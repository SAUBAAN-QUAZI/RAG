{
  "config": {
    "name": "default",
    "chunk_size": 1000,
    "chunk_overlap": 200,
    "splitter_type": "token"
  },
  "total_chunks": 206,
  "chunk_ids": [
    "01e5246396f56338",
    "25207c7e191f8381",
    "79218b39d9bfee9e",
    "ec390ce3398588ee",
    "f80f06574143b8ef",
    "ac1cb291ec58c4a7",
    "d0cd465442366d7c",
    "a0e4a15772a649f8",
    "5203c700f72bb77a",
    "820c085c90976181",
    "9460a9e4cd9644d5",
    "437e13aece52d8b6",
    "68006ce65c9ff3dc",
    "a6ba3b32c1e69074",
    "5737912b367745d7",
    "4927e1c52610f5a5",
    "da92b9cf88704082",
    "60ba03ec0847b8ec",
    "1fa4fc86cca1bd7d",
    "29a90ce803302b65",
    "e80d6bb96aec7af6",
    "055b193847ef103e",
    "27b3a01a5baec07e",
    "30b0d74d7770c44f",
    "d2e5380270ecb262",
    "eda120047cba6319",
    "50fe0eecc8412f45",
    "687b93711a140a6e",
    "286ed73423cf50ff",
    "aa3666c8db7b6bb4",
    "0cf2dfdd4ce3a2aa",
    "97b857bc836fd445",
    "2c6caf6389fb32e0",
    "eb7beb24aa451020",
    "86ebf89873b14c45",
    "72f80a0d0fe781a8",
    "808ec4ed6117434d",
    "99df9d134f8a74ab",
    "393d627c45da8706",
    "81f6ca27534022fc",
    "8d69b7f846ee6a8a",
    "9840b36b7c94b42e",
    "b84fecf89beab4e7",
    "cbc04304fcb8caed",
    "a5d7171d84d43e01",
    "0a3208c5f8fc188a",
    "8ca4de282f5f275c",
    "517aad6933c5c19b",
    "f7a831a2a96ee856",
    "a7e05628bd9ab888",
    "8b513febd3db9b3f",
    "84ea29514492a812",
    "cba032a57ada2df2",
    "1c9f40c6fb5ea6b5",
    "cf3c6b69b67aa5fc",
    "3ca0b9e6531168d4",
    "e491e7bd4b92e75a",
    "6014c03584f2973d",
    "09036916078608c3",
    "763b2f3380ad8dda",
    "551e1db0d73911c4",
    "7e50577222aac10b",
    "0f5b1ae9206c1d0a",
    "eef68698d6e2b7e9",
    "a34dac1c314d6229",
    "8a627bb391ca7d83",
    "ee5242fa2360c695",
    "3f490e04b5a4915f",
    "be358382e7747987",
    "65c2a760b17ed691",
    "d5c71306a8b3e294",
    "d1503231af49d40f",
    "570656a5c1117373",
    "fe3c1a3f7398b238",
    "76b84050f77cc89c",
    "88f30fb632c4a386",
    "c52bdb327d22a9b0",
    "b8a99487b1f9144b",
    "c8c47d1138c5929b",
    "3fd6b5df1afdc027",
    "d65dd2638b6cb922",
    "13b4755b3e012fcb",
    "5cc123bd7fb7b2c4",
    "4e4904128c16741b",
    "2d32fa95ac754270",
    "98db8678638c1357",
    "3f13571ca13d94cb",
    "5360e75e317c1f35",
    "a36d84b5c420a00f",
    "f257696922bd4260",
    "32bf0c193f3e4764",
    "ed08045f9c311b4d",
    "f6fc66525be5696b",
    "e52f7580d723a062",
    "861abae649aa2a09",
    "be314a178c9b0e52",
    "26e8ee7843084147",
    "c457f8f6cbfab4d2",
    "cb65b90676bfb2cf",
    "38911e37980be790",
    "e01f0144c0656587",
    "666d31179b8f04de",
    "93bd2b9b7ba5006e",
    "3fe9a7d210d62ef1",
    "7b2f045f08d0fcaf",
    "ec5914f81d4f31e1",
    "138e887253c810bf",
    "90d9629e1022454c",
    "13694fa6a3f41d2f",
    "1e0d6d459bcc5fbb",
    "f24556a89203c8d0",
    "64478d7bb8dd2381",
    "e7e560da217b236e",
    "78d6c5f6471e97b7",
    "17732afe23532edb",
    "37df10f9a8996d50",
    "c566ccde183f0793",
    "e7778dcda3ad25a6",
    "adc1201183398308",
    "a46cbae89730ca30",
    "b10a1b34378ce4de",
    "b1894a9b4f4cd94e",
    "ac5938bbe6766a2f",
    "80087bedd8d4d88b",
    "00966679a49d78ca",
    "d0b6b4fe5f4d6ddd",
    "b41eab3f2f73a7e3",
    "2fd6e4e168228288",
    "fd960eeb7b9b9927",
    "0572eee68659caa5",
    "c30fa9a5779a4356",
    "6054ad72af016f5a",
    "318685fe9a76ae1b",
    "b96222c82af990bc",
    "426b6f525963971c",
    "16f5880119b957bf",
    "fb6c68f16c8c9b55",
    "0161c673e9ed81fc",
    "a3fcfc1fed1c1dad",
    "20f154411d0897d0",
    "42f3a791c9377566",
    "bed1930b523cb428",
    "efa94e6b91ab13ec",
    "927df59dca750b33",
    "393349cb0aa2258f",
    "1719d2aa5bced58e",
    "b10543288ff134cc",
    "f7714dae01ad1569",
    "5fd942c44646a671",
    "92fb40d7fb2fe5df",
    "8ee988cfba969d31",
    "a1f7e5626d631961",
    "853c47e06a7c9f4e",
    "e91cb5d73a707d5d",
    "218447eb64eab097",
    "17970069e7db756e",
    "b6cadcb04b065ce1",
    "1ffe3733f7742664",
    "fccd271726b5dea2",
    "7999205516327413",
    "8311b5f9531399ff",
    "c19d420ce46bf814",
    "38b3e1293c27b1e8",
    "2b555c6848895566",
    "daac6181c47adf31",
    "d4439b723d5110f6",
    "afed922cc0ce4195",
    "fc87838b1ae5f138",
    "b661401b1d2a54ca",
    "3b4b0b063b3f82f5",
    "4ed8f42d9a7fb3d5",
    "ad058fe2a50641f0",
    "aaa1ae056816a6b5",
    "a432fba38cbbfa0a",
    "ba30681b895c70f0",
    "1f562b4ed986346f",
    "2505fe61af47a477",
    "3ce641ed14b3a247",
    "8948c9adc939738d",
    "70a3b4b98c58ff3a",
    "227e14e1fa5cc96d",
    "3c761c50708b44a8",
    "315f52337fb525a9",
    "19138dc2402890d3",
    "98db8abcf5ce2d1d",
    "fe0488a7ac522df1",
    "a3a111944ec9eb14",
    "c6fd27f99b4727a7",
    "17042f6fb6a8c62e",
    "e1dd881812a73465",
    "573bd4e0a39a965a",
    "0ecb503d2865bd8a",
    "4cac8de7b47bed9b",
    "eee723ad9edda4c5",
    "17f0a7828a7e8a51",
    "1b138fec891979f6",
    "9edbb7c0ea4ca1a1",
    "8fecccd1b4b85672",
    "6fa623cc7dda5f39",
    "3ccbc77058d2f7d2",
    "8dccbd177015dd73",
    "3eb0ec13b980aca6",
    "9ea187dbb860edea",
    "9e1b78beb88defc9",
    "78d6f3bbc8533901",
    "2d83549eda2ec324"
  ],
  "chunk_contents": {
    "01e5246396f56338": "\n\n--- Page 1 ---\n\narXiv:2501.09223v1  [cs.CL]  16 Jan 2025Foundations of\nLarge Language Models\nTong Xiao and Jingbo Zhu\nJanuary 17, 2025\nNLP Lab, Northeastern University & NiuTrans Research\n\n--- Page 2 ---\n\nCopyright \u00a9 2021-2025 Tong Xiao and Jingbo Zhu\nNATURAL LANGUAGE PROCESSING LAB, NORTHEASTERN UNIVERSITY\n&\nNIUTRANS RESEARCH\nLicensed under the Creative Commons Attribution-NonComme rcial 4.0 Unported License (the\n\u201cLicense\u201d). You may not use this \ufb01le except in compliance wit h the License. You may ob-\ntain a copy of the License at http://creativecommons.org/licenses/by-nc/4.0 . Unless\nrequired by applicable law or agreed to in writing, software distributed under the License is dis-\ntributed on an \u201c AS IS \u201dBASIS ,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND , either\nexpress or implied. See the License for the speci\ufb01c language governing permissions and limita-\ntions under the License.\nJanuary 17, 2025\n\n--- Page 3 ---\n\nPreface\nLarge language models originated from natural language pro cessing, but they have undoubtedly\nbecome one of the most revolutionary technological advance ments in the \ufb01eld of arti\ufb01cial intelli-\ngence in recent years. An important insight brought by large language models is that knowledge\nof the world and languages can be acquired through large-sca le language modeling tasks, and\nin this way, we can create a universal model that handles dive rse problems. This discovery has\nprofoundly impacted the research methodologies in natural language processing and many related\ndisciplines. We have shifted from training specialized sys tems from scratch using a large amount\nof labeled data to a new paradigm of using large-scale pre-tr aining to obtain foundation models,\nwhich are then \ufb01ne-tuned, aligned, and prompted.\nThis book aims to outline the basic concepts of large languag e models and introduce the\nrelated techniques. As the title suggests, the book focuses more on the foundational aspects of\nlarge language models rather than providing comprehensive coverage of all cutting-edge methods.\nThe book consists of four chapters:\n\u2022 Chapter 1 introduces the basics of pre-training. This is th e foundation of large language\nmodels, and common pre-training methods and model architec tures will be discussed here.\n\u2022 Chapter 2 introduces generative models, which are the larg e language models we commonly\nrefer to today. After presenting the basic process of buildi ng these models, we will also\nexplore how to scale up model training and handle long texts.\n\u2022 Chapter 3 introduces prompting methods for large language models. We will discuss var-\nious prompting strategies, along with more advanced method s such as chain-of-thought\nreasoning and automatic prompt design.\n\u2022 Chapter 4 introduces alignment methods for large language models. This chapter focuses\non instruction \ufb01ne-tuning and alignment based on human feed back.\nIf readers have some background in machine learning and natu ral language processing, along\nwith a certain understanding of neural networks like Transf ormers, reading this book will be quite\neasy. However, even without this prior knowledge, it is stil l perfectly \ufb01ne, as we have made the\ncontent of each chapter as self-contained as possible, ensu ring that readers will not be burdened\nwith too much reading dif\ufb01culty.\nIn writing this book, we have gradually realized that it is mo re like a compilation of \"notes\" we\nhave taken while learning about large language models. Thro ugh this note-taking writing style, we\nhope to offer readers a \ufb02exible learning path. Whether they w ish to dive deep into a speci\ufb01c area\nor gain a comprehensive understanding of large language mod els, they will \ufb01nd the knowledge\nand insights they need within these \"notes\".\nWe would like to thank the students in our laboratory and all o ur friends who have shared\nwith us their views on large language models and helped with c orrections of errors in writing. In\nparticular, we wish to thank Weiqiao Shan, Yongyu Mu, Chengl ong Wang, Kaiyan Chang, Yuchun\nFan, Hang Zhou, Xinyu Liu, Huiwen Bao, Tong Zheng, Junhao Rua n, and Qing Yang.\nii\n\n--- Page 4 ---\n\nNotation\navariable\narow vector or matrix\nf(a)function ofa\nmaxf(a)maximum value of f(a)\narg maxaf(a)value ofathat maximizes f(a)\nxinput token sequence to",
    "25207c7e191f8381": " Whether they w ish to dive deep into a speci\ufb01c area\nor gain a comprehensive understanding of large language mod els, they will \ufb01nd the knowledge\nand insights they need within these \"notes\".\nWe would like to thank the students in our laboratory and all o ur friends who have shared\nwith us their views on large language models and helped with c orrections of errors in writing. In\nparticular, we wish to thank Weiqiao Shan, Yongyu Mu, Chengl ong Wang, Kaiyan Chang, Yuchun\nFan, Hang Zhou, Xinyu Liu, Huiwen Bao, Tong Zheng, Junhao Rua n, and Qing Yang.\nii\n\n--- Page 4 ---\n\nNotation\navariable\narow vector or matrix\nf(a)function ofa\nmaxf(a)maximum value of f(a)\narg maxaf(a)value ofathat maximizes f(a)\nxinput token sequence to a model\nxjinput token at position j\nyoutput token sequence produced by a model\nyioutput token at position i\n\u03b8model parameters\nPr(a)probability of a\nPr(a|b)conditional probability of agivenb\nPr(\u00b7|b)probability distribution of a variable given b\nPr\u03b8(a)probability of aas parameterized by \u03b8\nhthidden state at time step tin sequential models\nH matrix of all hidden states over time in a sequence\nQ,K,V query, key, and value matrices in attention mechanisms\nSoftmax( A)Softmax function that normalizes the input vector or matrix A\nLloss function\nD dataset used for training or \ufb01ne-tuning a model\n\u2202L\n\u2202\u03b8gradient of the loss function Lwith respect to the parameters \u03b8\nKL(p||q)KL divergence between distributions pandq\niii\n\n--- Page 5 ---\n\nContents\n1 Pre-training 1\n1.1 Pre-training NLP Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\n1.1.1 Unsupervised, Supervised and Self-supervised Pre-t raining . . . . . . . . 2\n1.1.2 Adapting Pre-trained Models . . . . . . . . . . . . . . . . . . . . . . . . 3\n1.2 Self-supervised Pre-training Tasks . . . . . . . . . . . . . . . . . . . . . . . . . 7\n1.2.1 Decoder-only Pre-training . . . . . . . . . . . . . . . . . . . . . . . . . 7\n1.2.2 Encoder-only Pre-training . . . . . . . . . . . . . . . . . . . . . . . . . 8\n1.2.3 Encoder-Decoder Pre-training . . . . . . . . . . . . . . . . . . . . . . . 15\n1.2.4 Comparison of Pre-training Tasks . . . . . . . . . . . . . . . . . . . . . 20\n1.3 Example: BERT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n1.3.1 The Standard Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n1.3.2 More Training and Larger Models . . . . . . . . . . . . . . . . . . . . . 27\n1.3.3 More Ef\ufb01cient Models . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n1.3.4 Multi-lingual Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n1.4 Applying BERT Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n1.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n2 Generative Models 36\n2.1 A Brief Introduction to LLMs . . . . . . . . .",
    "79218b39d9bfee9e": " . . . . 27\n1.3.3 More Ef\ufb01cient Models . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n1.3.4 Multi-lingual Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n1.4 Applying BERT Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n1.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n2 Generative Models 36\n2.1 A Brief Introduction to LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n2.1.1 Decoder-only Transformers . . . . . . . . . . . . . . . . . . . . . . . . 38\n2.1.2 Training LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n2.1.3 Fine-tuning LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n2.1.4 Aligning LLMs with the World . . . . . . . . . . . . . . . . . . . . . . 46\n2.1.5 Prompting LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n2.2 Training at Scale . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\n2.2.1 Data Preparation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\n2.2.2 Model Modi\ufb01cations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n2.2.3 Distributed Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\n2.2.4 Scaling Laws . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\n2.3 Long Sequence Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\n2.3.1 Optimization from HPC Perspectives . . . . . . . . . . . . . . . . . . . 67\n2.3.2 Ef\ufb01cient Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . 68\n2.3.3 Cache and Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\n2.3.4 Sharing across Heads and Layers . . . . . . . . . . . . . . . . . . . . . 80\niv\n\n--- Page 6 ---\n\nv\n2.3.5 Position Extrapolation and Interpolation . . . . . . . . . . . . . . . . . . 82\n2.3.6 Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\n2.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94\n3 Prompting 96\n3.1 General Prompt Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97\n3.1.1 Basics . . .",
    "ec390ce3398588ee": " . . . . . . . . . . . 80\niv\n\n--- Page 6 ---\n\nv\n2.3.5 Position Extrapolation and Interpolation . . . . . . . . . . . . . . . . . . 82\n2.3.6 Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\n2.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94\n3 Prompting 96\n3.1 General Prompt Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97\n3.1.1 Basics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97\n3.1.2 In-context Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99\n3.1.3 Prompt Engineering Strategies . . . . . . . . . . . . . . . . . . . . . . . 101\n3.1.4 More Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106\n3.2 Advanced Prompting Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . 115\n3.2.1 Chain of Thought . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115\n3.2.2 Problem Decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . 117\n3.2.3 Self-re\ufb01nement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124\n3.2.4 Ensembling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130\n3.2.5 RAG and Tool Use . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134\n3.3 Learning to Prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138\n3.3.1 Prompt Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139\n3.3.2 Soft Prompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142\n3.3.3 Prompt Length Reduction . . . . . . . . . . . . . . . . . . . . . . . . . 152\n3.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153\n4 Alignment 155\n4.1 An Overview of LLM Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . 155\n4.2 Instruction Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157\n4.2.1 Supervised Fine-tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . 157\n4.2.2 Fine-tuning Data Acquisition . . . . . . . . . . . . . . . . . . . . . . . . 161\n4.2.3 Fine-tuning with Less Data . . . . . . . . . . . . . . . . . . . . . . . . . 166\n4.",
    "f80f06574143b8ef": " Alignment 155\n4.1 An Overview of LLM Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . 155\n4.2 Instruction Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157\n4.2.1 Supervised Fine-tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . 157\n4.2.2 Fine-tuning Data Acquisition . . . . . . . . . . . . . . . . . . . . . . . . 161\n4.2.3 Fine-tuning with Less Data . . . . . . . . . . . . . . . . . . . . . . . . . 166\n4.2.4 Instruction Generalization . . . . . . . . . . . . . . . . . . . . . . . . . 167\n4.2.5 Using Weak Models to Improve Strong Models . . . . . . . . . . . . . . 169\n4.3 Human Preference Alignment: RLHF . . . . . . . . . . . . . . . . . . . . . . . 172\n4.3.1 Basics of Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . 173\n4.3.2 Training Reward Models . . . . . . . . . . . . . . . . . . . . . . . . . . 179\n4.3.3 Training LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182\n4.4 Improved Human Preference Alignment . . . . . . . . . . . . . . . . . . . . . . 187\n4.4.1 Better Reward Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . 187\n\n--- Page 7 ---\n\nvi Notation\n4.4.2 Direct Preference Optimization . . . . . . . . . . . . . . . . . . . . . . 193\n4.4.3 Automatic Preference Data Generation . . . . . . . . . . . . . . . . . . 196\n4.4.4 Step-by-step Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . 198\n4.4.5 Inference-time Alignment . . . . . . . . . . . . . . . . . . . . . . . . . 200\n4.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201\nBibliography 203\n\n--- Page 8 ---\n\nCHAPTER 1\nPre-training\nThe development of neural sequence models, such as Transformers [Vaswani et al. ,2017 ], along\nwith the improvements in large-scale self-supervised lear ning, has opened the door to universal\nlanguage understanding and generation. This achievement i s largely motivated by pre-training:\nwe separate common components from many neural network-bas ed systems, and then train them\non huge amounts of unlabeled data using self-supervision. T hese pre-trained models serve as\nfoundation models that can be easily adapted to different ta sks via \ufb01ne-tuning or prompting. As a\nresult, the paradigm of NLP has been enormously changed. In m any cases, large-scale supervised\nlearning for speci\ufb01c tasks is no longer required, and instea d, we only need to adapt pre-trained\nfoundation models.\nWhile pre-training has gained popularity in recent NLP rese arch, this concept dates back\ndecades to the early days of deep learning. For example, earl y attempts to pre-train deep learning\nsystems include unsupervised learning for RNNs, deep feedf orward networks, autoencoders, and\nothers [ Schmidhuber ,2015 ]. In the modern era of deep learning, we experienced a resurg ence of\npre-training, caused in part by the large-scale unsupervis ed learning of various word embedding\nmodels [ Mikolov et al. ,2013b ;Pennington et al. ,",
    "ac1cb291ec58c4a7": " ta sks via \ufb01ne-tuning or prompting. As a\nresult, the paradigm of NLP has been enormously changed. In m any cases, large-scale supervised\nlearning for speci\ufb01c tasks is no longer required, and instea d, we only need to adapt pre-trained\nfoundation models.\nWhile pre-training has gained popularity in recent NLP rese arch, this concept dates back\ndecades to the early days of deep learning. For example, earl y attempts to pre-train deep learning\nsystems include unsupervised learning for RNNs, deep feedf orward networks, autoencoders, and\nothers [ Schmidhuber ,2015 ]. In the modern era of deep learning, we experienced a resurg ence of\npre-training, caused in part by the large-scale unsupervis ed learning of various word embedding\nmodels [ Mikolov et al. ,2013b ;Pennington et al. ,2014 ]. During the same period, pre-training also\nattracted signi\ufb01cant interest in computer vision, where th e backbone models were trained on rel-\natively large labeled datasets such as ImageNet, and then ap plied to different downstream tasks\n[He et al. ,2019 ;Zoph et al. ,2020 ]. Large-scale research on pre-training in NLP began with th e\ndevelopment of language models using self-supervised lear ning. This family of models covers\nseveral well-known examples like BERT [Devlin et al. ,2019 ] and GPT [Brown et al. ,2020 ], all\nwith a similar idea that general language understanding and generation can be achieved by train-\ning the models to predict masked words in a huge amount of text . Despite the simple nature of\nthis approach, the resulting models show remarkable capabi lity in modeling linguistic structure,\nthough they are not explicitly trained to achieve this. The g enerality of the pre-training tasks\nleads to systems that exhibit strong performance in a large v ariety of NLP problems, even outper-\nforming previously well-developed supervised systems. Mo re recently, pre-trained large language\nmodels have achieved a greater success, showing the excitin g prospects for more general arti\ufb01cial\nintelligence [ Bubeck et al. ,2023 ].\nThis chapter discusses the concept of pre-training in the co ntext of NLP. It begins with a gen-\neral introduction to pre-training methods and their applic ations. BERT is then used as an example\nto illustrate how a sequence model is trained via a self-supe rvised task, called masked language\nmodeling . This is followed by a discussion of methods for adapting pre -trained sequence mod-\nels for various NLP tasks. Note that in this chapter, we will f ocus primarily on the pre-training\nparadigm in NLP, and therefore, we do not intend to cover deta ils about generative large language\nmodels. A detailed discussion of these models will be left to subsequent chapters.\n1.1 Pre-training NLP Models\nThe discussion of pre-training issues in NLP typically invo lves two types of problems: sequence\nmodeling (or sequence encoding) and sequence generation. W hile these problems have different\n1\n\n--- Page 9 ---\n\n2 Pre-training\nforms, for simplicity, we describe them using a single model de\ufb01ned as follows:\no=g(x0,x1,...,x m;\u03b8)\n=g\u03b8(x0,x1,...,x m) (1.1)\nwhere {x0,x1,...,x m}denotes a sequence of input tokens1,x0denotes a special symbol ( \u27e8s\u27e9or\n[CLS] ) attached to the beginning of a sequence, g(\u00b7;\u03b8)(also written as g\u03b8(\u00b7)) denotes a neural\nnetwork with parameters \u03b8, and odenotes the output of the neural network. Different problem s\ncan vary based on the form of the output o. For example, in token prediction problems (as in\nlanguage modeling), ois a distribution over a vocabulary; in sequence encoding pr oblems, ois a\nrepresentation of the input sequence, often expressed as a r eal-valued vector sequence.\nThere are two fundamental issues here.\n\u2022 Optimizing \u03b8on a pre-training task. Unlike standard learning problems i n NLP, pre-training\ndoes not assume speci\ufb01c downstream tasks to which the model w ill be applied. Instead, the\ngoal is to train a model that can generalize across various ta sks.\n\u2022 Applying the pre-trained model g\u02c6\u03b8(\u00b7)to downstream tasks. To adapt the model to these\ntasks, we need to adjust the parameters \u02c6\u03b8slightly using labeled data or prompt the model\nwith task descriptions",
    "d0cd465442366d7c": "network with parameters \u03b8, and odenotes the output of the neural network. Different problem s\ncan vary based on the form of the output o. For example, in token prediction problems (as in\nlanguage modeling), ois a distribution over a vocabulary; in sequence encoding pr oblems, ois a\nrepresentation of the input sequence, often expressed as a r eal-valued vector sequence.\nThere are two fundamental issues here.\n\u2022 Optimizing \u03b8on a pre-training task. Unlike standard learning problems i n NLP, pre-training\ndoes not assume speci\ufb01c downstream tasks to which the model w ill be applied. Instead, the\ngoal is to train a model that can generalize across various ta sks.\n\u2022 Applying the pre-trained model g\u02c6\u03b8(\u00b7)to downstream tasks. To adapt the model to these\ntasks, we need to adjust the parameters \u02c6\u03b8slightly using labeled data or prompt the model\nwith task descriptions.\nIn this section, we discuss the basic ideas in addressing the se issues.\n1.1.1 Unsupervised, Supervised and Self-supervised Pre-t raining\nIn deep learning, pre-training refers to the process of opti mizing a neural network before it is\nfurther trained/tuned and applied to the tasks of interest. This approach is based on an assumption\nthat a model pre-trained on one task can be adapted to perform another task. As a result, we do\nnot need to train a deep, complex neural network from scratch on tasks with limited labeled data.\nInstead, we can make use of tasks where supervision signals a re easier to obtain. This reduces the\nreliance on task-speci\ufb01c labeled data, enabling the develo pment of more general models that are\nnot con\ufb01ned to particular problems.\nDuring the resurgence of neural networks through deep learn ing, many early attempts to\nachieve pre-training were focused on unsupervised learning . In these methods, the parame-\nters of a neural network are optimized using a criterion that is not directly related to speci\ufb01c tasks.\nFor example, we can minimize the reconstruction cross-entr opy of the input vector for each layer\n[Bengio et al. ,2006 ]. Unsupervised pre-training is commonly employed as a prel iminary step\nbefore supervised learning, offering several advantages, such as aiding in the discovery of better\nlocal minima and adding a regularization effect to the train ing process [ Erhan et al. ,2010 ]. These\nbene\ufb01ts make the subsequent supervised learning phase easi er and more stable.\nA second approach to pre-training is to pre-train a neural ne twork on supervised learning\ntasks. For example, consider a sequence model designed to en code input sequences into some\n1Here we assume that tokens are basic units of text that are sep arated through tokenization. Sometimes, we will use\nthe terms token andword interchangeably, though they have closely related but slig htly different meanings in NLP.\n\n--- Page 10 ---\n\n1.1 Pre-training NLP Models 3\nrepresentations. In pre-training, this model is combined w ith a classi\ufb01cation layer to form a clas-\nsi\ufb01cation system. This system is then trained on a pre-train ing task, such as classifying sentences\nbased on sentiment (e.g., determining if a sentence conveys a positive or negative sentiment).\nThen, we adapt the sequence model to a downstream task. We bui ld a new classi\ufb01cation system\nbased on this pre-trained sequence model and a new classi\ufb01ca tion layer (e.g., determining if a\nsequence is subjective or objective). Typically, we need to \ufb01ne-tune the parameters of the new\nmodel using task-speci\ufb01c labeled data, ensuring the model i s optimally adjusted to perform well\non this new type of data. The \ufb01ne-tuned model is then employed to classify new sequences for\nthis task. An advantage of supervised pre-training is that t he training process, either in the pre-\ntraining or \ufb01ne-tuning phase, is straightforward, as it fol lows the well-studied general paradigm\nof supervised learning in machine learning. However, as the complexity of the neural network\nincreases, the demand for more labeled data also grows. This , in turn, makes the pre-training task\nmore dif\ufb01cult, especially when large-scale labeled data is not available.\nA third approach to pre-training is self-supervised learning . In this approach, a neural net-\nwork is trained using the supervision signals generated by i tself, rather than those provided by\nhumans. This is generally done by constructing its own train ing tasks directly from unlabeled\ndata, such as having the system",
    "a0e4a15772a649f8": " the model i s optimally adjusted to perform well\non this new type of data. The \ufb01ne-tuned model is then employed to classify new sequences for\nthis task. An advantage of supervised pre-training is that t he training process, either in the pre-\ntraining or \ufb01ne-tuning phase, is straightforward, as it fol lows the well-studied general paradigm\nof supervised learning in machine learning. However, as the complexity of the neural network\nincreases, the demand for more labeled data also grows. This , in turn, makes the pre-training task\nmore dif\ufb01cult, especially when large-scale labeled data is not available.\nA third approach to pre-training is self-supervised learning . In this approach, a neural net-\nwork is trained using the supervision signals generated by i tself, rather than those provided by\nhumans. This is generally done by constructing its own train ing tasks directly from unlabeled\ndata, such as having the system create pseudo labels. While s elf-supervised learning has recently\nemerged as a very popular method in NLP, it is not a new concept . In machine learning, a related\nconcept is self-training where a model is iteratively improved by learning from the ps eudo labels\nassigned to a dataset. To do this, we need some seed data to bui ld an initial model. This model\nthen generates pseudo labels for unlabeled data, and these p seudo labels are subsequently used to\niteratively re\ufb01ne and bootstrap the model itself. Such a met hod has been successfully used in sev-\neral NLP areas, such as word sense disambiguation [ Yarowsky ,1995 ] and document classi\ufb01cation\n[Blum and Mitchell ,1998 ]. Unlike the standard self-training method, self-supervi sed pre-training\nin NLP does not rely on an initial model for annotating the dat a. Instead, all the supervision sig-\nnals are created from the text, and the entire model is traine d from scratch. A well-known example\nof this is training sequence models by successively predict ing a masked word given its preceding\nor surrounding words in a text. This enables large-scale sel f-supervised learning for deep neural\nnetworks, leading to the success of pre-training in many und erstanding, writing, and reasoning\ntasks.\nFigure 1.1shows a comparison of the above three pre-training approach es. Self-supervised\npre-training is so successful that most current state-of-t he-art NLP models are based on this\nparadigm. Therefore, in this chapter and throughout this bo ok, we will focus on self-supervised\npre-training. We will show how sequence models are pre-trai ned via self-supervision and how the\npre-trained models are applied.\n1.1.2 Adapting Pre-trained Models\nAs mentioned above, two major types of models are widely used in NLP pre-training.\n\u2022Sequence Encoding Models . Given a sequence of words or tokens, a sequence encoding\nmodel represents this sequence as either a real-valued vect or or a sequence of vectors, and\nobtains a representation of the sequence. This representat ion is typically used as input to\nanother model, such as a sentence classi\ufb01cation system.\n\n--- Page 11 ---\n\n4 Pre-training\nUnsupervised SupervisedPre-training Training\nUnlabeled\nDataLabeled\nData\n(a) Unsupervised Pre-trainingSupervised SupervisedPre-training Tuning\nLabeled\nData\nTask 1Labeled\nData\nTask 2\n(b) Supervised Pre-trainingSelf-\nSupervisedSupervisedZero/Few\nShot Learning\nPre-training TuningPrompting\nUnlabeled\nDataLabeled\nData\n(c) Self-supervised Pre-training\nFig. 1.1: Illustration of unsupervised, supervised, and self-super vised pre-training. In unsupervised pre-training, the\npre-training is performed on large-scale unlabeled data. I t can be viewed as a preliminary step to have a good starting\npoint for the subsequent optimization process, though cons iderable effort is still required to further train the model\nwith labeled data after pre-training. In supervised pre-tr aining, the underlying assumption is that different (super vised)\nlearning tasks are related. So we can \ufb01rst train the model on o ne task, and transfer the resulting model to another task\nwith some training or tuning effort. In self-supervised pre -training, a model is pre-trained on large-scale unlabeled data\nvia self-supervision. The model can be well trained in this w ay, and we can ef\ufb01ciently adapt it to new tasks through\n\ufb01ne-tuning or prompting.\n\u2022Sequence Generation Models . In NLP",
    "5203c700f72bb77a": " of unsupervised, supervised, and self-super vised pre-training. In unsupervised pre-training, the\npre-training is performed on large-scale unlabeled data. I t can be viewed as a preliminary step to have a good starting\npoint for the subsequent optimization process, though cons iderable effort is still required to further train the model\nwith labeled data after pre-training. In supervised pre-tr aining, the underlying assumption is that different (super vised)\nlearning tasks are related. So we can \ufb01rst train the model on o ne task, and transfer the resulting model to another task\nwith some training or tuning effort. In self-supervised pre -training, a model is pre-trained on large-scale unlabeled data\nvia self-supervision. The model can be well trained in this w ay, and we can ef\ufb01ciently adapt it to new tasks through\n\ufb01ne-tuning or prompting.\n\u2022Sequence Generation Models . In NLP, sequence generation generally refers to the prob-\nlem of generating a sequence of tokens based on a given contex t. The term context has\ndifferent meanings across applications. For example, it re fers to the preceding tokens in\nlanguage modeling, and refers to the source-language seque nce in machine translation2.\nWe need different techniques for applying these models to do wnstream tasks after pre-training.\nHere we are interested in the following two methods.\n1.1.2.1 Fine-tuning of Pre-trained Models\nFor sequence encoding pre-training, a common method of adap ting pre-trained models is \ufb01ne-\ntuning. Let Encode \u03b8(\u00b7)denote an encoder with parameters \u03b8, for example, Encode \u03b8(\u00b7)can be a\nstandard Transformer encoder. Provided we have pre-traine d this model in some way and obtained\nthe optimal parameters \u02c6\u03b8, we can employ it to model any sequence and generate the corre sponding\nrepresentation, like this\nH= Encode \u02c6\u03b8(x) (1.2)\nwhere xis the input sequence {x0,x1,...,x m}, and His the output representation which is a\nsequence of real-valued vectors {h0,h1,...,hm}. Because the encoder does not work as a stan-\ndalone NLP system, it is often integrated as a component into a bigger system. Consider, for\nexample, a text classi\ufb01cation problem in which we identify t he polarity (i.e., positive, negative,\n2More precisely, in auto-regressive decoding of machine tra nslation, each target-language token is generated based\non both its preceding tokens and source-language sequence.\n\n--- Page 12 ---\n\n1.1 Pre-training NLP Models 5\nand neutral) of a given text. We can build a text classi\ufb01catio n system by stacking a classi\ufb01er\non top of the encoder. Let Classify\u03c9(\u00b7)be a neural network with parameters \u03c9. Then, the text\nclassi\ufb01cation model can be expressed in the form\nPr\u03c9,\u02c6\u03b8(\u00b7|x) = Classify\u03c9(H)\n= Classify\u03c9(Encode \u02c6\u03b8(x)) (1.3)\nHere Pr\u03c9,\u02c6\u03b8(\u00b7|x)is a probability distribution over the label set {positive,negative,neutral }, and\nthe label with the highest probability in this distribution is selected as output. To keep the notation\nuncluttered, we will use F\u03c9,\u02c6\u03b8(\u00b7)to denote Classify\u03c9(Encode \u02c6\u03b8(\u00b7)).\nBecause the model parameters \u03c9and\u02c6\u03b8are not optimized for the classi\ufb01cation task, we cannot\ndirectly use this model. Instead, we must use a modi\ufb01ed versi on of the model that is adapted to\nthe task. A typical way is to \ufb01ne-tune the model by giving expl icit labeling in downstream tasks.\nWe can train F\u03c9,\u02c6\u03b8(\u00b7)on a labeled dataset, treating it as a common supervised lear ning task. The\noutcome of the \ufb01ne-tuning is the parameters \u02dc\u03c9and\u02dc\u03b8that are further optimized. Alternatively,\nwe can freeze the encoder parameters \u02c6\u03b8to maintain their pre-trained state, and focus solely on\noptimizing\u03c9. This allows the classi\ufb01er to be ef\ufb01ciently adapted to work i n tandem with the\npre-trained encoder.\nOnce we have obtained a \ufb01ne-tuned model, we can use it to class ify a new text. For example,\nsuppose we have a comment posted on a travel website:\nI love the food here. It\u2019s amazing!\nWe \ufb01rst tokenize this text into tokens3, and then",
    "820c085c90976181": " is adapted to\nthe task. A typical way is to \ufb01ne-tune the model by giving expl icit labeling in downstream tasks.\nWe can train F\u03c9,\u02c6\u03b8(\u00b7)on a labeled dataset, treating it as a common supervised lear ning task. The\noutcome of the \ufb01ne-tuning is the parameters \u02dc\u03c9and\u02dc\u03b8that are further optimized. Alternatively,\nwe can freeze the encoder parameters \u02c6\u03b8to maintain their pre-trained state, and focus solely on\noptimizing\u03c9. This allows the classi\ufb01er to be ef\ufb01ciently adapted to work i n tandem with the\npre-trained encoder.\nOnce we have obtained a \ufb01ne-tuned model, we can use it to class ify a new text. For example,\nsuppose we have a comment posted on a travel website:\nI love the food here. It\u2019s amazing!\nWe \ufb01rst tokenize this text into tokens3, and then feed the token sequence xnewinto the \ufb01ne-tuned\nmodelF\u02dc\u03c9,\u02dc\u03b8(\u00b7). The model generates a distribution over classes by\nF\u02dc\u03c9,\u02dc\u03b8(xnew) =[\nPr(positive |xnew) Pr(negative |xnew) Pr(neutral |xnew)]\n(1.4)\nAnd we select the label of the entry with the maximum value as o utput. In this example it is\npositive .\nIn general, the amount of labeled data used in \ufb01ne-tuning is s mall compared to that of the\npre-training data, and so \ufb01ne-tuning is less computational ly expensive. This makes the adaption\nof pre-trained models very ef\ufb01cient in practice: given a pre -trained model and a downstream task,\nwe just need to collect some labeled data, and slightly adjus t the model parameters on this data. A\nmore detailed discussion of \ufb01ne-tuning can be found in Secti on1.4.\n1.1.2.2 Prompting of Pre-trained Models\nUnlike sequence encoding models, sequence generation mode ls are often employed independently\nto address language generation problems, such as question a nswering and machine translation,\nwithout the need for additional modules. It is therefore str aightforward to \ufb01ne-tune these models\n3The text can be tokenized in many different ways. One of the si mplest is to segment the text into English words\nand punctuations {I,love,the,food,here,.,It,\u2019s,amazing,!}\n\n--- Page 13 ---\n\n6 Pre-training\nas complete systems on downstream tasks. For example, we can \ufb01ne-tune a pre-trained encoder-\ndecoder multilingual model on some bilingual data to improv e its performance on a speci\ufb01c trans-\nlation task.\nAmong various sequence generation models, a notable exampl e is the large language models\ntrained on very large amounts of data. These language models are trained to simply predict the next\ntoken given its preceding tokens. Although token predictio n is such a simple task that it has long\nbeen restricted to \u201clanguage modeling\u201d only, it has been fou nd to enable the learning of the general\nknowledge of languages by repeating the task a large number o f times. The result is that the\npre-trained large language models exhibit remarkably good abilities in token prediction, making\nit possible to transform numerous NLP problems into simple t ext generation problems through\nprompting the large language models. For example, we can fra me the above text classi\ufb01cation\nproblem as a text generation task\nI love the food here. It\u2019s amazing! I\u2019m\nHere indicates the word or phrase we want to predict (call it the completion ). If the predicted\nword is happy , orglad, orsatis\ufb01ed or a related positive word, we can classify the text as positive .\nThis example shows a simple prompting method in which we conc atenate the input text with I\u2019m\nto form a prompt. Then, the completion helps decide which lab el is assigned to the original text.\nGiven the strong performance of language understanding and generation of large language\nmodels, a prompt can instruct the models to perform more comp lex tasks. Here is a prompt where\nwe prompt the LLM to perform polarity classi\ufb01cation with an i nstruction.\nAssume that the polarity of a text is a label chosen from {posi tive, negative,\nneutral}. Identify the polarity of the input.\nInput : I love the food here. It\u2019s amazing!\nPolarity :\nThe \ufb01rst two sentences are a description of the task. Input andPolarity are indicators of the input\nand output, respectively. We expect the model to complete th e text and at the same time give the\ncorrect polarity label. By using instruction-based prompt",
    "9460a9e4cd9644d5": " can classify the text as positive .\nThis example shows a simple prompting method in which we conc atenate the input text with I\u2019m\nto form a prompt. Then, the completion helps decide which lab el is assigned to the original text.\nGiven the strong performance of language understanding and generation of large language\nmodels, a prompt can instruct the models to perform more comp lex tasks. Here is a prompt where\nwe prompt the LLM to perform polarity classi\ufb01cation with an i nstruction.\nAssume that the polarity of a text is a label chosen from {posi tive, negative,\nneutral}. Identify the polarity of the input.\nInput : I love the food here. It\u2019s amazing!\nPolarity :\nThe \ufb01rst two sentences are a description of the task. Input andPolarity are indicators of the input\nand output, respectively. We expect the model to complete th e text and at the same time give the\ncorrect polarity label. By using instruction-based prompt s, we can adapt large language models to\nsolve NLP problems without the need for additional training .\nThis example also demonstrates the zero-shot learning capa bility of large language models,\nwhich can perform tasks that were not observed during the tra ining phase. Another method for\nenabling new capabilities in a neural network is few-shot le arning. This is typically achieved\nthrough in-context learning (ICT). More speci\ufb01cally, we add some samples that demonstrate ho w\nan input corresponds to an output. These samples, known as demonstrations , are used to teach\nlarge language models how to perform the task. Below is an exa mple involving demonstrations\n\n--- Page 14 ---\n\n1.2 Self-supervised Pre-training Tasks 7\nAssume that the polarity of a text is a label chosen from {posi tive, negative,\nneutral}. Identify the polarity of the input.\nInput : The traf\ufb01c is terrible during rush hours, making it dif\ufb01cul t to reach the\nairport on time.\nPolarity : Negative\nInput : The weather here is wonderful.\nPolarity : Positive\nInput : I love the food here. It\u2019s amazing!\nPolarity :\nPrompting and in-context learning play important roles in t he recent rise of large language\nmodels. We will discuss these issues more deeply in Chapter 3 . However, it is worth noting\nthat while prompting is a powerful way to adapt large languag e models, some tuning efforts are\nstill needed to ensure the models can follow instructions ac curately. Additionally, the \ufb01ne-tuning\nprocess is crucial for aligning the values of these models wi th human values. More detailed\ndiscussions of \ufb01ne-tuning can be found in Chapter 4.\n1.2 Self-supervised Pre-training Tasks\nIn this section, we consider self-supervised pre-training approaches for different neural architec-\ntures, including decoder-only, encoder-only, and encoder -decoder architectures. We restrict our\ndiscussion to Transformers since they form the basis of most pre-trained models in NLP. How-\never, pre-training is a broad concept, and so we just give a br ief introduction to basic approaches\nin order to make this section concise.\n1.2.1 Decoder-only Pre-training\nThe decoder-only architecture has been widely used in devel oping language models [ Radford et al. ,\n2018 ]. For example, we can use a Transformer decoder as a language model by simply removing\ncross-attention sub-layers from it. Such a model predicts t he distribution of tokens at a position\ngiven its preceding tokens, and the output is the token with t he maximum probability. The stan-\ndard way to train this model, as in the language modeling prob lem, is to minimize a loss function\nover a collection of token sequences. Let Decoder \u03b8(\u00b7)denote a decoder with parameters \u03b8. At\neach position i, the decoder generates a distribution of the next tokens bas ed on its preceding\ntokens {x0,...,x i}, denoted by Pr\u03b8(\u00b7|x0,...,x i)(orp\u03b8\ni+1for short). Suppose we have the gold-\nstandard distribution at the same position, denoted by pgold\ni+1. For language modeling, we can think\nofpgold\ni+1as a one-hot representation of the correct predicted word. W e then de\ufb01ne a loss function\nL(p\u03b8\ni+1,pgold\ni+1)to measure the difference between the model prediction and t he true prediction. In\nNLP, the log-scale cross-entropy loss is typically used.\nGiven a sequence of mtokens {x0,...,x m}, the loss on this sequence is the sum of the loss\n\n--- Page 15 ---\n\n8 Pre-training\nover the positions {",
    "437e13aece52d8b6": ". At\neach position i, the decoder generates a distribution of the next tokens bas ed on its preceding\ntokens {x0,...,x i}, denoted by Pr\u03b8(\u00b7|x0,...,x i)(orp\u03b8\ni+1for short). Suppose we have the gold-\nstandard distribution at the same position, denoted by pgold\ni+1. For language modeling, we can think\nofpgold\ni+1as a one-hot representation of the correct predicted word. W e then de\ufb01ne a loss function\nL(p\u03b8\ni+1,pgold\ni+1)to measure the difference between the model prediction and t he true prediction. In\nNLP, the log-scale cross-entropy loss is typically used.\nGiven a sequence of mtokens {x0,...,x m}, the loss on this sequence is the sum of the loss\n\n--- Page 15 ---\n\n8 Pre-training\nover the positions {0,...,m \u22121}, given by\nLoss \u03b8(x0,...,x m) =m\u22121\u2211\ni=0L(p\u03b8\ni+1,pgold\ni+1)\n=m\u22121\u2211\ni=0LogCrossEntropy( p\u03b8\ni+1,pgold\ni+1) (1.5)\nwhere LogCrossEntropy( \u00b7)is the log-scale cross-entropy, and pgold\ni+1is the one-hot representation\nofxi+1.\nThis loss function can be extended to a set of sequences D. In this case, the objective of\npre-training is to \ufb01nd the best parameters that minimize the loss on D\n\u02c6\u03b8= arg min\n\u03b8\u2211\nx\u2208DLoss \u03b8(x) (1.6)\nNote that this objective is mathematically equivalent to ma ximum likelihood estimation, and can\nbe re-expressed as\n\u02c6\u03b8= arg max\n\u03b8\u2211\nx\u2208Dlog Pr \u03b8(x)\n= arg max\n\u03b8\u2211\nx\u2208Di\u22121\u2211\ni=0log Pr \u03b8(xi+1|x0,...,x i) (1.7)\nWith these optimized parameters \u02c6\u03b8, we can use the pre-trained language model Decoder \u02c6\u03b8(\u00b7)\nto compute the probability Pr\u02c6\u03b8(xi+1|x0,...,x i)at each position of a given sequence.\n1.2.2 Encoder-only Pre-training\nAs de\ufb01ned in Section 1.1.2.1 , an encoder Encoder \u03b8(\u00b7)is a function that reads a sequence of\ntokens x=x0...xmand produces a sequence of vectors H=h0...hm4. Training this model is\nnot straightforward, as we do not have gold-standard data fo r measuring how good the output of\nthe real-valued function is. A typical approach to encoder p re-training is to combine the encoder\nwith some output layers to receive supervision signals that are easier to obtain. Figure 1.2shows\na common architecture for pre-training Transformer encode rs, where we add a Softmax layer on\ntop of the Transformer encoder. Clearly, this architecture is the same as that of the decoder-based\nlanguage model, and the output is a sequence of probability d istributions\n\uf8ee\n\uf8ef\uf8ef\uf8f0pW,\u03b8\n1...\npW,\u03b8\nm\uf8f9\n\uf8fa\uf8fa\uf8fb= Softmax W(Encoder \u03b8(x)) (1.9)\n4If we view hias a row vector, Hcan be written as\nH=\uf8ee\n\uf8ef\uf8f0h0\n...\nhm\uf8f9\n\uf8fa\uf8fb (1.8)\n\n--- Page 16 ---\n\n1.2 Self-supervised Pre-training Tasks 9\nx0x1x2x3x4\n(masked)e0 e1 e2 e3 e4EncoderSoftmaxmodel reconstructs the masked tokenE.g., evaluate how well theSelf-supervision\n(a) Pre-trainingx0x1x2x3x4e0 e1 e2 e3 e4Pre-trained EncoderPrediction NetworkOutput for Downstream Tasks\n(b) Applying the Pre-trained Encoder\nFig. 1.2: Pre-training a Transformer encoder (left) and then applyin g the pre-trained encoder (right). In the pre-training\nphase, the encoder, together with a Softmax layer, is traine d via self-supervision. In the application phase, the Softm ax\nlayer is removed, and the pre-trained encoder is combined wi th a prediction network to address speci\ufb01c problems. In\ngeneral, for better adaptation to these tasks, the",
    "68006ce65c9ff3dc": "1.2 Self-supervised Pre-training Tasks 9\nx0x1x2x3x4\n(masked)e0 e1 e2 e3 e4EncoderSoftmaxmodel reconstructs the masked tokenE.g., evaluate how well theSelf-supervision\n(a) Pre-trainingx0x1x2x3x4e0 e1 e2 e3 e4Pre-trained EncoderPrediction NetworkOutput for Downstream Tasks\n(b) Applying the Pre-trained Encoder\nFig. 1.2: Pre-training a Transformer encoder (left) and then applyin g the pre-trained encoder (right). In the pre-training\nphase, the encoder, together with a Softmax layer, is traine d via self-supervision. In the application phase, the Softm ax\nlayer is removed, and the pre-trained encoder is combined wi th a prediction network to address speci\ufb01c problems. In\ngeneral, for better adaptation to these tasks, the system is \ufb01ne-tuned using labeled data.\nHere pW,\u03b8\ni is the output distribution Pr(\u00b7|x)at positioni. We use Softmax W(\u00b7)to denote that\nthe Softmax layer is parameterized by W, that is, Softmax W(H) = Softmax( H\u00b7W). For\nnotation simplicity, we will sometimes drop the superscrip tsWand\u03b8af\ufb01xed to each probability\ndistribution.\nThe difference between this model and standard language mod els is that the output pihas\ndifferent meanings in encoder pre-training and language mo deling. In language modeling, piis\nthe probability distribution of predicting the next word. T his follows an auto-regressive decoding\nprocess: a language model only observes the words up to posit ioniand predicts the next. By\ncontrast, in encoder pre-training, the entire sequence can be observed at once, and so it makes no\nsense to predict any of the tokens in this sequence.\n1.2.2.1 Masked Language Modeling\nOne of the most popular methods of encoder pre-training is masked language modeling , which\nforms the basis of the well-known BERT model [ Devlin et al. ,2019 ]. The idea of masked lan-\nguage modeling is to create prediction challenges by maskin g out some of the tokens in the input\nsequence and training a model to predict the masked tokens. I n this sense, the conventional lan-\nguage modeling problem, which is sometimes called causal language modeling , is a special case\nof masked language modeling: at each position, we mask the to kens in the right-context, and pre-\ndict the token at this position using its left context. Howev er, in causal language modeling we\nonly make use of the left-context in word prediction, while t he prediction may depend on tokens\nin the right-context. By contrast, in masked language model ing, all the unmasked tokens are used\nfor word prediction, leading to a bidirectional model that m akes predictions based on both left and\nright-contexts.\n\n--- Page 17 ---\n\n10 Pre-training\nMore formally, for an input sequence x=x0...xm, suppose that we mask the tokens at po-\nsitions A(x) = {i1,...,i u}. Hence we obtain a masked token sequence \u00afxwhere the token at\neach position in A(x)is replaced with a special symbol [MASK] . For example, for the following\nsequence\nThe early bird catches the worm\nwe may have a masked token sequence like this\nThe[MASK] bird catches the [MASK]\nwhere we mask the tokens early andworm (i.e.,i1= 2andi2= 6).\nNow we have two sequences xand\u00afx. The model is then optimized so that we can correctly\npredict xbased on \u00afx. This can be thought of as an autoencoding-like process, and the train-\ning objective is to maximize the reconstruction probabilit yPr(x|\u00afx). Note that there is a simple\nposition-wise alignment between xand\u00afx. Because an unmasked token in \u00afxis the same as the to-\nken in xat the same position, there is no need to consider the predict ion for this unmasked token.\nThis leads to a simpli\ufb01ed training objective which only maxi mizes the probabilities for masked\ntokens. We can express this objective in a maximum likelihoo d estimation fashion\n(\u02c6W,\u02c6\u03b8) = arg max\nW,\u03b8\u2211\nx\u2208D\u2211\ni\u2208A(x)log PrW,\u03b8\ni(xi|\u00afx) (1.10)\nor alternatively express it using the cross-entropy loss\n(\u02c6W,\u02c6\u03b8) = arg min\nW,\u03b8\u2211\nx",
    "a6ba3b32c1e69074": " as an autoencoding-like process, and the train-\ning objective is to maximize the reconstruction probabilit yPr(x|\u00afx). Note that there is a simple\nposition-wise alignment between xand\u00afx. Because an unmasked token in \u00afxis the same as the to-\nken in xat the same position, there is no need to consider the predict ion for this unmasked token.\nThis leads to a simpli\ufb01ed training objective which only maxi mizes the probabilities for masked\ntokens. We can express this objective in a maximum likelihoo d estimation fashion\n(\u02c6W,\u02c6\u03b8) = arg max\nW,\u03b8\u2211\nx\u2208D\u2211\ni\u2208A(x)log PrW,\u03b8\ni(xi|\u00afx) (1.10)\nor alternatively express it using the cross-entropy loss\n(\u02c6W,\u02c6\u03b8) = arg min\nW,\u03b8\u2211\nx\u2208D\u2211\ni\u2208A(x)LogCrossEntropy( pW,\u03b8\ni,pgold\ni) (1.11)\nwhere PrW,\u03b8\nk(xk|\u00afx)is the probability of the true token xkat positionkgiven the corrupted input\nx, and pW,\u03b8\nkis the probability distribution at position kgiven the corrupted input x. To illustrate,\nconsider the above example where two tokens of the sequence \u201c the early bird catches the worm \u201d\nare masked. For this example, the objective is to maximize th e sum of log-scale probabilities\nLoss = log Pr( x2=early|\u00afx= [CLS] The[MASK]\ued19\ued18\ued17\ued1a\n\u00afx2bird catches the [MASK]\ued19\ued18\ued17\ued1a\n\u00afx6) +\nlog Pr(x6=worm |\u00afx= [CLS] The[MASK]\ued19\ued18\ued17\ued1a\n\u00afx2bird catches the [MASK]\ued19\ued18\ued17\ued1a\n\u00afx6)(1.12)\nOnce we obtain the optimized parameters\u02c6Wand\u02c6\u03b8, we can drop\u02c6W. Then, we can further\n\ufb01ne-tune the pre-trained encoder Encoder \u02c6\u03b8(\u00b7)or directly apply it to downstream tasks.\n1.2.2.2 Permuted Language Modeling\nWhile masked language modeling is simple and widely applied , it introduces new issues. One\ndrawback is the use of a special token, [MASK] , which is employed only during training but not\n\n--- Page 18 ---\n\n1.2 Self-supervised Pre-training Tasks 11\nat test time. This leads to a discrepancy between training an d inference. Moreover, the auto-\nencoding process overlooks the dependencies between maske d tokens. For example, in the above\nexample, the prediction of x2(i.e., the \ufb01rst masked token) is made independently of x6(i.e., the\nsecond masked token), though x6should be considered in the context of x2.\nThese issues can be addressed using the permuted language modeling approach to pre-\ntraining [ Yang et al. ,2019 ]. Similar to causal language modeling, permuted language m odeling\ninvolves making sequential predictions of tokens. However , unlike causal modeling where predic-\ntions follow the natural sequence of the text (like left-to- right or right-to-left), permuted language\nmodeling allows for predictions in any order. The approach i s straightforward: we determine an\norder for token predictions and then train the model in a stan dard language modeling manner, as\ndescribed in Section 1.2.1 . Note that in this approach, the actual order of tokens in the text remains\nunchanged, and only the order in which we predict these token s differs from standard language\nmodeling. For example, consider a sequence of 5 tokens x0x1x2x3x4. Let eirepresent the em-\nbedding ofxi(i.e., combination of the token embedding and positional em bedding). In standard\nlanguage modeling, we would generate this sequence in the or der ofx0\u2192x1\u2192x2\u2192x3\u2192x4.\nThe probability of the sequence can be modeled via a generati on process.\nPr(x) = Pr(x0)\u00b7Pr(x1|x0)\u00b7Pr(x2|x0,x1)\u00b7Pr(x3|x0,x1,x2)\u00b7\nPr(x4|x0,x1,x2,x3)\n= Pr(x0)\u00b7Pr(x1|e0)\u00b7Pr(x2|e0,e1)\u00b7Pr(x3|e0,e1,e2)\u00b7\nPr(x4",
    "5737912b367745d7": ", and only the order in which we predict these token s differs from standard language\nmodeling. For example, consider a sequence of 5 tokens x0x1x2x3x4. Let eirepresent the em-\nbedding ofxi(i.e., combination of the token embedding and positional em bedding). In standard\nlanguage modeling, we would generate this sequence in the or der ofx0\u2192x1\u2192x2\u2192x3\u2192x4.\nThe probability of the sequence can be modeled via a generati on process.\nPr(x) = Pr(x0)\u00b7Pr(x1|x0)\u00b7Pr(x2|x0,x1)\u00b7Pr(x3|x0,x1,x2)\u00b7\nPr(x4|x0,x1,x2,x3)\n= Pr(x0)\u00b7Pr(x1|e0)\u00b7Pr(x2|e0,e1)\u00b7Pr(x3|e0,e1,e2)\u00b7\nPr(x4|e0,e1,e2,e3) (1.13)\nNow, let us consider a different order for token prediction: x0\u2192x4\u2192x2\u2192x1\u2192x3. The\nsequence generation process can then be expressed as follow s:\nPr(x) = Pr(x0)\u00b7Pr(x4|e0)\u00b7Pr(x2|e0,e4)\u00b7Pr(x1|e0,e4,e2)\u00b7\nPr(x3|e0,e4,e2,e1) (1.14)\nThis new prediction order allows for the generation of some t okens to be conditioned on a\nbroader context, rather than being limited to just the prece ding tokens as in standard language\nmodels. For example, in generating x3, the model considers both its left-context (i.e., e0,e1,e2)\nand right-context (i.e., e4). The embeddings e0,e1,e2,e4incorporate the positional information\nofx0,x1,x2,x4, preserving the original order of the tokens. As a result, th is approach is somewhat\nakin to masked language modeling: we mask out x3and use its surrounding tokens x0,x1,x2,x4\nto predict this token.\nThe implementation of permuted language models is relative ly easy for Transformers. Be-\ncause the self-attention model is insensitive to the order o f inputs, we do not need to explicitly\nreorder the sequence to have a factorization like Eq. ( 1.14). Instead, permutation can be done\nby setting appropriate masks for self-attention. For examp le, consider the case of computing\nPr(x1|e0,e4,e2). We can place x0,x1,x2,x3,x4in order and block the attention from x3tox1\nin self-attention, as illustrated below\n\n--- Page 19 ---\n\n12 Pre-training\nx0x1x2x3x4\nMasks for Self-attention:\nBlue box = valid attention\nGray box = blocked attention\nFor a more illustrative example, we compare the self-attent ion masking results of causal language\nmodeling, masked language modeling and permuted language m odeling in Figure 1.3.\n1.2.2.3 Pre-training Encoders as Classi\ufb01ers\nAnother commonly-used idea to train an encoder is to conside r classi\ufb01cation tasks. In self-\nsupervised learning, this is typically done by creating new classi\ufb01cation challenges from the unla-\nbeled text. There are many different ways to design the class i\ufb01cation tasks. Here we present two\npopular tasks.\nA simple method, called next sentence prediction (NSP ), is presented in BERT\u2019s original\npaper [ Devlin et al. ,2019 ]. The assumption of NSP is that a good text encoder should cap ture\nthe relationship between two sentences. To model such a rela tionship, in NSP we can use the\noutput of encoding two consecutive sentences Sent AandSent Bto determine whether Sent Bis\nthe next sentence following Sent A. For example, suppose Sent A=\u2019It is raining . \u2019 andSent B=\n\u2019I need an umbrella . \u2019 . The input sequence of the encoder could be\n[CLS] It is raining . [SEP] I need an umbrella . [SEP]\nwhere [CLS] is the start symbol (i.e., x0) which is commonly used in encoder pre-training, and\n[SEP] is a separator that separates the two sentences. The process ing of this sequence follows a\nstandard procedure of Transformer encoding: we \ufb01rst repres ent each token xias its corresponding\nembedding ei, and then feed the embedding sequence {e0,...,em}into the encoder to",
    "4927e1c52610f5a5": " NSP is that a good text encoder should cap ture\nthe relationship between two sentences. To model such a rela tionship, in NSP we can use the\noutput of encoding two consecutive sentences Sent AandSent Bto determine whether Sent Bis\nthe next sentence following Sent A. For example, suppose Sent A=\u2019It is raining . \u2019 andSent B=\n\u2019I need an umbrella . \u2019 . The input sequence of the encoder could be\n[CLS] It is raining . [SEP] I need an umbrella . [SEP]\nwhere [CLS] is the start symbol (i.e., x0) which is commonly used in encoder pre-training, and\n[SEP] is a separator that separates the two sentences. The process ing of this sequence follows a\nstandard procedure of Transformer encoding: we \ufb01rst repres ent each token xias its corresponding\nembedding ei, and then feed the embedding sequence {e0,...,em}into the encoder to obtain the\noutput sequence {h0,...,hm}. Since h0is generally considered as the representation of the entire\nsequence, we add a Softmax layer on top of it to construct a bin ary classi\ufb01cation system. This\nprocess is illustrated as follows\ntoken: [CLS] It is raining . [SEP] I need an umbrella . [SEP]\nembedding: e0e1e2 e3e4e5e6e7e8 e9 e10e11\n\u2193 \u2193 \u2193 \u2193 \u2193 \u2193 \u2193 \u2193 \u2193 \u2193 \u2193 \u2193\nEncoder\n\u2193 \u2193 \u2193 \u2193 \u2193 \u2193 \u2193 \u2193 \u2193 \u2193 \u2193 \u2193\nencoding: h0h1h2 h3h4h5h6h7h8 h9 h10h11\n\u2193\nSoftmax\n\u2193\nIs Next or Not?\n\n--- Page 20 ---\n\n1.2 Self-supervised Pre-training Tasks 13\nx0\nx0x1\nx1x2\nx2x3\nx3x4\nx4Pr(x0) = 1\nPr(x1|e0)\nPr(x2|e0,e1)\nPr(x3|e0,e1,e2)\nPr(x4|e0,e1,e2,e3)\n(a) Causal Language Modeling (order: x0\u2192x1\u2192x2\u2192x3\u2192x4)\nx0\nx0x1\nx1x2\nx2x3\nx3x4\nx4masked masked\nmaskedmasked1\nPr(x1|e0,emask,e2,emask,e4)\n1\nPr(x3|e0,emask,e2,emask,e4)\n1\n(b) Masked Language Modeling (order: x0,[MASK],x2,[MASK],x4\u2192x1,x3)\nx0\nx0x1\nx1x2\nx2x3\nx3x4\nx4Pr(x0) = 1\nPr(x1|e0,e4,e2)\nPr(x2|e0,e4)\nPr(x3|e0,e4,e2,e1)\nPr(x4|e0)\n(c) Permuted Language Modeling (order: x0\u2192x4\u2192x2\u2192x1\u2192x3)\nFig. 1.3: Comparison of self-attention masking results of causal lan guage modeling, masked language modeling and\npermuted language modeling. The gray cell denotes the token at positionjdoes not attend to the token at position i.\nThe blue cell (i,j)denotes that the token at position jattends to the token at position i.emask represents the embedding\nof the symbol [MASK] , which is a combination of the token embedding and the positi onal embedding.\nIn order to generate training samples, we need two sentences each time, one for Sent Aand\nthe other for Sent B. A simple way to do this is to utilize the natural sequence of t wo consecu-\ntive sentences in the text. For example, we obtain a positive sample by using actual consecutive\nsentences, and a negative sample by using randomly sampled s entences. Consequently, training\nthis model is the same as training a classi\ufb01er. Typically, NS P is used as an additional training loss\n\n--- Page 21 ---\n\n14 Pre-training\nfunction for pre-training based on masked language modelin g.\nA second example of training Transformer encoders as classi \ufb01ers is to apply classi\ufb01cation-\nbased supervision signals to each output of an encoder. For e xample, Clark et al. [2019 ] in their\nELECTRA model, propose training a Transformer encoder to id entify whether each input token\nis identical to the original input or has been altered in some manner. The \ufb01rst step",
    "da92b9cf88704082": "and\nthe other for Sent B. A simple way to do this is to utilize the natural sequence of t wo consecu-\ntive sentences in the text. For example, we obtain a positive sample by using actual consecutive\nsentences, and a negative sample by using randomly sampled s entences. Consequently, training\nthis model is the same as training a classi\ufb01er. Typically, NS P is used as an additional training loss\n\n--- Page 21 ---\n\n14 Pre-training\nfunction for pre-training based on masked language modelin g.\nA second example of training Transformer encoders as classi \ufb01ers is to apply classi\ufb01cation-\nbased supervision signals to each output of an encoder. For e xample, Clark et al. [2019 ] in their\nELECTRA model, propose training a Transformer encoder to id entify whether each input token\nis identical to the original input or has been altered in some manner. The \ufb01rst step of this method\nis to generate a new sequence from a given sequence of tokens, where some of the tokens are\naltered. To do this, a small masked language model (call it th e generator) is applied: we randomly\nmask some of the tokens, and train this model to predict the ma sked tokens. For each training\nsample, this masked language model outputs a token at each ma sked position, which might be\ndifferent from the original token. At the same time, we train another Transformer encoder (call it\nthe discriminator) to determine whether each predicted tok en is the same as the original token or\naltered. More speci\ufb01cally, we use the generator to generate a sequence where some of the tokens\nare replaced. Below is an illustration.\noriginal: [CLS] The boy spent hours working on toys .\n\u2193 \u2193 \u2193 \u2193 \u2193 \u2193 \u2193 \u2193 \u2193\nmasked: [CLS] The boy spent [MASK] working on [MASK] .\n\u2193 \u2193 \u2193 \u2193 \u2193 \u2193 \u2193 \u2193 \u2193\nGenerator (small masked language model)\n\u2193 \u2193 \u2193 \u2193 \u2193 \u2193 \u2193 \u2193 \u2193\nreplaced: [CLS] The boy spent decades working on toys .\nThen, we use the discriminator to label each of these tokens a sorginal orreplaced , as follows\nreplaced: [CLS] The boy spent decades working on toys .\n\u2193 \u2193 \u2193 \u2193 \u2193 \u2193 \u2193 \u2193 \u2193\nDiscriminator (the model we want)\n\u2193 \u2193 \u2193 \u2193 \u2193 \u2193 \u2193 \u2193 \u2193\nlabel: original original original original replaced original original original original\nFor training, the generator is optimized as a masked languag e model with maximum likelihood\nestimation, and the discriminator is optimized as a classi\ufb01 er using a classi\ufb01cation-based loss. In\nELECTRA, the maximum likelihood-based loss and the classi\ufb01 cation-based loss are combined for\njointly training both the generator and discriminator. An a lternative approach is to use generative\nadversarial networks (GANs), that is, the generator is trai ned to fool the discriminator, and the dis-\ncriminator is trained to distinguish the output of the gener ator from the true distribution. However,\nGAN-style training complicates the training task and is mor e dif\ufb01cult to scale up. Nevertheless,\nonce training is complete, the generator is discarded, and t he encoding part of the discriminator is\napplied as the pre-trained model for downstream tasks.\n\n--- Page 22 ---\n\n1.2 Self-supervised Pre-training Tasks 15\n1.2.3 Encoder-Decoder Pre-training\nIn NLP, encoder-decoder architectures are often used to mod el sequence-to-sequence problems,\nsuch as machine translation and question answering. In addi tion to these typical sequence-to-\nsequence problems in NLP, encoder-decoder models can be ext ended to deal with many other\nproblems. A simple idea is to consider text as both the input a nd output of a problem, and so\nwe can directly apply encoder-decoder models. For example, given a text, we can ask a model to\noutput a text describing the sentiment of the input text, suc h aspositive ,negative , and neutral .\nSuch an idea allows us to develop a single text-to-text syste m to address any NLP problem.\nWe can formulate different problems into the same text-to-t ext format. We \ufb01rst train an encoder-\ndecoder model to gain general-purpose knowledge of languag e via self-supervision. This model\nis then \ufb01ne-tuned for speci\ufb01c downstream tasks using target ed text-to-text data.\n1.2.3.1 Masked Encoder-Decoder Pre-training\nInRaffel et al. [2020 ]\u2019sT5model, many different tasks are framed as the same text-to-t ext task.\nEach sample in",
    "60ba03ec0847b8ec": " simple idea is to consider text as both the input a nd output of a problem, and so\nwe can directly apply encoder-decoder models. For example, given a text, we can ask a model to\noutput a text describing the sentiment of the input text, suc h aspositive ,negative , and neutral .\nSuch an idea allows us to develop a single text-to-text syste m to address any NLP problem.\nWe can formulate different problems into the same text-to-t ext format. We \ufb01rst train an encoder-\ndecoder model to gain general-purpose knowledge of languag e via self-supervision. This model\nis then \ufb01ne-tuned for speci\ufb01c downstream tasks using target ed text-to-text data.\n1.2.3.1 Masked Encoder-Decoder Pre-training\nInRaffel et al. [2020 ]\u2019sT5model, many different tasks are framed as the same text-to-t ext task.\nEach sample in T5 follows the format\nSource Text \u2192Target Text\nHere \u2192separates the source text, which consists of a task descript ion or instruction and the input\ngiven to the system, from the target text, which is the respon se to the input task. As an example,\nconsider a task of translating from Chinese to English. A tra ining sample can be expressed as\n[CLS] Translate from Chinese to English: \u4f60\u597d\uff01\u2192 \u27e8s\u27e9Hello!\nwhere [CLS] and\u27e8s\u27e9are the start symbols on the source and target sides, respect ively5.\nLikewise, we can express other tasks in the same way. For exam ple\n[CLS] Answer: when was Albert Einstein born?\n\u2192 \u27e8s\u27e9He was born on March 14, 1879.\n[CLS] Simplify: the professor, who has has published numerous pap ers in his \ufb01eld,\nwill be giving a lecture on the topic next week.\n\u2192 \u27e8s\u27e9The experienced professor will give a lecture next week.\n[CLS] Score the translation from English to Chinese. English: whe n in Rome, do as\nthe Romans do. Chinese: \u4eba\u5728\u7f57\u9a6c\u5c31\u50cf\u7f57\u9a6c\u4eba\u4e00\u6837\u505a\u4e8b\u3002\n\u2192 \u27e8s\u27e90.81\nwhere instructions are highlighted in gray. An interesting case is that in the last example we\n5We could use the same start symbol for different sequences. H ere we use different symbols to distinguish the\nsequences on the encoder and decoder-sides.\n\n--- Page 23 ---\n\n16 Pre-training\nreframe the scoring problem as the text generation problem. Our goal is to generate a text repre-\nsenting the number 0.81, rather than outputting it as a numerical value.\nThe approach described above provides a new framework of uni versal language understanding\nand generation. Both the task instructions and the problem i nputs are provided to the system\nin text form. The system then follows the instructions to com plete the task. This method puts\ndifferent problems together, with the bene\ufb01t of training a s ingle model that can perform many\ntasks simultaneously.\nIn general, \ufb01ne-tuning is necessary for adapting the pre-tr ained model to a speci\ufb01c downstream\ntask. In this process, one can use different ways to instruct the model for the task, such as using a\nshort name of the task as the pre\ufb01x to the actual input sequenc e or providing a detailed description\nof the task. Since the task instructions are expressed in tex t form and involved as part of the input,\nthe general knowledge of instruction can be gained through l earning the language understanding\nmodels in the pre-training phase. This may help enable zero- shot learning. For example, pre-\ntrained models can generalize to address new problems where the task instructions have never\nbeen encountered.\nThere have been several powerful methods of self-supervise d learning for either Transformer\nencoders or decoders. Applying these methods to pre-train e ncoder-decoder models is relatively\nstraightforward. One common choice is to train encoder-dec oder models as language models. For\nexample, the encoder receives a sequence pre\ufb01x, while the de coder generates the remaining se-\nquence. However, this differs from standard causal languag e modeling, where the entire sequence\nis autoregressively generated from the \ufb01rst token. In our ca se, the encoder processes the pre\ufb01x at\nonce, and then the decoder predicts subsequent tokens in the manner of causal language modeling.\nPut more precisely, this is a pre\ufb01x language modeling problem: a language model predicts the\nsubsequent sequence given a pre\ufb01x, which serves as the conte xt for prediction.\nConsider the following example\n[CLS] The puppies are frolicking\ufffd",
    "1fa4fc86cca1bd7d": "There have been several powerful methods of self-supervise d learning for either Transformer\nencoders or decoders. Applying these methods to pre-train e ncoder-decoder models is relatively\nstraightforward. One common choice is to train encoder-dec oder models as language models. For\nexample, the encoder receives a sequence pre\ufb01x, while the de coder generates the remaining se-\nquence. However, this differs from standard causal languag e modeling, where the entire sequence\nis autoregressively generated from the \ufb01rst token. In our ca se, the encoder processes the pre\ufb01x at\nonce, and then the decoder predicts subsequent tokens in the manner of causal language modeling.\nPut more precisely, this is a pre\ufb01x language modeling problem: a language model predicts the\nsubsequent sequence given a pre\ufb01x, which serves as the conte xt for prediction.\nConsider the following example\n[CLS] The puppies are frolicking\ued19 \ued18\ued17 \ued1a\nPre\ufb01x\u2192 \u27e8s\u27e9outside the house .\ued19\ued18\ued17\ued1a\nSubsequent Sequence\nWe can directly train an encoder-decoder model using exampl es like this. Then, the encoder learns\nto understand the pre\ufb01x, and the decoder learns to continue w riting based on this understanding.\nFor large-scale pre-training, it is easy to create a large nu mber of training examples from unlabeled\ntext.\nIt is worth noting that for pre-trained encoder-decoder mod els to be effective in multi-lingual\nand cross-lingual tasks, such as machine translation, they should be trained with multi-lingual\ndata. This typically requires that the vocabulary includes tokens from all the languages. By\ndoing so, the models can learn shared representations acros s different languages, thereby enabling\ncapabilities in both language understanding and generatio n in a multi-lingual and cross-lingual\ncontext.\nA second approach to pre-training encoder-decoder models i s masked language modeling. In\nthis approach, as discussed in Section 1.2.2 , tokens in a sequence are randomly replaced with a\nmask symbol, and the model is then trained to predict these ma sked tokens based on the entire\nmasked sequence.\nAs an illustration, consider the task of masking and reconst ructing the sentence\n\n--- Page 24 ---\n\n1.2 Self-supervised Pre-training Tasks 17\nThe puppies are frolicking outside the house .\nBy masking two tokens (say, frolicking andthe), we have the BERT-style input and output of the\nmodel, as follows\n[CLS] The puppies are [MASK] outside [MASK] house .\n\u2192 \u27e8s\u27e9 frolicking the\nHere denotes the masked position at which we do not make token pred ictions. By varying the\npercentage of the tokens in the text, this approach can be gen eralized towards either BERT-style\ntraining or language modeling-style training [ Song et al. ,2019 ]. For example, if we mask out all\nthe tokens, then the model is trained to generate the entire s equence\n[CLS] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MAS K]\n\u2192 \u27e8s\u27e9The puppies are frolicking outside the house .\nIn this case, we train the decoder as a language model.\nNote that, in the context of the encoder-decoder architectu re, we can use the encoder to read\nthe masked sequence, and use the decoder to predict the origi nal sequence. With this objective,\nwe essentially have a denoising autoencoder: the encoder tr ansforms a corrupted input into some\nhidden representation, and the decoder reconstructs the un corrupted input from this hidden repre-\nsentation. Here is an example of input and output for denoisi ng training.\n[CLS] The puppies are [MASK] outside [MASK] house .\n\u2192 \u27e8s\u27e9The puppies are frolicking outside the house .\nBy learning to map from this corrupted sequence to its uncorr upted counterpart, the model gains\nthe ability to understand on the encoder side and to generate on the decoder side. See Figure 1.4\nfor an illustration of how an encoder-decoder model is train ed with BERT-style and denoising\nautoencoding objectives.\nAs we randomly select tokens for masking, we can certainly ma sk consecutive tokens [ Joshi et al. ,\n2020 ]. Here is an example.\n[CLS] The puppies are [MASK] outside [MASK] [MASK] .\n\u2192 \u27e8s\u27e9The puppies are frolicking outside the house .\nAnother way to consider consecutive masked tokens is to repr esent them as spans. Here we\nfollow Raffel et al",
    "29a90ce803302b65": ". Here is an example of input and output for denoisi ng training.\n[CLS] The puppies are [MASK] outside [MASK] house .\n\u2192 \u27e8s\u27e9The puppies are frolicking outside the house .\nBy learning to map from this corrupted sequence to its uncorr upted counterpart, the model gains\nthe ability to understand on the encoder side and to generate on the decoder side. See Figure 1.4\nfor an illustration of how an encoder-decoder model is train ed with BERT-style and denoising\nautoencoding objectives.\nAs we randomly select tokens for masking, we can certainly ma sk consecutive tokens [ Joshi et al. ,\n2020 ]. Here is an example.\n[CLS] The puppies are [MASK] outside [MASK] [MASK] .\n\u2192 \u27e8s\u27e9The puppies are frolicking outside the house .\nAnother way to consider consecutive masked tokens is to repr esent them as spans. Here we\nfollow Raffel et al. [2020 ]\u2019s work, and use [X],[Y]and[Z]to denote sentinel tokens that cover\none or more consecutive masked tokens. Using this notation, we can re-express the above training\nexample as\n[CLS] The puppies are [X] outside [Y] .\n\u2192 \u27e8s\u27e9[X] frolicking [Y] the house [Z]\n\n--- Page 25 ---\n\n18 Pre-training\n[CLS] Thepuppies are [M] in[M] house .Encoder Decoder\nfrolicking[M] [M] [M] \u27e8s\u27e9 [M] the [M][M] [M] [M] frolicking [M] the [M] [M]Loss\n(a) Training an encoder-decoder model with BERT-style mask ed language modeling\n[CLS] Thepuppies are [M] in[M] house .Encoder Decoder\nfrolickingare puppies The \u27e8s\u27e9 in the houseThe puppies arefrolicking in the house .\n(b) Training an encoder-decoder model with denoising autoe ncodingLoss over the sequence\nFig. 1.4: Training an encoder-decoder model using BERT-style and den oising autoencoding methods. In both methods,\nthe input to the encoder is a corrupted token sequence where s ome tokens are masked and replaced with [MASK] (or\n[M]for short). The decoder predicts these masked tokens, but in different ways. In BERT-style training, the decoder\nonly needs to compute the loss for the masked tokens, while th e remaining tokens in the sequence can be simply treated\nas[MASK] tokens. In denoising autoencoding, the decoder predicts th e sequence of all tokens in an autoregressive\nmanner. As a result, the loss is obtained by accumulating the losses of all these tokens, as in standard language\nmodeling.\nThe idea is that we represent the corrupted sequence as a sequ ence containing placeholder\nslots. The training task is to \ufb01ll these slots with the correc t tokens using the surrounding context.\nAn advantage of this approach is that the sequences used in tr aining would be shorter, making the\ntraining more ef\ufb01cient. Note that masked language modeling provides a very general framework\nfor training encoder-decoder models. Various settings can be adjusted to have different training\nversions, such as altering the percentage of tokens masked a nd the maximum length of the masked\nspans.\n1.2.3.2 Denoising Training\nIf we view the problem of training encoder-decoder models as a problem of training denoising\nautoencoders, there will typically be many different metho ds for introducing input corruption and\nreconstructing the input. For instance, beyond randomly ma sking tokens, we can also alter some\nof them or rearrange their order.\nSuppose we have an encoder-decoder model that can map an inpu t sequence xto an output\n\n--- Page 26 ---\n\n1.2 Self-supervised Pre-training Tasks 19\nsequence y\ny= Decode \u03c9(Encode \u03b8(x))\n= Model \u03b8,\u03c9(x) (1.15)\nwhere\u03b8and\u03c9are the parameters of the encoder and the decoder, respectiv ely. In denoising\nautoencoding problems, we add some noise to xto obtain a noisy, corrupted input xnoise. By\nfeeding xnoise into the encoder, we wish the decoder to output the original i nput. The training\nobjective can be de\ufb01ned as\n(\u02c6\u03b8,\u02c6\u03c9) = arg max\n\u03b8,\u03c9Loss(Model \u03b8,\u03c9(xnoise),x) (1.16)\nHere the loss function Loss(Model \u03b8,\u03c9(xnoise),x)evaluates how well the model Model \u03b8,\u03c9(xnoise)\nre",
    "e80d6bb96aec7af6": "-decoder model that can map an inpu t sequence xto an output\n\n--- Page 26 ---\n\n1.2 Self-supervised Pre-training Tasks 19\nsequence y\ny= Decode \u03c9(Encode \u03b8(x))\n= Model \u03b8,\u03c9(x) (1.15)\nwhere\u03b8and\u03c9are the parameters of the encoder and the decoder, respectiv ely. In denoising\nautoencoding problems, we add some noise to xto obtain a noisy, corrupted input xnoise. By\nfeeding xnoise into the encoder, we wish the decoder to output the original i nput. The training\nobjective can be de\ufb01ned as\n(\u02c6\u03b8,\u02c6\u03c9) = arg max\n\u03b8,\u03c9Loss(Model \u03b8,\u03c9(xnoise),x) (1.16)\nHere the loss function Loss(Model \u03b8,\u03c9(xnoise),x)evaluates how well the model Model \u03b8,\u03c9(xnoise)\nreconstructs the original input x. We can choose the cross-entropy loss as usual.\nAs the model architecture and the training approach have bee n developed, the remaining issue\nis the corruption of the input. Lewis et al. [2020 ], in their BART model, propose corrupting the\ninput sequence in several different ways.\n\u2022Token Masking . This is the same masking method that we used in masked langua ge mod-\neling. The tokens in the input sequence are randomly selecte d and masked.\n\u2022Token Deletion . This method is similar to token masking. However, rather th an replacing\nthe selected tokens with a special symbol [MASK] , these tokens are removed from the\nsequence. See the following example for a comparison of the t oken masking and token\ndeletion methods.\nOriginal ( x): The puppies are frolicking outside the house .\nToken Masking ( xnoise): The puppies are [MASK] outside [MASK] house .\nToken Deletion ( xnoise): The puppies are frolicking outside thehouse .\nwhere the underlined tokens in the original sequence are mas ked or deleted.\n\u2022Span Masking . Non-overlapping spans are randomly sampled over the seque nce. Each\nspan is masked by [MASK] . We also consider spans of length 0, and, in such cases, [MASK]\nis simply inserted at a position in the sequence. For example , we can use span masking to\ncorrupt the above sequence as\nOriginal ( x): The 0 puppies are frolicking outside the house .\nSpan Masking ( xnoise): The [MASK] puppies are [MASK] house .\nHere the span frolicking outside the is replaced with a single [MASK] .0indicates a length-\n0 span, and so we insert an [MASK] between Theandpuppies . Span masking introduces\nnew prediction challenges in which the model needs to know ho w many tokens are gener-\nated from a span. This problem is very similar to fertility mo deling in machine translation\n[Brown et al. ,1993 ].\n\n--- Page 27 ---\n\n20 Pre-training\nIf we consider a sequence consisting of multiple sentences, additional methods of corruption\ncan be applied. In the BART model, there are two such methods.\n\u2022Sentence Reordering . This method randomly permutes the sentences so that the mod el can\nlearn to reorder sentences in a document. Consider, for exam ple, two consecutive sentences\nHard work leads to success . Success brings happiness .\nWe can reorder the two sentences to have a corrupted input seq uence\nSuccess brings happiness . Hard work leads to success .\n\u2022Document Rotation . The goal of this task is to identify the start token of the seq uence.\nFirst, a token is randomly selected from the sequence. Then, the sequence is rotated so that\nthe selected token is the \ufb01rst token. For example, suppose we select the token leads from\nthe above sequence. The rotated sequence is\nleads to success . Success brings happiness . Hard work Hard workselected\nwhere the subsequence Hard work before leads is appended to the end of the sequence.\nFor pre-training, we can apply multiple corruption methods to learn robust models, for ex-\nample, we randomly choose one of them for each training sampl e. In practice, the outcome of\nencoder-decoder pre-training depends heavily on the input corruption methods used, and so we\ntypically need to choose appropriate training objectives t hrough careful experimentation.\n1.2.4 Comparison of Pre-training Tasks\nSo far, we have discussed a number of pre-training tasks. Sin ce the same training objective can\napply to different architectures (e.g., using masked langu age modeling for both encoder-only and\nencoder-decoder pre-training), categorizing pre-traini ng tasks based solely on model architecture\ndoes not seem ideal. Instead, we summarize these tasks based on the training",
    "055b193847ef103e": " leads from\nthe above sequence. The rotated sequence is\nleads to success . Success brings happiness . Hard work Hard workselected\nwhere the subsequence Hard work before leads is appended to the end of the sequence.\nFor pre-training, we can apply multiple corruption methods to learn robust models, for ex-\nample, we randomly choose one of them for each training sampl e. In practice, the outcome of\nencoder-decoder pre-training depends heavily on the input corruption methods used, and so we\ntypically need to choose appropriate training objectives t hrough careful experimentation.\n1.2.4 Comparison of Pre-training Tasks\nSo far, we have discussed a number of pre-training tasks. Sin ce the same training objective can\napply to different architectures (e.g., using masked langu age modeling for both encoder-only and\nencoder-decoder pre-training), categorizing pre-traini ng tasks based solely on model architecture\ndoes not seem ideal. Instead, we summarize these tasks based on the training objectives.\n\u2022Language Modeling . Typically, this approach refers to an auto-regressive gen eration pro-\ncedure of sequences. At one time, it predicts the next token b ased on its previous context.\n\u2022Masked Language Modeling . Masked Language Modeling belongs to a general mask-\npredict framework. It randomly masks tokens in a sequence an d predicts these tokens using\nthe entire masked sequence.\n\n--- Page 28 ---\n\n1.3 Example: BERT 21\n\u2022Permuted Language Modeling . Permuted language modeling follows a similar idea to\nmasked language modeling, but considers the order of (maske d) token prediction. It reorders\nthe input sequence and predicts the tokens sequentially. Ea ch prediction is based on some\ncontext tokens that are randomly selected.\n\u2022Discriminative Training . In discriminative training, supervision signals are crea ted from\nclassi\ufb01cation tasks. Models for pre-training are integrat ed into classi\ufb01ers and trained to-\ngether with the remaining parts of the classi\ufb01ers to enhance their classi\ufb01cation performance.\n\u2022Denoising Autoencoding . This approach is applied to the pre-training of encoder-de coder\nmodels. The input is a corrupted sequence and the encoder-de coder models are trained to\nreconstruct the original sequence.\nTable 1.1illustrates these methods and their variants using example s. The use of these ex-\namples does not distinguish between models, but we mark the m odel architectures where the\npre-training tasks can be applied. In each example, the inpu t consists of a token sequence, and the\noutput is either a token sequence or some probabilities. For generation tasks, such as language\nmodeling, superscripts are used to indicate the generation order on the target side. If the super-\nscripts are omitted, it indicates that the output sequence c an be generated either autoregressively\nor simultaneously. On the source side, we assume that the seq uence undergoes a standard Trans-\nformer encoding process, meaning that each token can see the entire sequence in self-attention.\nThe only exception is in permuted language modeling, where a n autoregressive generation pro-\ncess is implemented by setting attention masks on the encode r side. To simplify the discussion,\nwe remove the token \u27e8s\u27e9from the target-side of each example.\nWhile these pre-training tasks are different, it is possibl e to compare them in the same frame-\nwork and experimental setup [ Dong et al. ,2019 ;Raffel et al. ,2020 ;Lewis et al. ,2020 ]. Note that\nwe cannot list all the pre-training tasks here as there are ma ny of them. For more discussions on\npre-training tasks, the interested reader may refer to some surveys on this topic [ Qiu et al. ,2020 ;\nHan et al. ,2021 ].\n1.3 Example: BERT\nIn this section, we introduce BERT models, which are among th e most popular and widely used\npre-trained sequence encoding models in NLP.\n1.3.1 The Standard Model\nThe standard BERT model, which is proposed in Devlin et al. [2019 ]\u2019s work, is a Transformer\nencoder trained using both masked language modeling and nex t sentence prediction tasks. The\nloss used in training this model is a sum of the loss of the two t asks.\nLoss BERT = Loss MLM + Loss NSP (1.17)\nAs is regular in training deep neural networks, we optimize t he model parameters by minimizing\nthis loss. To do this, a number of training samples are collec ted. During training, a batch of\n\n--- Page 29 ---\n\n22 Pre-training\nMethod Enc Dec E-D Input Output\nCausal LM \u2022 \u2022 The1kitten2is3chasing4the5ball6.7\nPre\ufffd",
    "27b3a01a5baec07e": "\nIn this section, we introduce BERT models, which are among th e most popular and widely used\npre-trained sequence encoding models in NLP.\n1.3.1 The Standard Model\nThe standard BERT model, which is proposed in Devlin et al. [2019 ]\u2019s work, is a Transformer\nencoder trained using both masked language modeling and nex t sentence prediction tasks. The\nloss used in training this model is a sum of the loss of the two t asks.\nLoss BERT = Loss MLM + Loss NSP (1.17)\nAs is regular in training deep neural networks, we optimize t he model parameters by minimizing\nthis loss. To do this, a number of training samples are collec ted. During training, a batch of\n\n--- Page 29 ---\n\n22 Pre-training\nMethod Enc Dec E-D Input Output\nCausal LM \u2022 \u2022 The1kitten2is3chasing4the5ball6.7\nPre\ufb01x LM \u2022 \u2022[C]The kitten is chasing1the2ball3.4\nMasked LM \u2022 \u2022[C]The kitten [M]chasing the [M]. is ball\nMASS-style \u2022 \u2022[C]The kitten [M] [M] [M] ball . is chasing the\nBERT-style \u2022 \u2022[C]The kitten [M]playing the[M]. kitten is chasing ball\nPermuted LM \u2022 [C]The kitten is chasing the ball . The5kitten7is6chasing1the4ball2.3\nNext Sentence \u2022 [C]The kitten is chasing the ball . Pr(IsNext |representation-of-[C])\nPrediction Birds eat worms .\nSentence \u2022 Encode a sentence as haand Score( ha,hb)\nComparison another sentence as hb\nToken Classi\ufb01cation \u2022 [C]The kitten is chasing the ball . Pr(\u00b7|The) Pr( \u00b7|kitten) ...Pr(\u00b7|.)\nToken Reordering \u2022[C]. kitten the chasing The is ball The1kitten2is3chasing4the5ball6.7\nToken Deletion \u2022[C]The kitten is chasing the ball . The1kitten2is3chasing4the5ball6.7\nSpan Masking \u2022[C]The kitten [M]is[M]. The1kitten2is3chasing4the5ball6.7\nSentinel Masking \u2022[C]The kitten [X]the[Y] [X]1is2chasing3[Y]4ball5.6\nSentence \u2022[C]The ball rolls away swiftly . The The1kitten2is3chasing4the5ball6.7\nReordering kitten is chasing the ball . The8ball9rolls10away11swiftly12.13\nDocument \u2022[C]chasing the ball . The ball rolls The1kitten2is3chasing4the5ball6.7\nRotation away swiftly . The kitten is The8ball9rolls10away11swiftly12.13\nTable 1.1: Comparison of pre-training tasks, including language modeling ,masked language modeling ,permuted\nlanguage modeling ,discriminative training , and denoising autoencoding .[C] = [CLS] ,[M] = [MASK] ,[X],[Y] =\nsentinel tokens . Enc, Dec and E-D indicate whether the approach can be applie d to encoder-only, decoder-only,\nencoder-decoder models, respectively. For generation tas ks, superscripts are used to represent the order of the token s.\ntraining samples is randomly selected from this collection at a time, and Loss BERT is accumulated\nover these training samples. Then, the model parameters are updated via gradient descent or its\nvariants. This process is repeated many times until some sto pping criterion is satis\ufb01ed, such as\nwhen the training loss converges.\n1.3.1.1 Loss Functions\nIn general, BERT models are used to represent a single senten ce or a pair of sentences, and thus\ncan handle various downstream language understanding prob lems. In this section we assume that\nthe input representation is a sequence containing two sente ncesSent AandSent B, expressed as\n[CLS] Sent A[SEP] Sent B[SEP]\nHere we follow the notation in BERT\u2019s paper and use [SEP] to denote the separator.\nGiven this sequence, we can obtain LossMLM andLossNSP separately. For masked language\nmodeling, we predict a subset of the tokens in the sequence. T ypically, a certain percentage of the\n\n--- Page 30 ---\n\n1.3 Example: BERT 23\ntokens are randomly selected, for example, in the standard B ERT model, 15% of the tokens in\neach sequence are",
    "30b0d74d7770c44f": "\nwhen the training loss converges.\n1.3.1.1 Loss Functions\nIn general, BERT models are used to represent a single senten ce or a pair of sentences, and thus\ncan handle various downstream language understanding prob lems. In this section we assume that\nthe input representation is a sequence containing two sente ncesSent AandSent B, expressed as\n[CLS] Sent A[SEP] Sent B[SEP]\nHere we follow the notation in BERT\u2019s paper and use [SEP] to denote the separator.\nGiven this sequence, we can obtain LossMLM andLossNSP separately. For masked language\nmodeling, we predict a subset of the tokens in the sequence. T ypically, a certain percentage of the\n\n--- Page 30 ---\n\n1.3 Example: BERT 23\ntokens are randomly selected, for example, in the standard B ERT model, 15% of the tokens in\neach sequence are selected. Then the sequence is modi\ufb01ed in t hree ways\n\u2022Token Masking .80% of the selected tokens are masked and replaced with the symbo l\n[MASK] . For example\nOriginal: [CLS] It is raining . [SEP] I need an umbrella . [SEP]\nMasked: [CLS] It is [MASK] . [SEP] I need [MASK] umbrella . [SE P]\nwhere the selected tokens are underlined. Predicting maske d tokens makes the model learn\nto represent tokens from their surrounding context.\n\u2022Random Replacement .10% of the selected tokens are changed to a random token. For\nexample,\nOriginal: [CLS] It is raining . [SEP] I need an umbrella . [SEP]\nRandom Token: [CLS] It is raining . [SEP] I need an hat. [SEP]\nThis helps the model learn to recover a token from a noisy inpu t.\n\u2022Unchanged .10% of the selected tokens are kept unchanged. For example,\nOriginal: [CLS] It is raining . [SEP] I need an umbrella . [SEP]\nUnchanged Token: [CLS] It is raining . [SEP] Ineed an umbrella . [SEP]\nThis is not a dif\ufb01cult prediction task, but can guide the mode l to use easier evidence for\nprediction.\nLetA(x)be the set of selected positions of a given token sequence x, and \u00afxbe the modi\ufb01ed\nsequence of x. The loss function of masked language modeling can be de\ufb01ned as\nLoss MLM =\u2212\u2211\ni\u2208A(x)log Pr i(xi|\u00afx) (1.18)\nwhere Pri(xi|\u00afx)is the probability of predicting xiat the position igiven \u00afx. Figure 1.5shows a\nrunning example of computing Loss MLM .\nFor next sentence prediction, we follow the method describe d in Section 1.2.2.3 . Each training\nsample is classi\ufb01ed into a label set {IsNext,NotNext }, for example,\nSequence: [CLS] It is raining . [SEP] I need an umbrella . [SEP ]\nLabel: IsNext\nSequence: [CLS] The cat sleeps on the windowsill . [SEP] Appl es grow on trees . [SEP]\nLabel: NotNext\n\n--- Page 31 ---\n\n24 Pre-training\n[CLS] It is raining . [SEP] I need an umbrella . [SEP] Input:\nSelect tokens with a probability of 15%\n[CLS] It is raining .[SEP] Ineed an umbrella .[SEP] Token Selection:\nMask selected tokens with a probability of 80%\n[CLS] It is [MASK] .[SEP] Ineed [MASK] umbrella .[SEP] Token Masking:\nAlter selected tokens with a probability of 10%\n[CLS] It is [MASK] .[SEP] Ineed [MASK] hat.[SEP] Token:\nReplacement\nKeep selected tokens unchanged with a probability of 10%\n[CLS] It is [MASK] .[SEP] Ineed [MASK] hat . [SEP] Unchanged:\nTrain the Transformer encoder with the modi\ufb01ed sequence\n[CLS] It is[MASK] .[SEP] I need [MASK] hat .[SEP]e0 e1 e2 e3 e4 e5 e6 e7 e8 e9 e10 e11h0 h1 h2 h3 h4 h5 h6 h7 h8 h9 h10 h11training I an umbrella\nTransformer Encoder\nFig. 1.5: A running example of BERT-style masked language modeling. F irst,15% tokens are randomly selected.\nThese selected tokens are then processed in one of three ways : replaced with a [MASK] token ( 80",
    "d2e5380270ecb262": " It is [MASK] .[SEP] Ineed [MASK] hat.[SEP] Token:\nReplacement\nKeep selected tokens unchanged with a probability of 10%\n[CLS] It is [MASK] .[SEP] Ineed [MASK] hat . [SEP] Unchanged:\nTrain the Transformer encoder with the modi\ufb01ed sequence\n[CLS] It is[MASK] .[SEP] I need [MASK] hat .[SEP]e0 e1 e2 e3 e4 e5 e6 e7 e8 e9 e10 e11h0 h1 h2 h3 h4 h5 h6 h7 h8 h9 h10 h11training I an umbrella\nTransformer Encoder\nFig. 1.5: A running example of BERT-style masked language modeling. F irst,15% tokens are randomly selected.\nThese selected tokens are then processed in one of three ways : replaced with a [MASK] token ( 80% of the time),\nreplaced with a random token ( 10% of the time), or kept unchanged ( 10% of the time). The model is trained to predict\nthese selected tokens based on the modi\ufb01ed sequence. eirepresents the embedding of the token at the position i. Gray\nboxes represent the Softmax layers.\nThe output vector of the encoder for the \ufb01rst token [CLS] is viewed as the sequence representation,\ndenoted by hcls(orh0). A classi\ufb01er is built on top of hcls. Then, we can compute the probability of\na labelcgiven hcls, i.e., Pr(c|hcls). There are many loss functions one can choose for classi\ufb01cat ion\nproblems. For example, in maximum likelihood training, we c an de\ufb01ne Loss NSPas\nLoss NSP =\u2212log Pr(cgold|hcls) (1.19)\n\n--- Page 32 ---\n\n1.3 Example: BERT 25\nwherecgoldis the correct label for this sample.\n1.3.1.2 Model Setup\nAs shown in Figure 1.6, BERT models are based on the standard Transformer encoder a rchitecture.\nThe input is a sequence of embeddings, each being the sum of th e token embedding, the positional\nembedding, and the segment embedding.\ne=x+epos+eseg (1.20)\nBoth the token embedding ( x) and positional embedding ( epos) are regular, as in Transformer\nmodels. The segment embedding ( eseg) is a new type of embedding that indicates whether a token\nbelongs to Sent AorSent B. This can be illustrated by the following example.\nToken [CLS] It is raining . [SEP] I need an umbrella . [SEP]\nxx0 x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 x11\neposPE(0) PE(1) PE(2) PE(3) PE(4) PE(5) PE(6) PE(7) PE(8) PE(9) P E(10) PE(11)\neseg eA eA eA eA eA eA eB eB eB eB eB eB\nThe main part of BERT models is a multi-layer Transformer net work. A Transformer layer\nconsists of a self-attention sub-layer and an FFN sub-layer . Both of them follow the post-norm\narchitecture: output = LNorm( F(input) + input) , whereF(\u00b7)is the core function of the sub-\nlayer (either a self-attention model or an FFN), and LNorm( \u00b7)is the layer normalization unit.\nTypically, a number of Transformer layers are stacked to for m a deep network. At each position of\nthe sequence, the output representation is a real-valued ve ctor which is produced by the last layer\nof the network.\nThere are several aspects one may consider in developing BER T models.\n\u2022Vocabulary Size (|V|). In Transformers, each input token is represented as an ent ry in a\nvocabularyV. Large vocabularies can cover more surface form variants of words, but may\nlead to increased storage requirements.\n\u2022Embedding Size (de). Every token is represented as a de-dimensional real-valued vector.\nAs presented above, this vector is the sum of the token embedd ing, positional embedding,\nand segment embedding, all of which are also de-dimensional real-valued vectors.\n\u2022Hidden Size (d). The input and output of a sub-layer are of ddimensions. Besides, most\nof the hidden states of a sub-layer are d-dimensional vectors. In general, dcan be roughly\nviewed as the width of the network.\n\u2022Number of Heads (nhead",
    "eda120047cba6319": " the output representation is a real-valued ve ctor which is produced by the last layer\nof the network.\nThere are several aspects one may consider in developing BER T models.\n\u2022Vocabulary Size (|V|). In Transformers, each input token is represented as an ent ry in a\nvocabularyV. Large vocabularies can cover more surface form variants of words, but may\nlead to increased storage requirements.\n\u2022Embedding Size (de). Every token is represented as a de-dimensional real-valued vector.\nAs presented above, this vector is the sum of the token embedd ing, positional embedding,\nand segment embedding, all of which are also de-dimensional real-valued vectors.\n\u2022Hidden Size (d). The input and output of a sub-layer are of ddimensions. Besides, most\nof the hidden states of a sub-layer are d-dimensional vectors. In general, dcan be roughly\nviewed as the width of the network.\n\u2022Number of Heads (nhead). In self-attention sub-layers, one needs to specify the nu mber\nof heads used in multi-head self-attention. The larger this number is, the more sub-spaces\nattention is performed. In practical systems, we often set nhead\u22654.\n\u2022FFN Hidden Size (d\ufb00n). The size of the hidden layer of the FFNs used in Transformer s is\ntypically larger than d. For example, a typical setting is d\ufb00n= 4d. For larger Transformers,\nsuch as recent large models, d\ufb00nmay be set to a very large value.\n\n--- Page 33 ---\n\n26 Pre-training\nSelf-attention\nPosition Token Segment\nx0x1...xmLayer NormalizationFFNLayer NormalizationOutput Layer...\nh0h1...hm\nlayers\nInput\nxicorresponds to an entry of VEmbedding\ne=x+epos+eseg\u2208RdeSelf-attention Sub-layer\nhidden size: d\nnumber of heads: nheadFFN Sub-layer\nhidden size: d\nFFN hidden size: d\ufb00nEncoder Output\nhi\u2208Rdis the contextual\nrepresentation of xi\nFig. 1.6: The model Architecture of BERT (Transformer encoder). The i nput tokens are \ufb01rst represented as embed-\ndings, each of which is the sum of the corresponding token emb edding, positional embedding and segment embedding.\nThen, the embedding sequence is processed by a stack of Trans former layers. Each layer in this stack includes a self-\nattention sub-layer and a FFN sub-layer. The output of the BE RT model is a sequence of vectors produced by the \ufb01nal\nTransformer layer.\n\u2022Model Depth (L). Using deep networks is an effective way to improve the expr essive power\nof Transformers. For BERT models, Lis typically set to 12or24. However, networks with\neven greater depth are also feasible and can be applied for fu rther enhancements.\nDifferent settings of these hyper-parameters lead to diffe rent model sizes. There are two\nwidely-used BERT models.\n\u2022 BERT base:d= 768 ,L= 12 ,nhead= 12 , total number of parameters = 110 M.\n\u2022 BERT large:d= 1,024,L= 14 ,nhead= 16 , total number of parameters = 340 M.\nTraining BERT models follows the standard training process of Transformers. Training larger\nmodels such as BERT large requires more training effort and time. This is a common prob lem\nfor pre-training, especially when a model is trained on a ver y large amount of data. In practice,\n\n--- Page 34 ---\n\n1.3 Example: BERT 27\nthere are often considerations of training ef\ufb01ciency. For e xample, a practice is to \ufb01rst train a\nBERT model on relatively short sequences for a large number o f training steps, and then continue\ntraining it on full-length sequences for the remaining trai ning steps.\n1.3.2 More Training and Larger Models\nBERT is a milestone model in NLP, sparking many subsequent ef forts to improve it. One direction\nis to scale up the model itself, including increasing traini ng data and developing larger models.\nRoBERTa , an extension of the standard BERT model, is an example of suc h efforts [ Liu et al. ,\n2019 ]. It introduces two major improvements. First, simply usin g more training data and more\ncompute can improve BERT models without need of changing the model architectures. Second,\nremoving the NSP loss does not decrease the performance on do wnstream tasks if the training is\nscaled up. These \ufb01ndings suggest exploring a general direct ion of pre-training: we can continue\nto improve pre-training by",
    "50fe0eecc8412f45": "\ufffd\ufffdrst train a\nBERT model on relatively short sequences for a large number o f training steps, and then continue\ntraining it on full-length sequences for the remaining trai ning steps.\n1.3.2 More Training and Larger Models\nBERT is a milestone model in NLP, sparking many subsequent ef forts to improve it. One direction\nis to scale up the model itself, including increasing traini ng data and developing larger models.\nRoBERTa , an extension of the standard BERT model, is an example of suc h efforts [ Liu et al. ,\n2019 ]. It introduces two major improvements. First, simply usin g more training data and more\ncompute can improve BERT models without need of changing the model architectures. Second,\nremoving the NSP loss does not decrease the performance on do wnstream tasks if the training is\nscaled up. These \ufb01ndings suggest exploring a general direct ion of pre-training: we can continue\nto improve pre-training by scaling it up on simple pre-train ing tasks.\nA second approach to improving BERT models is to increase the number of model parame-\nters. For example, in He et al. [2021 ]\u2019s work, a 1.5 billion-parameter BERT-like model is built b y\nincreasing both the model depth and hidden size. However, sc aling up BERT and various other\npre-trained models introduces new challenges in training, for example, training very large models\noften becomes unstable and dif\ufb01cult to converge. This makes the problem more complicated, and\nrequires careful consideration of various aspects, includ ing model architecture, parallel computa-\ntion, parameter initialization, and so on. In another examp le,Shoeybi et al. [2019 ] successfully\ntrained a 3.9 billion-parameter BERT-like model, where hun dreds of GPUs were used to manage\nthe increased computational demands.\n1.3.3 More Ef\ufb01cient Models\nCompared to its predecessors, BERT is a relatively large mod el for the time it was proposed.\nThis increase in model size results in larger memory require ments and a consequent slowdown in\nsystem performance. Developing smaller and faster BERT mod els is part of the broader challenge\nof building ef\ufb01cient Transformers, which has been extensiv ely discussed in Tay et al. [2020 ]\u2019s\nwork and Xiao and Zhu [2023 ]\u2019s work. However, a deeper discussion of this general topic is\nbeyond the scope of our current discussion. Here we instead c onsider a few ef\ufb01cient variants of\nBERT.\nSeveral threads of research are of interest to NLP researche rs in developing ef\ufb01cient BERT\nmodels. First, work on knowledge distillation, such as trai ning student models with the output\nof well-trained teacher models, shows that smaller BERT mod els can be obtained by transferring\nknowledge from larger BERT models. Given that BERT models ar e multi-layer networks with\nseveral different types of layers, knowledge distillation can be applied at different levels of repre-\nsentation. For example, beyond distilling knowledge from t he output layers, it is also possible to\nincorporate training loss that measures the difference in o utput of hidden layers between teacher\nmodels and student models [ Sun et al. ,2020 ;Jiao et al. ,2020 ]. Indeed, knowledge distillation has\nbeen one of the most widely-used techniques for learning sma ll pre-trained models.\nSecond, conventional model compression methods can be dire ctly applied to compress BERT\nmodels. One common approach is to use general-purpose pruni ng methods to prune the Trans-\nformer encoding networks [ Gale et al. ,2019 ]. This generally involves removing entire layers\n\n--- Page 35 ---\n\n28 Pre-training\n[Fan et al. ,2019 ] or a certain percentage of parameters in the networks [ Sanh et al. ,2020 ;Chen et al. ,\n2020 ]. Pruning is also applicable to multi-head attention model s. For example, Michel et al.\n[2019 ] show that removing some of the heads does not signi\ufb01cantly d ecrease the performance\nof BERT models, but speeds up the inference of these models. A nother approach to compress-\ning BERT models is quantization [ Shen et al. ,2020 ]. By representing model parameters as low-\nprecision numbers, the models can be greatly compressed. Wh ile this method is not speci\ufb01c to\nBERT models, it proves effective for large Transformer-bas ed architectures.\nThird, considering that BERT models are relatively deep and large networks, another thread\nof research uses dynamic networks to adapt these models for e f\ufb01cient inference. An idea in this\nparad",
    "687b93711a140a6e": " al. ,2019 ] or a certain percentage of parameters in the networks [ Sanh et al. ,2020 ;Chen et al. ,\n2020 ]. Pruning is also applicable to multi-head attention model s. For example, Michel et al.\n[2019 ] show that removing some of the heads does not signi\ufb01cantly d ecrease the performance\nof BERT models, but speeds up the inference of these models. A nother approach to compress-\ning BERT models is quantization [ Shen et al. ,2020 ]. By representing model parameters as low-\nprecision numbers, the models can be greatly compressed. Wh ile this method is not speci\ufb01c to\nBERT models, it proves effective for large Transformer-bas ed architectures.\nThird, considering that BERT models are relatively deep and large networks, another thread\nof research uses dynamic networks to adapt these models for e f\ufb01cient inference. An idea in this\nparadigm is to dynamically choose the layers for processing a token, for example, in depth-\nadaptive models we exit at some optimal depth and thus skip th e rest of the layers in the layer\nstack [ Xin et al. ,2020 ;Zhou et al. ,2020 ]. Similarly, we can develop length-adaptive models in\nwhich the length of the input sequence is dynamically adjust ed. For example, we can skip some of\nthe tokens in the input sequence so that the model can reduce c omputational load on less important\ntokens, enhancing overall ef\ufb01ciency.\nFourth, it is also possible to share parameters across layer s to reduce the size of BERT models.\nA simple way to do this is to share the parameters of a whole Tra nsformer layer across the layer\nstack [ Dehghani et al. ,2018 ;Lan et al. ,2020 ]. In addition to the reduced number of parameters,\nthis enables reuse of the same layer in a multi-layer Transfo rmer network, leading to savings of\nmemory footprint at test time.\n1.3.4 Multi-lingual Models\nThe initial BERT model was primarily focused on English. Soo n after this model was proposed,\nit was extended to many languages. One simple way to do this is to develop a separate model\nfor each language. Another approach, which has become more p opular in recent work on large\nlanguage models, is to train multi-lingual models directly on data from all the languages. In\nresponse, multi-lingual BERT (mBERT ) models were developed by training them on text from\n104 languages6. The primary difference from monolingual BERT models is tha t mBERT models\nuse larger vocabularies to cover tokens from multiple langu ages. As a result, the representations\nof tokens from different languages are mapped into the same s pace, allowing for the sharing of\nknowledge across languages via this universal representat ion model.\nOne important application of multi-lingual pre-trained mo dels is cross-lingual learning. In the\ncross-lingual setting, we learn a model on tasks in one langu age, and apply it to the same tasks\nin another language. In cross-lingual text classi\ufb01cation, for example, we \ufb01ne-tune a multi-lingual\npre-trained model on English annotated documents. Then, we use the \ufb01ne-tuned model to classify\nChinese documents.\nAn improvement to multi-lingual pre-trained models like mB ERT is to introduce bilingual data\ninto pre-training. Rather than training solely on monoling ual data from multiple languages, bilin-\ngual training explicitly models the relationship between t okens in two languages. The resulting\nmodel will have innate cross-lingual transfer abilities, a nd thus can be easily adapted to different\nlanguages. Lample and Conneau [2019 ] propose an approach to pre-training cross-lingual lan-\nguage models (XLMs ). In their work, a cross-lingual language model can be train ed in either the\ncausal language modeling or masked language modeling manne r. For masked language modeling\n6https://github.com/google-research/bert/\n\n--- Page 36 ---\n\n1.3 Example: BERT 29\npre-training, the model is treated as an encoder. The traini ng objective is the same as BERT: we\nmaximize the probabilities of some randomly selected token s which are either masked, replaced\nwith random tokens, or kept unchanged in the input. If we cons ider bilingual data in pre-training,\nwe sample a pair of aligned sentences each time. Then, the two sentences are packed together to\nform a single sequence used for training. For example, consi der an English-Chinese sentence pair\n\u9cb8\u9c7c\u662f\ufffd",
    "286ed73423cf50ff": " [2019 ] propose an approach to pre-training cross-lingual lan-\nguage models (XLMs ). In their work, a cross-lingual language model can be train ed in either the\ncausal language modeling or masked language modeling manne r. For masked language modeling\n6https://github.com/google-research/bert/\n\n--- Page 36 ---\n\n1.3 Example: BERT 29\npre-training, the model is treated as an encoder. The traini ng objective is the same as BERT: we\nmaximize the probabilities of some randomly selected token s which are either masked, replaced\nwith random tokens, or kept unchanged in the input. If we cons ider bilingual data in pre-training,\nwe sample a pair of aligned sentences each time. Then, the two sentences are packed together to\nform a single sequence used for training. For example, consi der an English-Chinese sentence pair\n\u9cb8\u9c7c\u662f\u54fa\u4e73\u52a8\u7269\u3002\u2194Whales are mammals .\nWe can pack them to obtain a sequence, like this\n[CLS]\u9cb8\u9c7c\u662f\u54fa\u4e73\u52a8\u7269\u3002[SEP] Whales are mammals . [SEP]\nWe then select a certain percentage of the tokens and replace them with [MASK] .\n[CLS] [MASK]\u662f[MASK]\u52a8\u7269\u3002[SEP] Whales [MASK] [MASK] .[SEP]\nThe goal of pre-training is to maximize the product of the pro babilities of the masked tokens given\nthe above sequence. By performing training in this way, the m odel can learn to represent both the\nEnglish and Chinese sequences, as well as to capture the corr espondences between tokens in the\ntwo languages. For example, predicting the Chinese token \u9cb8\u9c7c may require the information\nfrom the English token Whales . Aligning the representations of the two languages essenti ally\ntransforms the model into a \u201ctranslation\u201d model. So this tra ining objective is also called transla-\ntion language modeling . Figure 1.7shows an illustration of this approach.\nA bene\ufb01t of multi-lingual pre-trained models is their inher ent capability of handling code-\nswitching. In NLP and linguistics, code-switching refers t o switching among languages in a text.\nFor example, the following is a mixed language text containi ng both Chinese and English:\n\u5468\u672b\u6211\u4eec\u6253\u7b97\u53bb\u505ahiking\uff0c\u4f60\u60f3\u4e00\u8d77\u6765\u5417\uff1f\n(We plan to go hiking this weekend, would you like to join us?)\nFor multi-lingual pre-trained models, we do not need to iden tify whether a token is Chinese or\nEnglish. Instead, every token is just an entry of the shared v ocabulary. This can be imagined as\ncreating a \u201cnew\u201d language that encompasses all the language s we want to process.\nThe result of multi-lingual pre-training is in\ufb02uenced by se veral factors. Given that the model\narchitecture is \ufb01xed, one needs to specify the size of the sha red vocabulary, the number (or per-\ncentage) of samples in each language, the size of the model, a nd so on. Conneau et al. [2020 ]\npoint out several interesting issues regarding large-scal e multi-lingual pre-training for XLM-like\nmodels. First, as the number of supported languages increas es, a larger model is needed to handle\nthese languages. Second, a larger shared vocabulary is help ful for modeling the increased diver-\nsity in languages. Third, low-resource languages more easi ly bene\ufb01t from cross-lingual transfer\nfrom high-resource languages, particularly when similar h igh-resource languages are involved in\npre-training. However, interference may occur if the model is trained for an extended period,\n\n--- Page 37 ---\n\n30 Pre-training\n[CLS] [MASK]\u662f[MASK]\u52a8\u7269\u3002 [SEP] Whales [MASK] [MASK] .[SEP]\n(zh) (zh) (zh) (zh) (zh) (zh) (zh) (en) (en) (en) (en) (en)e0 e1 e2 e3 e4 e5 e6 e7 e8 e9 e10 e11h0 h1 h2 h3 h4 h5 h6 h7 h8 h9 h10 h11\u9cb8\u9c7c\u54fa\u4e73 are mammals\nTransformer Encoder\nFig. 1.7: An illustration of translation language modeling. For ease of understanding, we present a simple example\nwhere all the selected tokens are masked. The model is traine d to predict these masked tokens. As the sequence\ncontains tokens in two languages, predicting a token in",
    "aa3666c8db7b6bb4": " an extended period,\n\n--- Page 37 ---\n\n30 Pre-training\n[CLS] [MASK]\u662f[MASK]\u52a8\u7269\u3002 [SEP] Whales [MASK] [MASK] .[SEP]\n(zh) (zh) (zh) (zh) (zh) (zh) (zh) (en) (en) (en) (en) (en)e0 e1 e2 e3 e4 e5 e6 e7 e8 e9 e10 e11h0 h1 h2 h3 h4 h5 h6 h7 h8 h9 h10 h11\u9cb8\u9c7c\u54fa\u4e73 are mammals\nTransformer Encoder\nFig. 1.7: An illustration of translation language modeling. For ease of understanding, we present a simple example\nwhere all the selected tokens are masked. The model is traine d to predict these masked tokens. As the sequence\ncontains tokens in two languages, predicting a token in one l anguage allows access to tokens in the other language,\nthereby enabling cross-lingual modeling. In Lample and Conneau [2019 ]\u2019s work, an input embedding (i.e., ei) is the\nsum of the token embedding, positional embedding, and langu age embedding. This requires that each token is assigned\nwith a language label. Thus we can distinguish tokens in diff erent languages. In multi-lingual pre-training, particul arly\nin work using shared vocabularies, specifying the language to which a token belongs is not necessary. The use of\nlanguage embeddings in turn makes it dif\ufb01cult to handle code -switching. Therefore, we assume here that all token\nrepresentations are language-independent.\nmeaning the overall performance of the pre-trained model st arts decreasing at a certain point dur-\ning pre-training. Thus, in practical systems, one may need t o stop the pre-training early to prevent\ninterference.\n1.4 Applying BERT Models\nOnce a BERT model is pre-trained, it can then be used to solve N LP problems. But BERT models\nare not immediately ready for performing speci\ufb01c downstrea m tasks. In general, additional \ufb01ne-\ntuning work is required to make them adapt. As a \ufb01rst step, we n eed a predictor to align the\noutput of the model with the problem of interest. Let BERT \u02c6\u03b8(\u00b7)be a BERT model with pre-\ntrained parameters \u02c6\u03b8, and Predict \u03c9(\u00b7)be a prediction network with parameters \u03c9. By integrating\nthe prediction network with the output of the BERT model, we d evelop a model to tackle the\ndownstream tasks. This model can be expressed as\ny= Predict \u03c9(BERT \u02c6\u03b8(x)) (1.21)\nwhere xis the input and yis the output that \ufb01ts the problem. For example, in classi\ufb01ca tion\nproblems, the model outputs a probability distribution ove r labels.\nThen, we collect a set of labeled samples D, and \ufb01ne-tune the model by\n(\u02dc\u03c9,\u02dc\u03b8) = arg min\n\u03c9,\u02c6\u03b8+\u2211\n(x,ygold)\u2208DLoss( y\u03c9,\u02c6\u03b8+,ygold) (1.22)\n\n--- Page 38 ---\n\n1.4 Applying BERT Models 31\nwhere (x,ygold)represents a tuple of an input and its corresponding output. The notation of this\nequation seems a bit complicated, but the training/tuning p rocess is standard. We optimize the\nmodel by minimizing the loss over the tuning samples. The out come is the optimized parameters\n\u02dc\u03c9and\u02dc\u03b8. The optimization starts with the pre-trained parameters \u02c6\u03b8. Here we use \u02c6\u03b8+to indicate that\nthe parameters are initialized with \u02c6\u03b8, and use y\u03c9,\u02c6\u03b8+to denote the model output computed using\nthe parameters \u03c9and\u02c6\u03b8+.\nWith the \ufb01ne-tuned parameters \u02dc\u03c9and\u02dc\u03b8, we can apply the model Predict \u02dc\u03c9(BERT \u02dc\u03b8(\u00b7))to new\ndata of the same tasks for which the model was \ufb01ne-tuned. The f orm of the downstream tasks\ndetermines the input and output formats of the model, as well as the architecture of the prediction\nnetwork. In the following we list some tasks to which BERT mod els are generally suited.\n\u2022Classi\ufb01cation (Single Text). One of the most widely-used applications of B ERT models is\ntext classi\ufb01cation. In this task, a BERT model receives a seq uence of tokens and encodes\nit",
    "0cf2dfdd4ce3a2aa": " \u02c6\u03b8+to indicate that\nthe parameters are initialized with \u02c6\u03b8, and use y\u03c9,\u02c6\u03b8+to denote the model output computed using\nthe parameters \u03c9and\u02c6\u03b8+.\nWith the \ufb01ne-tuned parameters \u02dc\u03c9and\u02dc\u03b8, we can apply the model Predict \u02dc\u03c9(BERT \u02dc\u03b8(\u00b7))to new\ndata of the same tasks for which the model was \ufb01ne-tuned. The f orm of the downstream tasks\ndetermines the input and output formats of the model, as well as the architecture of the prediction\nnetwork. In the following we list some tasks to which BERT mod els are generally suited.\n\u2022Classi\ufb01cation (Single Text). One of the most widely-used applications of B ERT models is\ntext classi\ufb01cation. In this task, a BERT model receives a seq uence of tokens and encodes\nit as a sequence of vectors. The \ufb01rst output vector hcls(orh0) is typically used as the\nrepresentation of the entire text. The prediction network t akes hclsas input to produce a\ndistribution of labels. Let [CLS]x1x2...xmbe an input text. See below for an illustration of\nBERT-based text classi\ufb01cation.\n[CLS]x1x2...xm[SEP]ecls e1 e2... emem+1hcls h1 h2... hmhm+1Class\nBERT\nHere the gray box denotes the prediction network. Many NLP pr oblems can be categorized\nas text classi\ufb01cation tasks, and there have been several tex t classi\ufb01cation benchmarks for\nevaluating pre-trained models. For example, we can classif y texts by their grammatical cor-\nrectness (grammaticality) or emotional tone (sentiment) [ Socher et al. ,2013 ;Warstadt et al. ,\n2019 ]. Note that the prediction network could be any classi\ufb01cati on model, such as a deep\nneural network or a more traditional classi\ufb01cation model. T he entire model can then be\ntrained or \ufb01ne-tuned in the manner of a standard classi\ufb01cati on model. For example, the pre-\ndiction network can be simply a Softmax layer and the model pa rameters can be optimized\nby maximizing the probabilities of the correct labels.\n\u2022Classi\ufb01cation (Pair of Texts). Classi\ufb01cation can also be performed on a pai r of texts. Sup-\npose we have two texts, x1...xmandy1...yn. We can concatenate these texts to form a single\nsequence with a length len. Then, we predict a label for this combined text sequence bas ed\non the hclsvector, as follows\n\n--- Page 39 ---\n\n32 Pre-training\n[CLS]x1x2...xm[SEP]y1y2...yn[SEP]\nText 1 Text 2ecls e1 e2... emem+1em+2em+3... elen\u22121elenhcls h1 h2... hmhm+1hm+2hm+3... hlen\u22121hlenClass\nBERT\nwherelen=n+m+ 2. Text pair classi\ufb01cation covers several problems, includi ng se-\nmantic equivalence judgement (determine whether two texts are semantically equivalent)\n[Dolan and Brockett ,2005 ], text entailment judgement (determine whether a hypothes is\ncan be logically inferred or entailed from a premise) [ Bentivogli and Giampiccolo ,2011 ;\nWilliams et al. ,2018 ], grounded commonsense inference (determine whether an ev ent is\nlikely to happen given its context) [ Zellers et al. ,2018 ], and question-answering inference\n(determine whether an answer corresponds to a given questio n).\n\u2022Regression . Instead of generating a label distribution, we can have the prediction network\noutput a real-valued score. For example, by adding a Sigmoid layer to the prediction net-\nwork, the system can be employed to compute the similarity be tween two given sentences.\nThe architecture is the same as that of BERT-based classi\ufb01ca tion systems, with only the\nchange of the output layer.\n[CLS]x1x2...xm[SEP]y1y2...yn[SEP]\nText 1 Text 2ecls e1 e2... emem+1em+2em+3... elen\u22121elenhcls h1 h2... hmhm+1hm+2hm+3... hlen\u22121hlenNumber (similarity, evaluation score, etc.)\n",
    "97b857bc836fd445": " context) [ Zellers et al. ,2018 ], and question-answering inference\n(determine whether an answer corresponds to a given questio n).\n\u2022Regression . Instead of generating a label distribution, we can have the prediction network\noutput a real-valued score. For example, by adding a Sigmoid layer to the prediction net-\nwork, the system can be employed to compute the similarity be tween two given sentences.\nThe architecture is the same as that of BERT-based classi\ufb01ca tion systems, with only the\nchange of the output layer.\n[CLS]x1x2...xm[SEP]y1y2...yn[SEP]\nText 1 Text 2ecls e1 e2... emem+1em+2em+3... elen\u22121elenhcls h1 h2... hmhm+1hm+2hm+3... hlen\u22121hlenNumber (similarity, evaluation score, etc.)\nBERT\nFor training or \ufb01ne-tuning, we can minimize the regression l oss of the model output as\nusual.\n\u2022Sequence Labeling . Sequence labeling is a machine learning approach applicab le to a wide\nrange of NLP problems. This approach assigns a label to each t oken in an input sequence,\nand some linguistic annotations can then be derived from thi s sequence of labels. An ex-\nample of sequence labeling in NLP is part-of-speech (POS) ta gging. We label each word\nin a sentence with its corresponding POS tag. Another exampl e is named entity recognition\n(NER) in which we label each word with an NER tag, and named ent ities are identi\ufb01ed\nusing these tags. See below for an illustration of the model a rchitecture for NER.\n\n--- Page 40 ---\n\n1.4 Applying BERT Models 33\n[CLS]x1x2...xm[SEP]ecls e1 e2... emem+1hcls h1 h2... hmhm+1{B,I,O}{B,I,O} { B,I,O}Tag Tag Tag\nBERT\nHere {B,I,O}is the tag set of NER. For example, B-ORG means the beginning of an\norganization, I-ORG means the word is inside an organization, and Omeans the word does\nnot belong to any named entity. This NER model can output a dis tribution over the tag set\nat each position, denoted as pi. The training or \ufb01ne-tuning of the model can be performed\nover these distributions {p1,...,pm}. For example, suppose pi(tagi)is the probability of\nthe correct tag at position i. The training loss can be de\ufb01ned to be the negative likelihoo d\nLoss = \u22121\nmm\u2211\ni=1logpi(tagi) (1.23)\nFinding the best label sequence given a trained NER model is a well-studied issue in NLP.\nThis is often achieved via dynamic programming, which, in th e context of path \ufb01nding over\na lattice, has linear complexity [ Huang ,2009 ].\n\u2022Span Prediction . Some NLP tasks require predicting a span in a text. A common e xample\nis reading comprehension. In this task, we are given a query x1...xmand a context text\ny1...yn. The goal is to identify a continuous span in y1...ynthat best answers the query.\nThis problem can be framed as a sequence labeling-like task i n which we predict a label for\neachyjto indicate the beginning or ending of the span. Following Seo et al. [2017 ], we add\ntwo networks on top of the BERT output for yj: one for generating the probability of yj\nbeing the beginning of the span (denoted by pbeg\nj), and one for generating the probability\nofyjbeing the ending of the span (denoted by pend\nj). The resulting model architecture is\nshown as follows\n[CLS]x1x2...xm[SEP]y1y2...yn[SEP]\nQuery Context Textecls e1 e2... emem+1em+2em+3... elen\u22121elenhcls h1 h2... hmhm+1hm+2hm+3... hlen\u22121hlen(pbeg\n1)Beg(pend\n1)End\n(pbeg\n2)Beg(pend\n2)End\n(pbeg\nn)Beg(pend\nn)End\nBERT\n\n--- Page 41 ---\n\n34 Pre-training\nWe pack the query and context text together to obtain the inpu t sequence. The prediction\nnetworks are only applied to outputs for the",
    "2c6caf6389fb32e0": " the beginning of the span (denoted by pbeg\nj), and one for generating the probability\nofyjbeing the ending of the span (denoted by pend\nj). The resulting model architecture is\nshown as follows\n[CLS]x1x2...xm[SEP]y1y2...yn[SEP]\nQuery Context Textecls e1 e2... emem+1em+2em+3... elen\u22121elenhcls h1 h2... hmhm+1hm+2hm+3... hlen\u22121hlen(pbeg\n1)Beg(pend\n1)End\n(pbeg\n2)Beg(pend\n2)End\n(pbeg\nn)Beg(pend\nn)End\nBERT\n\n--- Page 41 ---\n\n34 Pre-training\nWe pack the query and context text together to obtain the inpu t sequence. The prediction\nnetworks are only applied to outputs for the context text, ge nerating the probabilities pbeg\nj\nandpend\njat each position. The loss can be computed by summing the log l ikelihoods of the\ntwo models across the entire context text.\nLoss = \u22121\nnn\u2211\nj=1(logpbeg\nj+ logpend\nj)(1.24)\nAt test time, we search for the best span by\n(\u02c6j1,\u02c6j2) = arg max\n1\u2264j1\u2264j2\u2264n(logpbeg\nj1+ logpend\nj2)(1.25)\n\u2022Encoding for Encoder-decoder Models . While our focus in this section has been primarily\non language understanding problems, it is worth noting that BERT models can be applied\nto a broader range of NLP tasks. In fact, BERT models can be use d in all the scenarios\nwhere we need to encode a piece of text. One application that w e have not mentioned is\ntext generation which includes a range of tasks such as machi ne translation, summarization,\nquestion answering, and dialogue generation. These tasks c an be formulated as sequence-\nto-sequence problems: we use an encoder to represent the sou rce text, and a decoder to\ngenerate the corresponding target text. A straightforward method to apply BERT models\nis to consider them as encoders. Before \ufb01ne-tuning, we can in itialize the parameters of the\nencoder with those from a pre-trained BERT model. Then, the e ncoder-decoder model can\nbe \ufb01ne-tuned on pairs of texts as usual. The following shows t he architecture of a neural\nmachine translation system where a BERT model is applied on t he source side.\n[CLS]x1...xm[SEP]\nSource Textex\ncls ex\n1... ex\nmex\nm+1BERT (Encoder)Adapter\n\u27e8s\u27e9y1y2...yn\u22121ey\n0 ey\n1 ey\n2... ey\nn\u22121Decodery1y2y3...ynTarget Text\nHerex1...xmdenotes the source sequence, y1...yndenotes the target sequence, ex\n1...ex\nm\ndenotes the embedding sequence of x1...xm, and ey\n1...ey\nndenotes the embedding sequence\nofy1...yn. The adapter, which is optional, maps the output of the BERT m odel to the form\nthat is better suited to the decoder.\nFine-tuning BERT models is a complicated engineering probl em, in\ufb02uenced by many factors,\nsuch as the amount of \ufb01ne-tuning data, the model size, and the optimizer used in \ufb01ne-tuning.\nIn general, we wish to \ufb01ne-tune these models suf\ufb01ciently so t hat they can perform well in the\ndownstream tasks. However, \ufb01ne-tuning BERT models for spec i\ufb01c tasks may lead to over\ufb01tting,\n\n--- Page 42 ---\n\n1.5 Summary 35\nwhich in turn reduces their ability to generalize to other ta sks. For example, suppose we have a\nBERT model that performs well on a particular task. If we then \ufb01ne-tune it for new tasks, this\nmay decrease its performance on the original task. This prob lem is related to the catastrophic\nforgetting problem in continual training, where a neural network forge ts previously learned in-\nformation when updated on new samples. In practical applica tions, a common way to alleviate\ncatastrophic forgetting is to add some old data into \ufb01ne-tun ing and train the model with more\ndiverse data. Also, one may use methods specialized to catas trophic forgetting, such as",
    "eb7beb24aa451020": " hat they can perform well in the\ndownstream tasks. However, \ufb01ne-tuning BERT models for spec i\ufb01c tasks may lead to over\ufb01tting,\n\n--- Page 42 ---\n\n1.5 Summary 35\nwhich in turn reduces their ability to generalize to other ta sks. For example, suppose we have a\nBERT model that performs well on a particular task. If we then \ufb01ne-tune it for new tasks, this\nmay decrease its performance on the original task. This prob lem is related to the catastrophic\nforgetting problem in continual training, where a neural network forge ts previously learned in-\nformation when updated on new samples. In practical applica tions, a common way to alleviate\ncatastrophic forgetting is to add some old data into \ufb01ne-tun ing and train the model with more\ndiverse data. Also, one may use methods specialized to catas trophic forgetting, such as experi-\nence replay [ Rolnick et al. ,2019 ] and elastic weight consolidation [ Kirkpatrick et al. ,2017 ]. The\ninterested reader can refer to some surveys for more detaile d discussions of this issue in continual\nlearning [ Parisi et al. ,2019 ;Wang et al. ,2023a ;e].\n1.5 Summary\nIn this chapter we have discussed the general idea of pre-tra ining in NLP. In particular, we have dis-\ncussed self-supervised pre-training and its application t o encode-only, decoder-only, and encoder-\ndecoder architectures. Moreover, we have presented and com pared a variety of pre-training tasks\nfor these architectures. As an example, BERT is used to illus trate how sequence models are pre-\ntrained via masked language modeling and applied to differe nt downstream tasks.\nRecent years have shown remarkable progress in NLP, led by th e large-scale use of self-\nsupervised pre-training. And sweeping advances are being m ade across many tasks, not only\nin NLP but also in computer vision and other areas of AI. One id ea behind these advances is that a\nsigni\ufb01cant amount of knowledge about the world can be learne d by simply training these AI sys-\ntems on huge amounts of unlabeled data. For example, a langua ge model can learn some general\nknowledge of a language by repeatedly predicting masked wor ds in large-scale text. As a result,\nthis pre-trained language model can serve as a foundation mo del, which can be easily adapted to\naddress speci\ufb01c downstream NLP tasks. This paradigm shift i n NLP has enabled the development\nof incredibly powerful systems for language understanding , generation, and reasoning [ Manning ,\n2022 ]. However, it is important to recognize that we are still in t he early stages of creating truly in-\ntelligent systems, and there is a long way to go. Nevertheles s, large-scale pre-training has opened\na door to intelligent systems that researchers have long asp ired to develop, though several key re-\nsearch areas remain open for exploration, such as learning i ntelligence ef\ufb01ciently using reasonably\nsmall-sized data and acquiring complex reasoning and plann ing abilities.\nNote that this chapter is mostly introductory and cannot cov er all aspects of pre-training. For\nexample, there are many methods to \ufb01ne-tune a pre-trained mo del, offering different ways to better\nadapt the model to diverse situations. Moreover, large lang uage models, which are considered one\nof the most signi\ufb01cant achievements in AI in recent years, ar e skipped in this section. We leave\nthe discussion of these topics to the following chapters.\n\n--- Page 43 ---\n\nCHAPTER 2\nGenerative Models\nOne of the most signi\ufb01cant advances in NLP in recent years mig ht be the development of large\nlanguage models (LLMs). This has helped create systems that can understand and generate nat-\nural languages like humans. These systems have even been fou nd to be able to reason, which\nis considered a very challenging AI problem. With these achi evements, NLP made big strides\nand entered a new era of research in which dif\ufb01cult problems a re being solved, such as building\nconversational systems that can communicate with humans sm oothly.\nThe concept of language modeling or probabilistic language modeling dates back to early ex-\nperiments conducted by Shannon [1951 ]. In his work, a language model was designed to estimate\nthe predictability of English \u2014 how well can the next letter of a text be predicted when the pre -\ncedingNletters are known . Although Shannon\u2019s experiments were preliminary, the fun damental\ngoals and methods of language modeling have remained largel y unchanged over the decades since\nthen. For quite a long period",
    "86ebf89873b14c45": " development of large\nlanguage models (LLMs). This has helped create systems that can understand and generate nat-\nural languages like humans. These systems have even been fou nd to be able to reason, which\nis considered a very challenging AI problem. With these achi evements, NLP made big strides\nand entered a new era of research in which dif\ufb01cult problems a re being solved, such as building\nconversational systems that can communicate with humans sm oothly.\nThe concept of language modeling or probabilistic language modeling dates back to early ex-\nperiments conducted by Shannon [1951 ]. In his work, a language model was designed to estimate\nthe predictability of English \u2014 how well can the next letter of a text be predicted when the pre -\ncedingNletters are known . Although Shannon\u2019s experiments were preliminary, the fun damental\ngoals and methods of language modeling have remained largel y unchanged over the decades since\nthen. For quite a long period, particularly before 2010, the dominant approach to language mod-\neling was the n-gram approach [ Jurafsky and Martin ,2008 ]. Inn-gram language modeling, we\nestimate the probability of a word given its preceding n\u22121words, and thus the probability of a\nsequence can be approximated by the product of a series of n-gram probabilities. These proba-\nbilities are typically estimated by collecting smoothed re lative counts of n-grams in text. While\nsuch an approach is straightforward and simple, it has been e xtensively used in NLP. For example,\nthe success of modern statistical speech recognition and ma chine translation systems has largely\ndepended on the utilization of n-gram language models [ Jelinek ,1998 ;Koehn ,2010 ].\nApplying neural networks to language modeling has long been attractive, but a real break-\nthrough appeared as deep learning techniques advanced. A wi dely cited study is Bengio et al.\n[2003 ]\u2019s work where n-gram probabilities are modeled via a feed-forward network and learned\nby training the network in an end-to-end fashion. A by-produ ct of this neural language model\nis the distributed representations of words, known as word e mbeddings. Rather than represent-\ning words as discrete variables, word embeddings map words i nto low-dimensional real-valued\nvectors, making it possible to compute the meanings of words and wordn-grams in a continu-\nous representation space. As a result, language models are n o longer burdened with the curse of\ndimensionality, but can represent exponentially many n-grams via a compact and dense neural\nmodel.\nThe idea of learning word representations through neural la nguage models inspired subsequent\nresearch in representation learning in NLP. However, this a pproach did not attract signi\ufb01cant in-\nterest in developing NLP systems in the \ufb01rst few years after i ts proposal. Starting in about 2012,\nthough, advances were made in learning word embeddings from large-scale text via simple word\nprediction tasks. Several methods, such as Word2Vec, were p roposed to effectively learn such\nembeddings, which were then successfully applied in a varie ty of NLP systems [ Mikolov et al. ,\n2013a ;b]. As a result of these advances, researchers began to think o f learning representations of\nsequences using more powerful language models, such as LSTM -based models [ Sutskever et al. ,\n2014 ;Peters et al. ,2018 ]. And further progress and interest in sequence representa tion exploded\nafter Transformer was proposed. Alongside the rise of Trans former, the concept of language mod-\neling was generalized to encompass models that learn to pred ict words in various ways. Many\n36\n\n--- Page 44 ---\n\n2.1 A Brief Introduction to LLMs 37\npowerful Transformer-based models were pre-trained using these word prediction tasks, and suc-\ncessfully applied to a variety of downstream tasks [ Devlin et al. ,2019 ].\nIndeed, training language models on large-scale data has le d NLP research to exciting times.\nWhile language modeling has long been seen as a foundational technique with no direct link to\nthe goals of arti\ufb01cial intelligence that researchers had ho ped for, it helps us see the emergence of\nintelligent systems that can learn a certain degree of gener al knowledge from repeatedly predicting\nwords in text. Recent research demonstrates that a single, w ell-trained LLM can handle a large\nnumber of tasks and generalize to perform new tasks with a sma ll adaptation effort [ Bubeck et al. ,\n2023 ]. This suggests a step towards more advanced forms of arti\ufb01c ial intelligence, and inspires\nfurther exploration into developing more powerful languag e models as foundation",
    "72f80a0d0fe781a8": "s 37\npowerful Transformer-based models were pre-trained using these word prediction tasks, and suc-\ncessfully applied to a variety of downstream tasks [ Devlin et al. ,2019 ].\nIndeed, training language models on large-scale data has le d NLP research to exciting times.\nWhile language modeling has long been seen as a foundational technique with no direct link to\nthe goals of arti\ufb01cial intelligence that researchers had ho ped for, it helps us see the emergence of\nintelligent systems that can learn a certain degree of gener al knowledge from repeatedly predicting\nwords in text. Recent research demonstrates that a single, w ell-trained LLM can handle a large\nnumber of tasks and generalize to perform new tasks with a sma ll adaptation effort [ Bubeck et al. ,\n2023 ]. This suggests a step towards more advanced forms of arti\ufb01c ial intelligence, and inspires\nfurther exploration into developing more powerful languag e models as foundation models.\nIn this chapter, we consider the basic concepts of generativ e LLMs. For simplicity, we use the\nterms large language models orLLMs to refer to generative models like GPT, though this term\ncan broadly cover other types of models like BERT. We begin by giving a general introduction\nto LLMs, including the key steps of building such models. We t hen discuss two scaling issues of\nLLMs: how LLMs are trained at scale, and how LLMs can be improv ed to handle very long texts.\nFinally, we give a summary of these discussions.\n2.1 A Brief Introduction to LLMs\nIn this section we give an introduction to the basic ideas of L LMs as required for the rest of this\nchapter and the following chapters. We will use terms word andtoken interchangeably. Both\nof them refer to the basic units used in language modeling, th ough their original meanings are\ndifferent.\nBefore presenting details, let us \ufb01rst consider how languag e models work. The goal of lan-\nguage modeling is to predict the probability of a sequence of tokens occurring. Let {x0,x1,...,x m}\nbe a sequence of tokens, where x0is the start symbol \u27e8s\u27e9(or\u27e8SOS\u27e9)1. The probability of this se-\nquence can be de\ufb01ned using the chain rule\nPr(x0,...,x m) = Pr(x0)\u00b7Pr(x1|x0)\u00b7Pr(x2|x0,x1)\u00b7 \u00b7 \u00b7Pr(xm|x0,...,x m\u22121)\n=m\u220f\ni=0Pr(xi|x0,...,x i\u22121) (2.1)\nor alternatively in a logarithmic form\nlog Pr(x0,...,x m) =m\u2211\ni=0log Pr(xi|x0,...,x i\u22121) (2.2)\nHere Pr(xi|x0,...,x i\u22121)is the probability of the token xigiven all its previous tokens {x0,...,x i\u22121}\n2. In the era of deep learning, a typical approach to language m odeling is to estimate this\n1The start symbol can also be [CLS] following BERT models.\n2We assume that when i= 0 ,Pr(xi|x0,...,x i\u22121) = Pr(x0) = 1 . Hence Pr(x0,...,x m) =\nPr(x0) Pr(x1,...,x m|x0) = Pr(x1,...,x m|x0).\n\n--- Page 45 ---\n\n38 Generative Models\nContext Predict Decision Rule Sequence Probability\n\u27e8s\u27e9ab arg maxx2\u2208VPr(x2|\u27e8s\u27e9a) Pr(\u27e8s\u27e9)\u00b7Pr(a|\u27e8s\u27e9)\u00b7Pr(b|\u27e8s\u27e9a)\n\u27e8s\u27e9a bc arg maxx3\u2208VPr(x3|\u27e8s\u27e9ab) Pr(\u27e8s\u27e9)\u00b7Pr(a|\u27e8s\u27e9)\u00b7Pr(b|\u27e8s\u27e9a)\u00b7\nPr(c|\u27e8s\u27e9ab)\n\u27e8s\u27e9a b cd arg maxx4\u2208VPr(x4|\u27e8s\u27e9abc)Pr(\u27e8s\u27e9)\u00b7Pr(a|\u27e8s\u27e9)\u00b7Pr(b|\u27e8s\u27e9a)\u00b7\nPr(c|\u27e8s\u27e9ab)\u00b7Pr(d|\u27e8s\u27e9abc)\nTable 2.1: Illustration of generating the three tokens b c d given the pre\ufb01x \u27e8s\u27e9avia a language model. In each step,\nthe model picks a token xifromVso that Pr(xi|x0,...,x i\u22121)is maximized. This token is then appended to the",
    "808ec4ed6117434d": "s\u27e9a)\n\u27e8s\u27e9a bc arg maxx3\u2208VPr(x3|\u27e8s\u27e9ab) Pr(\u27e8s\u27e9)\u00b7Pr(a|\u27e8s\u27e9)\u00b7Pr(b|\u27e8s\u27e9a)\u00b7\nPr(c|\u27e8s\u27e9ab)\n\u27e8s\u27e9a b cd arg maxx4\u2208VPr(x4|\u27e8s\u27e9abc)Pr(\u27e8s\u27e9)\u00b7Pr(a|\u27e8s\u27e9)\u00b7Pr(b|\u27e8s\u27e9a)\u00b7\nPr(c|\u27e8s\u27e9ab)\u00b7Pr(d|\u27e8s\u27e9abc)\nTable 2.1: Illustration of generating the three tokens b c d given the pre\ufb01x \u27e8s\u27e9avia a language model. In each step,\nthe model picks a token xifromVso that Pr(xi|x0,...,x i\u22121)is maximized. This token is then appended to the end\nof the context sequence. In the next step, we repeat the same p rocess, but based on the new context.\nprobability using a deep neural network. Neural networks tr ained to accomplish this task re-\nceive a sequence of tokens x0,...,x i\u22121and produce a distribution over the vocabulary V(de-\nnoted by Pr(\u00b7|x0,...,x i\u22121)). The probability Pr(xi|x0,...,x i\u22121)is the value of the i-th entry of\nPr(\u00b7|x0,...,x i\u22121).\nWhen applying a trained language model, a common task is to \ufb01n d the most likely token given\nits previous context tokens. This token prediction task can be described as\n\u02c6xi= arg max\nxi\u2208VPr(xi|x0,...,x i\u22121) (2.3)\nWe can perform word prediction multiple times to generate a c ontinuous text: each time we\npredict the best token \u02c6xi, and then add this predicted token to the context for predict ing the next\ntoken \u02c6xi+1. This results in a left-to-right generation process implem enting Eqs. ( 2.1) and ( 2.2). To\nillustrate, consider the generation of the following three words given the pre\ufb01x \u2018 \u27e8s\u27e9a\u2019, as shown\nin Table 2.1. Now we discuss how LLMs are constructed, trained, and appli ed.\n2.1.1 Decoder-only Transformers\nAs is standard practice, the input of a language model is a seq uence of tokens (denoted by\n{x0,...,x m\u22121}). For each step, an output token is generated, shifting the s equence one po-\nsition forward for the next prediction. To do this, the langu age model outputs a distribution\nPr(\u00b7|x0,...,x i\u22121)at each position i, and the token xiis selected according to this distribution.\nThis model is trained by maximizing the log likelihood\u2211m\ni=1log Pr(xi|x0,...,x i\u22121)3.\nHere, we focus on the decoder-only Transformer architectur e, as it is one of the most popular\nmodel architectures used in LLMs. The input sequence of toke ns is represented by a sequence\nofde-dimensional vectors {e0,...,em\u22121}.eiis the sum of the token embedding of xiand the\npositional embedding of i. The major body of the model is a stack of Transformer blocks ( or\nlayers). Each Transformer block has two stacked sub-layers , one for self-attention modeling and\none for FFN modeling. These sub-layers can be de\ufb01ned using th e post-norm architecture\noutput = LNorm( F(input) + input) (2.4)\n3Note that\u2211m\ni=1log Pr(xi|x0,...,x i\u22121) =\u2211m\ni=0log Pr(xi|x0,...,x i\u22121)since log Pr(x0) = 0 .\n\n--- Page 46 ---\n\n2.1 A Brief Introduction to LLMs 39\nor the pre-norm architecture\noutput = LNorm( F(input)) + input (2.5)\nwhere input andoutput denote the input and output, both being an m\u00d7dmatrix. The i-th rows\nofinput andoutput can be seen as contextual representations of the i-th token in the sequence.\nF(\u00b7)is the core function of a sub-layer. For FFN sub-layers, F(\u00b7)is a multi-layer FFN. For\nself-attention sub-layers, F(\u00b7)is a multi-head self-attention function. In general, self- attention is",
    "99df9d134f8a74ab": " (2.4)\n3Note that\u2211m\ni=1log Pr(xi|x0,...,x i\u22121) =\u2211m\ni=0log Pr(xi|x0,...,x i\u22121)since log Pr(x0) = 0 .\n\n--- Page 46 ---\n\n2.1 A Brief Introduction to LLMs 39\nor the pre-norm architecture\noutput = LNorm( F(input)) + input (2.5)\nwhere input andoutput denote the input and output, both being an m\u00d7dmatrix. The i-th rows\nofinput andoutput can be seen as contextual representations of the i-th token in the sequence.\nF(\u00b7)is the core function of a sub-layer. For FFN sub-layers, F(\u00b7)is a multi-layer FFN. For\nself-attention sub-layers, F(\u00b7)is a multi-head self-attention function. In general, self- attention is\nexpressed in a form of QKV attention\nAttqkv(Q,K,V) = Softmax(QKT\n\u221a\nd+Mask )V (2.6)\nwhere Q,KandV\u2208Rm\u00d7dare the queries, keys, and values, respectively. It is impor tant to\nnote that only previous tokens are considered when predicti ng a token. So a masking variable\nMask \u2208Rm\u00d7mis incorporated into self-attention to achieve this. The en try(i,k)ofMask has\na value of 0 if i\u2264k, and a value of \u2212infotherwise.\nGiven a representation H\u2208Rm\u00d7d, the multi-head self-attention function can be de\ufb01ned as\nF(H) = Merge(head 1,...,head \u03c4)Whead(2.7)\nwhere Merge( \u00b7)representees a concatenation of its inputs, and Whead\u2208Rd\u00d7drepresents a pa-\nrameter matrix. head jis the output of QKV attention on a sub-space of representati on\nhead j= Att qkv(Q[j],K[j],V[j]) (2.8)\nQ[j],K[j],andV[j]are the queries, keys, and values projected onto the j-th sub-space via linear\ntransformations\nQ[j]=HWq\nj (2.9)\nK[j]=HWk\nj (2.10)\nV[j]=HWv\nj (2.11)\nwhere Wq\nj,Wk\nj, and Wv\nj\u2208Rd\u00d7d\n\u03c4are the parameter matrices of the transformations.\nSuppose we have LTransformer blocks. A Softmax layer is built on top of the out put of the\nlast block. The Softmax layer outputs a sequence of mdistributions over the vocabulary, like this\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0Pr(\u00b7|x0,...,x m\u22121)\n...\nPr(\u00b7|x0,x1)\nPr(\u00b7|x0)\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb= Softmax( HLWo) (2.12)\nwhere HLis the output of the last Transformer block, and Wo\u2208Rd\u00d7|V|is the parameter matrix.\nFigure 2.1shows the Transformer architecture for language modeling. Applying this language\n\n--- Page 47 ---\n\n40 Generative Models\nx0x1...xm\u22121e0 e1... em\u22121hL\n0 hL\n1... hL\nm\u22121...Pr(x1|x0)Pr(x2|x0x1)Pr(xm|x0x1...xm\u22121)x1x2...xm\nLanguage Model\nz0 z1... zm\u22121Post-norm or Pre-normPost-norm or Pre-norm\nSelf-attentionFFNLBlocks\nFig. 2.1: The Transformer-decoder architecture for language modeli ng. The central components are Lstacked Trans-\nformer blocks, each comprising a self-attention sub-layer and an FFN sub-layer. To prevent the model from accessing\nthe right-context, a masking variable is incorporated into self-attention. The output layer uses a Softmax function to\ngenerate a probability distribution for the next token, giv en the sequence of previous tokens. During inference, the\nmodel takes the previously predicted token to predict the ne xt one, repeating this process until the end of the sequence\nis reached. {z0,...,zm\u22121}denote the inputs of a Transformer block, and {hL\n0,...,hL\nm\u22121}denote the outputs of the\nlast Transformer block.\nmodel follows an autoregressive process. Each time the lang u",
    "393d627c45da8706": " Pre-normPost-norm or Pre-norm\nSelf-attentionFFNLBlocks\nFig. 2.1: The Transformer-decoder architecture for language modeli ng. The central components are Lstacked Trans-\nformer blocks, each comprising a self-attention sub-layer and an FFN sub-layer. To prevent the model from accessing\nthe right-context, a masking variable is incorporated into self-attention. The output layer uses a Softmax function to\ngenerate a probability distribution for the next token, giv en the sequence of previous tokens. During inference, the\nmodel takes the previously predicted token to predict the ne xt one, repeating this process until the end of the sequence\nis reached. {z0,...,zm\u22121}denote the inputs of a Transformer block, and {hL\n0,...,hL\nm\u22121}denote the outputs of the\nlast Transformer block.\nmodel follows an autoregressive process. Each time the lang uage model takes a token xi\u22121as\ninput and predicts a token xithat maximizes the probability Pr(xi|x0,...,x i\u22121). It is important\nto note that, despite different implementation details, ma ny LLMs share the same architecture\ndescribed above. These models are called large because both their depth and width are signi\ufb01cant.\nTable 2.2shows the model sizes for a few LLMs, as well as their model set ups.\n2.1.2 Training LLMs\nNow suppose that we are given a training set DcomprisingKsequences. The log-likelihood of\neach sequence x=x0...xminDcan be calculated using a language model\nL\u03b8(x) =m\u2211\ni=1log Pr \u03b8(xi|x0,...,x i\u22121) (2.13)\nHere the subscript \u03b8af\ufb01xed to L(\u00b7)andPr(\u00b7)denotes the parameters of the language model. Then,\nthe objective of maximum likelihood training is de\ufb01ned as\n\u02c6\u03b8= arg max\n\u03b8\u2211\nx\u2208DL\u03b8(x) (2.14)\nTraining Transformer-based language models with the above objective is commonly viewed\nas a standard optimization process for neural networks. Thi s can be achieved using gradient de-\nscent algorithms, which are widely supported by off-the-sh elf deep learning toolkits. Somewhat\n\n--- Page 48 ---\n\n2.1 A Brief Introduction to LLMs 41\nLLM # of Parameters DepthLWidthd# of Heads\n(Q/KV)\nGPT-1 [ Radford et al. ,2018 ] 0.117B 12 768 12/12\nGPT-2 [ Radford et al. ,2019 ] 1.5B 48 1,600 25/25\nGPT-3 [ Brown et al. ,2020 ] 175B 96 12,288 96/96\nLLaMA2 [ Touvron et al. ,2023b ]7B 32 4,096 32/32\n13B 40 5,120 40/40\n70B 80 8,192 64/64\nLLaMA3/3.1 [ Dubey et al. ,2024 ]8B 32 4,096 32/8\n70B 80 8,192 64/8\n405B 126 16,384 128/8\nGemma2 [ Team et al. ,2024 ]2B 26 2,304 8/4\n9B 42 3,584 16/8\n37B 46 4,608 32/16\nQwen2.5 [ Yang et al. ,2024 ]0.5B 24 896 14/2\n7B 28 3,584 28/4\n72B 80 8,192 64/8\nDeepSeek-V3 [ Liu et al. ,2024a ] 671B 61 7,168 128/128\nFalcon [ Penedo et al. ,2023 ]7B 32 4,544 71/71\n40B 60 8,192 128/128\n180B 80 14,848 232/232\nMistral [ Jiang et al. ,2023a ] 7B 32 4,096 32/32\nTable 2.2: Comparison of some LLMs in terms of model size, model depth, m odel width, and number of heads ( a/b\nmeansaheads for queries and bheads for both keys and values).\nsurprisingly, better results were continuously yielded as language models",
    "81f6ca27534022fc": "24 896 14/2\n7B 28 3,584 28/4\n72B 80 8,192 64/8\nDeepSeek-V3 [ Liu et al. ,2024a ] 671B 61 7,168 128/128\nFalcon [ Penedo et al. ,2023 ]7B 32 4,544 71/71\n40B 60 8,192 128/128\n180B 80 14,848 232/232\nMistral [ Jiang et al. ,2023a ] 7B 32 4,096 32/32\nTable 2.2: Comparison of some LLMs in terms of model size, model depth, m odel width, and number of heads ( a/b\nmeansaheads for queries and bheads for both keys and values).\nsurprisingly, better results were continuously yielded as language models were evolved into more\ncomputationally intensive models and trained on larger dat asets [ Kaplan et al. ,2020 ]. These suc-\ncesses have led NLP researchers to continue increasing both the training data and model size in\norder to build more powerful language models.\nHowever, as language models become larger, we confront new t raining challenges, which\nsigni\ufb01cantly change the problem compared to training relat ively small models. One of these\nchallenges arises from the need for large-scale distribute d systems to manage the data, model\nparameters, training routines, and so on. Developing and ma intaining such systems requires a\nsigni\ufb01cant amount of work in both software and hardware engi neering, as well as expertise in deep\nlearning. A related issue is that when the training is scaled up, we need more computing resources\nto ensure the training process can be completed in an accepta ble time. For example, it generally\nrequires hundreds or thousands of GPUs to train an LLM with te ns of billions of parameters\nfrom scratch. This requirement drastically increases the c ost of training such models, especially\nconsidering that many training runs are needed as these mode ls are developed. Also, from the\nperspective of deep learning, the training process can beco me unstable if the neural networks are\nvery deep and/or the model size is very large. In response, we typically need to modify the model\narchitecture to adapt LLMs to large-scale training. In Sect ion2.2we will present more discussions\non these issues.\n\n--- Page 49 ---\n\n42 Generative Models\n2.1.3 Fine-tuning LLMs\nOnce we have pre-trained an LLM, we can then apply it to perfor m various NLP tasks. Tradi-\ntionally language models are used as components of other sys tems, for example, they are widely\napplied to score translations in statistical machine trans lation systems. By contrast, in generative\nAI, LLMs are considered complete systems and are employed to address NLP problems by mak-\ning use of their generation nature. A common approach is to de scribe the task we want to address\nin text and then prompt LLMs to generate text based on this des cription. This is a standard text\ngeneration task where we continue or complete the text start ing from a given context.\nMore formally, let x=x0...xmdenote a token sequence of context given by users, and\ny=y1...yndenote a token sequence following the context. Then, the inf erence of LLMs can be\nde\ufb01ned as a problem of \ufb01nding the most likely sequence ybased on x:\n\u02c6y= arg max\nylog Pr( y|x)\n= arg max\nyn\u2211\ni=1log Pr(yi|x0,...,x m,y1,...,y i\u22121) (2.15)\nHere\u2211n\ni=1log Pr(yi|x0,...,x m,y1,...,y i\u22121)essentially expresses the same thing as the right-\nhand side of Eq. ( 2.2). It models the log probability of predicting tokens from po sitionm+ 1,\nrather than position 0. Throughout this chapter and subsequent ones, we will emplo y separate\nvariables xandyto distinguish the input and output of an LLM, though they can be seen as sub-\nsequences from the same sequence. By adopting such notation , we see that the form of the above\nequation closely resembles those used in other text generat ion models in NLP, such as neural\nmachine translation models.\nTo illustrate how LLMs are applied, consider the problem of d etermining the grammaticality\nfor a given sentence. We can de\ufb01",
    "8d69b7f846ee6a8a": "x m,y1,...,y i\u22121) (2.15)\nHere\u2211n\ni=1log Pr(yi|x0,...,x m,y1,...,y i\u22121)essentially expresses the same thing as the right-\nhand side of Eq. ( 2.2). It models the log probability of predicting tokens from po sitionm+ 1,\nrather than position 0. Throughout this chapter and subsequent ones, we will emplo y separate\nvariables xandyto distinguish the input and output of an LLM, though they can be seen as sub-\nsequences from the same sequence. By adopting such notation , we see that the form of the above\nequation closely resembles those used in other text generat ion models in NLP, such as neural\nmachine translation models.\nTo illustrate how LLMs are applied, consider the problem of d etermining the grammaticality\nfor a given sentence. We can de\ufb01ne a template like this\n{*sentence*}\nQuestion: Is this sentence grammatically correct?\nAnswer:\nHere represents the text we intend to generate. {*sentence*} is a placeholder variable that\nwill be replaced by the actual sentence provided by the users . For example, suppose we have a\nsentence \u201c John seems happy today. \u201d. We can replace the {*sentence*} in the template with this\nsentence to have an input to the language model\nJohn seems happy today.\nQuestion: Is this sentence grammatically correct?\nAnswer:\nTo perform the task, the language model is given the context x=\u201cJohn seems happy today . \\n\nQuestion : Is this sentence grammatically correct? \\n Answer :\u201d4. It then generates the following\n4\\n is a special character used for line breaks.\n\n--- Page 50 ---\n\n2.1 A Brief Introduction to LLMs 43\ntext as the answer, based on the context. For example, the lan guage model may output \u201cYes\u201d (i.e.,\ny= \u201cYes\u201d ) if this text is the one with the maximum probability of predi ction given this context.\nLikewise, we can de\ufb01ne more templates to address other tasks . For example, we can translate\nan English sentence into Chinese using the following templa te\n{*sentence*}\nQuestion: What is the Chinese translation of this English se ntence?\nAnswer:\nor using an instruction-like template\n{*sentence*}\nTranslate this sentence from English into Chinese.\nor using a code-like template.\n[src-lang] = English [tgt-lang] = Chinese [input] = {*sente nce*}\n[output] =\nThe above templates provide a simple but effective method to \u201cprompt\u201d a single LLM to per-\nform various tasks without adapting the structure of the mod el. However, this approach requires\nthat the LLM can recognize and follow the instructions or que stions. One way to do this is to incor-\nporate training samples with instructions and their corres ponding responses into the pre-training\ndataset. While this method is straightforward, building an d training LLMs from scratch is com-\nputationally expensive. Moreover, making instruction-fo llowing data effective for pre-training\nrequires a signi\ufb01cant amount of such data, but collecting la rge-scale labeled data for all tasks of\ninterest is very dif\ufb01cult.\nA second method, which has been a de facto standard in recent r esearch, is to adapt LLMs\nvia \ufb01ne-tuning. As such, the token prediction ability learn ed in the pre-training phase can be\ngeneralized to accomplish new tasks. The idea behind \ufb01ne-tu ning is that some general knowledge\nof language has been acquired in pre-training, but we need a m echanism to activate this knowledge\nfor applying it to new tasks. To achieve this, we can slightly \ufb01ne-tune the model parameters using\ninstruction-following data. This approach is called instruction \ufb01ne-tuning .\nAn instruction \ufb01ne-tuning sample, which is represented by a sequence of tokens, can be seen\nas a tuple consisting of an input and the desired output. Here , the input includes instructions,\nsystem information (or system pre\ufb01x), and any other user-pr ovided information5. To illustrate,\nconsider the following examples (blue text = input and under lined text = output).\n5System information refers to a sequence of tokens added at th e beginning of an input in order to guide the behavior\nof an LLM, such as, you are a helpful assistant and should not output toxic conte nt.\n\n--- Page 51 ---\n\n44 Generative Models\nRead the following sentence, and answer whether it is gramma tically correct.\nLLMs are powerful models but are expensive to build.\nYes\nLLMs are powerful models but are expensive to build.\n",
    "9840b36b7c94b42e": "ne-tune the model parameters using\ninstruction-following data. This approach is called instruction \ufb01ne-tuning .\nAn instruction \ufb01ne-tuning sample, which is represented by a sequence of tokens, can be seen\nas a tuple consisting of an input and the desired output. Here , the input includes instructions,\nsystem information (or system pre\ufb01x), and any other user-pr ovided information5. To illustrate,\nconsider the following examples (blue text = input and under lined text = output).\n5System information refers to a sequence of tokens added at th e beginning of an input in order to guide the behavior\nof an LLM, such as, you are a helpful assistant and should not output toxic conte nt.\n\n--- Page 51 ---\n\n44 Generative Models\nRead the following sentence, and answer whether it is gramma tically correct.\nLLMs are powerful models but are expensive to build.\nYes\nLLMs are powerful models but are expensive to build.\nDoes this sentence make sense grammatically?\nAnswer Option\n- Yes\n- No\nYes\nYes or No? Yes = the following sentence is grammatically corr ect. No = it con-\ntains grammatical errors.\nLLMs are powerful models but are expensive to build.\nYes\nAll these samples describe the same binary classi\ufb01cation ta sk, but with different instructions.\nTo increase the diversity in the \ufb01ne-tuning data and make LLM s generalize, we can de\ufb01ne more\ntasks using instructions.\nWrite a short story about three characters.\nCharacter 1: A boy whose name is Andy.\nCharacter 2: Andy\u2019s dog.\nCharacter 3: Andy\u2019s friend, Jane.\nIntheheart ofabustling city, Andy, hisloyal dog Rocket, and hisfriend\nJane discovered ahidden garden beneath theglow oftwilight. Surrounded by\nancient trees andwhispering winds, they shared dreams andsecrets, forging an\nunbreakable bond. Asthenight deepened, they made apact under thestars to\nalways \ufb01ndmagic inthemundane. Together, they realized thattrueadventure lay\ninthemoments spent with friends, andevery daywas atreasure waiting tobe\ndiscovered.\n\n--- Page 52 ---\n\n2.1 A Brief Introduction to LLMs 45\nShow me a recipe for making ice cream.\nIngredients:\n2 cups heavy cream; 1 cup milk; 3/4 cup sugar; 2 tsp vanilla ext ract;\nPinch of salt.\nSteps:\n1. Whisk all ingredients until sugar dissolves.\n2. Chill mixture for 2 hours in the fridge.\n3. Freeze in a shallow dish, stirring every 30 minutes, 3-4 ti mes.\n4. Freeze solid for 2 hours.\n5. Serve your homemade ice cream.\nIf you buy 5 apples and each apple costs $1.20, how much do you s pend in total?\n$6.00\nWrite a Python program to calculate the sum of squares of the f ollowing numbers.\n1 , 2 , 10 , -9 , 78\nnumbers = [1,2,10,-9 ,78]\nsum_of_squares = sum(x**2 for x in numbers)\nprint(sum_of_squares)\nTo acquire instruction-following abilities, a certain amo unt of \ufb01ne-tuning data is required. This\ndata may include diverse instructions and possible respons es. It has been found that scaling the\nnumber of \ufb01ne-tuning tasks is bene\ufb01cial for improving the pe rformance of LLMs [ Chung et al. ,\n2022 ]. Note that although more \ufb01ne-tuning data is favorable, the amount of this data is generally\norders of magnitude smaller than that of the pre-training da ta. For example, LLMs can be \ufb01ne-\ntuned with tens or hundreds of thousands of samples, or even f ewer if these samples are of high\nquality [ Zhou et al. ,2023a ;Chen et al. ,2023b ], whereas pre-training such models may require\nbillions or trillions of tokens, resulting in signi\ufb01cantly larger computational demands and longer\ntraining times [ Touvron et al. ,2023a ].\nIt is also worth noting that we should not expect the \ufb01ne-tuni ng data to cover all the down-\nstream tasks to which we intend to apply LLMs. A common unders tanding of how the pre-training\n+ \ufb01ne-tuning approach works is that LLMs have gained knowled ge for understanding instructions\nand generating responses in the pre-training phase. Howeve r, these abilities are not fully activated\nuntil we introduce some form of supervision. The general ins truction-following behavior emerges\nas we \ufb01",
    "b84fecf89beab4e7": "-\ntuned with tens or hundreds of thousands of samples, or even f ewer if these samples are of high\nquality [ Zhou et al. ,2023a ;Chen et al. ,2023b ], whereas pre-training such models may require\nbillions or trillions of tokens, resulting in signi\ufb01cantly larger computational demands and longer\ntraining times [ Touvron et al. ,2023a ].\nIt is also worth noting that we should not expect the \ufb01ne-tuni ng data to cover all the down-\nstream tasks to which we intend to apply LLMs. A common unders tanding of how the pre-training\n+ \ufb01ne-tuning approach works is that LLMs have gained knowled ge for understanding instructions\nand generating responses in the pre-training phase. Howeve r, these abilities are not fully activated\nuntil we introduce some form of supervision. The general ins truction-following behavior emerges\nas we \ufb01ne-tune the models with a relatively small amount of la beled data. As a result, we can\nachieve some level of zero-shot learning : the \ufb01ne-tuned models can handle new tasks that they\nhave not been explicitly trained or \ufb01ne-tuned for [ Sanh et al. ,2022 ;Wei et al. ,2022a ]. This zero-\nshot learning ability distinguishes generative LLMs from e arlier pre-trained models like BERT,\nwhich are primarily \ufb01ne-tuned for speci\ufb01c tasks.\nOnce we have prepared a collection of instruction-describe d data, the \ufb01ne-tuning process is\nrelatively simple. This process can be viewed as a standard t raining process as pre-training, but on\na much smaller training dataset. Let Dtunebe the \ufb01ne-tuning dataset and \u02c6\u03b8be the model parameters\n\n--- Page 53 ---\n\n46 Generative Models\noptimized via pre-training. We can modify Eq. ( 2.14) to obtain the objective of \ufb01ne-tuning\n\u02dc\u03b8= arg max\n\u02c6\u03b8+\u2211\nsample \u2208DtuneL\u02c6\u03b8+(sample) (2.16)\nHere \u02dc\u03b8denotes the optimal parameters. The use of notation \u02c6\u03b8+means that the \ufb01ne-tuning starts\nwith the pre-trained parameters \u02c6\u03b8.\nFor each sample \u2208 D tune, we divide it into an input segment xsample and an output segment\nysample , that is,\nsample = [ ysample,xsample ] (2.17)\nWe then de\ufb01ne the loss function to be\nL\u02c6\u03b8+(sample) = \u2212log Pr \u02c6\u03b8+(ysample |xsample ) (2.18)\nIn other words, we compute the loss over the sub-sequence ysample , rather than the entire sequence.\nIn a practical implementation of back-propagation for this equation, the sequence [ysample,xsample ]\nis constructed in the forward pass as usual. However, in the b ackward pass, error gradients are\npropagated back only through the parts of the network that co rrespond to ysample , leaving the rest\nof the network unchanged. As an example, consider a sequence\n\u27e8s\u27e9Square this number . 2 .\ued19 \ued18\ued17 \ued1a\nContext (Input)The result is 4 .\ued19\ued18\ued17\ued1a\nPrediction (Output)\nThe loss is calculated and back propagated only for The resul t is4..\nInstruction \ufb01ne-tuning also requires substantial enginee ring work. In order to achieve satis-\nfactory results, one may experiment with different setting s of the learning rate, batch size, number\nof \ufb01ne-tuning steps, and so on. This typically requires many \ufb01ne-tuning runs and evaluations. The\ncost and experimental effort of \ufb01ne-tuning remain critical and should not be overlooked, though\nthey are much lower than those of the pre-training phase.\nWhile we focus on instruction \ufb01ne-tuning for an illustrativ e example here, \ufb01ne-tuning tech-\nniques play an important role in developing various LLMs and are more widely used. Examples\ninclude \ufb01ne-tuning LLMs as chatbots using dialog data, and a dapting these models to handle very\nlong sequences. The wide application of \ufb01ne-tuning has led r esearchers to improve these tech-\nniques, such as designing more ef\ufb01cient \ufb01ne-tuning algorit hms. While the research on \ufb01ne-tuning\nis fruitful, in this section we just give a \ufb02avour of the key st eps involved. We will see more detailed\ndiscussions on this",
    "cbc04304fcb8caed": " many \ufb01ne-tuning runs and evaluations. The\ncost and experimental effort of \ufb01ne-tuning remain critical and should not be overlooked, though\nthey are much lower than those of the pre-training phase.\nWhile we focus on instruction \ufb01ne-tuning for an illustrativ e example here, \ufb01ne-tuning tech-\nniques play an important role in developing various LLMs and are more widely used. Examples\ninclude \ufb01ne-tuning LLMs as chatbots using dialog data, and a dapting these models to handle very\nlong sequences. The wide application of \ufb01ne-tuning has led r esearchers to improve these tech-\nniques, such as designing more ef\ufb01cient \ufb01ne-tuning algorit hms. While the research on \ufb01ne-tuning\nis fruitful, in this section we just give a \ufb02avour of the key st eps involved. We will see more detailed\ndiscussions on this topic in the following chapters.\n2.1.4 Aligning LLMs with the World\nInstruction \ufb01ne-tuning provides a simple way to adapt LLMs t o tasks that can be well de\ufb01ned. This\nproblem can broadly be categorized as an alignment problem. Here, alignment is referred to as a\nprocess of guiding LLMs to behave in ways that align with huma n intentions. The guidance can\ncome from labeled data, human feedback, or any other form of h uman preferences. For example,\n\n--- Page 54 ---\n\n2.1 A Brief Introduction to LLMs 47\nwe want LLMs not only to be accurate in following instruction s, but also to be unbiased, truthful,\nand harmless. So we need to supervise the models towards huma n values and expectations. A\ncommon example is that when we ask an LLM how to build a weapon, it may provide a list of key\nsteps to do so if it is not carefully aligned. However, a respo nsible model should recognize and\navoid responding to requests for harmful or illegal informa tion. Alignment in this case is crucial\nfor ensuring that LLMs act responsibly and in accordance wit h ethical guidelines.\nA related concept to alignment is AI safety. One ultimate goa l of AI is to build intelligent\nsystems that are safe and socially bene\ufb01cial. To achieve thi s goal we should keep these systems\nrobust, secure, and subjective, in any conditions of real-w orld use, even in conditions of misuse\nor adverse use. For LLMs, the safety can be increased by align ing them with appropriate human\nguidance, such as human labeled data and interactions with u sers during application.\nAlignment is dif\ufb01cult as human values and expectations are d iverse and shifting. Sometimes,\nit is hard to describe precisely what humans want, unless we s ee the response of LLMs to user\nrequests. This makes alignment no longer a problem of tuning LLMs on prede\ufb01ned tasks, but a\nbigger problem of training them with the interactions with t he real world.\nAs a result of the concerns with controlling AI systems, ther e has been a surge in research\non the alignment issue for LLMs. Typically, two alignment st eps are adopted after LLMs are\npre-trained on large-scale unlabeled data.\n\u2022Supervised Fine-tuning (SFT). This involves continuing the training of pre-trained LLM s\non new, task-oriented, labelled data. A commonly used SFT te chnique is instruction \ufb01ne-\ntuning. As described in the previous subsection, by learnin g from instruction-response\nannotated data, LLMs can align with the intended behaviors f or following instructions,\nthereby becoming capable of performing various instructio n-described tasks. Supervised\n\ufb01ne-tuning can be seen as following the pre-training + \ufb01ne-t uning paradigm, and offers a\nrelatively straightforward method to adapt LLMs.\n\u2022Learning from Human Feedback . After an LLM \ufb01nishes pre-training and supervised \ufb01ne-\ntuning, it can be used to respond to user requests if appropri ately prompted. But this model\nmay generate content that is unfactual, biased, or harmful. To make the LLM more aligned\nwith the users, one simple approach is to directly learn from human feedback. For example,\ngiven some instructions and inputs provided by the users, ex perts are asked to evaluate how\nwell the model responds in accordance with their preference s and interests. This feedback\nis then used to further train the LLM for better alignment.\nA typical method for learning from human feedback is to consi der it as a reinforcement learn-\ning (RL) problem, known as reinforcement learning from human feedback (RLHF ) [Ouy",
    "a5d7171d84d43e01": " be seen as following the pre-training + \ufb01ne-t uning paradigm, and offers a\nrelatively straightforward method to adapt LLMs.\n\u2022Learning from Human Feedback . After an LLM \ufb01nishes pre-training and supervised \ufb01ne-\ntuning, it can be used to respond to user requests if appropri ately prompted. But this model\nmay generate content that is unfactual, biased, or harmful. To make the LLM more aligned\nwith the users, one simple approach is to directly learn from human feedback. For example,\ngiven some instructions and inputs provided by the users, ex perts are asked to evaluate how\nwell the model responds in accordance with their preference s and interests. This feedback\nis then used to further train the LLM for better alignment.\nA typical method for learning from human feedback is to consi der it as a reinforcement learn-\ning (RL) problem, known as reinforcement learning from human feedback (RLHF ) [Ouyang et al. ,\n2022 ]. The RLHF method was initially proposed to address general sequential decision-making\nproblems [ Christiano et al. ,2017 ], and was later successfully employed in the development of\nthe GPT series models [ Stiennon et al. ,2020 ]. As a reinforcement learning approach, the goal of\nRLHF is to learn a policy by maximizing some reward from the en vironment. Speci\ufb01cally, two\ncomponents are built in RLHF:\n\u2022Agent . An agent, also called an LM agent, is the LLM that we want to tr ain. This agent\noperates by interacting with its environment: it receives a text from the environment and\n\n--- Page 55 ---\n\n48 Generative Models\noutputs another text that is sent back to the environment. Th e policy of the agent is the\nfunction de\ufb01ned by the LLM, that is, Pr(y|x).\n\u2022Reward Model . A reward model is a proxy of the environment. Each time the ag ent\nproduces an output sequence, the reward model assigns this o utput sequence a numerical\nscore (i.e., the reward). This score tells the agent how good the output sequence is.\nIn RLHF, we need to perform two learning tasks: 1) reward mode l learning, which involves\ntraining a reward model using human feedback on the output of the agent, and 2) policy learning,\nwhich involves optimizing a policy guided by the reward mode l using reinforcement learning\nalgorithms. Here is a brief outline of the key steps involved in RLHF.\n\u2022 Build an initial policy using pre-training and instructio n \ufb01ne-tuning.\n\u2022 Use the policy to generate multiple outputs for each input, and then collect human feedback\non these outputs (e.g., comparisons of the outputs).\n\u2022 Learn a reward model from the human feedback.\n\u2022 Fine-tune the policy with the supervision from the reward m odel.\nFigure 2.2shows an overview of RLHF. Given that this section serves onl y as a brief intro-\nduction to concepts of LLMs, a detailed discussion of RLHF te chniques will not be included. We\ninstead illustrate the basic ideas behind RLHF using a simpl e example.\nSuppose we have trained an LLM via pre-training and instruct ion \ufb01ne-tuning. This LLM is\ndeployed to respond to requests from users. For example, a us er may input\nHow can I live a more environmentally friendly life?\nWe use the LLM to generate 4 different outputs (denoted by {y1,...,y4}) by sampling the\noutput space\nOutput 1 ( y1): Consider switching to an electric vehicle or bicycle inst ead of\ntraditional cars to reduce carbon emissions and protect our planet.\nOutput 2 ( y2): Adopt a minimalist lifestyle. Own fewer possessions to re duce\nconsumption and the environmental impact of manufacturing and\ndisposal.\nOutput 3 ( y3): Go off-grid. Generate your own renewable energy and colle ct\nrainwater to become completely self-suf\ufb01cient and reduce r eliance\non non-renewable resources.\nOutput 4 ( y4): Support local farm products to reduce the carbon footprin t of\ntransporting food, while enjoying fresh, healthy food.\n\n--- Page 56 ---\n\n2.1 A Brief Introduction to LLMs 49\nLLMPre-training Data\nI love the food here! ...How can I get there? ...SFT Data\nweather in London . ...Write a poem about the\nPre-training &\nSupervised \ufb01ne-tuning\n(a) Learning an Initial LLMLLMUser Input\nenvironmentally friendly?How can I live moreModel Output\n3. ............ 4. ............1. ............ 2. ............\nPredictingComparisons\n",
    "0a3208c5f8fc188a": "consumption and the environmental impact of manufacturing and\ndisposal.\nOutput 3 ( y3): Go off-grid. Generate your own renewable energy and colle ct\nrainwater to become completely self-suf\ufb01cient and reduce r eliance\non non-renewable resources.\nOutput 4 ( y4): Support local farm products to reduce the carbon footprin t of\ntransporting food, while enjoying fresh, healthy food.\n\n--- Page 56 ---\n\n2.1 A Brief Introduction to LLMs 49\nLLMPre-training Data\nI love the food here! ...How can I get there? ...SFT Data\nweather in London . ...Write a poem about the\nPre-training &\nSupervised \ufb01ne-tuning\n(a) Learning an Initial LLMLLMUser Input\nenvironmentally friendly?How can I live moreModel Output\n3. ............ 4. ............1. ............ 2. ............\nPredictingComparisons\ny1\u227by4\u227by2\u227by3\nAnnotating Data with Human Preferences\n(b) Annotating Data with Human Preferences\nReward ModelComparison Data\n{(x,yk1\u227byk2)}\nTraining\n(c) Training the Reward ModelLLM\n(Policy)Dataset D\nx\u223c DInput-output Pairs\n{x,y}\nSampling yvia the Policy Pr(y|x)Reward ModelReward Scores\n{r(x,y)}\nEvaluate the Input-output Pairs\n(d) Training/Fine-tuning the PolicyRL Fine-tuning\nFig. 2.2: An overview of RLHF. There are 4 key steps involved: a) traini ng an initial LLM (i.e., policy) using pre-\ntraining and supervised \ufb01ne-tuning; b) collecting human pr eference data by ranking the outputs of the LLM; c) training\na reward model using the ranking results; d) RL \ufb01ne-tuning of the policy based on the reward model. Double line\narrows mean training or \ufb01ne-tuning.\nWe then ask annotators to evaluate these outputs. One straig htforward way is to assign a rating\nscore to each output. In this case, the reward model learning problem can be framed as a task of\ntraining a regression model. But giving numerical scores to LLM outputs is not an easy task for\nannotators. It is usually dif\ufb01cult to design an annotation s tandard that all annotators can agree on\nand easily follow. An alternative method, which is more popu lar in the development of LLMs, is\nto rank these outputs. For example, a possible ranking of the above outputs is\ny1\u227by4\u227by2\u227by3\n\n--- Page 57 ---\n\n50 Generative Models\nA reward model is then trained using this ranking result. In g eneral, a reward model in RLHF\nis a language model that shares the same architecture as the t arget LLM, but with a smaller model\nsize. Given the input xand output yk, we concatenate them to form a sequence seqk= [x,yk].\nThis sequence is processed from left to right using forced de coding. Since each position can\nonly access its left context in language modeling, the outpu t of the top-most Transformer layer at\nthe \ufb01rst position cannot be used as the representation of the sequence. Instead, a special symbol\n(e.g., \u27e8\\s\u27e9) is added to the end of the sequence, and the corresponding ou tput of the Transformer\nlayer stack is considered as the representation of the entir e sequence. An output layer, such as a\nlinear transformation layer, is built on top of this represe ntation to generate the reward, denoted\nbyR(seqk)orR(x,yk).\nWe train this reward model using ranking loss. For example, a pair-wise ranking loss function\ncan be written in the form\nLoss \u03c9(Dr) = \u2212E(x,yk1,yk2)\u223cD rlog(Sigmoid( R\u03c9(x,yk1)\u2212R\u03c9(x,yk2))) (2.19)\nwhere\u03c9represents the parameters of the reward model, and Drrepresents a set of tuples of an\ninput and a pair of outputs. (x,yk1,yk2)\u223c D ris a sampling operation which draws a sample\n(x,yk1,yk2)from Drwith some probability. As an example, suppose we \ufb01rst draw a m odel\ninput xwith a uniform distribution and then draw a pair of model outp uts with a probability of\nyk1\u227byk2given x(denoted by Pr(yk1\u227byk2|x)). The corresponding loss function is given by\nLoss",
    "8ca4de282f5f275c": " reward model using ranking loss. For example, a pair-wise ranking loss function\ncan be written in the form\nLoss \u03c9(Dr) = \u2212E(x,yk1,yk2)\u223cD rlog(Sigmoid( R\u03c9(x,yk1)\u2212R\u03c9(x,yk2))) (2.19)\nwhere\u03c9represents the parameters of the reward model, and Drrepresents a set of tuples of an\ninput and a pair of outputs. (x,yk1,yk2)\u223c D ris a sampling operation which draws a sample\n(x,yk1,yk2)from Drwith some probability. As an example, suppose we \ufb01rst draw a m odel\ninput xwith a uniform distribution and then draw a pair of model outp uts with a probability of\nyk1\u227byk2given x(denoted by Pr(yk1\u227byk2|x)). The corresponding loss function is given by\nLoss \u03c9(Dr)\n=\u2212\u2211\nPr(x)\u00b7Pr(yk1\u227byk2|x)\u00b7log(Sigmoid( R\u03c9(x,yk1)\u2212R\u03c9(x,yk2)))\n=\u22121\nK\u2211\nPr(yk1\u227byk2|x)\u00b7log(Sigmoid( R\u03c9(x,yk1)\u2212R\u03c9(x,yk2))) (2.20)\nwhereKrepresents the number of model inputs involved in sampling. While the form of these\nfunctions may seem complex, their idea is simple: we penaliz e the model if the predicted ranking\nof two outputs differs from the human-labeled ranking. By co ntrast, the model receives a bonus,\nif the predicted ranking matches the human-labeled ranking .\nWe can train the reward model by minimizing the above ranking loss\n\u02c6\u03c9= arg min\n\u03c9Loss \u03c9(Dr) (2.21)\nThe resulting model R\u02c6\u03c9(\u00b7)can be employed to evaluate any given pair of input and output . Note\nthat although the reward model is trained using a ranking-ba sed objective, it is used for scoring.\nThis allows it to provide continuous supervision signals, w hich is very bene\ufb01cial for training other\nmodels.\nWe now turn to the policy learning problem. A commonly adopte d objective is to maximize\nthe reward on a set of input-output pairs. Following an analo gous form of Eq. ( 2.16), we obtain a\nsimple training objective for RL \ufb01ne-tuning\n\u02dc\u03b8= arg max\n\u02c6\u03b8+E(x,y\u02c6\u03b8+)\u223cD rlftR\u02c6\u03c9(x,y\u02c6\u03b8+) (2.22)\nwhere the optimal parameters \u02dc\u03b8are obtained by \ufb01ne-tuning the pre-trained parameters \u02c6\u03b8.Drlftis\n\n--- Page 58 ---\n\n2.1 A Brief Introduction to LLMs 51\nthe RL \ufb01ne-tuning dataset. For each sample (x,y\u02c6\u03b8+),xis sampled from a prepared dataset of\ninput sequences, and y\u02c6\u03b8+is sampled from the distribution Pr\u02c6\u03b8+(y|x)given by the policy.\nIn practice, more advanced reinforcement learning algorit hms, such as proximal policy opti-\nmization (PPO ), are often used for achieving more stable training, as well as better performance.\nWe leave the detailed discussion of reinforcement learning algorithms to the following parts of\nthis book where RLHF is extensively used for alignment.\nAn interesting question arises here: why not consider learn ing from human preferences as\na standard supervised learning problem? This question is cl osely related to our aforementioned\ndiscussion on the dif\ufb01culty of data annotation. Often, desc ribing human values and goals is chal-\nlenging, and it is even more dif\ufb01cult for humans to provide ou tputs that are well aligned. As an\nalternative, annotating the preferences of a given list of m odel outputs offers a simpler task. By\ndoing so, we can create a model that understands human prefer ences, which can then be used as\na reward model for training policies. From the perspective o f machine learning, RLHF is par-\nticularly useful for scenarios where the desired behavior o f an agent is dif\ufb01cult to demonstrate\nbut can be easily recognized by humans. Another advantage of RLHF is its ability to explore the\nsample space. By employing sampling techniques, models tra ined with reinforcement learning\ncan venture beyond the annotated data set to explore additio nal samples. This exploratory ability\nallows RLHF to discover potentially bene\ufb01cial policies tha t are not immediately apparent from\nthe labeled data alone.\n2.1.5 Prompting LLMs\nWe have so far shown that LLM",
    "517aad6933c5c19b": "cult for humans to provide ou tputs that are well aligned. As an\nalternative, annotating the preferences of a given list of m odel outputs offers a simpler task. By\ndoing so, we can create a model that understands human prefer ences, which can then be used as\na reward model for training policies. From the perspective o f machine learning, RLHF is par-\nticularly useful for scenarios where the desired behavior o f an agent is dif\ufb01cult to demonstrate\nbut can be easily recognized by humans. Another advantage of RLHF is its ability to explore the\nsample space. By employing sampling techniques, models tra ined with reinforcement learning\ncan venture beyond the annotated data set to explore additio nal samples. This exploratory ability\nallows RLHF to discover potentially bene\ufb01cial policies tha t are not immediately apparent from\nthe labeled data alone.\n2.1.5 Prompting LLMs\nWe have so far shown that LLMs can be used to perform various ta sks by giving them appropriate\nprompts. There are no restrictions on these prompts, which c an include any information we wish\nto ask or communicate with LLMs, such as natural language ins tructions and the context of con-\nversations. Since this approach requires no additional tra ining or tuning, adapting LLMs becomes\nhighly ef\ufb01cient once they are developed. This somewhat in\ufb02u ences the paradigms in NLP: we\nno longer need to develop speci\ufb01c systems for individual tas ks but can instead use a single, well-\ntrained LLM to perform different tasks by prompting it. An ap pealing aspect of LLM prompting\narises as a result: users can easily have \u201ccustomized\u201d syste ms by designing their own prompts\nfor LLMs. Given the important role played by prompting in LLM s,prompting engineering has\nbecome a very active area of research in NLP.\nThe term prompt is used in many different ways in the literature. In this chap ter, this term\nrefers to the entire input to LLMs, and so we use the terms prompt andmodel input interchange-\nably. Before discussing prompting further, let us \ufb01rst see a few examples where the prompts\nare more complex than those presented in the previous subsec tions. Note that this subsection is\nnot aimed at writing high-quality prompts but rather at high lighting some interesting issues in\nprompting LLMs.\nOne of the popular ways to use LLMs is to assign them a \u201crole\u201d pl ayed in generating responses.\nFor example, LLMs can act as a psychologist when answering qu estions.\n\n--- Page 59 ---\n\n52 Generative Models\nPlease explain what delayed grati\ufb01cation is?\nNote, you are a researcher with a deep background in psycholo gy. Your own\n\ufb01eld of study is developmental psychology in children, with a focus on aspects\nof control and intelligence. You are very familiar with the c oncept of delayed\ngrati\ufb01cation and have applied it to your own research. You ha ve published papers\non delayed grati\ufb01cation and conducted related experiments .\nDelayed grati\ufb01cation istheprocess ofresisting animmediate reward in\nanticipation ofreceiving amore valuable reward inthefuture. Itisasigni\ufb01cant\nconcept indevelopmental psychology, especially inrelation tohow children learn\nself-control anddevelop cognitive abilities. Research shows thattheability to\ndelay grati\ufb01cation isassociated with positive outcomes inacademic performance,\nsocial competence, andemotional well-being. Studies andexperiments inthis\narea highlight theimportance ofcultivating this skill early inlifetosupport\nlong-term success andresilience.\nAnother example is the use of LLMs in detecting and correctin g errors such as syntactic or\nsemantic mistakes in text. For an LLM which is trained on both code and natural language data,\nwe may use it for code debugging6.\nFix the bugs in this C language program.\n#include<stdio.h>\nint main() {\nprintg(\"Hello, World!\")\nreturn 0;\n}\n1. The function name \u2019printg\u2019 is incorrect . It should be \u2019pri ntf\u2019.\n2. There is a missing semicolon at the end of the \u2019printf\u2019 func tion call.\nHere is the corrected version of the program:\n#include<stdio.h>\nint main() {\nprintf(\"Hello, World!\");\nreturn 0;\n}\nAlso, LLMs can be \ufb01ne-tuned to act as a conversation partner7.\n6In this example, the code is not tokenized for easier reading .\n7To \ufb01ne-tune an LLM for multi-turn dialogue, one needs to cons ider conversation history in the context for pre-\n",
    "f7a831a2a96ee856": "in g errors such as syntactic or\nsemantic mistakes in text. For an LLM which is trained on both code and natural language data,\nwe may use it for code debugging6.\nFix the bugs in this C language program.\n#include<stdio.h>\nint main() {\nprintg(\"Hello, World!\")\nreturn 0;\n}\n1. The function name \u2019printg\u2019 is incorrect . It should be \u2019pri ntf\u2019.\n2. There is a missing semicolon at the end of the \u2019printf\u2019 func tion call.\nHere is the corrected version of the program:\n#include<stdio.h>\nint main() {\nprintf(\"Hello, World!\");\nreturn 0;\n}\nAlso, LLMs can be \ufb01ne-tuned to act as a conversation partner7.\n6In this example, the code is not tokenized for easier reading .\n7To \ufb01ne-tune an LLM for multi-turn dialogue, one needs to cons ider conversation history in the context for pre-\ndicting the response in the current round of conversation. T his makes the actual prompt used in response generation\nrelatively longer than that used in single-turn dialogue.\n\n--- Page 60 ---\n\n2.1 A Brief Introduction to LLMs 53\nHi! I\u2019m an assistant. How can I help you?\nWho won the FIFA World Cup 2022?\nArgentina won the FIFA World Cup 2022.\nWhere was it held?\nThe 2022 FIFA World Cup was held in Qatar.\nHow many times has Argentina won the World Cup?\nArgentina has won the FIFA World Cup three times.\n...\nThese examples and previous ones have shown that appropriat e responses can be generated\nvia prompts involving clear instructions and questions. Ho wever, when problem solving requires\nknowledge that is not explicitly speci\ufb01ed, LLMs may make mis takes, even though the instructions\nare suf\ufb01ciently clear and precise. A family of challenging t asks for LLMs involves arithmetic\nreasoning and commonsense reasoning. For example, we can as k an LLM to solve primary school\nmath problems presented in natural language.\nJack has 7 apples. He ate 2 of them for dinner, but then his mom g ave him 5 more\napples. The next day, Jack gave 3 apples to his friend John. Ho w many apples\ndoes Jack have left in the end?\nThe answer is 10.\nThe correct answer should be 7, so the model output is incorre ct.\nOne approach to addressing such issues is to incorporate lea rning into prompts, called in-\ncontext learning or (ICL). The idea of ICL is to demonstrate the ways to solve problems in\nprompts, and condition predictions on these demonstration s. Here is an example where a similar\nproblem and the corresponding answer are presented in the pr ompt (green = demonstrations).\nTom has 12 marbles. He wins 7 more marbles in a game with his fri end but then\nloses 5 marbles the next day. His brother gives him another 3 m arbles as a gift.\nHow many marbles does Tom have now?\nThe answer is 17.\nJack has 7 apples. He ate 2 of them for dinner, but then his mom g ave him 5 more\napples. The next day, Jack gave 3 apples to his friend John. Ho w many apples\ndoes Jack have left in the end?\nThe answer is 12.\nBut the LLM still made mistakes this time. A reason for this mi ght be that solving math\nproblems does not only involve problem-answer mappings but also, to a larger extent, the under-\nlying logical inference in multiple steps. A method to impro ve the inference abilities of LLMs\nischain-of-thought prompting (COT prompting ) [Wei et al. ,2022c ]. In COT prompting, we\ndecompose complex reasoning problems into multiple proble m-solving intermediate steps. These\n\n--- Page 61 ---\n\n54 Generative Models\nsteps are demonstrated in prompts so that LLMs can be prompte d to learn to mimic the underlying\nthought processes in solving the problems. COT prompting ha s been found to be very useful for\nachieving good results on challenging tasks, such as the GSM 8K mathematical reasoning bench-\nmark.\nConsider the above demonstration example Tom has 12 marbles. He wins 7 more marbles in\na game with his friend but then loses 5 marbles the next day. Hi s brother gives him another 3\nmarbles as a gift. How many marbles does Tom have now ? . The corresponding logical reasoning\nsteps can be structured:\n1. Tom starts with 12 marbles.\n2. He wins 7 more marbles in a game, so we add those to his initia l count: 12 marbles",
    "a7e05628bd9ab888": "\ndecompose complex reasoning problems into multiple proble m-solving intermediate steps. These\n\n--- Page 61 ---\n\n54 Generative Models\nsteps are demonstrated in prompts so that LLMs can be prompte d to learn to mimic the underlying\nthought processes in solving the problems. COT prompting ha s been found to be very useful for\nachieving good results on challenging tasks, such as the GSM 8K mathematical reasoning bench-\nmark.\nConsider the above demonstration example Tom has 12 marbles. He wins 7 more marbles in\na game with his friend but then loses 5 marbles the next day. Hi s brother gives him another 3\nmarbles as a gift. How many marbles does Tom have now ? . The corresponding logical reasoning\nsteps can be structured:\n1. Tom starts with 12 marbles.\n2. He wins 7 more marbles in a game, so we add those to his initia l count: 12 marbles + 7\nmarbles = 19 marbles.\n3. The next day, Tom loses 5 marbles, so we subtract those from his current total: 19 marbles\n- 5 marbles = 14 marbles.\n4. His brother gives him 3 more marbles, so we add those to his c urrent total: 14 marbles + 3\nmarbles = 17 marbles.\n5. After all these events, Tom has 17 marbles.\nWe can add these reasoning steps into the prompt, and get the c orrect answer.\nTom has 12 marbles. He wins 7 more marbles in a game with his fri end but then\nloses 5 marbles the next day. His brother gives him another 3 m arbles as a gift.\nHow many marbles does Tom have now?\nTom initially has 12 marbles. After winning a game, he gains 7 more, increasing\nhis total to 19 marbles. He loses 5 marbles the next day, bring ing his total down\nto 14 marbles. His brother gifts him 3 more marbles, increasi ng his total to 17\nmarbles. Therefore, Tom now has 17 marbles. So the answer is 1 7.\nJack has 7 apples. He ate 2 of them for dinner, but then his mom g ave him 5 more\napples. The next day, Jack gave 3 apples to his friend John. Ho w many apples\ndoes Jack have left in the end?\nJack starts with 7apples. Heeats 2apples fordinner, sowesubtract 2from 7,\nleaving him with 5apples. Hismom gives him 5more apples, adding 5tohis\ncurrent total. Now hehas5+5=10apples. Thenext day, Jack gives 3apples to\nhisfriend John. Subtracting 3from 10,Jack isleftwith 7apples. Therefore, Jack\nhas7apples left. Sotheanswer is7.\nThis approach is also called one-shot COT prompting . Likewise, we can add more demon-\nstrations to have a few-shot COT prompting approach.\nFurthermore, a special kind of COT, zero-shot COT , introduces instructions that provoke\nLLMs to think, without presenting any intermediate reasoni ng steps in the prompts [ Kojima et al. ,\n\n--- Page 62 ---\n\n2.1 A Brief Introduction to LLMs 55\n2022 ]. An interesting example is that by appending a simple sente nce \u201cLet\u2019s think step by step.\u201d\nto the end of a prompt, LLMs can generate reasoning steps to re ach the \ufb01nal answer.\nJack has 7 apples. He ate 2 of them for dinner, but then his mom g ave him 5 more\napples. The next day, Jack gave 3 apples to his friend John. Ho w many apples\ndoes Jack have left in the end?\nLet\u2019s think step by step.\n1. Initial Quantity: Jack starts with 7 apples.\n2. After Dinner: He eats 2 apples, so 7\u22122 = 5 apples remain.\n3. His Mom Gives More: His mom gives him 5 more apples, so 5 + 5 = 10\napples are now with him.\n4. Giving to John: The next day, Jack gives 3 apples to his frie nd John,\nso10\u22123 = 7 apples are left.\nIn the end, Jack has 7 apples left.\nZero-shot, one-shot, and few-shot learning are common conc epts in the area of in-context\nlearning for LLMs and are not restricted to COT prompting. Br oadly speaking, any prompting\nthat",
    "8b513febd3db9b3f": " ave him 5 more\napples. The next day, Jack gave 3 apples to his friend John. Ho w many apples\ndoes Jack have left in the end?\nLet\u2019s think step by step.\n1. Initial Quantity: Jack starts with 7 apples.\n2. After Dinner: He eats 2 apples, so 7\u22122 = 5 apples remain.\n3. His Mom Gives More: His mom gives him 5 more apples, so 5 + 5 = 10\napples are now with him.\n4. Giving to John: The next day, Jack gives 3 apples to his frie nd John,\nso10\u22123 = 7 apples are left.\nIn the end, Jack has 7 apples left.\nZero-shot, one-shot, and few-shot learning are common conc epts in the area of in-context\nlearning for LLMs and are not restricted to COT prompting. Br oadly speaking, any prompting\nthat involves only simple instructions without any demonst rations can be considered a form of\nzero-shot learning. This zero-shot learning ability emerg es as LLMs are pre-trained and/or \ufb01ne-\ntuned. Also, one-shot and few-shot learning methods are mor e often considered when LLMs do\nnot acquire the corresponding zero-shot learning ability. These methods are therefore important\nfor in-context learning when addressing new tasks. Example s include those for performing various\nNLP tasks by demonstrating task-formatted samples. See the following examples for sentiment\nsentence classi\ufb01cation and phrase translation via few-sho t learning.\nGiven the following text snippets, classify their sentimen t as Positive, Negative,\nor Neutral.\nExample 1: \u201cI had an amazing day at the park!\u201d\nSentiment: Positive\nExample 2: \u201cThe service at the restaurant was terrible.\u201d\nSentiment: Negative\nExample 3: \u201cI think it\u2019s going to rain today.\u201d\nSentiment: Neutral\nText: \u201cThis movie was a fantastic journey through imaginati on.\u201d\nSentiment: Positive\n\n--- Page 63 ---\n\n56 Generative Models\nTranslate the following Chinese phrases into English.\nExample 1: \u201c\u4f60\u597d\u201d\nTranslation: \u201cHello\u201d\nExample 2: \u201c\u8c22\u8c22\u4f60\u201d\nTranslation: \u201cThank you\u201d\nPhrase to translate: \u201c \u65e9\u4e0a\u597d\u201d\nTranslation: \u201cGood Morning\u201d\nAbove, we have presented examples to illustrate the fundame ntal in-context learning capa-\nbilities of prompting LLMs. This section, however, does not include more advanced prompting\ntechniques in order to keep the content concise and compact. More discussions on prompting can\nbe found in Chapter 3.\n2.2 Training at Scale\nAs a \ufb01rst step in developing LLMs, we need to train these model s on large amounts of data.\nThe training task is itself standard: the objective is to max imize the likelihood, which can be\nachieved via gradient descent. However, as we scale up both t he model size and the amount\nof data, the problem becomes very challenging, for example, large models generally make the\ntraining unstable. In this section, we discuss several issu es of large-scale training for LLMs,\nincluding data preparation, model modi\ufb01cation, and distri buted training. We also discuss the\nscaling laws for LLMs, which help us understand their traini ng ef\ufb01ciency and effectiveness.\n2.2.1 Data Preparation\nThe importance of data cannot be overstated in NLP. As larger neural networks are developed,\nthe demand for data continues to increase. For example, deve loping LLMs may require trillions\nof tokens in pre-training (see Table 2.3), orders of magnitude larger than those used in training\nconventional NLP models. In general, we may want to gather as much training data as possible.\nHowever, larger training datasets do not mean better traini ng results, and the development of\nLLMs raises new issues in creating or collecting these datas ets.\nA \ufb01rst issue is the quality of data. High-quality data has lon g been seen as crucial for training\ndata-driven NLP systems. Directly using raw text from vario us sources is in general undesirable.\nFor example, a signi\ufb01cant portion of the data used to train re cent LLMs comes from web scraping,\nwhich may contain errors and inappropriate content, such as toxic information and fabricated\nfacts. Also, the internet is \ufb02ooded with machine-generated content due to the widespread use of\nAI, presenting further challenges for processing and using web-scraped data. Researchers have\nfound that training LLMs on un\ufb01ltered data is harmful [ Raffel et al. ,2020 ]. Improving data quality\ntypically involves incorporating",
    "84ea29514492a812": " much training data as possible.\nHowever, larger training datasets do not mean better traini ng results, and the development of\nLLMs raises new issues in creating or collecting these datas ets.\nA \ufb01rst issue is the quality of data. High-quality data has lon g been seen as crucial for training\ndata-driven NLP systems. Directly using raw text from vario us sources is in general undesirable.\nFor example, a signi\ufb01cant portion of the data used to train re cent LLMs comes from web scraping,\nwhich may contain errors and inappropriate content, such as toxic information and fabricated\nfacts. Also, the internet is \ufb02ooded with machine-generated content due to the widespread use of\nAI, presenting further challenges for processing and using web-scraped data. Researchers have\nfound that training LLMs on un\ufb01ltered data is harmful [ Raffel et al. ,2020 ]. Improving data quality\ntypically involves incorporating \ufb01ltering and cleaning st eps in the data processing work\ufb02ow. For\nexample, Penedo et al. [2023 ] show that by adopting a number of data processing technique s, 90%\nof their web-scraped data can be removed for LLM training. In addition to large-scale web-scraped\ndata, LLM training data often includes books, papers, user- generated data on social media, and\nso on. Most of the latest LLMs are trained on such combined dat asets, which are found to be\n\n--- Page 64 ---\n\n2.2 Training at Scale 57\nLLM # of Tokens Data\nGPT3-175B [ Brown et al. ,2020 ] 0.5T Webpages, Books, Wikipedia\nFalcon-180B [ Almazrouei et al. ,2023 ] 3.5T Webpages, Books, Conversations,\nCode, Technical Articles\nLLaMA2-65B [ Touvron et al. ,2023a ]1.0T \u223c1.4T Webpages, Code, Wikipedia,\nBooks, Papers, Q&As\nPaLM-450B [ Chowdhery et al. ,2022 ] 0.78T Webpages, Books, Conversations,\nCode, Wikipedia, News\nGemma-7B [ Gemma Team ,2024 ] 6T Webpages, Mathematics, Code\nTable 2.3: Amounts of training data used in some LLMs in terms of the numb er of tokens.\nimportant for the strong performance of the resulting model s.\nA second issue is the diversity of data. We want the training d ata to cover as many types of\ndata as possible, so that the trained models can adapt to diff erent downstream tasks easily. It has\nbeen widely recognized that the quality and diversity of tra ining data both play very important\nroles in LLMs. An interesting example is that incorporating programming code into training data\nhas been found to be bene\ufb01cial for LLMs. The bene\ufb01ts are demon strated not only in enhancing the\nprogramming abilities of LLMs, but also in improving reason ing for complex problems, especially\nthose requiring COT prompting. The concept \u201cdiversity\u201d can be extended to include language\ndiversity as well. For example, many LLMs are trained on mult i-lingual data, and therefore we\ncan handle multiple languages using a single model. While th is approach shows strong abilities\nin multi-lingual and cross-lingual tasks, its performance on speci\ufb01c languages largely depends on\nthe volume and quality of the data for those languages. It has been shown in some cases to provide\npoor results for low-resource languages.\nA third issue is the bias in training data. This is not a proble m that is speci\ufb01c to LLMs but\nexists in many NLP systems. A common example is gender bias, w here LLMs show a preference\nfor one gender over another. This can partly be attributed to class imbalance in the training data,\nfor example, the term nurses is more often associated with women. In order to debias the da ta,\nit is common practice to balance the categories of different language phenomena, such as gender,\nethnicity, and dialects. The bias in data is also related to t he diversity issue mentioned above.\nFor example, since many LLMs are trained and aligned with Eng lish-centric data, they are bi-\nased towards the cultural values and perspectives prevalen t among English-speaking populations.\nIncreasing language diversity in training data can somewha t mitigate the bias.\nAnother issue with collecting large-scale data is the priva cy concern. If LLMs are trained\non data from extensive sources, this",
    "cba032a57ada2df2": " not a proble m that is speci\ufb01c to LLMs but\nexists in many NLP systems. A common example is gender bias, w here LLMs show a preference\nfor one gender over another. This can partly be attributed to class imbalance in the training data,\nfor example, the term nurses is more often associated with women. In order to debias the da ta,\nit is common practice to balance the categories of different language phenomena, such as gender,\nethnicity, and dialects. The bias in data is also related to t he diversity issue mentioned above.\nFor example, since many LLMs are trained and aligned with Eng lish-centric data, they are bi-\nased towards the cultural values and perspectives prevalen t among English-speaking populations.\nIncreasing language diversity in training data can somewha t mitigate the bias.\nAnother issue with collecting large-scale data is the priva cy concern. If LLMs are trained\non data from extensive sources, this potentially leads to ri sks regarding the exposure of sensitive\ninformation, such as intellectual property and personal da ta. This is particularly concerning given\nthe capacity of LLMs to represent patterns from the data they are trained on, which might in-\nadvertently involve memorizing and reproducing speci\ufb01c de tails. A simple approach to privacy\nprotection is to remove or anonymize sensitive information . For example, anonymization tech-\nniques can be applied to remove personally identi\ufb01able info rmation from training data to prevent\nLLMs from learning from such data. However, in practice, era sing or redacting all sensitive data\nis dif\ufb01cult. Therefore, many LLMs, particularly those laun ched for public service, typically work\nwith systems that can detect the potential exposure of sensi tive data, or are \ufb01ne-tuned to reject\n\n--- Page 65 ---\n\n58 Generative Models\ncertain requests that could lead to information leakage.\n2.2.2 Model Modi\ufb01cations\nTraining LLMs is dif\ufb01cult. A commonly encountered problem i s that the training process be-\ncomes more unstable as LLMs get bigger. For example, one need s to choose a small learning rate\nto achieve stable training with gradient descent, but this i n turn results in much longer training\ntimes. Sometimes, even when the training con\ufb01guration is ca refully designed, training may di-\nverge at certain points during optimization. The training o f LLMs is generally in\ufb02uenced by many\nfactors, such as parameter initialization, batching, and r egularization. Here, we focus on common\nmodi\ufb01cations and improvements to the standard Transformer architecture, which are considered\nimportant in developing trainable LLMs.\n2.2.2.1 Layer Normalization with Residual Connections\nLayer normalization is used to stabilize training for deep n eural networks. It is a process of\nsubtracting the mean and dividing by the standard deviation . By normalizing layer output in\nthis way, we can effectively reduce the covariate shift prob lem and improve the training stability.\nIn Transformers, layer normalization is typically used tog ether with residual connections. As\ndescribed in Section 2.1.1 , a sub-layer can be based on either the post-norm architectu re, in which\nlayer normalization is performed right after a residual blo ck, or the pre-norm architecture, in\nwhich layer normalization is performed inside a residual bl ock. While both of these architectures\nare widely used in Transformer-based systems [ Wang et al. ,2019 ], the pre-norm architecture has\nproven to be especially useful in training deep Transformer s. Given this, most LLMs are based on\nthe pre-norm architecture, expressed as output = LNorm( F(input)) + input .\nA widely-used form of the layer normalization function is gi ven by\nLNorm( h) =\u03b1\u00b7h\u2212\u00b5\n\u03c3+\u01eb+\u03b2 (2.23)\nwhere his ad-dimensional real-valued vector, \u00b5is the mean of all the entries of h, and\u03c3is the\ncorresponding standard deviation. \u01ebis introduced for the sake of numerical stability. \u03b1\u2208Rdand\n\u03b2\u2208Rdare the gain and bias terms.\nA variant of layer normalization, called root mean square (R MS) layer normalization, only\nre-scales the input vector but does not re-center it [ Zhang and Sennrich ,2019 ]. The RMS layer\nnormalization function is given by\nLNorm( h) =\u03b1\u00b7h\n\u03c3rms+\u01eb+\u03b2 (2.24)\nwhere\u03c3rmsis the root mean square of h, that is,\u03c3rms= (1\nd\u2211d\nk",
    "1c9f40c6fb5ea6b5": " .\nA widely-used form of the layer normalization function is gi ven by\nLNorm( h) =\u03b1\u00b7h\u2212\u00b5\n\u03c3+\u01eb+\u03b2 (2.23)\nwhere his ad-dimensional real-valued vector, \u00b5is the mean of all the entries of h, and\u03c3is the\ncorresponding standard deviation. \u01ebis introduced for the sake of numerical stability. \u03b1\u2208Rdand\n\u03b2\u2208Rdare the gain and bias terms.\nA variant of layer normalization, called root mean square (R MS) layer normalization, only\nre-scales the input vector but does not re-center it [ Zhang and Sennrich ,2019 ]. The RMS layer\nnormalization function is given by\nLNorm( h) =\u03b1\u00b7h\n\u03c3rms+\u01eb+\u03b2 (2.24)\nwhere\u03c3rmsis the root mean square of h, that is,\u03c3rms= (1\nd\u2211d\nk=1h2\nk)1\n2. This layer normalization\nfunction is used in LLMs like the LLaMA series.\n\n--- Page 66 ---\n\n2.2 Training at Scale 59\n2.2.2.2 Activation Functions in FFNs\nIn Transformers, FFN sub-layers are designed to introduce n on-linearities into representation\nlearning, and are found to be useful for preventing the repre sentations learned by self-attention\nfrom degeneration8[Dong et al. ,2021 ]. A standard form of the FFNs used in these sub-layers can\nbe expressed as\nFFN( h) =\u03c3(hW h+bh)Wf+bf (2.25)\nwhere Wh\u2208Rd\u00d7dh,bh\u2208Rdh,Wf\u2208Rdh\u00d7d, and bf\u2208Rdare the parameters, and dhis the\nhidden size. \u03c3(\u00b7)is the activation function of the hidden layer. A common choi ce for\u03c3(\u00b7)is the\nrecti\ufb01ed linear unit (ReLU ), given by\n\u03c3relu(h) = max(0 ,h) (2.26)\nIn practical implementations, increasing dhis helpful and thus it is often set to a larger number\nin LLMs. But a very large hidden size poses challenges for bot h training and deployment. In this\ncase, the design of the activation function plays a relative ly more important role in wide FFNs.\nThere are several alternatives to the ReLU in LLMs. One of the se is the gaussian error linear\nunit (GeLU ) which can be seen as a smoothed version of the ReLU. Rather th an controlling the\noutput by the sign of the input, the GeLU function weights its input by the percentile Pr(h\u2264h).\nHerehis ad-dimensional vector whose entries are drawn from the standa rd normal distribution\nGaussian(0,1)9. Speci\ufb01cally, the GeLU function is de\ufb01ned to be\n\u03c3gelu(h) = hPr(h\u2264h)\n=h\u03a6(h) (2.27)\nwhere \u03a6(h)is the cumulative distribution function of Gaussian(0,1), which can be implemented\nin convenient ways [ Hendrycks and Gimpel ,2016 ]. The GeLU function has been adopted in\nseveral LLMs, such as BERT, GPT-3, and BLOOM.\nAnother family of activation functions which is popular in L LMs is gated linear unit (GLU )-\nbased functions. The basic form of GLUs is given by\n\u03c3glu(h) =\u03c3(hW 1+b1)\u2299(W2+b2) (2.28)\nwhere W1\u2208Rd\u00d7d,b1\u2208Rd,W2\u2208Rd\u00d7d, and b2\u2208Rdare model parameters. Different choices\nof\u03c3(\u00b7)result in different versions of GLU functions. For example, if\u03c3(\u00b7)is de\ufb01ned to be the\nGeLU function, we will have the GeGLU function\n\u03c3geglu(h) =\u03c3gelu(hW 1+b1)\u2299(W2+b2) (2.29)\nThis activation function has been successfully applied in L LMs like Gemma.\nAs another example, consider \u03c3(\u00b7)to be the Swish function \u03c3swish(h) =h\u2299Sigmoid(ch)\n8Here degeneration refers to the phenomenon in which the rank of a matrix is reduced after some processing.\n9Pr(h\u2264h)is an informal notation. It refers to a vector, with each entr y representing the percentile for the\ncorresponding entry of h.\n\n--- Page 67 ---\n\n60 Generative Models",
    "cf3c6b69b67aa5fc": ", and b2\u2208Rdare model parameters. Different choices\nof\u03c3(\u00b7)result in different versions of GLU functions. For example, if\u03c3(\u00b7)is de\ufb01ned to be the\nGeLU function, we will have the GeGLU function\n\u03c3geglu(h) =\u03c3gelu(hW 1+b1)\u2299(W2+b2) (2.29)\nThis activation function has been successfully applied in L LMs like Gemma.\nAs another example, consider \u03c3(\u00b7)to be the Swish function \u03c3swish(h) =h\u2299Sigmoid(ch)\n8Here degeneration refers to the phenomenon in which the rank of a matrix is reduced after some processing.\n9Pr(h\u2264h)is an informal notation. It refers to a vector, with each entr y representing the percentile for the\ncorresponding entry of h.\n\n--- Page 67 ---\n\n60 Generative Models\n[Ramachandran et al. ,2017 ]. Then, the SwiGLU function is given by\n\u03c3swiglu (h) =\u03c3swish(hW 1+b1)\u2299(W2+b2) (2.30)\nBoth the PaLM and LLaMA series are based on the SwiGLU functio n. For more discussions of\nGLUs, the reader can refer to Shazeer [2020 ]\u2019s work.\n2.2.2.3 Removing Bias Terms\nAnother popular model design is to remove the bias terms in af \ufb01ne transformations used in LLMs.\nThis treatment can be applied to layer normalization, trans formations of the inputs to QKV atten-\ntion, and FFNs. For example, we can modify Eq. ( 2.25) to obtain an FFN with no bias terms\nFFN( h) =\u03c3(hW h)Wf (2.31)\nChowdhery et al. [2022 ] report that removing bias terms helps improve the training stability\nof LLMs. This method has been used in several recent LLMs, suc h as LLaMA and Gemma.\n2.2.2.4 Other Issues\nMany LLMs also involve modi\ufb01cations to their positional emb edding models. For example, one\ncan replace sinusoidal positional encodings with rotary po sition embeddings so that the learned\nLLMs can handle long sequences better. These models will be d iscussed in Section 2.3.\nNote that while model modi\ufb01cations are common in training LL Ms, the stability of training\ncan be improved in many different ways. For example, increas ing the batch size as the training\nproceeds has been found to be useful for some LLMs. In general , achieving stable and ef\ufb01cient\nlarge-scale LLM training requires carefully designed setu ps, including learning schedules, opti-\nmizer choices, training parallelism, mixed precision trai ning, and so on. Some of these issues are\nhighly engineered, and therefore, we typically need a numbe r of training runs to obtain satisfactory\nLLMs.\n2.2.3 Distributed Training\nTraining LLMs requires signi\ufb01cant amounts of computationa l resources. A common approach to\nimproving training ef\ufb01ciency is to use large-scale distrib uted systems. Fortunately, alongside the\nrise of neural networks in AI, deep learning-oriented softw are and hardware have been developed,\nmaking it easier to implement LLMs and perform computations . For example, one can now easily\n\ufb01ne-tune an LLM using deep learning software frameworks and a machine with multiple GPUs.\nHowever, scaling up the training of LLMs is still challengin g, and requires signi\ufb01cant efforts in\ndeveloping hardware and software systems for stable and ef\ufb01 cient distributed training.\nAn important consideration of distributed training is para llelism. There are several forms\nof parallelism: data parallelism, model parallelism, tens or parallelism, and pipeline parallelism.\nDespite different ways to distribute computations across d evices, these parallelism methods are\nbased on a similar idea: the training problem can be divided i nto smaller tasks that can be ex-\necuted simultaneously. The issue of parallelism in trainin g LLMs has been extensively studied\n\n--- Page 68 ---\n\n2.2 Training at Scale 61\n[Narayanan et al. ,2021 ;Fedus et al. ,2022 ]. Here we sketch the basic concepts.\n\u2022Data Parallelism . This method is one of the most widely used parallelism metho ds for\ntraining neural networks. To illustrate, consider the simp lest case where the standard delta\nrule is used in",
    "3ca0b9e6531168d4": " in\ndeveloping hardware and software systems for stable and ef\ufb01 cient distributed training.\nAn important consideration of distributed training is para llelism. There are several forms\nof parallelism: data parallelism, model parallelism, tens or parallelism, and pipeline parallelism.\nDespite different ways to distribute computations across d evices, these parallelism methods are\nbased on a similar idea: the training problem can be divided i nto smaller tasks that can be ex-\necuted simultaneously. The issue of parallelism in trainin g LLMs has been extensively studied\n\n--- Page 68 ---\n\n2.2 Training at Scale 61\n[Narayanan et al. ,2021 ;Fedus et al. ,2022 ]. Here we sketch the basic concepts.\n\u2022Data Parallelism . This method is one of the most widely used parallelism metho ds for\ntraining neural networks. To illustrate, consider the simp lest case where the standard delta\nrule is used in gradient descent\n\u03b8t+1=\u03b8t\u2212lr\u00b7\u2202L\u03b8t(Dmini)\n\u2202\u03b8t(2.32)\nwhere the new parameters \u03b8t+1is obtained by updating the latest parameters \u03b8twith a small\nsteplrin the direction of the negative loss gradient.\u2202L\u03b8t(Dmini)\n\u2202\u03b8tis the gradient of the loss\nwith respect to the parameters \u03b8t, and is computed on a minibatch of training sample Dmini.\nIn data parallelism, we divide Dmini intoNsmaller batches, denoted by {D1,...,DN}.\nThen, we distribute these batches to Nworkers, each with a corresponding batch. Once\nthe data is distributed, these workers can work at the same ti me. The gradient of the entire\nminibatch is obtained by aggregating the gradients compute d by the workers, like this\n\u2202L\u03b8t(Dmini)\n\u2202\u03b8t=\u2202L\u03b8t(D1)\n\u2202\u03b8t\ued19\ued18\ued17\ued1a\nworker 1+\u2202L\u03b8t(D2)\n\u2202\u03b8t\ued19\ued18\ued17\ued1a\nworker 2+\u00b7 \u00b7 \u00b7+\u2202L\u03b8t(DN)\n\u2202\u03b8t\ued19\ued18\ued17\ued1a\nworker N(2.33)\nIn ideal cases where the workers coordinate well and the comm unication overhead is small,\ndata parallelism can achieve nearly an N-fold speed-up for training.\n\u2022Model Parallelism . Although data parallelism is simple and effective, it requ ires each\nworker to run the entire LLM and perform the complete forward and backward process.\nAs LLMs grow larger, it sometimes becomes unfeasible to load and execute an LLM on a\nsingle device. In this case, we can decouple the LLM into smal ler components and run these\ncomponents on different devices. One simple way to do this is to group consecutive layers\nin the layer stack and assign each group to a worker. The worke rs operate in the order of\nthe layers in the stack, that is, in the forward pass we proces s the input from lower-level to\nupper-level layers, and in the backward pass we propagate th e error gradients from upper-\nlevel to lower-level layers. Consider, for example, a Trans former decoder with Lstacked\nblocks. To distribute the computation load, each block is as signed to a worker. See the\nfollowing illustration for a single run of the forward and ba ckward passes of this model.\nWorkerL BL(\u2191)BL(\u2193)\n... ... ...\nWorker 2 B2(\u2191) B2(\u2193)\nWorker 1B1(\u2191) B1(\u2193)\nHere Bldenotes the computation of block l, and the symbols \u2191and\u2193denote the forward and\nbackward passes, respectively. Note that this parallelism method forces the workers to run\nin sequence, so a worker has to wait for the previous worker to \ufb01nish their job. This results\nin the devices being idle for most of the time. In practical sy stems, model parallelism is\ngenerally used together with other parallelism mechanisms to maximize the use of devices.\n\n--- Page 69 ---\n\n62 Generative Models\n\u2022Tensor Parallelism . Parallelism can also be performed in a single computation s tep. A\ncommon example is splitting a large parameter matrix into ch unks, multiplying an input\ntensor with each of these chunks separately, and then concat enating the results of these\nmultiplications to form the output. For example, consider t he multiplication of the repre-\nsentation h\u2208Rdwith the parameter matrix Wh\u2208Rd\u00d7dhin an FFN sub-layer (see Eq.\n(2",
    "e491e7bd4b92e75a": "and\u2193denote the forward and\nbackward passes, respectively. Note that this parallelism method forces the workers to run\nin sequence, so a worker has to wait for the previous worker to \ufb01nish their job. This results\nin the devices being idle for most of the time. In practical sy stems, model parallelism is\ngenerally used together with other parallelism mechanisms to maximize the use of devices.\n\n--- Page 69 ---\n\n62 Generative Models\n\u2022Tensor Parallelism . Parallelism can also be performed in a single computation s tep. A\ncommon example is splitting a large parameter matrix into ch unks, multiplying an input\ntensor with each of these chunks separately, and then concat enating the results of these\nmultiplications to form the output. For example, consider t he multiplication of the repre-\nsentation h\u2208Rdwith the parameter matrix Wh\u2208Rd\u00d7dhin an FFN sub-layer (see Eq.\n(2.25)). We can slice the matrix Wh\u2208Rd\u00d7dhvertically to a sequence of Msub-matrices\nWh=[\nW1\nhW2\nh...WM\nh]\n(2.34)\nwhere each sub-matrix Wk\nhhas a shape of d\u00d7dh\nM. The multiplication of hwithWhcan be\nexpressed as\nhW h=h[\nW1\nhW2\nh...WM\nh]\n=[\nhW1\nhhW2\nh...hWM\nh]\n(2.35)\nWe can perform matrix multiplications {hW1\nh,hW2\nh,...,hWM\nh}onMdevices separately.\nAs a result, we distribute a large matrix multiplication acr oss multiple devices, each of\nwhich may have relatively small memory. From the perspectiv e of the design of modern\nGPUs, tensor parallelism over GPUs provides a two-level, ti le-based approach to parallel\ncomputing. First, at a higher level, we decompose a matrix mu ltiplication into sub-matrix\nmultiplications that can directly \ufb01t into the memory of GPUs . Then, at a lower level, we\nexecute these sub-matrix multiplications on GPUs using til e-based parallel algorithms that\nare speci\ufb01cally optimized for GPUs.\n\u2022Pipeline Parallelism . Above, in model parallelism, we have described a simple app roach\nto spreading groups of model components across multiple dev ices. But this method is in-\nef\ufb01cient because only one device is activated at a time durin g processing. Pipeline par-\nallelism addresses this issue by introducing overlaps betw een computations on different\ndevices [ Harlap et al. ,2018 ;Huang et al. ,2019 ]. To do this, a batch of samples is divided\ninto a number of micro-batches, and then these micro-batche s are processed by each worker\nas usual. Once a micro-batch is processed by a worker and pass ed to the next one, the\nfollowing micro-batch immediately occupies the same worke r. In other words, we create\na pipeline in which different computation steps can overlap if multiple jobs are given to\nthe pipeline. The following shows an illustration of pipeli ne parallelism for processing 3\nmicro-batches.\nWorkerL BL,1BL,2BL,3BL,1BL,2BL,3\n... ... ...\nWorker 2 B2,1B2,2B2,3 B2,1B2,2B2,3\nWorker 1B1,1B1,2B1,3 B1,1B1,2B1,3\nHere Bl,krepresents the processing of the k-th micro-batch by the l-th worker. Ideally we\nwould like to maximize the number of micro-batches, and thus minimize the idle time of the\n\n--- Page 70 ---\n\n2.2 Training at Scale 63\nworkers. However, in practice, using small micro-batches o ften reduces GPU utilization and\nincreases task-switching costs. This may, in turn, decreas e the overall system throughput.\nThe ultimate goal of parallel processing is to achieve linea r growth in ef\ufb01ciency, that is, the\nnumber of samples that can be processed per unit of time incre ases linearly with the number of\ndevices. However, distributed training is complicated, an d in\ufb02uenced by many factors in addition\nto the parallelism method we choose. One problem, which is of ten associated with distributed\nsystems, is the cost of communication. We can think of a distr ibuted system as a group of net-\nworked nodes. Each of these nodes can perform local computat ion or pass data to other nodes. If\n",
    "6014c03584f2973d": "-batches, and thus minimize the idle time of the\n\n--- Page 70 ---\n\n2.2 Training at Scale 63\nworkers. However, in practice, using small micro-batches o ften reduces GPU utilization and\nincreases task-switching costs. This may, in turn, decreas e the overall system throughput.\nThe ultimate goal of parallel processing is to achieve linea r growth in ef\ufb01ciency, that is, the\nnumber of samples that can be processed per unit of time incre ases linearly with the number of\ndevices. However, distributed training is complicated, an d in\ufb02uenced by many factors in addition\nto the parallelism method we choose. One problem, which is of ten associated with distributed\nsystems, is the cost of communication. We can think of a distr ibuted system as a group of net-\nworked nodes. Each of these nodes can perform local computat ion or pass data to other nodes. If\nthere are a large number of such nodes, it will be expensive to distribute and collect data across\nthem. Sometimes, the time savings brought about by parallel ism are offset by the communica-\ntion overhead of a large network. Another problem with large -scale distributed systems is that\nthe synchronization of nodes introduces additional costs. As is often the case, some nodes may\ntake longer to work, causing others to wait for the slowest on es. While we can use asynchronous\ntraining to handle heterogeneity in computational resourc es, this may lead to stale gradients and\nnon-guaranteed convergence. Moreover, as more nodes are ad ded to the network, there is more\nchance to have crashed nodes during training. In this case, w e need to ensure that the whole\nsystem is fault tolerant. In many practical settings, to inc rease scalability, one needs to take into\naccount additional issues, including architecture design , data transfer and computation overlap,\nload balancing, memory bandwidth and so on.\nTraining LLMs is so computationally expensive that, even th ough distributed training is al-\nready in use, researchers and engineers often still employ v arious model compression and speed-\nup methods to improve training ef\ufb01ciency [ Weng ,2021 ]. One example is mixed precision training,\nin which low precision data (such as FP16 and FP8 data) is used for gradient computation on each\nindividual node, and single or double precision data (such a s FP32/FP64 data) is used for updating\nthe model [ Micikevicius et al. ,2018 ]. A key operation in this approach is gradient accumulation\nwhere gradients need to be accumulated and synchronized acr oss nodes. However, due to the\nnon-associativity of \ufb02oating-point addition, this can lea d to slight numerical differences in accu-\nmulated gradients on different nodes, which may affect mode l convergence and \ufb01nal performance.\nThis problem is more obvious if there are a large number of nod es involved in distributed training,\nespecially given that low-precision numerical computatio ns may encounter over\ufb02ow and under-\n\ufb02ow issues, as well as inconsistencies across different har dware devices. Therefore, the design of\ndistributed systems needs to consider these numerical comp utation issues to ensure satisfactory\nresults and convergence.\n2.2.4 Scaling Laws\nThe success of LLMs reveals that training larger language mo dels using more resources can lead\nto improved model performance. Researchers have explained this as scaling laws of LLMs. More\nspeci\ufb01cally, scaling laws describe the relationships betw een the performance of LLMs and the\nattributes of LLM training, such as the model size, the amoun t of computation used for training,\nand the amount of training data. For example, Hestness et al. [2017 ] show that the performance of\ndeep neural networks is a power-law-like function of the tra ining data size. In the beginning, when\nthe amount of training data is not large, the performance of t he model improves slowly. Afterward,\nwhen more training data is used, the model enters a phase of ra pid performance improvement, and\nthe performance curve resembles a power-law curve. Ultimat ely, the improvement in performance\n\n--- Page 71 ---\n\n64 Generative Models\nSlow Reduction\nPhasePower-law Reduction\nPhaseConvergence\nPhase\n(Irreducible Error)\nTraining Dataset Size (Log-scale)Number of Test Errors (Log-scale)\nFig. 2.3: A scaling law of test error against a variable of interest (e. g., training dataset size) [ Hestness et al. ,2017 ]. The\ncurve of the scaling law can be divided into three phases. At t he beginning, the number of test errors decreases slowly\n",
    "09036916078608c3": " show that the performance of\ndeep neural networks is a power-law-like function of the tra ining data size. In the beginning, when\nthe amount of training data is not large, the performance of t he model improves slowly. Afterward,\nwhen more training data is used, the model enters a phase of ra pid performance improvement, and\nthe performance curve resembles a power-law curve. Ultimat ely, the improvement in performance\n\n--- Page 71 ---\n\n64 Generative Models\nSlow Reduction\nPhasePower-law Reduction\nPhaseConvergence\nPhase\n(Irreducible Error)\nTraining Dataset Size (Log-scale)Number of Test Errors (Log-scale)\nFig. 2.3: A scaling law of test error against a variable of interest (e. g., training dataset size) [ Hestness et al. ,2017 ]. The\ncurve of the scaling law can be divided into three phases. At t he beginning, the number of test errors decreases slowly\nwhen more training data is used, but this only lasts for a shor t period. In the second phase, the number of test errors\ndecreases drastically, and the curve becomes a power law cur ve. After that, the error reduction slows down again in the\nthird phase. Note that there are irreducible errors that can not be eliminated, regardless of the amount of training data .\nbecomes slow again, and more data does not lead to signi\ufb01cant gains. Figure 2.3shows an example\nof such curves.\nIn NLP, a traditional view holds that the performance gains w ill disappear at a certain point\nas the training is scaled up. However, recent results show th at, if we consider the problem on\na larger scale, scaling up training is still a very effective method for obtaining stronger LLMs.\nFor example, both closed-source and open-source LLMs can be ne\ufb01t from more data, even though\ntrillions of tokens have already been used for training.\nWith the increase in the scale of model training, LLMs exhibi t new capabilities, known as the\nemergent abilities of LLMs. For example, Wei et al. [2022b ] studied the scaling properties of\nLLMs across different model sizes and amounts of computatio nal resources. Their work shows\nthat some abilities emerge when we scale the model size to cer tain level. The appearance of\nemergent abilities has demonstrated the role of scaled trai ning in enhancing the performance of\nLLMs, and it has also, to some extent, motivated researchers to continuously attempt to train larger\nmodels. As larger and stronger LMs continue to appear, our un derstanding of the scaling laws\ncontinues to mature. This helps researchers predict the per formance of LLMs during training and\nestimate the minimal computational resources required to a chieve a given level of performance.\nTo understand how model performance scales with various fac tors considered during training,\nit is common to express the model performance as a function of these factors. For example, in\nthe simplest case, we can express the loss or error of an LLM as a function of a single variable of\ninterest. However, there are no universal scaling laws that can describe this relationship. Instead,\ndifferent functions are proposed to \ufb01t the learning curves o f LLMs.\nLetxbe the variable of interest (such as the number of model param eters) and L(x)be the\nloss of the model given x(such as the cross-entropy loss on test data). The simplest f orm of L(x)\nis a power law\nL(x) =axb(2.36)\n\n--- Page 72 ---\n\n2.2 Training at Scale 65\n1051071092.43.24.04.85.6\nNumber of ParametersTest LossL(N) = (N\n8.8\u00b71013)\u22120.076\n1081092.733.33.63.94.2\nDataset SizeTest LossL(D) = (D\n5.4\u00b71013)\u22120.095\nFig. 2.4: Test loss against model size ( N) and training dataset size ( D) (data points are plotted for illustrative purposes).\nWe plot test loss as a function of N, which is de\ufb01ned as L(N) =(N\n8.8\u00d71013)\u22120.076, and a function of D, which is\nde\ufb01ned as L(D) =(D\n5.4\u00d71013)\u22120.095[Kaplan et al. ,2020 ].\nwhereaandbare parameters that are estimated empirically. Despite its simplicity, this func-\ntion has successfully interpreted the scaling ability of la nguage models and machine transla-\ntion systems in terms of model size (den",
    "763b2f3380ad8dda": "\u00b71013)\u22120.076\n1081092.733.33.63.94.2\nDataset SizeTest LossL(D) = (D\n5.4\u00b71013)\u22120.095\nFig. 2.4: Test loss against model size ( N) and training dataset size ( D) (data points are plotted for illustrative purposes).\nWe plot test loss as a function of N, which is de\ufb01ned as L(N) =(N\n8.8\u00d71013)\u22120.076, and a function of D, which is\nde\ufb01ned as L(D) =(D\n5.4\u00d71013)\u22120.095[Kaplan et al. ,2020 ].\nwhereaandbare parameters that are estimated empirically. Despite its simplicity, this func-\ntion has successfully interpreted the scaling ability of la nguage models and machine transla-\ntion systems in terms of model size (denoted by N) and training dataset size (denoted by D)\n[Gordon et al. ,2021 ;Hestness et al. ,2017 ]. For example, Kaplan et al. [2020 ] found that the per-\nformance of their language model improves as a power law of ei therNorDafter an initial\ntransient period, and expressed these relationships using L(N) =(N\n8.8\u00d71013)\u22120.076andL(D) =\n(D\n5.4\u00d71013)\u22120.095(see Figure 2.4).\nAn improvement to this scaling law is to add an irreducible error term to the power law. The\nform of L(x)is then given by\nL(x) =axb+\u01eb\u221e (2.37)\nwhere\u01eb\u221eis the irreducible error that accounts for the error due to un known variables, which is\npresent even as x\u2192 \u221e . Eq. ( 2.37) is one of the most widely used forms for designing scaling\nlaws of LLMs. For example, Rosenfeld et al. [2020 ] developed a scaling law that involves both\nmodel scaling and dataset scaling, like this\nL(N,D ) =aNb+cDd+\u01eb\u221e (2.38)\nAn example of such formulation is the Chinchilla scaling law . It states that the test loss per\ntoken is the sum of the inverse proportion functions of NandD, with an additional irreducible\nerror term. Hoffmann et al. [2022 ] express this scaling law as\nL(N,D ) =406.4\nN0.34\ued19\ued18\ued17\ued1a\nmodel scaling+410.7\nD0.28\ued19\ued18\ued17\ued1a\ndataset scaling+ 1.69\ued19\ued18\ued17\ued1a\nirreducible error(2.39)\nAll the scaling laws mentioned above are based on monotonic f unctions. So they cannot cover\nfunctions with in\ufb02ection points, such as double descent cur ves. In response, researchers have\nexplored more sophisticated functions to \ufb01t the learning cu rves. Examples of such functions can\n\n--- Page 73 ---\n\n66 Generative Models\nbe found in Alabdulmohsin et al. [2022 ] and Caballero et al. [2023 ]\u2019s work.\nThe signi\ufb01cance of scaling laws lies in providing direction al guidance for LLM research: if\nwe are still in the region of the power law curve, using more re sources to train larger models is a\nvery promising direction. While this result \u201cforces\u201d big re search groups and companies to invest\nmore in computational resources to train larger models, whi ch is very expensive, scaling laws\ncontinuously push the boundaries of AI further away. On the o ther hand, understanding scaling\nlaws helps researchers make decisions in training LLMs. For example, given the computational\nresources at hand, the performance of LLMs may be predicted.\nOne last note on scaling laws in this section. For LLMs, a lowe r test loss does not always\nimply better performance on all downstream tasks. To adapt L LMs, there are several steps such\nas \ufb01ne-tuning and prompting that may in\ufb02uence the \ufb01nal resul t. Therefore, the scaling laws for\ndifferent downstream tasks might be different in practice.\n2.3 Long Sequence Modeling\nWe have already seen that, in large-scale training, larger l anguage models can be developed by us-\ning more data and computational resources. However, scalin g up can also occur in other directions.\nFor instance, in many applications, LLMs are",
    "551e1db0d73911c4": " laws\ncontinuously push the boundaries of AI further away. On the o ther hand, understanding scaling\nlaws helps researchers make decisions in training LLMs. For example, given the computational\nresources at hand, the performance of LLMs may be predicted.\nOne last note on scaling laws in this section. For LLMs, a lowe r test loss does not always\nimply better performance on all downstream tasks. To adapt L LMs, there are several steps such\nas \ufb01ne-tuning and prompting that may in\ufb02uence the \ufb01nal resul t. Therefore, the scaling laws for\ndifferent downstream tasks might be different in practice.\n2.3 Long Sequence Modeling\nWe have already seen that, in large-scale training, larger l anguage models can be developed by us-\ning more data and computational resources. However, scalin g up can also occur in other directions.\nFor instance, in many applications, LLMs are adapted to proc ess signi\ufb01cantly long sequences. An\ninteresting example is that we pre-train an LLM on extensive texts of normal length and then ap-\nply it to deal with very long token sequences, far beyond the l ength encountered in pre-training.\nHere we use Pr(y|x)to denote the text generation probability where xis the context and yis the\ngenerated text. There are broadly three types of long sequen ce modeling problems.\n\u2022Text generation based on long context (i.e., xis a long sequence). For example, we\ngenerate a short summary for a very long text.\n\u2022Long text generation (i.e., yis a long sequence). For example, we generate a long story\nbased on a few keywords.\n\u2022Long text generation based on long context (i.e., both xandyare long sequences). For\nexample, we translate a long document from Chinese to Englis h.\nRecently, NLP researchers have been more interested in appl ying and evaluating LLMs on\ntasks where extremely long input texts are involved. Imagin e an LLM, which reads a C++ source\n\ufb01le containing tens of thousands of lines, and outlines the f unctionality of the program correspond-\ning to the source \ufb01le. Such models, capable of handling exten sive textual contexts, are sometimes\ncalled long-context LLMs . In this section we will restrict ourselves to long-context LLMs, but\nthe methods discussed here can be applicable to other proble ms.\nFor Transformers, dealing with long sequences is computati onally expensive, as the computa-\ntional cost of self-attention grows quadratically with the sequence length. This makes it infeasible\nto train and deploy such models for very long inputs. Two stra nds of research have tried to adapt\nTransformers to long-context language modeling.\n\u2022 The \ufb01rst explores ef\ufb01cient training methods and model arch itectures to learn self-attention\nmodels from long-sequence data.\n\n--- Page 74 ---\n\n2.3 Long Sequence Modeling 67\n\u2022 The other adapts pre-trained LLMs to handle long sequences with modest or no \ufb01ne-tuning\nefforts.\nHere, we will discuss the former brie\ufb02y since it can be found i n general discussions of ef\ufb01cient\nTransformer architectures [ Tay et al. ,2020 ;Xiao and Zhu ,2023 ]. We will focus on the latter,\nhighlighting popular methods in recent LLMs. We will also di scuss the strengths and limitations\nof these long-sequence models.\n2.3.1 Optimization from HPC Perspectives\nWe begin our discussion by considering improvements to stan dard Transformer models from the\nperspectives of high-performance computing. Most of these improvements, though not speci\ufb01-\ncally designed for LLMs, have been widely applied across var ious deep learning models [ Kim et al. ,\n2023 ]. A commonly used approach is to adopt a low-precision imple mentation of Transformers.\nFor example, we can use 8-bit or 16-bit \ufb01xed-point data types for arithmetic operations, instead\nof 32-bit or 64-bit \ufb02oating-point data types. Using these lo w-precision data types can increase\nthe ef\ufb01ciency and memory throughput, so that longer sequenc es can be processed more easily.\nAn alternative approach is to improve Transformers by using hardware-aware techniques. For\nexample, on modern GPUs, the ef\ufb01ciency of Transformers can b e improved by using IO-aware\nimplementations of the self-attention function [ Dao et al. ,2022 ;Kwon et al. ,2023 ].\nAnother way to handle long sequences is through sequence par allelism [ Li et al. ,2023b ;\nKorth",
    "7e50577222aac10b": " applied across var ious deep learning models [ Kim et al. ,\n2023 ]. A commonly used approach is to adopt a low-precision imple mentation of Transformers.\nFor example, we can use 8-bit or 16-bit \ufb01xed-point data types for arithmetic operations, instead\nof 32-bit or 64-bit \ufb02oating-point data types. Using these lo w-precision data types can increase\nthe ef\ufb01ciency and memory throughput, so that longer sequenc es can be processed more easily.\nAn alternative approach is to improve Transformers by using hardware-aware techniques. For\nexample, on modern GPUs, the ef\ufb01ciency of Transformers can b e improved by using IO-aware\nimplementations of the self-attention function [ Dao et al. ,2022 ;Kwon et al. ,2023 ].\nAnother way to handle long sequences is through sequence par allelism [ Li et al. ,2023b ;\nKorthikanti et al. ,2023 ]. Speci\ufb01cally, consider the general problem of attending t he query qi\nat the position ito the keys Kand values V. We can divide Kby rows and obtain a set of sub-\nmatrices {K[1],...,K[nu]}, each corresponding to a segment of the sequence. Similarly , we can\nobtain the sub-matrices of V, denoted by {V[1],...,V[nu]}. Then, we assign each pair of K[u]and\nV[u]to a computing node (e.g., a GPU of a GPU cluster). The assigne d nodes can run in parallel,\nthereby parallelizing the attention operation.\nRecall that the output of the self-attention model can be wri tten as\nAttqkv(qi,K,V) =m\u22121\u2211\nj=0\u03b1i,jvj (2.40)\nwhere\u03b1i,jis the attention weight between positions iandj. In Transformers, \u03b1i,jis obtained\nby normalizing the rescaled version of the dot product betwe enqiandkj. Let\u03b2i,jdenote the\nattention score between qiandkj. We have\n\u03b2i,j=qi\u00b7kj\u221a\nd+ Mask(i,j) (2.41)\nwhere Mask(i,j)is the masking variable for (i,j). Then, we de\ufb01ne the attention weight \u03b1i,jto\nbe\n\u03b1i,j= Softmax( \u03b2i,j)\n=exp(\u03b2i,j)\u2211\nj\u2032exp(\u03b2i,j\u2032)(2.42)\n\n--- Page 75 ---\n\n68 Generative Models\nOn each computing node, we need to implement these equations . Given the keys and values\nassigned to this node, computing the numerator of the right- hand side of Eq. ( 2.42) (i.e., exp(\u03b2i,j))\nis straightforward, as all the required information is stor ed on the node. However, computing the\ndenominator of the right-hand side of Eq. ( 2.42) involves a sum of exp(\u03b2i,j\u2032)over allj\u2032s, which\nrequires transferring data to and from other nodes. To illus trate, suppose that vjandkjare placed\non nodeu. We can rewrite Eq. ( 2.42) as\n\u03b1i,j\n=node u\ued17\ued1a\ued19\ued18\nexp(\u03b2i,j)\u2211\nkj\u2032\u2208K[1]exp(\u03b2i,j\u2032)\n\ued19\ued18\ued17\ued1a\nnode 1+\u00b7 \u00b7 \u00b7+\u2211\nkj\u2032\u2208K[u]exp(\u03b2i,j\u2032)\n\ued19\ued18\ued17\ued1a\nnode u+\u00b7 \u00b7 \u00b7+\u2211\nkj\u2032\u2208K[nu]exp(\u03b2i,j\u2032)\n\ued19\ued18\ued17\ued1a\nnode nu(2.43)\nwhere the notation kj\u2032\u2208K[u]represents that kj\u2032is a row vector of K[u]. In a straightforward\nimplementation, we \ufb01rst perform the summations {\u2211\nkj\u2032\u2208K[u]exp(\u03b2i,j\u2032)}separately on the corre-\nsponding nodes. Then, we collect these summation results fr om different nodes to combine them\ninto a \ufb01nal result. This corresponds to a collective operati on in the context of parallel processing.\nThere are many ef\ufb01cient implementations of such operations , such as the all-reduce algorithms.\nHence the sum of all exp(\u03b2i,j)values can be computed using optimized routines in collecti ve\ncommunication toolkits.\nGiven the attention weights {\u03b1i,j},",
    "0f5b1ae9206c1d0a": "\u2211\nkj\u2032\u2208K[nu]exp(\u03b2i,j\u2032)\n\ued19\ued18\ued17\ued1a\nnode nu(2.43)\nwhere the notation kj\u2032\u2208K[u]represents that kj\u2032is a row vector of K[u]. In a straightforward\nimplementation, we \ufb01rst perform the summations {\u2211\nkj\u2032\u2208K[u]exp(\u03b2i,j\u2032)}separately on the corre-\nsponding nodes. Then, we collect these summation results fr om different nodes to combine them\ninto a \ufb01nal result. This corresponds to a collective operati on in the context of parallel processing.\nThere are many ef\ufb01cient implementations of such operations , such as the all-reduce algorithms.\nHence the sum of all exp(\u03b2i,j)values can be computed using optimized routines in collecti ve\ncommunication toolkits.\nGiven the attention weights {\u03b1i,j}, we then compute the attention results using Eq. ( 2.40).\nThe problem can be re-expressed as\nAttqkv(qi,K,V)\n=\u2211\nvj\u2032\u2208V[1]\u03b1i,j\u2032vj\u2032\n\ued19\ued18\ued17\ued1a\nnode 1+\u00b7 \u00b7 \u00b7+\u2211\nvj\u2032\u2208V[u]\u03b1i,j\u2032vj\u2032\n\ued19\ued18\ued17\ued1a\nnode u+\u00b7 \u00b7 \u00b7+\u2211\nvj\u2032\u2208V[nu]\u03b1i,j\u2032vj\u2032\n\ued19\ued18\ued17\ued1a\nnode nu(2.44)\nLike Eq. ( 2.43), Eq. ( 2.44) can be implemented as a summation program in parallel proce ss-\ning. First, perform the weighted summations of values on dif ferent nodes simultaneously. Then,\nwe collect the results from these nodes via collective opera tions.\nNote that, although this section primarily focuses on long s equence modeling, much of the mo-\ntivation for sequence parallelism comes from the distribut ed training methods of deep networks,\nas discussed in Section 2.2.3 . As a result, the implementation of these methods can be base d on\nthe same parallel processing library.\n2.3.2 Ef\ufb01cient Architectures\nOne dif\ufb01culty of applying Transformers to long sequences is that self-attention has a quadratic\ntime complexity with respect to the sequence length. Moreov er, a key-value cache (orKV cache\nfor short) is maintained during inference, and its size incr eases as more tokens are processed. Al-\nthough the KV cache grows linearly with the sequence length, for extremely long input sequences,\nthe memory footprint becomes signi\ufb01cant and it is even infea sible to deploy LLMs for such tasks.\nAs a result, the model architecture of long-context LLMs gen erally moves away from the standard\n\n--- Page 76 ---\n\n2.3 Long Sequence Modeling 69\nTransformer, turning instead to the development of more ef\ufb01 cient variants and alternatives.\nOne approach is to use sparse attention instead of standard s elf-attention. This family of\nmodels is based on the idea that only a small number of tokens a re considered important when\nattending to a given token, and so most of the attention weigh ts between tokens are close to zero.\nAs a consequence, we can prune most of the attention weights a nd represent the attention model\nin a compressed form. To illustrate, consider the self-atte ntion model\nAttqkv(Q,K,V) =\u03b1(Q,K)V (2.45)\nwhere the attention weight matrix \u03b1(Q,K)\u2208Rm\u00d7mis obtained by\n\u03b1(Q,K) = Softmax(QKT\n\u221a\nd+Mask )\n=\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\u03b10,0 0 0 ... 0\n\u03b11,0\u03b11,1 0... 0\n\u03b12,0\u03b12,1\u03b12,2... 0\n...............\n\u03b1m\u22121,0\u03b1m\u22121,1\u03b1m\u22121,2... \u03b1 m\u22121,m\u22121\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb(2.46)\nEach row vector[\n\u03b1i,0... \u03b1 i,i0...0]\ncorresponds to a distribution of attending the i-th\ntoken to every token of the sequence. Since language models p redict next tokens only based on\ntheir left-context, we normally write the output of the atte ntion model at position ias\nAtt",
    "eef68698d6e2b7e9": "d+Mask )\n=\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\u03b10,0 0 0 ... 0\n\u03b11,0\u03b11,1 0... 0\n\u03b12,0\u03b12,1\u03b12,2... 0\n...............\n\u03b1m\u22121,0\u03b1m\u22121,1\u03b1m\u22121,2... \u03b1 m\u22121,m\u22121\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb(2.46)\nEach row vector[\n\u03b1i,0... \u03b1 i,i0...0]\ncorresponds to a distribution of attending the i-th\ntoken to every token of the sequence. Since language models p redict next tokens only based on\ntheir left-context, we normally write the output of the atte ntion model at position ias\nAttqkv(qi,K\u2264i,V\u2264i) =[\n\u03b1i,0... \u03b1 i,i]\uf8ee\n\uf8ef\uf8ef\uf8f0v0\n...\nvi\uf8f9\n\uf8fa\uf8fa\uf8fb\n=i\u2211\nj=0\u03b1i,jvj (2.47)\nwhere K\u2264i=\uf8ee\n\uf8ef\uf8ef\uf8f0k0\n...\nki\uf8f9\n\uf8fa\uf8fa\uf8fbandV\u2264i=\uf8ee\n\uf8ef\uf8ef\uf8f0v0\n...\nvi\uf8f9\n\uf8fa\uf8fa\uf8fbare the keys and values up to position i.\nIn the original version of self-attention[\n\u03b1i,0... \u03b1 i,i]\nis assumed to be dense, that is, most of\nthe values are non-zero. In sparse attention, some of the ent ries of[\n\u03b1i,0... \u03b1 i,i]\nare considered\nnon-zero, and the remaining entries are simply ignored in co mputation. Suppose G\u2286 {0,...,i}is\nthe set of indices of the non-zero entries. For language mode ls, the output of the sparse attention\nmodel at position iis given by\nAttsparse (qi,K\u2264i,V\u2264i) =\u2211\nj\u2208G\u03b1\u2032\ni,jvj (2.48)\nHere {\u03b1\u2032\ni,j}are normalized over G. Hence their values are different from the original attenti on\nweights (in fact we have \u03b1\u2032\ni,j>\u03b1 i,j). The sparsity of the model is determined by how large Gis.\nSparse attention models differ in the way we de\ufb01ne G. One simple approach is to de\ufb01ne Gbased\n\n--- Page 77 ---\n\n70 Generative Models\non heuristically designed patterns. For example, a widely- used pattern involves having Gcover a\nwindow of tokens located near position i[Parmar et al. ,2018 ].\nWhile sparse attention reduces the computation through the use of sparse operations, such\nmodels still have signi\ufb01cant limitations as we must keep the entire KV cache (i.e., K\u2264iandV\u2264i)\nduring inference. If the sequence is very long, storing this cache will become highly memory-\nintensive. To address this, we can consider a different form of attention models where the KV\ncache is not explicitly retained. Linear attention is one su ch approach [ Katharopoulos et al. ,2020 ].\nIt uses a kernel function \u03c6(\u00b7)to project each query and key onto points q\u2032\ni=\u03c6(qi)andk\u2032\ni=\u03c6(ki),\nrespectively. By removing the Softmax function under such t ransformations10, the form of the\nresulting attention model is given by\nAttqkv(qi,K\u2264i,V\u2264i)\u2248Attlinear(q\u2032\ni,K\u2032\n\u2264i,V\u2264i)\n=q\u2032\ni\u00b5i\nq\u2032\ni\u03bdi(2.49)\nwhere\u00b5iand\u03bdiare variables that are computed in the recurrent forms\n\u00b5i=\u00b5i\u22121+k\u2032T\nivi (2.50)\n\u03bdi=\u03bdi\u22121+k\u2032T\ni (2.51)\n\u00b5iand\u03bdican be seen as representations of the history up to position i. A bene\ufb01t of this model is\nthat we need not keep all past queries and values. Instead onl y the latest representations \u00b5iand\n\u03bdiare used. So the computational cost of each step is a constant , and the model can be easily\nextended to deal with long sequences.\nIn fact, this sequential approach to long sequence",
    "a34dac1c314d6229": " the form of the\nresulting attention model is given by\nAttqkv(qi,K\u2264i,V\u2264i)\u2248Attlinear(q\u2032\ni,K\u2032\n\u2264i,V\u2264i)\n=q\u2032\ni\u00b5i\nq\u2032\ni\u03bdi(2.49)\nwhere\u00b5iand\u03bdiare variables that are computed in the recurrent forms\n\u00b5i=\u00b5i\u22121+k\u2032T\nivi (2.50)\n\u03bdi=\u03bdi\u22121+k\u2032T\ni (2.51)\n\u00b5iand\u03bdican be seen as representations of the history up to position i. A bene\ufb01t of this model is\nthat we need not keep all past queries and values. Instead onl y the latest representations \u00b5iand\n\u03bdiare used. So the computational cost of each step is a constant , and the model can be easily\nextended to deal with long sequences.\nIn fact, this sequential approach to long sequence modeling arises naturally when we adopt a\nviewpoint of recurrent models. Such models read one token (o r a small number of tokens) at a\ntime, update the recurrent state using these inputs, and the n discard them before the next token\narrives. The output at each step is generated based only on th e recurrent state, rather than on all the\nprevious states. The memory footprint is determined by the r ecurrent state which has a \ufb01xed size.\nRecurrent models can be used in real-time learning scenario s where data arrives in a stream and\npredictions can be made at any time step. In NLP, applying rec urrent models to language mod-\neling is one of the earliest successful attempts to learn rep resentations of sequences. Although\nTransformer has been used as the foundational architecture in LLMs, recurrent models are still\npowerful models, especially for developing ef\ufb01cient LLMs. More recently, recurrent models have\nstarted their resurgence in language modeling and have been reconsidered as a promising alterna-\ntive to Transformers [ Gu and Dao ,2023 ]. Figure 2.5shows a comparison of the models discussed\nin this subsection.\n10In the new space after this transformation, the Softmax norm alization can be transformed into the simple scaling\nnormalization.\n\n--- Page 78 ---\n\n2.3 Long Sequence Modeling 71\nqi ki ki\u22121 ki\u22122 \u00b7 \u00b7 \u00b7 k1 k0\nvi vi\u22121 vi\u22122 \u00b7 \u00b7 \u00b7 v1 v0Att qkv(qi,K\u2264i,V\u2264i)\n(a) Standard Self-attention\nqi ki ki\u22121 ki\u22122 \u00b7 \u00b7 \u00b7 k1 k0\nvi vi\u22121 vi\u22122 \u00b7 \u00b7 \u00b7 v1 v0Att qkv(qi,{k1,ki},{v1,vi})\n(b) Sparse Attention\nqi ki ki\u22121 ki\u22122 \u00b7 \u00b7 \u00b7 k1 k0\nvi vi\u22121 vi\u22122 \u00b7 \u00b7 \u00b7 v1 v0\u03bdi=\u03bdi\u22121+k\u2032T\ni\u00b5i=\u00b5i\u22121+k\u2032T\nivi\n\u03bdi\u00b5i \u21d2\n\u21d2Att linear(qi,K\u2264i,V\u2264i) =q\u2032\ni\u00b5i\nq\u2032\ni\u03bdi\n(c) Linear Attention\nhi hi\u22121 hi\u22122 hi\u22123 \u00b7 \u00b7 \u00b7 h1 h0\ninputihi=f(hi\u22121,inputi)\n(d) Recurrent Models\nFig. 2.5: Illustrations of self-attention, sparse attention, linea r attention and recurrent models. Blue boxes = cached\nstates for producing the output at position i.f(\u00b7)= a recurrent cell.\n2.3.3 Cache and Memory\nLLMs based on the standard Transformer architecture are glo bal models. The inference for these\nmodels involves storing the entire left-context in order to make predictions for future tokens. This\nrequires a KV cache where the representations (i.e., keys an d values) of all previously-generated\n\n--- Page 79 ---\n\n72 Generative Models\ntokens are kept, and the cost of caching grows as the inferenc e proceeds. Above, we have dis-\ncussed methods for optimizing this cache via ef\ufb01cient atten tion approaches, such as sparse atten-\ntion and linear attention. Another idea, which may have over lap with the previous discussion, is\nto explicitly encode the context via an additional memory mo del.\n2.3.3.1 Fixed-size KV Cache\nA straightforward approach is to represent the keys and valu es using a \ufb01xed-size memory model.\nSuppose we have a memory Mem which retains the contextual information. We can write the\nattention operation at position iin a general form\nAtt(qi,Mem) = Att qkv(qi,K\u2264i,V\u2264i) (",
    "8a627bb391ca7d83": " make predictions for future tokens. This\nrequires a KV cache where the representations (i.e., keys an d values) of all previously-generated\n\n--- Page 79 ---\n\n72 Generative Models\ntokens are kept, and the cost of caching grows as the inferenc e proceeds. Above, we have dis-\ncussed methods for optimizing this cache via ef\ufb01cient atten tion approaches, such as sparse atten-\ntion and linear attention. Another idea, which may have over lap with the previous discussion, is\nto explicitly encode the context via an additional memory mo del.\n2.3.3.1 Fixed-size KV Cache\nA straightforward approach is to represent the keys and valu es using a \ufb01xed-size memory model.\nSuppose we have a memory Mem which retains the contextual information. We can write the\nattention operation at position iin a general form\nAtt(qi,Mem) = Att qkv(qi,K\u2264i,V\u2264i) (2.52)\nIn this model, Mem is simply the KV cache, i.e., Mem = ( K\u2264i,V\u2264i). Thus the size of\nMem is determined by i. If we de\ufb01ne Mem as a \ufb01xed-size variable, then the cost of performing\nAtt(qi,Mem) will be \ufb01xed. There are several alternative ways to design Mem .\n\u2022 One of the simplest methods is to consider a \ufb01xed-size windo w of previous keys and values.\nMem is therefore given by\nMem = ( K[i\u2212nc+1,i],V[i\u2212nc+1,i]) (2.53)\nwherencdenotes the size of the window. The notation K[i\u2212nc+1,i]andV[i\u2212nc+1,i]denote\nthe keys and values over positions from i\u2212nc+ 1toi.11This model can be seen as a type\nof local attention model.\n\u2022 It is also possible to de\ufb01ne Mem as a pair of summary vectors, which leads to a more\ncompressed representation of the history. A simple way to su mmarize the previous keys\nand values is to use the moving average of them. For example, Mem can be de\ufb01ned as the\nunweighted moving average of the previous nckeys and values\nMem =(\u2211i\nj=i\u2212nc+1kj\nnc,\u2211i\nj=i\u2212nc+1vj\nnc)\n(2.54)\nAlternatively, we can use a weighted version of moving avera ge\nMem =(\u2211i\nj=i\u2212nc+1\u03b2j\u2212i+nckj\u2211nc\nj=1\u03b2j,\u2211i\nj=i\u2212nc+1\u03b2j\u2212i+ncvj\u2211nc\nj=1\u03b2j)\n(2.55)\nHere {\u03b21,...,\u03b2 nc}are the coef\ufb01cients, which can be either learned as model par ameters\nor determined via heuristics. For example, they can be set to increasing coef\ufb01cients (i.e.,\n\u03b21<\u03b2 2<...<\u03b2 nc\u22121<\u03b2 nc) in order to give larger weight to positions that are closer t o\n11More formally, we write K[i\u2212nc+1,i]=\uf8ee\n\uf8ef\uf8f0ki\u2212nc+1\n...\nki\uf8f9\n\uf8fa\uf8fbandV[i\u2212nc+1,i]=\uf8ee\n\uf8ef\uf8f0vi\u2212nc+1\n...\nvi\uf8f9\n\uf8fa\uf8fb. Sometimes we denote\nK[i\u2212nc+1,i]by{ki\u2212nc+1,...,ki}andV[i\u2212nc+1,i]by{vi\u2212nc+1,...,vi}for notation simplicity.\n\n--- Page 80 ---\n\n2.3 Long Sequence Modeling 73\ni. We can extend the moving average to include all the position s up toi. This leads to the\ncumulative average of the keys and values, given in the form\nMem =(\u2211i\nj=0kj\ni+ 1,\u2211i\nj=0vj\ni+ 1)\n(2.56)\nIn general, the cumulative average can be written using a rec ursive formula\nMem i=(ki,vi) +i\u00b7Mem i\u22121\ni+ 1(2.57)\nwhere Mem iandMem i\u22121denote the cumulative averages of the current and previous p o-\nsitions, respectively. An advantage of this model is that we only need to store a single\nkey-value pair during inference, rather than storing all th e key-value pairs. Note that the\nabove memory models are related to recurrent models, and mor e advanced techniques",
    "ee5242fa2360c695": " Page 80 ---\n\n2.3 Long Sequence Modeling 73\ni. We can extend the moving average to include all the position s up toi. This leads to the\ncumulative average of the keys and values, given in the form\nMem =(\u2211i\nj=0kj\ni+ 1,\u2211i\nj=0vj\ni+ 1)\n(2.56)\nIn general, the cumulative average can be written using a rec ursive formula\nMem i=(ki,vi) +i\u00b7Mem i\u22121\ni+ 1(2.57)\nwhere Mem iandMem i\u22121denote the cumulative averages of the current and previous p o-\nsitions, respectively. An advantage of this model is that we only need to store a single\nkey-value pair during inference, rather than storing all th e key-value pairs. Note that the\nabove memory models are related to recurrent models, and mor e advanced techniques have\nbeen used to develop alternatives to self-attention mechan isms in Transformers [ Ma et al. ,\n2023 ].\n\u2022 The memory Mem can also be a neural network. At each step, it takes both the pr evious\noutput of the memory and the current states of the model as inp ut, and produces the new\noutput of the memory. This neural network can be formulated a s the function\nMem = Update( Skv,Mem pre) (2.58)\nHere Mem andMem prerepresent the outputs of the memory at the current step and th e\nprevious step, respectively. Skvis a set of key-value pairs, representing the recent states o f\nthe model. This formulation is general and allows us to devel op various memory models by\nselecting different Update( \u00b7)andSkvcon\ufb01gurations. For example, if Skvonly contains the\nlatest key-value pair (ki,vi)andUpdate( \u00b7)is de\ufb01ned as a recurrent cell, then Eq. ( 2.58)\ncan be expressed as an RNN-like model\nMem =f((ki,vi),Mem pre) (2.59)\nwheref(\u00b7)is a recurrent cell. Recurrence can also be applied to segmen t-level modeling\nfor ef\ufb01ciency consideration. A simple approach is that we ca n divide the sequence into\nsegments, and treat Skvas a segment. Applying recurrent models to Update( \u00b7)will result in\nmemory models that operate on segments. A special example is that we de\ufb01ne Update( \u00b7)as\nan FIFO function that adds Skvinto the memory and removes the oldest key-value segment\nfrom the memory, given by\nMem = FIFO( Skv,Mem pre) (2.60)\nConsider a memory which includes two segments, one for curre nt segment, and one for the\nprevious segment. In the attention operation, each positio n can access the history key-value\npairs in two closest consecutive segments. This essentiall y de\ufb01nes a local memory, but it\nand its variants have been widely used segment-level recurr ent models [ Dai et al. ,2019 ;\nHutchins et al. ,2022 ;Bulatov et al. ,2022 ].\n\u2022 The above memory models can be extended to involve multiple memories. An example\n\n--- Page 81 ---\n\n74 Generative Models\nof this approach is compressive Transformer [ Rae et al. ,2019 ]. It employs two distinct\n\ufb01xed-size memories: one for modeling local context (denote d by Mem ), and the other for\nmodeling and compressing long-term history (denoted by CMem ). The KV cache in this\nmodel is the combination of Mem andCMem . The attention function can be written as\nAttcom(qi,Mem,CMem) = Att qkv(qi,[Mem,CMem]) (2.61)\nwhere [Mem,CMem] is a combined memory of Mem andCMem . As with other segment-\nlevel models, the compressive Transformer model operates o n segments of the sequence.\nEach segment is a sequence of nsconsecutive tokens, and we denote Sk\nkvas the key-value\npairs corresponding to the tokens of the k-th segment. When a new segment arrives, Mem\nis updated in an FIFO fashion: we append the nckey-value pairs in Sk\nkvtoMem , and then\npop thensoldest key-value pairs from Mem , which is given by\nMem = FIFO( Sk\nkv,Mem pre) (2.62)\nThe popped key-value pairs are then used to update the compre ssive memory CMem . These\nnskey-value pairs are compressed intons\nckey-value pairs via a compression network.\nCMem is an FIFO which appends the compressedns\n",
    "3f490e04b5a4915f": "(qi,[Mem,CMem]) (2.61)\nwhere [Mem,CMem] is a combined memory of Mem andCMem . As with other segment-\nlevel models, the compressive Transformer model operates o n segments of the sequence.\nEach segment is a sequence of nsconsecutive tokens, and we denote Sk\nkvas the key-value\npairs corresponding to the tokens of the k-th segment. When a new segment arrives, Mem\nis updated in an FIFO fashion: we append the nckey-value pairs in Sk\nkvtoMem , and then\npop thensoldest key-value pairs from Mem , which is given by\nMem = FIFO( Sk\nkv,Mem pre) (2.62)\nThe popped key-value pairs are then used to update the compre ssive memory CMem . These\nnskey-value pairs are compressed intons\nckey-value pairs via a compression network.\nCMem is an FIFO which appends the compressedns\nckey-value pairs to the tail of the\nqueue, and drops the \ufb01rstns\nckey-value pairs of the queue. It is given by\nCMem = FIFO( Ck\nkv,CMem pre) (2.63)\nwhereCk\nkvrepresents the set of compressed key-value pairs. Implicit in the compressive\nTransformer model is that local context should be represent ed explicitly with minimal in-\nformation loss, while long-range context can be more compre ssed.\n\u2022 We have already seen that both global and local contexts are useful and can be mod-\neled using attention models. This view motivates the extens ion to attention models for\ncombining both local and long-term memories [ Ainslie et al. ,2020 ;Zaheer et al. ,2020 ;\nGupta and Berant ,2020 ]. A simple but widely-used approach is to involve the \ufb01rst fe w to-\nkens of the sequence in attention, serving as global tokens. This approach is usually applied\nalong with other sparse attention models. An advantage of in corporating global tokens of\nthe sequence is that it helps smooth the output distribution of the Softmax function used in\nattention weight computation, and thus stabilizes model pe rformance when the context size\nis very large [ Xiao et al. ,2024 ]. One drawback, however, is that using a \ufb01xed-size global\nmemory may result in information loss. When dealing with lon g sequences, we need to\nenlarge the KV cache for suf\ufb01cient representations of the co ntext, but this in turn increases\nthe computational cost.\nFigure 2.6shows illustrations of the above approaches. Note that, whi le we focus on optimiza-\ntion of the KV cache here, this issue is closely related to tho se discussed in the previous section.\nAll of the methods we have mentioned so far can broadly be cate gorized as ef\ufb01cient attention\napproaches, which are widely used in various Transformer va riants.\n\n--- Page 82 ---\n\n2.3 Long Sequence Modeling 75\n\u00b7 \u00b7 \u00b7\n\u00b7 \u00b7 \u00b7\ni i \u22121 i\u22122 i\u22123 i\u22124 i\u22125 i\u22126 i\u22127Keys\nValuesSize = 4 \u00d72Memory\n(a) Window-based Cache\n\u00b7 \u00b7 \u00b7\n\u00b7 \u00b7 \u00b7\ni i \u22121 i\u22122 i\u22123 i\u22124 i\u22125 i\u22126 i\u22127Keys\nValuesSize = 1 \u00d72Memory ki\u22123+ki\u22122+ki\u22121+ki\n4\u21d2\nvi\u22123+vi\u22122+vi\u22121+vi\n4\u21d2\n(b) Moving Average-based Cache\n\u00b7 \u00b7 \u00b7\n\u00b7 \u00b7 \u00b7\ni i \u22121 i\u22122 i\u22123 i\u22124 i\u22125 i\u22126 i\u22127Keys\nValuesSize = 1 \u00d72MemoryMem = Update( Skv,Mem pre)\u21d2\n(c) Recurrent Network as Cache\n\u00b7 \u00b7 \u00b7\n\u00b7 \u00b7 \u00b7\ni i \u22121 i\u22122 i\u22123 i\u22124 i\u22125 i\u22126 i\u22127Keys\nValuesSize = 4 \u00d72Memory\nSize = 2 \u00d72MemoryCompressed\n(d) Hybrid Cache (Compressed Memory + Local Memory)\nFig. 2.6: Illustrations of \ufb01xed-size KV caches in LLMs. Blue boxes rep resent the keys and values generated during\nLLM inference, green boxes represent the keys and values sto red or encoded in the primary memory, and orange boxes\nrepresent the keys and values stored or encoded in the compre ssed memory.\n\n--- Page 83 ---\n\n76 Generative Models\n2.3.3.2 Memory-based Models\nThe modeling of memories discussed above was based on update s to the KV",
    "be358382e7747987": "Size = 1 \u00d72MemoryMem = Update( Skv,Mem pre)\u21d2\n(c) Recurrent Network as Cache\n\u00b7 \u00b7 \u00b7\n\u00b7 \u00b7 \u00b7\ni i \u22121 i\u22122 i\u22123 i\u22124 i\u22125 i\u22126 i\u22127Keys\nValuesSize = 4 \u00d72Memory\nSize = 2 \u00d72MemoryCompressed\n(d) Hybrid Cache (Compressed Memory + Local Memory)\nFig. 2.6: Illustrations of \ufb01xed-size KV caches in LLMs. Blue boxes rep resent the keys and values generated during\nLLM inference, green boxes represent the keys and values sto red or encoded in the primary memory, and orange boxes\nrepresent the keys and values stored or encoded in the compre ssed memory.\n\n--- Page 83 ---\n\n76 Generative Models\n2.3.3.2 Memory-based Models\nThe modeling of memories discussed above was based on update s to the KV cache, and the re-\nsulting models are typically referred to as internal memories . We now consider another family\nof models, called external memories , which operate as independent models to access large-scale\ncontexts for LLMs. Many such models are based on memory-based methods which have been\nextensively discussed in machine learning [ Bishop ,2006 ]. A common example is nearest neigh-\nbor algorithms: we store context representations in a datas tore, and try to \ufb01nd the most similar\nstored representations to match a given query. The retrieve d context representations are then used\nto improve attention for this query.\nHere, we consider the k-nearest neighbors (k-NN) method which is one of the most popular\nmemory-based methods. Since our focus is language modeling in this section, we de\ufb01ne a sample\nin the datastore as a key-value pair corresponding to some co ntext state. Note that \u201ccontext\u201d is a\nbroad concept here, not just a sequence pre\ufb01x in text generat ion. One might, for example, view\nthe entire dataset as the context for predicting tokens. Thi s allows us to retrieve the closest context\nsituation in a set of sequences, rather than a given sequence pre\ufb01x. Although we will restrict\nourselves to context modeling for a single sequence, in this subsection, we discuss a relatively\nmore general case.\nSuppose we have a set of keys {kj}with corresponding values {vj}, and suppose we store\nthese key-value pairs in a vector database12. For each query qi, we \ufb01nd itsknearest neighbours by\ngrowing the radius of the sphere centered as qiuntil it contains kdata points in {kj}. This results\nin a set ofkkeys along with their corresponding values, denoted by Mem knn. As before, we\ndenote Mem as the local memory for the query, such as the KV cache of neigh boring tokens. Our\ngoal is to attend query qito both the local memory Mem and the long-term memory Mem knn.\nThere are, of course, several ways to incorporate Mem andMem knninto the attention model.\nFor example, we might simply combine them to form a single KV c ache [Mem,Mem knn], and\nattend qito[Mem,Mem knn]via standard QKV attention. Or we might use Mem andMem knn\nin separate attention steps. An example of such approaches i s the model developed by Wu et al.\n[2021 ]. It linearly combines the two types of attention, given by\nAtt(qi,Mem,Mem knn) = g\u2299Attlocal+ (1 \u2212g)\u2299Attknn (2.64)\nAttlocal = Att( qi,Mem) (2.65)\nAttknn= Att( qi,Mem knn) (2.66)\nHere g\u2208Rdis the coef\ufb01cient vector, which can be the output of a learned gate.\nGiven thek-NN-based memory model described above, the remaining task is to determine\nwhich key-value pairs are retained in the datastore. For sta ndard language modeling tasks, we\nconsider the previously seen tokens in a sequence as the cont ext, so we can add the keys and\nvalues of all these tokens into the datastore. In this case, t he resulting k-NN-based attention\nmodel is essentially equivalent to a sparse attention model [Gupta et al. ,2021 ].\nAlternatively, we can extend the context from one sequence t o a collection of sequences.\nFor example, we might collect all key-value pairs across the sequences in a training dataset and\nadd them to the datastore to model a larger context. Thus, LLM s can predict tokens based on a\n12A vector database, or vector store, is a database that provid es highly optimized retrieval interfaces",
    "65c2a760b17ed691": "\ufffdRdis the coef\ufb01cient vector, which can be the output of a learned gate.\nGiven thek-NN-based memory model described above, the remaining task is to determine\nwhich key-value pairs are retained in the datastore. For sta ndard language modeling tasks, we\nconsider the previously seen tokens in a sequence as the cont ext, so we can add the keys and\nvalues of all these tokens into the datastore. In this case, t he resulting k-NN-based attention\nmodel is essentially equivalent to a sparse attention model [Gupta et al. ,2021 ].\nAlternatively, we can extend the context from one sequence t o a collection of sequences.\nFor example, we might collect all key-value pairs across the sequences in a training dataset and\nadd them to the datastore to model a larger context. Thus, LLM s can predict tokens based on a\n12A vector database, or vector store, is a database that provid es highly optimized retrieval interfaces for \ufb01nding stored\nvectors that closely match a query vector.\n\n--- Page 84 ---\n\n2.3 Long Sequence Modeling 77\ngeneralized context. A problem with this approach is that th e computational cost would be large\nif many sequences are involved. Since these sequences are pa rt of our training data, we can build\nand optimize an index for the vectors in the datastore before running the LLMs. As a result, the\nretrieval of similar vectors can be very ef\ufb01cient, as in most vector databases.\nIn fact, all the above-mentioned methods can be viewed as ins tances of a retrieval-based ap-\nproach. Instead of using retrieval results to improve atten tion, we can apply this approach in other\nways as well. One application of k-NN-based search is k-NN language modeling (ork-NN LM )\n[Khandelwal et al. ,2020 ]. The idea is that, although it is attempting to extend the co ntext used\nin self-attention by incorporating nearest neighbors in re presentation learning, in practice, similar\nhidden states in Transformers are often highly predictive o f similar tokens in subsequent positions.\nInk-NN LM, each item in the datastore is a key-value tuple (z,w), where zrepresents a hidden\nstate of the LLM at a position, and wrepresents the corresponding prediction. A typical way to\ncreate the datastore is to collect the output vector of the Tr ansformer layer stack and the corre-\nsponding next token for each position of each sequence in a tr aining dataset. During inference,\nwe have a representation higiven a pre\ufb01x. Given this representation, we \ufb01rst search the datastore\nforkclosest matching data items {(z1,w1),...,(zk,wk)}. Here {w1,...,w k}are thought of as\nreference tokens for prediction, and thus can be used to guid e the token prediction based on hi.\nOne common way to make use of reference tokens is to de\ufb01ne a dis tribution over the vocabulary\nV,\nPrknn(\u00b7|hi) = Softmax([\n\u2212d0\u00b7 \u00b7 \u00b7 \u2212d|V|]\n) (2.67)\nwheredvequals the distance between hiandzjifwjequals thev-th entry of V, and equals 0\notherwise. We use a linear function with a coef\ufb01cient \u03bbthat interpolates between the retrieval-\nbased distribution Prknn(\u00b7|hi)and the LLM output distribution Prlm(\u00b7|hi)\nPr(\u00b7|hi) =\u03bb\u00b7Prknn(\u00b7|hi) + (1 \u2212\u03bb)\u00b7Prlm(\u00b7|hi) (2.68)\nThen, as usual, we can choose the next token yby maximizing the probability Pr(y|hi).\nAs with information retrieval (IR) systems, the datastore c an also manage texts and provide\naccess to relevant texts for a query. For example, we can stor e a collection of text documents\nin a search engine with full-text indexing, and then search i t for documents that match a given\ntext-based query. Applying IR techniques to LLMs leads to a g eneral framework called retrieval-\naugmented generation (RAG ). The RAG framework works as follows. We use the context xas\nthe query and \ufb01nd the kmost relevant document pieces {c1,...,ck}from the datastore via ef\ufb01cient\nIR techniques13. These search results are combined with the original contex t via a prompting\n13In piratical applications, queries are typically generate d using a query generation system, which may expand it\nwith variations of tokens and query intent.\n\n--- Page 85 ---\n\n78 Generative Models\ntemplateg(\u00b7)14, resulting in an augmented input for the",
    "d5c71306a8b3e294": ") systems, the datastore c an also manage texts and provide\naccess to relevant texts for a query. For example, we can stor e a collection of text documents\nin a search engine with full-text indexing, and then search i t for documents that match a given\ntext-based query. Applying IR techniques to LLMs leads to a g eneral framework called retrieval-\naugmented generation (RAG ). The RAG framework works as follows. We use the context xas\nthe query and \ufb01nd the kmost relevant document pieces {c1,...,ck}from the datastore via ef\ufb01cient\nIR techniques13. These search results are combined with the original contex t via a prompting\n13In piratical applications, queries are typically generate d using a query generation system, which may expand it\nwith variations of tokens and query intent.\n\n--- Page 85 ---\n\n78 Generative Models\ntemplateg(\u00b7)14, resulting in an augmented input for the LLM\nx\u2032=g(c1,...,ck,x) (2.69)\nThen, we use x\u2032as the context and predict the following text using the model Pr(y|x\u2032). One\nadvantage of RAG is that we need not modify the architecture o f LLMs, but instead augment the\ninput to LLMs via an additional IR system. Figure 2.7shows a comparison of the use of different\nexternal memories in LLMs.\n2.3.3.3 Memory Capacity\nA memory model in LLMs, in the form of a simple key-value cache or a datastore, can broadly\nbe seen as an encoder of contextual information. Ideally, be fore we say that a memory model\nis representative of the entire context in token prediction , we need to make sure that the model\ncan accurately represent any part of the context. The standa rd KV cache is one such model that\ncompletely stores all past history. In this case, the model i s said to have adequate capacity for\nmemorizing the context. In many practical applications, ho wever, complete memorization is not\nrequired. Instead, the goal is to enable LLMs to access impor tant contextual information. As a\nresult, ef\ufb01cient and compressed memory models are develope d, as described in this section. Note\nthat, the longer the sequence, the more dif\ufb01cult it becomes f or a low-capacity memory model to\ncapture important contextual information. It is therefore common practice to simply increase the\nmodel capacity when processing long contexts.\nWhile high-capacity models are generally favorable, they a re dif\ufb01cult to train and deploy. A\nchallenging scenario is that the tokens arrive in a stream an d the context continuously grows.\nDeveloping LLMs for such tasks is dif\ufb01cult as we need to train Transformers on extremely long\nsequences. A possible way to address this dif\ufb01culty is to use non-parametric methods, such as\nretrieval-based methods. For example, as discussed above, we can use a vector database to store\npreviously generated key-value pairs, and thus represent t he context by this external memory\nmodel. Although this approach side-steps the challenge of r epresenting long context in Trans-\nformers, building and updating external memory models are c omputationally expensive. These\nmodels are more often used in problems where the context is gi ven in advance and \ufb01xed during\ninference, and hence unsuitable for streaming context mode ling.\nIn cases where the size of the context continuously grows, ap plying \ufb01xed-size memory models\nis a commonly used approach. For example, in recurrent model s, a sequence of arbitrary length\ncan be summarized into a set of hidden states by which we have a \ufb01xed computational cost per\nstep. While recurrent models were initially found to be not v ery good at handling long-distance\ndependencies in sequence modeling in early applications of deep learning to NLP, recent advance-\nments have shown that their variants are now effective in mod eling extremely long sequences.\n[Bulatov et al. ,2022 ;Hutchins et al. ,2022 ;Munkhdalai et al. ,2024 ;Ma et al. ,2024 ].\n14For example, the template could be:\nmessage = {* c1*} ... {* ck*}\ninput: {* x*}\noutput:\n\n--- Page 86 ---\n\n2.3 Long Sequence Modeling 79\n\u00b7 \u00b7 \u00b7\n\u00b7 \u00b7 \u00b7\nDatastoreqi\nSearchKV Cache kNearest\nNeighborsAtt(qi,Mem) Att(qi,Mem knn)g\u2299Att(qi,Mem) + (1 \u2212g)\u2299Att(qi,Mem knn)\nKeys/values in LLM\nKeys/values",
    "d1503231af49d40f": " to be not v ery good at handling long-distance\ndependencies in sequence modeling in early applications of deep learning to NLP, recent advance-\nments have shown that their variants are now effective in mod eling extremely long sequences.\n[Bulatov et al. ,2022 ;Hutchins et al. ,2022 ;Munkhdalai et al. ,2024 ;Ma et al. ,2024 ].\n14For example, the template could be:\nmessage = {* c1*} ... {* ck*}\ninput: {* x*}\noutput:\n\n--- Page 86 ---\n\n2.3 Long Sequence Modeling 79\n\u00b7 \u00b7 \u00b7\n\u00b7 \u00b7 \u00b7\nDatastoreqi\nSearchKV Cache kNearest\nNeighborsAtt(qi,Mem) Att(qi,Mem knn)g\u2299Att(qi,Mem) + (1 \u2212g)\u2299Att(qi,Mem knn)\nKeys/values in LLM\nKeys/values in Datastore\n(a)k-NN Search Augmented Attention\n\u00b7 \u00b7 \u00b7\n\u00b7 \u00b7 \u00b7\nDatastoreqi\nSearchKV Cache kNearest\nNeighborsAtt(qi,Mem) Att(qi,Mem)\u00b7 \u00b7 \u00b7Distribution Pr(\u00b7)\nDistribution Prknn(\u00b7)Output Distribution\nKeys/values in LLM\nKeys in Datastore\nPredicted Tokens\n(b)k-NN Language Modeling\nInput Context:\nx=What is deep learning?DatastoreSearch\u00b7 \u00b7 \u00b7c2=Machine learning is ...c1=Deep network is ...\nkNearest\nNeighborsWhat is deep learning?Message: deep network ... machine learning ...LLM\n(c) Retrieval-augmented Generation\nFig. 2.7: Illustrations of external memories (or datastores) for lan guage modeling.\n\n--- Page 87 ---\n\n80 Generative Models\nThere is no general de\ufb01nition of memory capacity in LLMs. A si mple approach might consider\nhow much storage is used to retain contextual information. F or example, memory capacity could\nbe de\ufb01ned by the size of the KV cache in Transformers or the vec tor database used in retrieval-\nbased methods. A related concept is model complexity. In mac hine learning, there are several\nways to de\ufb01ne the model complexity of a model. One of the simpl est methods is by counting the\nnumber of parameters. However, it should be emphasized that the memory models discussed here\nprimarily serve to store information, rather than add train able parameters. Therefore, a model with\na large memory capacity is not necessarily more complex. Nev ertheless, in practice determining\nthe capacity of a memory model is not straightforward. In gen eral, we need to control the trade-off\nbetween maximizing the performance and controlling the mem ory footprint.\n2.3.4 Sharing across Heads and Layers\nIn Transformers, the KV cache is a data structure that can be d ynamically adjusted along multiple\ndimensions, such as heads, layers, and sequence length. For example, consider an LLM with L\nlayers. Each layer has \u03c4attention heads, and each head produces a dh-dimensional output. During\ninference, we store the keys and values for up to mtokens. The space complexity of this caching\nmechanism is O(L\u00b7\u03c4\u00b7dh\u00b7m). As we have seen previously, this complexity can be reduced b y\ncaching the keys and values for fewer tokens. For example, in sliding window attention, a \ufb01xed-\nsize window is used to cache the keys and values in local conte xt. And this model has a space\ncomplexity of O(L\u00b7\u03c4\u00b7dh\u00b7mw), withmwbeing the size of the window.\nIn addition to reducing m, we can also decrease the size of the KV cache along other di-\nmensions. A widely-used approach is to enable sharing acros s heads in multi-head self-attention.\nRecall from Section 2.1.1 that multi-head self-attention uses multiple sets of queri es, keys, and\nvalues (each set is called a head), each performing the QKV at tention mechanism as usual. This\ncan be expressed as\nOutput = Merge(head 1,...,head \u03c4)Whead(2.70)\nwhere head j\u2208Rdhis computed using the standard QKV attention function\nhead j= Att qkv(q[j]\ni,K[j]\n\u2264i,V[j]\n\u2264i) (2.71)\nHere, q[j]\ni,K[j]\n\u2264i, and V[j]\n\u2264iare the query, keys, and values that are projected onto the j-th feature\nsub-space. So this model can be interpreted as performing at tention on a group of feature sub-\nspaces in parallel (see Figure 2.8(b)).",
    "570656a5c1117373": " sharing acros s heads in multi-head self-attention.\nRecall from Section 2.1.1 that multi-head self-attention uses multiple sets of queri es, keys, and\nvalues (each set is called a head), each performing the QKV at tention mechanism as usual. This\ncan be expressed as\nOutput = Merge(head 1,...,head \u03c4)Whead(2.70)\nwhere head j\u2208Rdhis computed using the standard QKV attention function\nhead j= Att qkv(q[j]\ni,K[j]\n\u2264i,V[j]\n\u2264i) (2.71)\nHere, q[j]\ni,K[j]\n\u2264i, and V[j]\n\u2264iare the query, keys, and values that are projected onto the j-th feature\nsub-space. So this model can be interpreted as performing at tention on a group of feature sub-\nspaces in parallel (see Figure 2.8(b)). The KV cache needs to retain the keys and values for all\nthese heads, that is, {(K[1]\n\u2264i,V[1]\n\u2264i),...,(K[\u03c4]\n\u2264i,V[\u03c4]\n\u2264i)}.\nOne re\ufb01nement to the multi-head attention model, called multi-query attention (MQA ), is to\nshare keys and values across heads, while allowing queries t o be unique for each head [ Shazeer ,\n2019 ]. In MQA, there is a single set of keys and values (K\u2264i,V\u2264i). In addition, there are \u03c4\nqueries {q[1]\ni,...,q[\u03c4]\ni}, each corresponding to a different head. For each head, we ha ve\nhead j= Att qkv(q[j]\ni,K\u2264i,V\u2264i) (2.72)\nFigure 2.8(c) illustrates this model. By sharing keys and values, the s ize of the KV cache would\n\n--- Page 88 ---\n\n2.3 Long Sequence Modeling 81\nquery key value\n(a) Single-head Attentionquery key value\n(b) Multi-head Attention\nquery key value\n(c) Multi-query Attentionquery key value\n(d) Grouped Query Attention\nquery key value\nLayerl\nLayerl\u22121Sharing\n(e) Cross-layer Multi-head Attention\nFig. 2.8: Illustration of QKV attention based on different multi-hea d and sharing mechanisms. (a) = single-head\nattention, and (b-e) = attention with multiple heads.\nbeO(L\u00b7dh\u00b7m).\nGrouped query attention (GQA ) is a natural extension to multi-head attention and MQA\n[Ainslie et al. ,2023 ]. In GQA, heads are divided into nggroups, each corresponding to a shared\nset of keys and values. Hence we have ngsets of keys and values {(K[1]\n\u2264i,V[1]\n\u2264i),...,(K[ng]\n\u2264i,V[ng]\n\u2264i)}.\nSee Figure 2.8(d) for an illustration. Let g(j)be the group id for the j-th head. The GQA model\ncan be expressed as\nhead j= Att qkv(q[j]\ni,K[g(j)]\n\u2264i,V[g(j)]\n\u2264i) (2.73)\nThe size of the KV cache of GQA is O(L\u00b7ng\u00b7dh\u00b7m). One bene\ufb01t of GQA is that we can trade-off\nbetween computational ef\ufb01ciency and model expressiveness by adjusting ng. Whenng=\u03c4, the\nmodel becomes the standard multi-head attention model. By c ontrast, when ng= 1, it becomes\n\n--- Page 89 ---\n\n82 Generative Models\nthe GQA model.\nSharing can also be performed across layers. Such a method fa lls into the family of shared\nweight and shared activation methods, which have been exten sively used in Transformers [ Dehghani et al. ,\n2018 ;Lan et al. ,2020 ]. For example, one can share KV activations or attention wei ghts across\nlayers to reduce both computation and memory footprints [ Xiao et al. ,2019 ;Brandon et al. ,2024 ].\nFigure 2.8(e) shows an illustration of this method, where a query in a la yer directly accesses the\nKV cache of a lower-level layer.\n2.3.5 Position Extrapolation and Interpolation\nSince Transformer layers are order-insensitive to input, w e need some way to encode positional\ninformation in the input tokens. To do this, it is common to ad d positional embeddings to token\nembeddings, and then feed these combined embeddings into th e Transformer layer stack as input",
    "fe3c1a3f7398b238": " be performed across layers. Such a method fa lls into the family of shared\nweight and shared activation methods, which have been exten sively used in Transformers [ Dehghani et al. ,\n2018 ;Lan et al. ,2020 ]. For example, one can share KV activations or attention wei ghts across\nlayers to reduce both computation and memory footprints [ Xiao et al. ,2019 ;Brandon et al. ,2024 ].\nFigure 2.8(e) shows an illustration of this method, where a query in a la yer directly accesses the\nKV cache of a lower-level layer.\n2.3.5 Position Extrapolation and Interpolation\nSince Transformer layers are order-insensitive to input, w e need some way to encode positional\ninformation in the input tokens. To do this, it is common to ad d positional embeddings to token\nembeddings, and then feed these combined embeddings into th e Transformer layer stack as input.\nIn this case, the embedding at position ican be expressed as\nei=xi+ PE(i) (2.74)\nwhere xi\u2208Rddenotes the token embedding, and PE(i)\u2208Rddenotes the positional embedding.\nIn general, the token embedding xiis a position-independent vector, and so the positional emb ed-\ndingPE(i)is used to encode the positional context. A straightforward approach is to treat PE(i)\nas a learnable variable and train it alongside other model pa rameters. In this way, we can learn\na unique representation for each position, and thus disting uish the tokens appearing at different\npositions of a sequence.\nRepresentations of positions using learned vectors can wor k well in tasks where the sequences\nat training and test times are of similar lengths. In practic e, however, we often impose length\nrestrictions on sequences during training to prevent exces sive computational costs, but wish to\napply the trained models to much longer sequences during inf erence. In this case, using learned\npositional embeddings has obvious drawbacks, as there are n o trained embeddings for positions\nthat are not observed in the training phase.\nAn alternative approach to modeling positional informatio n is to develop positional embed-\ndings that can generalize: once trained, the embedding mode l can be used to handle longer se-\nquences. Suppose that we train a positional embedding model on sequences with a maximum\nlength ofml, and we wish to apply the trained model to a sequence of length m(m >> m l). If\nthe embedding model is limited in the range of positions that we can observe from training data,\nthen this model will simply fail to deal with new data outside that range. See Figure 2.9(a) for\nan illustration where the learned embedding model cannot mo del data points outside the training\ndomain if it lacks the ability to extrapolate.\nThere are several approaches to making positional embeddin g models generalize. They can\nbe grouped into two classes.\n\u2022Extrapolation . The model learned on observed data points (i.e., positions ) can be directly\nemployed to assign meaningful values to data points beyond t he original range. For ex-\nample, suppose we have a series of numbers 1,2,...,10, and we want to understand the\nmeaning of a new number, 15. Knowing that these numbers are na tural numbers used for\nordering, we can easily infer that 15 is a number that follows 10, even though 15 has not\n\n--- Page 90 ---\n\n2.3 Long Sequence Modeling 83\n0 1 ,024 2 ,048\u2212101\nSequence Length\n(a) Encoding with No GeneralizationValue\n0 1 ,024 2 ,048\u2212101\nSequence Length\n(b) ExtrapolationValue\n0 1 ,024 2 ,048\u2212101\nSequence Length\n(c) InterpolationValue\nFig. 2.9: Illustrations of different positional embedding methods f or a range of positions. Blue points represent the\npositions that have been observed during training, and red p oints represent the positions that are newly observed at tes t\ntime. In sub-\ufb01gure (a), the encoding model only memorizes th e points seen during training, and cannot generalize. In\nsub-\ufb01gures (b) and (c), the model can generalize through ext rapolation and interpolation.\nbeen observed before. Figure 2.9(b) shows an example of this approach, where a function\nis learned to \ufb01t the data points within a speci\ufb01c range and the n applied to estimate the values\nof data points outside that range.\n\u2022Interpolation . This approach maps a larger range of data points into the ori ginal obser-\nvation range. For example, suppose we have a model designed f or numbers in the range",
    "76b84050f77cc89c": "\nFig. 2.9: Illustrations of different positional embedding methods f or a range of positions. Blue points represent the\npositions that have been observed during training, and red p oints represent the positions that are newly observed at tes t\ntime. In sub-\ufb01gure (a), the encoding model only memorizes th e points seen during training, and cannot generalize. In\nsub-\ufb01gures (b) and (c), the model can generalize through ext rapolation and interpolation.\nbeen observed before. Figure 2.9(b) shows an example of this approach, where a function\nis learned to \ufb01t the data points within a speci\ufb01c range and the n applied to estimate the values\nof data points outside that range.\n\u2022Interpolation . This approach maps a larger range of data points into the ori ginal obser-\nvation range. For example, suppose we have a model designed f or numbers in the range\n[1,10]. When given a new range of [1,20], we can scale this down by dividing every num-\nber by 2, thereby \ufb01tting all numbers into [1,10]. This scaling allows us to use the model\ntrained on the range [1,10]to describe data points in the expanded range of [1,20]. See\nFigure 2.9(c) for an illustration of this approach.\nIn fact, positional embeddings in many systems have achieve d some level of generalization.\nFor example, sinusoidal encoding, the most common position al embedding method, employs sine\nand cosine functions that can naturally extend to sequences of any length. Although this approach\nmight seem direct and simple, it does not perform well when we signi\ufb01cantly extend the sequences\nfor processing. In this subsection, we will discuss several alternative methods based on either\nextrapolation or interpolation.\n\n--- Page 91 ---\n\n84 Generative Models\n2.3.5.1 Attention with Learnable Biases\nOne problem with Eq. ( 2.74) is that the embedding model treats each token independentl y and\ntherefore ignores the distance between different tokens. A common improvement to this model,\ncalled relative positional embedding, is to consider the pa irwise relationship between tokens\n[Shaw et al. ,2018 ]. The general idea behind this is to obtain the offset betwee n any pair of posi-\ntions and incorporate it into the self-attention model. One of the simplest forms of self-attention\nwith relative positional embedding is given by\nAttqkv(qi,K\u2264i,V\u2264i) =i\u2211\nj=0\u03b1(i,j)vj (2.75)\n\u03b1(i,j) = Softmax(qikT\nj+PE(i,j)\u221a\nd+ Mask(i,j)) (2.76)\nThe only difference between this model and the original self -attention model is that a bias term\nPE(i,j)is added to the query-key product in this new model. Intuitiv ely,PE(i,j)can be inter-\npreted as a distance penalty for the pair of positions iandj. Asimoves away from j, the value of\nPE(i,j)decreases.\nPE(i,j)can be de\ufb01ned in several different ways. Here, we consider th e T5 version of relative\npositional embedding, called the T5 bias [ Raffel et al. ,2020 ]. For each pair of query qiand key\nkj, the offset between them is de\ufb01ned to be15\nd(i,j) =i\u2212j (2.77)\nA simple design for the bias PE(i,j)is to share the same learnable variable for all query-key\npairs with the same offset, i.e., PE(i,j) =ui\u2212j, whereui\u2212jis the variable corresponding to\nthe offseti\u2212j. However, simply assigning a unique value to each offset wil l restrict this model\nto observed offsets. When i\u2212jis larger than the maximum trained offset, the model cannot\ngeneralize.\nThe T5 bias instead adopts a generalization of this model. Ra ther than assigning each query-\nkey offset a unique bias term, it groups difference offsets i nto \u201cbuckets\u201d, each corresponding to\none learnable parameter. More speci\ufb01cally, the bias terms f ornb+ 1buckets are given as follows.\n\u2022 For buckets 0 tonb+1\n2\u22121, each bucket corresponds to one offset, that is, bucket 0 \u2194offset\n0, bucket 1 \u2194offset 1, bucket 2 \u2194offset 2, and so on. We express this as b(i\u2212j) =i\u2212j.\n\u2022 For bucketsnb+1\n2tonb,",
    "88f30fb632c4a386": "j, whereui\u2212jis the variable corresponding to\nthe offseti\u2212j. However, simply assigning a unique value to each offset wil l restrict this model\nto observed offsets. When i\u2212jis larger than the maximum trained offset, the model cannot\ngeneralize.\nThe T5 bias instead adopts a generalization of this model. Ra ther than assigning each query-\nkey offset a unique bias term, it groups difference offsets i nto \u201cbuckets\u201d, each corresponding to\none learnable parameter. More speci\ufb01cally, the bias terms f ornb+ 1buckets are given as follows.\n\u2022 For buckets 0 tonb+1\n2\u22121, each bucket corresponds to one offset, that is, bucket 0 \u2194offset\n0, bucket 1 \u2194offset 1, bucket 2 \u2194offset 2, and so on. We express this as b(i\u2212j) =i\u2212j.\n\u2022 For bucketsnb+1\n2tonb, the size of each bucket increases logarithmically. For exa mple, the\nbucket number for a given offset i\u2212j\u2265nb+1\n2can be de\ufb01ned as\nb(i\u2212j) =nb+ 1\n2+\u230alog(i\u2212j)\u2212log(nb+1\n2)\nlog(dist max)\u2212log(nb+1\n2)\u00b7nb+ 1\n2\u230b (2.78)\nwhere the parameter dist maxis typically set to a relatively large number to indicate the\n15For language modeling, a query is only allowed to attend to it s left-context, and so we have i\u2212j\u22650. In the more\ngeneral case of self-attention, where a token can attend to a ll tokens in the sequence, we may have negative offsets\nwheni<j .\n\n--- Page 92 ---\n\n2.3 Long Sequence Modeling 85\n0 1 2 3\u00b7 \u00b7 \u00b7 14 15 16 17 18 \u00b7 \u00b7 \u00b7 32 Bucket\nOffset\n(i\u2212j)0 1 2 3 14 15 16 \u223c20 21 \u223c26 27 \u223c33 802 \u223c \u221e\ufb01xed bucket size logarithmically increased bucket size\nFig. 2.10: Illustration of distributing query-key offsets into bucke ts in the T5 model ( nb= 32 anddist max= 1024 ).\nBoxes represent buckets. In the \ufb01rst half of the buckets, we u se a \ufb01xed bucket size. In the second half of the buckets,\nwe increase the bucket size logarithmically. The last bucke t contains all the query-key offsets that are not covered by\nprevious buckets.\nmaximum offset we may encounter.\n\u2022 Wheni\u2212j >dist max, we placei\u2212jin the last bucket. In other words, bucket nbcontains\nall the offsets that are not assigned to the previous buckets .\nTogether, these can be expressed as the function\nb(i\u2212j)\n=\uf8f1\n\uf8f4\uf8f2\n\uf8f4\uf8f3i\u2212j 0\u2264i\u2212j <nb+1\n2\nmin(nb,nb+1\n2+\u230alog(i\u2212j)\u2212log(nb+1\n2)\nlog(dist max)\u2212log(nb+1\n2)\u00b7nb+1\n2\u230b)i\u2212j\u2265nb+1\n2(2.79)\nFigure 2.10 shows an illustration of these buckets. We see that in the \ufb01rs t half of the buckets,\neach bucket is associated with only one value of i\u2212j, while in the second half, the bucket size\nincreases as i\u2212jgrows. The last bucket is designed to handle sequences of arb itrarily long\nlengths.\nAllPE(i,j)s in a bucket share the same bias term ub(i\u2212j). Substituting PE(i,j) =ub(i\u2212j)\ninto Eq. ( 2.76), the attention weight for qiandkjbecomes16\n\u03b1(i,j) = Softmax(qikT\nj+ub(i\u2212j)\u221a\nd+ Mask(i,j)) (2.81)\nThe parameters {u0,...,u nb}are learned as common parameters during training. It should\nbe emphasized that this model can generalize to long sequenc es. This is because PE(i,j)s with\nsimilar query-key offsets share the same parameter, and thi s sharing strategy is particularly im-\nportant for achieving good generalization, given that larg e query-key offsets are rare in training.\nIn practice, we often set nbto a moderate number, and thus it can help control the over\ufb01tt ing of\npositional embedding models.\n16Note that, in Raff",
    "c52bdb327d22a9b0": "(i,j)s in a bucket share the same bias term ub(i\u2212j). Substituting PE(i,j) =ub(i\u2212j)\ninto Eq. ( 2.76), the attention weight for qiandkjbecomes16\n\u03b1(i,j) = Softmax(qikT\nj+ub(i\u2212j)\u221a\nd+ Mask(i,j)) (2.81)\nThe parameters {u0,...,u nb}are learned as common parameters during training. It should\nbe emphasized that this model can generalize to long sequenc es. This is because PE(i,j)s with\nsimilar query-key offsets share the same parameter, and thi s sharing strategy is particularly im-\nportant for achieving good generalization, given that larg e query-key offsets are rare in training.\nIn practice, we often set nbto a moderate number, and thus it can help control the over\ufb01tt ing of\npositional embedding models.\n16Note that, in Raffel et al. [2020 ]\u2019s T5 model, the rescaling operation for the query-key prod uct is removed. The\nattention weight \u03b1(i,j)is then given by\n\u03b1(i,j) = Softmax( qikT\nj+ub(i\u2212j)+ Mask(i,j)) (2.80)\n\n--- Page 93 ---\n\n86 Generative Models\n2.3.5.2 Attention with Non-learned Biases\nRelative positional embedding models are based on a set of le arned biases for the query-key prod-\nuct in self-attention. An alternative approach is to give th ese biases \ufb01xed values via heuristics,\nrather than training them on a particular dataset. One bene\ufb01 t of this heuristics-based approach is\nthat it does not rely on a training process and thus can be dire ctly applied to any sequences once\nthe biases are set.\nOne example of such an approach is Press et al. [2022 ]\u2019s approach, called attention with\nlinear biases orALiBi for short. In the ALiBi approach, the bias term is de\ufb01ned as th e negative\nscaled query-key offset\nPE(i,j) = \u2212\u03b2\u00b7(i\u2212j)\n=\u03b2\u00b7(j\u2212i) (2.82)\nwhere\u03b2is the scaling factor. Adding this term to the query-key prod uct, we obtain a new form of\nattention weights\n\u03b1(i,j) = Softmax(qikT\nj+\u03b2\u00b7(j\u2212i)\u221a\nd+ Mask(i,j)) (2.83)\nThis model can be interpreted as adding a \ufb01xed penalty to qikT\njwheneverjmoves one step\naway fromi. So we do not need to adapt it to a range of sequence lengths, an d can employ it to\nmodel arbitrarily long sequences. See Figure 2.11 for a comparison of the T5 bias and the ALiBi\nbias.\nIn general, the scalar \u03b2should be tuned on a validation dataset. However, Press et al. [2022 ]\nfound that setting \u03b2to values decreasing geometrically by a factor of1\n2afor multi-head attention\nperforms well on a variety of tasks. Speci\ufb01cally, for a self- attention sub-layer involving nhead\nheads, the scalar for the k-th head is given by\n\u03b2k=1\n28\nk(2.84)\nThe ALiBi approach provides a simple form of relative positi onal embeddings. There are\nother similar methods for designing query-key biases using the offseti\u2212j. Table 2.4shows a\ncomparison of such biases. As an aside it is worth noting that the form of the right-hand side\nof Eq. ( 2.82) is very similar to length features used in conventional fea ture-based systems. For\nexample, in statistical machine translation systems, such features are widely used to model word\nreordering problems, resulting in models that can generali ze well across different translation tasks\n[Koehn ,2010 ].\n2.3.5.3 Rotary Positional Embedding\nAs with sinusoidal embeddings, rotary positional embeddin gs are based on hard-coded values for\nall dimensions of an embedding [ Su et al. ,2024 ]. Recall that in the sinusoidal embedding model,\npositions are represented as combinations of sine and cosin e functions with different frequencies.\nThese embeddings are then added to token embeddings to form t he inputs to the Transformer\n\n--- Page 94 ---\n\n2.3 Long Sequence Modeling 87\nq0kT\n0\nq1kT\n0q1kT\n1\nq2kT\n0q2kT\n1q2kT\n2\nq3kT\n0q3kT\n",
    "b8a99487b1f9144b": ". For\nexample, in statistical machine translation systems, such features are widely used to model word\nreordering problems, resulting in models that can generali ze well across different translation tasks\n[Koehn ,2010 ].\n2.3.5.3 Rotary Positional Embedding\nAs with sinusoidal embeddings, rotary positional embeddin gs are based on hard-coded values for\nall dimensions of an embedding [ Su et al. ,2024 ]. Recall that in the sinusoidal embedding model,\npositions are represented as combinations of sine and cosin e functions with different frequencies.\nThese embeddings are then added to token embeddings to form t he inputs to the Transformer\n\n--- Page 94 ---\n\n2.3 Long Sequence Modeling 87\nq0kT\n0\nq1kT\n0q1kT\n1\nq2kT\n0q2kT\n1q2kT\n2\nq3kT\n0q3kT\n1q3kT\n2q3kT\n3\nq4kT\n0q4kT\n1q4kT\n2q4kT\n3q4kT\n4\nq5kT\n0q5kT\n1q5kT\n2q5kT\n3q5kT\n4q5kT\n5\nq6kT\n0q6kT\n1q6kT\n2q6kT\n3q6kT\n4q6kT\n5q6kT\n6qikT\nj\nu0\nu1u0\nu2u1u0\nu2u2u1u0\nu3u2u2u1u0\nu3u3u2u2u1u0\nu3u3u3u2u2u1u0Bias ( ub(i\u2212j))\n+\n(a) The T5 bias ( nb= 3anddist max= 5)\nq0kT\n0\nq1kT\n0q1kT\n1\nq2kT\n0q2kT\n1q2kT\n2\nq3kT\n0q3kT\n1q3kT\n2q3kT\n3\nq4kT\n0q4kT\n1q4kT\n2q4kT\n3q4kT\n4\nq5kT\n0q5kT\n1q5kT\n2q5kT\n3q5kT\n4q5kT\n5\nq6kT\n0q6kT\n1q6kT\n2q6kT\n3q6kT\n4q6kT\n5q6kT\n6qikT\nj\n0\n\u22121\u03b20\n\u22122\u03b2\u22121\u03b20\n\u22123\u03b2\u22122\u03b2\u22121\u03b20\n\u22124\u03b2\u22123\u03b2\u22122\u03b2\u22121\u03b20\n\u22125\u03b2\u22124\u03b2\u22123\u03b2\u22122\u03b2\u2212\u03b2 0\n\u22126\u03b2\u22125\u03b2\u22124\u03b2\u22123\u03b2\u22122\u03b2\u2212\u03b2 0Bias ( \u2212\u03b2(i\u2212j))\n+\n(b) The ALiBi bias\nFig. 2.11: Query-key products with biases (above = the T5 bias and below = the ALiBi bias). The color scale of the\nbiases ranges from light blue denoting small absolute value s to deep blue denoting large absolute values.\nlayer stack. Rotary positional embeddings instead model po sitional context as rotations to token\nembeddings in a complex space. This leads to a model expresse d in the form of multiplicative\nembeddings\nei=xiR(i) (2.85)\nwhereR(i)\u2208Rd\u00d7dis the rotation matrix representing the rotations performe d on the token\nembedding xi\u2208Rd.\nFor simplicity, we will \ufb01rst consider embeddings with only t wo dimensions and return to a\ndiscussion of the more general formulation later. Suppose w e have a 2-dimensional token embed-\nding x=[\nx1x2]\n. We can represent it as a vector in a plane, originating at the origin (0,0)\nand terminating at (x1,x2). A counterclockwise rotation of this vector refers to an ope ration of\n\n--- Page 95 ---\n\n88 Generative Models\nEntry Query-Key Bias ( PE(i,j))\nT5 [Raffel et al. ,2020 ]ub(i\u2212j)\nALiBi [ Press et al. ,2022 ] \u2212\u03b2\u00b7(i\u2212j)\nKerple [ Chi et al. ,2022 ] \u2212\u03b21(i\u2212j)",
    "c8c47d1138c5929b": "R(i)\u2208Rd\u00d7dis the rotation matrix representing the rotations performe d on the token\nembedding xi\u2208Rd.\nFor simplicity, we will \ufb01rst consider embeddings with only t wo dimensions and return to a\ndiscussion of the more general formulation later. Suppose w e have a 2-dimensional token embed-\nding x=[\nx1x2]\n. We can represent it as a vector in a plane, originating at the origin (0,0)\nand terminating at (x1,x2). A counterclockwise rotation of this vector refers to an ope ration of\n\n--- Page 95 ---\n\n88 Generative Models\nEntry Query-Key Bias ( PE(i,j))\nT5 [Raffel et al. ,2020 ]ub(i\u2212j)\nALiBi [ Press et al. ,2022 ] \u2212\u03b2\u00b7(i\u2212j)\nKerple [ Chi et al. ,2022 ] \u2212\u03b21(i\u2212j)\u03b22(power)\n\u2212\u03b21log(1 +\u03b22(i\u2212j))(logarithmic)\nSandwich [ Chi et al. ,2023 ]\u2211\u00afd/2\nk=1cos((i\u2212j)/100002k/\u00afd)\nFIRE [ Li et al. ,2024 ]f(\u03c8(i\u2212j)/\u03c8(max(mlen,i)))\nTable 2.4: Query-key biases as relative positional embeddings. \u03b2,\u03b21,\u03b22,\u00afd, andmlenare hyper-parameters. In the T5\nmodel,b(i\u2212j)denotes the bucket assigned to i\u2212j. In the FIRE model, \u03c8(\u00b7)is a monotonically increasing function\nsuch as\u03c8(x) = log(cx+ 1) , andf(\u00b7)is an FFN.\nmoving the vector around the origin while maintaining its ma gnitude, as shown in Figure 2.12 (a).\nThe degree of rotation is usually de\ufb01ned by a speci\ufb01c angle, d enoted by\u03b8. The rotation can be\nexpressed mathematically in the form\nRo(x,\u03b8) = xR\u03b8\n=[\nx1x2][\ncos\u03b8sin\u03b8\n\u2212sin\u03b8cos\u03b8]\n=[\ncos\u03b8\u00b7x1\u2212sin\u03b8\u00b7x2sin\u03b8\u00b7x1+ cos\u03b8\u00b7x2]\n(2.86)\nwhereR\u03b8=[\ncos\u03b8sin\u03b8\n\u2212sin\u03b8cos\u03b8]\nis the rotation matrix. If two or more rotations are performe d on the\nsame vector, we can rotate the vector further. This follows f rom the fact that the composition of\nsuccessive rotations is itself a rotation. More formally, r otating a vector by an angle \u03b8forttimes\ncan be expressed as\nRo(x,t\u03b8) = xRt\u03b8\n=[\ncost\u03b8\u00b7x1\u2212sint\u03b8\u00b7x2sint\u03b8\u00b7x1+ cost\u03b8\u00b7x2]\n(2.87)\nIf we interpret tas the position of a token represented by xin a sequence, then we will \ufb01nd\nthat the above equation de\ufb01nes a simple positional embeddin g model. As shown in Figure 2.12\n(b), we start moving the token from position 0. Each time we mo ve one step forward, the vector\nis rotated by the angle \u03b8. Upon arriving at the position t, the representation of the token with\npositional context is given by Ro(x,i\u03b8). As the rotations do not change the magnitude of the\nembedding, the original \u201cmeaning\u201d of the token is retained. The positional information is injected\ninto the embedding, when it gets rotated.\nA popular way to understand vector rotation is to de\ufb01ne it in c omplex spaces. It is easy\nto transform each vector x=[\nx1x2]\nin the 2D Euclidean space R2to a complex number\nx\u2032=x1+ix2in the complex space Cvia a bijective linear map. Then, the rotation of xwith the\nanglet\u03b8corresponds to the multiplication by eit\u03b8. Given that eit\u03b8= cost\u03b8+isint\u03b8, the rotation\n\n--- Page 96 ---\n\n2.3 Long Sequence Modeling 89\nx1x2\n\u03b8vector x\nxR\u03b8rotated vector\n(a) Single-step Rotationx1x2\n\u03b8\n\u03b8\n\u03b8xxR\u03b8\nxR2\u03b8\nxR3\u03b8\n(b) Multi-step Rotation\nx1x2\n7\u03b8\n7\u03b8sleeping 4\nsleeping 11cat2\ncat9The 1cat2is3sleeping 4peacefully 5\nin6the7warm 8sunlight 9.10\nEvery 1afternoon ",
    "3fd6b5df1afdc027": "in the 2D Euclidean space R2to a complex number\nx\u2032=x1+ix2in the complex space Cvia a bijective linear map. Then, the rotation of xwith the\nanglet\u03b8corresponds to the multiplication by eit\u03b8. Given that eit\u03b8= cost\u03b8+isint\u03b8, the rotation\n\n--- Page 96 ---\n\n2.3 Long Sequence Modeling 89\nx1x2\n\u03b8vector x\nxR\u03b8rotated vector\n(a) Single-step Rotationx1x2\n\u03b8\n\u03b8\n\u03b8xxR\u03b8\nxR2\u03b8\nxR3\u03b8\n(b) Multi-step Rotation\nx1x2\n7\u03b8\n7\u03b8sleeping 4\nsleeping 11cat2\ncat9The 1cat2is3sleeping 4peacefully 5\nin6the7warm 8sunlight 9.10\nEvery 1afternoon 2,3you 4\u2019ll5\ufb01nd 6that 7\nthe8cat9is10sleeping 11on12my13bed 14.15\n(c) Angles between embeddings of two tokens at different pos itions\nFig. 2.12: Illustrations of vector rotations in a plane. Sub-\ufb01gures (a ) and (b) show rotations of a vector in a single\nstep and multiple steps, respectively. Sub-\ufb01gure (c) shows the embeddings of tokens catandsleeping in two different\nsentences. We show these sentences with a subscript af\ufb01xed t o each token to indicate its position. If we represent\ntokens as vectors, we can add positional information by rota ting these vectors. This rotation preserves the \u201cdistances \u201d\nbetween the vectors. For example, given that the distance be tween catandsleeping is the same in both sentences, the\nangle between their embeddings also remains the same during rotation.\noperation can be re-expressed in the form\nxRt\u03b8\u21a6\u2192x\u2032eit\u03b8\n= (x1+ix2)(cost\u03b8+isint\u03b8)\n= cost\u03b8\u00b7x1\u2212sint\u03b8\u00b7x2+i(sint\u03b8\u00b7x1+ cost\u03b8\u00b7x2) (2.88)\nHere we denote the token representation x\u2032eit\u03b8byC(x,t\u03b8). The inner product of the representa-\ntions of the tokens at positions tandscan be written as\n\u27e8C(x,t\u03b8),C(y,s\u03b8)\u27e9= (x\u2032y\u2032)ei(t\u2212s)\u03b8(2.89)\nwhere y\u2032is the complex conjugate of y\u2032. As can be seen, the result of this inner product involves\na termt\u2212s, and so it can model the offset between the two tokens.\n\n--- Page 97 ---\n\n90 Generative Models\nNow we go back to representations in the 2D Euclidean space. T he dot-product of Ro(x,t\u03b8)\nandRo(y,s\u03b8)is can be written as a function of (t\u2212s)\u03b8\nRo(x,t\u03b8)[Ro( y,s\u03b8)]T=xRt\u03b8[yRs\u03b8]T\n=xRt\u03b8[Rs\u03b8]TyT\n=xR(t\u2212s)\u03b8yT(2.90)\nGiven this result, if we consider Ro(x,t\u03b8)andRo(y,s\u03b8)as the query and the key, then the self-\nattention operation will implicitly involve the modeling o f relative positional context.\nThis rotary positional embedding can be extended to multi-d imensional embeddings. For\nad-dimensional token embedding x=[\nx1x2... x d]\n, we can treat it as ad\n2-dimensional\ncomplex vector x\u2032=[\nx\u2032\n1x\u2032\n2... x\u2032\nd/2]\n=[\nx1+ix2x3+ix4... x d\u22121+ixd]\n, where\neach consecutive pair of items forms a complex number. Then, the rotary positional embedding in\nthe complex space is given by\nC(x,t\u03b8) =d/2\u2211\nk=1x\u2032\nkeit\u03b8k\u20d7 ek (2.91)\nwhere\u20d7 ekis the standard basis vector with a single non-zero value in t hek-th coordinate and 0\u2019s\nelsewhere [ Biderman et al. ,2021 ].\nAlthough this formula involves a complicated expression, i ts equivalent form in the d-dimensional\nEuclidean space is relatively easy to understand. We can wri te it as\nRo(x,t\u03b8) =[\nx1x2... x d]\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0Rt\u03b81\nRt\u03b82\n...\nRt\u03b8d",
    "d65dd2638b6cb922": "1+ix2x3+ix4... x d\u22121+ixd]\n, where\neach consecutive pair of items forms a complex number. Then, the rotary positional embedding in\nthe complex space is given by\nC(x,t\u03b8) =d/2\u2211\nk=1x\u2032\nkeit\u03b8k\u20d7 ek (2.91)\nwhere\u20d7 ekis the standard basis vector with a single non-zero value in t hek-th coordinate and 0\u2019s\nelsewhere [ Biderman et al. ,2021 ].\nAlthough this formula involves a complicated expression, i ts equivalent form in the d-dimensional\nEuclidean space is relatively easy to understand. We can wri te it as\nRo(x,t\u03b8) =[\nx1x2... x d]\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0Rt\u03b81\nRt\u03b82\n...\nRt\u03b8d/2\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb(2.92)\nwhereRt\u03b8k=[\ncost\u03b8ksint\u03b8k\n\u2212sint\u03b8kcost\u03b8k]\n.\u03b8=[\n\u03b81,...,\u03b8 d/2]\nare the parameters for controlling the an-\ngles of rotations in different dimensions. Typically, \u03b8kis set to 10000\u22122(k\u22121)\nd, which is analogous\nto the setting in sinusoidal embeddings.\nIn a practical implementation, Eq. ( 2.92) can be rewritten into a form that relies solely on the\nelement-wise product and addition of vectors.\nRo(x,t\u03b8) =\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0x1\nx2\n...\nxd\u22121\nxd\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fbT\n\u2299\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0cost\u03b81\ncost\u03b81\n...\ncost\u03b8d/2\ncost\u03b8d/2\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fbT\n+\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\u2212x2\nx1\n...\n\u2212xd\nxd\u22121\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fbT\n\u2299\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0sint\u03b81\nsint\u03b81\n...\nsint\u03b8d/2\nsint\u03b8d/2\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fbT\n(2.93)\nFinally, we rewrite Eq. ( 2.85) to obtain the form of the embedding at position i\n\n--- Page 98 ---\n\n2.3 Long Sequence Modeling 91\nei= Ro( xi,i\u03b8) (2.94)\n2.3.5.4 Position Interpolation\nIn position interpolation, our goal is to map the positions i n the new sequence to match the ob-\nserved range in training. Suppose the sequence length for tr aining ranges from 0toml. When\nm>m lat test time, we represent the positions in [0,m]such that our representations \ufb01t [0,ml].\nTo illustrate, consider the rotary positional embedding mo del described above. The embedding\nof each token is described by a model Ro(xi,i\u03b8)in which\u03b8=[\n\u03b81,...,\u03b8 d/2]\nare the parameters.\nRo(xi,i\u03b8)can be cast in the form of a linear combination of two periodic functions (see Eq.\n(2.93))\ncosi\u03b8=[\ncosi\u03b81...cosi\u03b8d/2]\n(2.95)\nsini\u03b8=[\nsini\u03b81...sini\u03b8d/2]\n(2.96)\n\u03b8kis a exponential function of kand takes the form\n\u03b8k=b\u22122(k\u22121)\nd (2.97)\nwherebis the base. The period of cosi\u03b8kandsini\u03b8kis\nTk= 2\u03c0\u00b7b2(k\u22121)\nd (2.98)\nThe key idea behind position interpolation is to adjust this period so that the new positions can\nbe encoded within the range [0,ml]. One way to achieve this is to scale up Tkbym\nml, given by\nT\u2032\nk=m\n",
    "13b4755b3e012fcb": "=[\n\u03b81,...,\u03b8 d/2]\nare the parameters.\nRo(xi,i\u03b8)can be cast in the form of a linear combination of two periodic functions (see Eq.\n(2.93))\ncosi\u03b8=[\ncosi\u03b81...cosi\u03b8d/2]\n(2.95)\nsini\u03b8=[\nsini\u03b81...sini\u03b8d/2]\n(2.96)\n\u03b8kis a exponential function of kand takes the form\n\u03b8k=b\u22122(k\u22121)\nd (2.97)\nwherebis the base. The period of cosi\u03b8kandsini\u03b8kis\nTk= 2\u03c0\u00b7b2(k\u22121)\nd (2.98)\nThe key idea behind position interpolation is to adjust this period so that the new positions can\nbe encoded within the range [0,ml]. One way to achieve this is to scale up Tkbym\nml, given by\nT\u2032\nk=m\nml\u00b72\u03c0\u00b7b2(k\u22121)\nd (2.99)\nHence all points in [0,m]are compressed into [0,ml]. This linear scaling can be easily realized\nby modifying the input to the embedding model [ Chen et al. ,2023c ]. The new model with linear\npositional interpolation is given by\nRo\u2032(xi,i\u03b8) = Ro( xi,ml\nmi\u03b8) (2.100)\nAnother method of positional interpolation is to scale the b ase17. Suppose that the base bis\nscaled by\u03bb. We wish the period of this new model in the last dimension of \u03b8(i.e., dimensiond\n2)\nto be equal to that of the linear positional interpolation mo del. This can be expressed as\n2\u03c0\u00b7(\u03bbb)2(d\n2\u22121)\nd =m\nml\u00b72\u03c0\u00b7b2(d\n2\u22121)\nd (2.101)\n17This method was \ufb01rst proposed in https://www.reddit.com/r/LocalLLaMA/comments/14lz7j 5/\nntkaware_scaled_rope_allows_llama_models_to_have/\n\n--- Page 99 ---\n\n92 Generative Models\nSolving this equation, we obtain\n\u03bb=(m\nml)d\n2(d\n2\u22121)\n=(m\nml)d\nd\u22122 (2.102)\nThis gives an embedding model\nRo\u2032(xi,i\u03b8) = Ro( xi,i\u03b8\u2032) (2.103)\nwhere\n\u03b8\u2032=[\n(\u03bbb)\u22120\nd,(\u03bbb)\u22122\nd,...,(\u03bbb)\u2212d\u22122\nd]\n(2.104)\nNote that scaling the base provides a non-uniform method for scaling the periods across dif-\nferent dimensions of \u03b8. This method has been found to be helpful for extending LLMs t o longer\nsequences, and several improvements have been developed [ Peng et al. ,2024 ;Ding et al. ,2024 ].\n2.3.6 Remarks\nIn this section, we have presented a variety of methods for lo ng-context language modeling. We\nclose this section by discussing some interesting issues re lated to these methods.\n2.3.6.1 Need for Long Context\nOne of the ultimate goals of long-context LLMs is that these m odels can precisely encode in\ufb01nite\ncontext. The so-called in\ufb01nite context refers more to the fa ct that an LLM can continuously read\nwords. This motivates LLMs that can handle extremely long co ntext or stream data. As discussed\nin Section 2.3.3 , it is common to use \ufb01xed-size memory models to process conti nuously expanding\ncontext. Many such systems are based on recurrent architect ures or their variants, because they\nare inherently suited to model time series problems where th e effects of past inputs continue\ninde\ufb01nitely. Another way to achieve in\ufb01nite memory is to dev elop alternatives to self-attention\nmodels, for example, one can use continuous-space attentio n models to encode context, which\nremoves the dependency on context length [ Martins et al. ,2022 ].\nWhen studying long-context LLMs, it is natural to wonder wha t mechanisms may explain the\nuse of long context in language modeling. Can we compress the representation of in\ufb01nite context\ninto a relatively small-sized model? Are all context tokens useful for predicting next tokens? How\ndo LLMs prepare for token prediction when they see the contex t? Can we know in advance which\ncontextual information will be critical for prediction? Ge neral answers to all these questions\nare not obvious, but they inspire follow-on research of expl ainable models, and some interesting\nresults have been found.",
    "5cc123bd7fb7b2c4": " series problems where th e effects of past inputs continue\ninde\ufb01nitely. Another way to achieve in\ufb01nite memory is to dev elop alternatives to self-attention\nmodels, for example, one can use continuous-space attentio n models to encode context, which\nremoves the dependency on context length [ Martins et al. ,2022 ].\nWhen studying long-context LLMs, it is natural to wonder wha t mechanisms may explain the\nuse of long context in language modeling. Can we compress the representation of in\ufb01nite context\ninto a relatively small-sized model? Are all context tokens useful for predicting next tokens? How\ndo LLMs prepare for token prediction when they see the contex t? Can we know in advance which\ncontextual information will be critical for prediction? Ge neral answers to all these questions\nare not obvious, but they inspire follow-on research of expl ainable models, and some interesting\nresults have been found. For example, Deletang et al. [2024 ] conducted extensive experiments\nto show that LLMs are powerful in-context compressors. Alth ough viewing predictive models\nas compression models has long been studied in machine learn ing, it also provides insights into\nour understanding of the LLM scaling laws. Pal et al. [2023 ] and Wu et al. [2024 ] investigated\nwhether the features learned up to the current step, though n ot intentionally, are already suf\ufb01cient\n\n--- Page 100 ---\n\n2.3 Long Sequence Modeling 93\nfor predicting tokens at the following steps. Note that the n eed for long-context in language\nmodeling is highly dependent on the problem that we address. A related issue is where to apply\nLLMs and how to evaluate them. For example, in summarization tasks we may only need to distill\nand focus on a few key aspects of the text, while in retrieval- like tasks we need to \u201cmemorize\u201d\nthe entire context so that the relevant information can be ac cessed. We will discuss the evaluation\nissue later in this subsection.\n2.3.6.2 Pre-training or Adapting LLMs?\nTraining LLMs requires signi\ufb01cant computational costs. Al though it is straightforward to train\nLLMs on long sequence data, the training becomes computatio nally unwieldy for large data sets. It\nis common practice to pre-train LLMs on general datasets, an d then adapt them with modest \ufb01ne-\ntuning effort. For example, LLMs with relative or rotary pos itional embeddings can be directly\ntrained on large-scale data in the pre-training phase. Whil e the resulting models may exhibit some\nabilities to extrapolate lengths in the inference phase, it may be more effective to \ufb01ne-tune them\non longer sequences.\nIdeally, we would like to pre-train LLMs with standard Trans former architectures and adapt\nthem to new tasks. This allows us to use many off-the-shelf LL Ms and ef\ufb01ciently adapt them to\nhandle long sequences. However, when new architectures are adopted, it seems inevitable that\nwe need to train these models from scratch. This poses practi cal dif\ufb01culties for developing long-\ncontext LLMs, as we cannot leverage well-developed, pre-tr ained models and must instead train\nthem ourselves. On the other hand, \ufb01ne-tuning is still an eff ective way to adapt LLMs with certain\narchitectures that are different from those in pre-trainin g. An example is models augmented with\nexternal memories. In these models, the pre-trained LLMs ar e \ufb01xed, and the focus is on how\nto make these LLMs collaborate with the memory models. In RAG , for instance, it is common\nto \ufb01ne-tune LLMs to improve their use of retrieval-augmente d inputs. Another example of \ufb01ne-\ntuning LLMs for long-context modeling is that we train an LLM with full attention models, and\nthen replace them with sparse attention models in the \ufb01ne-tu ning phase. The pre-trained LLM\nprovides initial values of model parameters used in a differ ent model, and this model is then \ufb01ne-\ntuned as usual.\n2.3.6.3 Evaluating Long-context LLMs\nEvaluating long-context LLMs is important, but it is a new is sue in NLP. The general idea is that,\nif we input a long context to an LLM, then we can check from the o utput of the LLM whether it\nunderstands the entire context and makes use of it in predict ing following tokens. In conventional\nresearch of NLP, such evaluations",
    "4e4904128c16741b": " common\nto \ufb01ne-tune LLMs to improve their use of retrieval-augmente d inputs. Another example of \ufb01ne-\ntuning LLMs for long-context modeling is that we train an LLM with full attention models, and\nthen replace them with sparse attention models in the \ufb01ne-tu ning phase. The pre-trained LLM\nprovides initial values of model parameters used in a differ ent model, and this model is then \ufb01ne-\ntuned as usual.\n2.3.6.3 Evaluating Long-context LLMs\nEvaluating long-context LLMs is important, but it is a new is sue in NLP. The general idea is that,\nif we input a long context to an LLM, then we can check from the o utput of the LLM whether it\nunderstands the entire context and makes use of it in predict ing following tokens. In conventional\nresearch of NLP, such evaluations are often aimed at examini ng the ability of NLP models in\nhandling long-range dependencies. However, the size of con texts used in recent LLMs is much\nlarger than that used in NLP systems a few years ago. This moti vates researchers to develop new\nevaluation benchmarks and metrics for long-context LLMs.\nOne approach is to use the perplexity metric. However, in spi te of its apparent simplicity, this\nmethod tends to re\ufb02ect more on the LLMs\u2019 ability to make use of local context rather than global\ncontext. It is therefore tempting to develop evaluation met hods that are speci\ufb01c to long-context\nLLMs. Popular methods include various synthetic tasks wher e arti\ufb01cially generated or modi\ufb01ed\n\n--- Page 101 ---\n\n94 Generative Models\ndata is used to evaluate speci\ufb01c capabilities of long-conte xt LLMs. In needle-in-a-haystack18and\npasskey retrieval tasks [ Mohtashami and Jaggi ,2024 ;Chen et al. ,2023c ], for instance, LLMs are\nrequired to identify and extract a small, relevant piece of i nformation from a large volume of given\ntext. The assumption here is that an LLM with suf\ufb01cient memor y should remember earlier parts\nof the text as it processes new information. This LLM can thus pick out the relevant details, which\nmight be sparse and hidden among much irrelevant informatio n, from the text. Alternatively,\nin copy memory tasks (or copy tasks for short), LLMs are used t o repeat the input text or a\nspeci\ufb01c segment multiple times. These tasks were initially proposed to test the extent to which\nrecurrent models can retain and recall previously seen toke ns [Hochreiter and Schmidhuber ,1997 ;\nArjovsky et al. ,2016 ], and have been adopted in evaluating recent LLMs [ Bulatov et al. ,2022 ;\nGu and Dao ,2023 ].\nAnother approach to evaluating long-context LLMs is to test them on NLP tasks that involve\nvery long input sequences. Examples include long-document or multi-document summarization,\nlong-document question answering, code completion, and so on. A bene\ufb01t of this approach is that\nit can align evaluations with user expectations.\nAlthough many methods have been developed, there is still no general way to evaluate long-\ncontext LLMs [ Liu et al. ,2024c ]. One problem is that most of these methods focus on speci\ufb01c\naspects of LLMs, rather than their fundamental ability to mo del very long contexts. Even though\nan LLM can pick out the appropriate piece of text from the inpu t, we cannot say that it truly un-\nderstands the entire context. Instead, it might just rememb er some important parts of the context,\nor even simply recall the answer via the model learned in pre- training. Moreover, the data used\nin many tasks is small-scale and relatively preliminary, le ading to discrepancies between evalu-\nation results and actual application performance. A more in teresting issue is that the results of\nLLMs are in\ufb02uenced by many other factors and experimental se tups, for example, using different\nprompts can lead to very different outcomes. This makes eval uation even more challenging be-\ncause improvements may not solely result from better modeli ng of long contexts, and there is a\nrisk of overclaiming our results. Nevertheless, many open q uestions remain in the development\nand evaluation of long-context LLMs. For example, these mod els still suffer from limitations\nsuch as restricted context length and high latency. Studyin g these issues is likely to prove valuable",
    "2d32fa95ac754270": " entire context. Instead, it might just rememb er some important parts of the context,\nor even simply recall the answer via the model learned in pre- training. Moreover, the data used\nin many tasks is small-scale and relatively preliminary, le ading to discrepancies between evalu-\nation results and actual application performance. A more in teresting issue is that the results of\nLLMs are in\ufb02uenced by many other factors and experimental se tups, for example, using different\nprompts can lead to very different outcomes. This makes eval uation even more challenging be-\ncause improvements may not solely result from better modeli ng of long contexts, and there is a\nrisk of overclaiming our results. Nevertheless, many open q uestions remain in the development\nand evaluation of long-context LLMs. For example, these mod els still suffer from limitations\nsuch as restricted context length and high latency. Studyin g these issues is likely to prove valuable\nfuture directions.\n2.4 Summary\nIn this chapter, we have discussed the concept of LLMs and rel ated techniques. This can be consid-\nered a general, though not comprehensive, introduction to L LMs, laying the foundation for further\ndiscussions on more advanced topics in subsequent chapters . Furthermore, we have explored two\nways to scale up LLMs. The \ufb01rst focuses on the large-scale pre -training of LLMs, which is cru-\ncial for developing state-of-the-art models. The second fo cuses on methods for adapting LLMs to\nlong inputs, including optimizing attention models, desig ning more ef\ufb01cient and compressed KV\ncaches, incorporating memory models, and exploring better positional embeddings.\nThe strength of LLMs lies in their ability to break the constr aints of training NLP models for\na limited number of speci\ufb01c tasks. Instead, LLMs learn from l arge amounts of text through the\nsimple task of token prediction \u2014 we predict the next token in a sentence given its prior tokens.\n18https://github.com/gkamradt/LLMTest_NeedleInAHaysta ck\n\n--- Page 102 ---\n\n2.4 Summary 95\nA general view is that, by repeating this token prediction ta sk a large number of times, LLMs can\nacquire some knowledge of the world and language, which can t hen be applied to new tasks. As a\nresult, LLMs can be prompted to perform any task by framing it as a task of predicting subsequent\ntokens given prompts. This emergent ability in language mod els comes from several dimensions,\nsuch as scaling up training, model size, and context size. It is undeniable that scaling laws are\ncurrently the fundamental principle adopted in developing large language models, although sim-\nply increasing model size has yet to prove suf\ufb01cient for achi eving AGI. These continuously scaled\nLLMs have been found to show capabilities in general-purpos e language understanding, genera-\ntion, and reasoning. More recently, it has been found that sc aling up the compute at inference time\ncan also lead to signi\ufb01cant improvements in complex reasoni ng tasks [ OpenAI ,2024 ].\nGiven their amazing power, LLMs have attracted considerabl e interest, both in terms of tech-\nniques and applications. As a result, the explosion of resea rch interest in LLMs has also led to a\nvast number of new techniques and models. However, we do not a ttempt to provide a comprehen-\nsive literature review on all aspects of LLMs, given the rapi d evolution of the \ufb01eld. Nevertheless,\none can still gain knowledge about LLMs from general reviews [Zhao et al. ,2023 ;Minaee et al. ,\n2024 ] or more focused discussions on speci\ufb01c topics [ Ruan et al. ,2024 ].\n\n--- Page 103 ---\n\nCHAPTER 3\nPrompting\nIn the context of LLMs, prompting refers to the method of providing an LLM with a speci\ufb01c input\nor cue to generate a desired output or perform a task. For exam ple, if we want the LLM to translate\na sentence from English to Chinese, we can prompt it like this\nTranslate the text from English to Chinese.\nText: The early bird catches the worm.\nTranslation:\nPrompting is crucial for LLMs because it directly in\ufb02uences how effectively these models under-\nstand and respond to user queries. A well-crafted prompt can guide an LLM to generate more\naccurate, relevant, and contextually appropriate respons es. Furthermore, this process can be iter-\natively re\ufb01ned. By analyzing the responses of the LLM, users",
    "98db8678638c1357": " or more focused discussions on speci\ufb01c topics [ Ruan et al. ,2024 ].\n\n--- Page 103 ---\n\nCHAPTER 3\nPrompting\nIn the context of LLMs, prompting refers to the method of providing an LLM with a speci\ufb01c input\nor cue to generate a desired output or perform a task. For exam ple, if we want the LLM to translate\na sentence from English to Chinese, we can prompt it like this\nTranslate the text from English to Chinese.\nText: The early bird catches the worm.\nTranslation:\nPrompting is crucial for LLMs because it directly in\ufb02uences how effectively these models under-\nstand and respond to user queries. A well-crafted prompt can guide an LLM to generate more\naccurate, relevant, and contextually appropriate respons es. Furthermore, this process can be iter-\natively re\ufb01ned. By analyzing the responses of the LLM, users can adjust their prompts to align\nmore closely with their speci\ufb01c needs. Given the importance of prompting in applying LLMs,\nprompt design has become an essential skill for users and dev elopers working with LLMs. This\nleads to an active research area, called prompt engineering , in which we design effective prompts\nto make better use of LLMs and enhance their practical utilit y in real-world applications.\nAn important concept related to prompting is in-context learning . When prompting an LLM,\nwe can add new information to the context, such as demonstrat ions of problem-solving. This\nallows the LLM to learn from this context how to solve the prob lem. Here is an example of\nprompting LLMs with a few demonstrations of how to classify t ext based on sentiment polarity.\nHere are some examples of text classi\ufb01cation.\nExample 1: We had a delightful dinner together. \u2192Label: Positive\nExample 2: I\u2019m frustrated with the delays. \u2192Label: Negative\nWhat is the label for \u201cThat comment was quite hurtful.\u201d?\nLabel:\nIn-context learning is often seen as an emergent ability of L LMs that arises after pre-training.\nThough LLMs can be trained or tuned to perform new tasks, in-c ontext learning provides a very\nef\ufb01cient way to adapt these models without any training or tu ning effort. Perhaps this is one of\nthe most notable features of LLMs: they indeed learn general knowledge about the world and\nlanguage during pre-training, which we can easily apply to n ew challenges. Moreover, in-context\nlearning re\ufb02ects the broader trend of making AI systems more generalizable and user-friendly.\nInstead of requiring specialized engineers to \ufb01ne-tune mod els for every unique task, users can\ninteract with LLMs in a more intuitive way, simply providing examples or adjusting the context\nas needed.\nIn this chapter, we focus on prompting techniques in LLMs. We begin by considering several\ninteresting prompt designs commonly used in prompt enginee ring. Then, we discuss a series of\n96\n\n--- Page 104 ---\n\n3.1 General Prompt Design 97\nre\ufb01nements to these methods. Finally, we explore approache s for automating prompt design.\n3.1 General Prompt Design\nThis section presents basic concepts in prompt design, alon g with examples of how to prompt\nLLMs for various NLP tasks. Since the effectiveness of promp ting is highly dependent on the\nLLMs being used, prompts often vary across different LLMs, m aking it dif\ufb01cult to provide a\ncomprehensive list of prompts for all LLMs and downstream ta sks. Therefore, this discussion is\nnot focused on any speci\ufb01c LLM. Instead, the goal is to provid e guiding principles for prompt\ndesign.\n3.1.1 Basics\nThe term prompt is used in many different ways. In this chapter we de\ufb01ne a prom pt as the input\ntext to an LLM, denoted by x. The LLM generates a text yby maximizing the probability Pr(y|x).\nIn this generation process, the prompt acts as the condition on which we make predictions, and it\ncan contain any information that helps describe and solve th e problem.\nA prompt can be obtained using a prompt template (or template for short) [ Liu et al. ,2023a ].\nA template is a piece of text containing placeholders or vari ables, where each placeholder can\nbe \ufb01lled with speci\ufb01c information. Here are two templates fo r asking the LLM for weekend\nsuggestions.\nPlease give me some suggestions for a fun weekend.\nIf{\u2217premise \u2217}, what are your suggestions for a fun weekend.\nIn the \ufb01rst template, we simply",
    "3f13571ca13d94cb": "1.1 Basics\nThe term prompt is used in many different ways. In this chapter we de\ufb01ne a prom pt as the input\ntext to an LLM, denoted by x. The LLM generates a text yby maximizing the probability Pr(y|x).\nIn this generation process, the prompt acts as the condition on which we make predictions, and it\ncan contain any information that helps describe and solve th e problem.\nA prompt can be obtained using a prompt template (or template for short) [ Liu et al. ,2023a ].\nA template is a piece of text containing placeholders or vari ables, where each placeholder can\nbe \ufb01lled with speci\ufb01c information. Here are two templates fo r asking the LLM for weekend\nsuggestions.\nPlease give me some suggestions for a fun weekend.\nIf{\u2217premise \u2217}, what are your suggestions for a fun weekend.\nIn the \ufb01rst template, we simply instruct the LLM to return som e suggestions. So the template\nis just a piece of text with no variables. In the second templa te, the variable {\u2217premise \u2217}needs to\nbe speci\ufb01ed by the users to provide a premise for making sugge stions. For example, if we input\npremise = the weather is nice this weekend\nthen we can generate a prompt\nIf the weather is nice this weekend,\nwhat are your suggestions for a fun weekend.\nWe can also design a template with multiple variables. Here i s an example in which we\ncompare the two sentences in terms of their semantic similar ity.\n\n--- Page 105 ---\n\n98 Prompting\nHere is a sentence\n{\u2217sentence1 \u2217}\nHere is another sentence\n{\u2217sentence2 \u2217}\nCompute the semantic similarity between the two sentences\nA popular way to format prompts is to write each input or outpu t in a \u201cname:content\u201d style.\nFor example, we can describe a conversation between two peop le, named John and David, and use\nthe LLM to continue the conversation. A template of such prom pts is given by\nJohn: {\u2217utterance1 \u2217}\nDavid: {\u2217utterance2 \u2217}\nJohn: {\u2217utterance3 \u2217}\nDavid: {\u2217utterance4 \u2217}\nJohn: {\u2217utterance5 \u2217}\nDavid: {\u2217utterance6 \u2217}\nJohn: {\u2217utterance7 \u2217}\nDavid:\nThe \u201cname:content\u201d format can be used to de\ufb01ne the task that w e want the LLM to perform.\nFor example, given that \u201cQ\u201d and \u201cA\u201d are commonly used abbrevi ations for \u201cQuestion\u201d and \u201cAn-\nswer\u201d, respectively, we can use the following template to do question-answering.\nQ:{\u2217question \u2217}\nA:\nThis format can be used to describe more complex tasks. For ex ample, the following is an\nexample of providing a speci\ufb01cation for a translation task\nTask: Translation\nSource language: English\nTarget language: Chinese\nStyle: Formal text\nTemplate: Translate the following sentence: {\u2217sentence \u2217}\nIn practical systems, it is common to represent and store suc h data in key-value pairs, such as the\nJSON format1.\nWhen the problem is dif\ufb01cult to describe in an attribute-bas ed manner, it is more common\nto instruct LLMs with a clear and detailed description. Ther e are many ways to do this. One\n1The JSON representation is\n\n--- Page 106 ---\n\n3.1 General Prompt Design 99\nexample is to assign a role to LLMs and provide suf\ufb01cient cont ext. The following is a template\nthat instructs an LLM to act as an expert and answer questions from children.\nYou are a computer scientist with extensive knowledge in the \ufb01eld\nof deep learning.\nPlease explain the following computer-related concept to a child\naround 10 years old, using simple examples whenever possibl e.\n{\u2217concept \u2217}\nHere the text \u201cYou are a computer scientist ... deep learning . \u201d is sometimes called system\ninformation, and is provided to help the LLM understand the c ontext or constraints of the task it\nis being asked to perform.\n3.1.2 In-context Learning\nLearning can occur during inference. In-context learning i s one such method, where prompts\ninvolve demonstrations of problem-solving, and LLMs can le arn from these demonstrations how\nto solve new problems. Since we do not update model parameter s in this process, in-context\nlearning can be viewed as a way to ef\ufb01ciently activate and reo rganize the knowledge learned in\n",
    "5360e75e317c1f35": " act as an expert and answer questions from children.\nYou are a computer scientist with extensive knowledge in the \ufb01eld\nof deep learning.\nPlease explain the following computer-related concept to a child\naround 10 years old, using simple examples whenever possibl e.\n{\u2217concept \u2217}\nHere the text \u201cYou are a computer scientist ... deep learning . \u201d is sometimes called system\ninformation, and is provided to help the LLM understand the c ontext or constraints of the task it\nis being asked to perform.\n3.1.2 In-context Learning\nLearning can occur during inference. In-context learning i s one such method, where prompts\ninvolve demonstrations of problem-solving, and LLMs can le arn from these demonstrations how\nto solve new problems. Since we do not update model parameter s in this process, in-context\nlearning can be viewed as a way to ef\ufb01ciently activate and reo rganize the knowledge learned in\npre-training without additional training or \ufb01ne-tuning. T his enables quick adaptation of LLMs to\nnew problems, pushing the boundaries of what pre-trained LL Ms can achieve without task-speci\ufb01c\nadjustments.\nIn-context learning can be illustrated by comparing three m ethods: zero-shot learning, one-\nshot learning and few-shot learning. Zero-shot learning, a s its name implies, does not involve a\ntraditional \u201clearning\u201d process. It instead directly appli es LLMs to address new problems that were\nnot observed during training. In practice, we can repetitiv ely adjust prompts to guide the LLMs in\ngenerating better responses, without demonstrating probl em-solving steps or providing examples.\nConsider the following example. Suppose we want to use an LLM as an assistant that can help\ncorrect English sentences. A zero-shot learning prompt is g iven by\n{\n\"Task\": \"Translation\"\n\"Source language\": \"English\"\n\"Target language\": \"Chinese\"\n\"Style\": \"Formal text\"\n\"Template\": \"Translate the following sentence: {\u2217sentence \u2217}\"\n}\n\n--- Page 107 ---\n\n100 Prompting\nSYSTEM You are a helpful assistant, and are great at grammar correct ion.\nUSER You will be provided with a sentence in English. The task is\nto output the correct sentence.\nInput: She don\u2019t like going to the park.\nOutput:\nHere the gray words are used to indicate different \ufb01elds of th e prompt.\nIn one-shot learning, we extend this prompt by adding a demon stration of how to correct\nsentences, thereby allowing the LLM to learn from this newly -added experience.\nSYSTEM You are a helpful assistant, and are great at grammar correct ion.\nDEMO You will be provided with a sentence in English. The task is\nto output the correct sentence.\nInput: There is many reasons to celebrate.\nOutput: There are many reasons to celebrate.\nUSER You will be provided with a sentence in English. The task is\nto output the correct sentence.\nInput: She don\u2019t like going to the park.\nOutput:\nFurthermore, we can add more demonstrations to enable few-s hot learning.\nSYSTEM You are a helpful assistant, and are great at grammar correct ion.\nDEMO1 You will be provided with a sentence in English. The task is\nto output the correct sentence.\nInput: There is many reasons to celebrate.\nOutput: There are many reasons to celebrate.\nDEMO2 You will be provided with a sentence in English. The task is\nto output the correct sentence.\nInput: Me and my friend goes to the gym every day.\nOutput: My friend and I go to the gym every day.\nUSER You will be provided with a sentence in English. The task is\nto output the correct sentence.\nInput: She don\u2019t like going to the park.\nOutput:\nIn few-shot learning, we essentially provide a pattern that maps some inputs to the corre-\nsponding outputs. The LLM attempts to follow this pattern in making predictions, provided that\nthe prompt includes a suf\ufb01cient number of demonstrations, a lthough generally small. It is also\n\n--- Page 108 ---\n\n3.1 General Prompt Design 101\npossible to use simpler patterns to achieve this. For exampl e, one can use the following few-shot\nlearning prompt for translating words from Chinese to Engli sh.\nDEMO\u73b0\u5728\u2192 now\n\u6765\u2192 come\n\u53bb\u2192 go\n\u7537\u5b69\u2192 boy\nUSER\u5973\u5b69\u2192\nIf the LLM is powerful enough, few-shot learning can enable i t to address complex prob-\nlems, such as mathematical reasoning. For example, conside r the following task of summing two\nnumbers and then dividing the sum by their product.\nDEMO 12 5 \u2192 (12 + 5)/(12\u00d75) = 0.283\n3",
    "a36d84b5c420a00f": " maps some inputs to the corre-\nsponding outputs. The LLM attempts to follow this pattern in making predictions, provided that\nthe prompt includes a suf\ufb01cient number of demonstrations, a lthough generally small. It is also\n\n--- Page 108 ---\n\n3.1 General Prompt Design 101\npossible to use simpler patterns to achieve this. For exampl e, one can use the following few-shot\nlearning prompt for translating words from Chinese to Engli sh.\nDEMO\u73b0\u5728\u2192 now\n\u6765\u2192 come\n\u53bb\u2192 go\n\u7537\u5b69\u2192 boy\nUSER\u5973\u5b69\u2192\nIf the LLM is powerful enough, few-shot learning can enable i t to address complex prob-\nlems, such as mathematical reasoning. For example, conside r the following task of summing two\nnumbers and then dividing the sum by their product.\nDEMO 12 5 \u2192 (12 + 5)/(12\u00d75) = 0.283\n3 1 \u2192 (3 + 1)/(3\u00d71) = 1.33\n\u22129 4 \u2192 (\u22129 + 4)/(\u22129\u00d74) = 0.138\n15 15 \u2192 (15 + 15)/(15\u00d715) = 0.133\nUSER 19 73 \u2192\nIn many practical applications, the effectiveness of in-co ntext learning relies heavily on the\nquality of prompts and the fundamental abilities of pre-tra ined LLMs. On one hand, we need a\nsigni\ufb01cant prompt engineering effort to develop appropria te prompts that help LLMs learn more\neffectively from demonstrations. On the other hand, strong er LLMs can make better use of in-\ncontext learning for performing new tasks. For example, sup pose we wish to use an LLM to\ntranslate words from Inuktitut to English. If the LLM lacks p re-training on Inuktitut data, its\nunderstanding of Inuktitut will be weak, and it will be dif\ufb01c ult for the model to perform well in\ntranslation regardless of how we prompt it. In this case, we n eed to continue training the LLM\nwith more Inuktitut data, rather than trying to \ufb01nd better pr ompts.\nIt might be interesting to explore how in-context learning e merges during pre-training and\nwhy it works during inference. One simple understanding is t hat LLMs have gained some knowl-\nedge of problem-solving, but there are many possible predic tions, which are hard to distinguish\nwhen the models confront new problems. Providing demonstra tions can guide the LLMs to fol-\nlow the \u201ccorrect\u201d paths. Furthermore, some researchers hav e tried to interpret in-context learn-\ning from several different perspectives, including Bayesi an inference [ Xie et al. ,2022 ], gradient\ndecent [ Dai et al. ,2023 ;V on Oswald et al. ,2023 ], linear regression [ Aky\u00fcrek et al. ,2023 ], meta\nlearning [ Garg et al. ,2022 ], and so on.\n3.1.3 Prompt Engineering Strategies\nDesigning prompts is highly empirical. In general, there ar e many ways to prompt an LLM for\nperforming the same task, and we need to perform a number of tr ial-and-error runs to \ufb01nd a\nsatisfactory prompt. To write good prompts more ef\ufb01ciently , one can follow certain strategies.\nExamples of common prompting principles include\n\n--- Page 109 ---\n\n102 Prompting\n\u2022Describing the task as clearly as possible . When we apply an LLM to solve a problem,\nwe need to provide a precise, speci\ufb01c, and clear description of the problem and instruct the\nLLM to perform as we expect. This is particularly important w hen we want the output of\nthe LLM to meet certain expectations. For example, suppose w e are curious about climate\nchange. A simple prompt for asking the LLM to provide some inf ormation is\nTell me about climate change.\nSince this instruction is too general, the LLM may generate a response that addresses any\naspect of climate change, which may not align with our speci\ufb01 c interests. In this case, we\ncan instead use prompts that are speci\ufb01c and detailed. One su ch example is\nProvide a detailed explanation of the causes and effects of c limate change,\nincluding the impact on global temperatures, weather patte rns, and sea\nlevels. Also, discuss possible solutions and actions being taken to mitigate\nthese effects.\nNow suppose we intend to explain climate change to a 10-year- old child. We can adjust the\nabove prompt further.\nExplain the causes and effects of climate change to a ",
    "f257696922bd4260": " important w hen we want the output of\nthe LLM to meet certain expectations. For example, suppose w e are curious about climate\nchange. A simple prompt for asking the LLM to provide some inf ormation is\nTell me about climate change.\nSince this instruction is too general, the LLM may generate a response that addresses any\naspect of climate change, which may not align with our speci\ufb01 c interests. In this case, we\ncan instead use prompts that are speci\ufb01c and detailed. One su ch example is\nProvide a detailed explanation of the causes and effects of c limate change,\nincluding the impact on global temperatures, weather patte rns, and sea\nlevels. Also, discuss possible solutions and actions being taken to mitigate\nthese effects.\nNow suppose we intend to explain climate change to a 10-year- old child. We can adjust the\nabove prompt further.\nExplain the causes and effects of climate change to a 10-year -old child.\nTalk about how it affects the weather, sea levels, and temper atures. Also,\nmention some things people are doing to help. Try to explain i n simple\nterms and do not exceed 500 words.\n\u2022Guiding LLMs to think . LLMs have exhibited surprisingly good capabilities to \u201cth ink\u201d.\nA common example is that well-developed LLMs have achieved i mpressive performance\nin mathematical reasoning tasks, which are considered chal lenging. In prompt engineering,\nthe \u201cthinking\u201d ability of LLMs needs to be activated through appropriate prompting, espe-\ncially for problems that require signi\ufb01cant reasoning effo rts. In many cases, an LLM that\nis instructed to \u201cthink\u201d can produce completely different r esults compared with the same\nLLM that is instructed to perform the task straightforwardl y. For example, Kojima et al.\n[2022 ] found that simply appending \u201cLet\u2019s think step by step\u201d to th e end of each prompt\ncan improve the performance of LLMs on several reasoning tas ks. LLMs can be prompted\nto \u201cthink\u201d in a number of ways. One method is to instruct LLMs t o generate steps for rea-\nsoning about the problem before reaching the \ufb01nal answer. Fo r example, consider a task of\nsolving mathematical problems. See below for a simple promp t for this task.\n\n--- Page 110 ---\n\n3.1 General Prompt Design 103\nYou are a mathematician. You will be provided with a math prob lem.\nPlease solve the problem.\nSince solving math problems requires a detailed reasoning p rocess, LLMs would probably\nmake mistakes if they attempted to work out the answer direct ly. So we can explicitly ask\nLLMs to follow a given reasoning process before coming to a co nclusion.\nYou are a mathematician. You will follow these detailed reas oning steps\nwhen solving math problems.\nStep 1: Problem Interpretation.\nThe mathematician carefully listens to your query and under stands the in-\ntricate details of the mathematical challenge you have pres ented.\nStep 2: Strategy Formulation.\nDrawing upon their extensive knowledge, the mathematician chooses the\nmost effective strategy tailored to the type of math problem , whether it is\nalgebra, calculus, or geometry.\nStep 3: Detailed Calculation.\nWith precision and expertise, the mathematician performs t he necessary\ncalculations step by step, adhering to all mathematical pri nciples.\nStep 4: Solution Review.\nBefore providing the \ufb01nal answer, the mathematician meticu lously checks\nthe calculations for accuracy and offers a concise explanat ion or rationale\nfor the solution.\nYou will be provided with a math problem. Please solve the pro blem.\n{\u2217problem \u2217}\nAnother method to guide LLMs to \u201cthink\u201d is through multiple r ounds of interaction with\nLLMs. For example, as a \ufb01rst step, we can instruct LLMs to solv e the problem directly\nYou will be provided with a math problem. Please solve the pro blem.\n{\u2217problem \u2217}\nNow we have an initial answer to the problem. As a second step, we prompt LLMs to\nevaluate the correctness of the answer and, if necessary, re work it to \ufb01nd a better solution.\n\n--- Page 111 ---\n\n104 Prompting\nYou will be provided with a math problem, along with a solutio n. Evaluate\nthe correctness of this solution, and identify any errors if present. Then,\nwork out your own solution.\nProblem: {\u2217problem \u2217}\nSolution: {\u2217solution \u2217}\nThe prompts presented here are closely related to a long line of research on reasoning prob-\nlems in LLMs. It",
    "32bf0c193f3e4764": " to \u201cthink\u201d is through multiple r ounds of interaction with\nLLMs. For example, as a \ufb01rst step, we can instruct LLMs to solv e the problem directly\nYou will be provided with a math problem. Please solve the pro blem.\n{\u2217problem \u2217}\nNow we have an initial answer to the problem. As a second step, we prompt LLMs to\nevaluate the correctness of the answer and, if necessary, re work it to \ufb01nd a better solution.\n\n--- Page 111 ---\n\n104 Prompting\nYou will be provided with a math problem, along with a solutio n. Evaluate\nthe correctness of this solution, and identify any errors if present. Then,\nwork out your own solution.\nProblem: {\u2217problem \u2217}\nSolution: {\u2217solution \u2217}\nThe prompts presented here are closely related to a long line of research on reasoning prob-\nlems in LLMs. It is impossible to provide a complete discussi on of all related issues because\nthis topic covers a large family of methods. But we will see a r elatively more detailed dis-\ncussion on how to improve prompting through more reasoning i n Section 3.2.\n\u2022Providing reference information . As discussed in the previous section, we can include\ndemonstrations in prompts and allow LLMs to in-context lear n from these demonstrations\nhow to perform the task. In fact, given the remarkable abilit y of language understanding of\nLLMs, we can add any type of text into the prompts and so these m odels can predict based\non enriched contexts. In many applications, we have various information that is relevant\nto user queries. Instead of using LLMs to make unconstrained predictions, we often want\nLLMs to produce outputs that are con\ufb01ned to the relevant text . One such example is RAG,\nwhere the relevant text for the user query is provided by call ing an IR system, and we\nprompt LLMs to generate responses based on this provided rel evant text. The following\nprompt shows an example.\nYou are an expert that can generate answers to input queries. You have now\nbeen provided with a query and the corresponding context inf ormation.\nPlease generate an answer based on this context information . Note that\nyou need to provide the answer in your own words, not just copy from the\ncontext provided.\nContext information: {\u2217IR-result \u2217}\nQuery: {\u2217query \u2217}\nIf the context information is highly reliable, we can even re strict LLMs to answering using\nonly the provided text. An example prompt is shown as follows\n\n--- Page 112 ---\n\n3.1 General Prompt Design 105\nYou are an expert tasked with generating answers from input q ueries. You\nhave been provided with a query and corresponding context in formation,\norganized in a table where each row represents a useful recor d. Please\ngenerate an answer using only this context information. Ens ure that you\nprovide the answer in your own words.\nContext information: {\u2217table \u2217}\nQuery: {\u2217query \u2217}\nWhen dealing with real-world problems, we often have prior k nowledge and additional\ninformation about the problems that help produce better ans wers. Considering such infor-\nmation in prompting is generally helpful in improving the re sult.\n\u2022Paying attention to prompt formats . In general, the performance of LLMs is highly\nsensitive to the prompts we input. Sometimes a small modi\ufb01ca tion to a prompt can lead to a\nbig change in model output. An interesting example is that ch anging the order of sentences\nin a prompt may cause LLMs to generate different results. To m ake prompts easy to read\nand reduce ambiguity, it is common to format them in a way that ensures clarity. One\nexample is that we de\ufb01ne several \ufb01elds for prompts and \ufb01ll dif ferent information in each\n\ufb01eld. Another example is we can use code-style prompts for LL Ms which can understand\nand generate both natural language and code. See the followi ng for a code-style prompt that\nperforms translation where one demonstration is presented .\n[English] = [I have an apple.]\n[German] = [Ich habe einen Apfel.]\n[English] = [I have an orange.]\n[German] =\nLLMs can receive text in various formats. This allows us to us e control characters, XML\ntags, and speci\ufb01c formatting to represent complex data. And it is useful to specify how the\ninput and output should be formatted or structured. For exam ple, we can delimit sections of\ntext using quotes and prompt LLMs accordingly (e.g., adding a sentence like \u201cthe",
    "ed08045f9c311b4d": " ensures clarity. One\nexample is that we de\ufb01ne several \ufb01elds for prompts and \ufb01ll dif ferent information in each\n\ufb01eld. Another example is we can use code-style prompts for LL Ms which can understand\nand generate both natural language and code. See the followi ng for a code-style prompt that\nperforms translation where one demonstration is presented .\n[English] = [I have an apple.]\n[German] = [Ich habe einen Apfel.]\n[English] = [I have an orange.]\n[German] =\nLLMs can receive text in various formats. This allows us to us e control characters, XML\ntags, and speci\ufb01c formatting to represent complex data. And it is useful to specify how the\ninput and output should be formatted or structured. For exam ple, we can delimit sections of\ntext using quotes and prompt LLMs accordingly (e.g., adding a sentence like \u201cthe input text\nis delimited by double quotes\u201d to the prompt).\nAbove, we have discussed only a few strategies for writing go od prompts. There are, of course,\nmany such methods, and one needs to develop their own through practice. Interested readers can\nrefer to various online documents for more information, suc h as OpenAI\u2019s manual on the GPT\nseries models2.\n2See https://platform.openai.com/docs/guides/prompt-engi neering/\nsix-strategies-for-getting-better-results .\n\n--- Page 113 ---\n\n106 Prompting\n3.1.4 More Examples\nIn this subsection, we consider more examples of prompting L LMs to perform various NLP tasks.\nThe motivation here is not to give standard prompts for these tasks, but rather to use simple\nexamples to illustrate how LLMs can be prompted to deal with N LP problems.\n3.1.4.1 Text Classi\ufb01cation\nText classi\ufb01cation is perhaps one of the most common problem s in NLP. Many tasks can be\nbroadly categorized as assigning pre-de\ufb01ned labels to a giv en text. Here we consider the polarity\nclassi\ufb01cation problem in sentiment analysis. We choose pol arity classi\ufb01cation for illustration be-\ncause it is one of the most popular and well-de\ufb01ned text class i\ufb01cation tasks. In a general setup of\npolarity classi\ufb01cation, we are required to categorize a giv en text into one of three categories: neg-\native, positive, or neutral. Below is a simple prompt for doi ng this (for easy reading, we highlight\nthe task description in the prompt).\nAnalyze thepolarityofthefollowingtextandclassifyitaspositive, negative, or\nneutral.\nText:\nThe service at the restaurant was slower than expected, whic h was a bit frustrat-\ning.\nThe polarity of the text can be classi\ufb01ed as positive.\nTo make the example complete, we show the response generated by the LLM (underlined text).\nAlthough the answer is correct, the LLM gives this answer not in labels but in text describing\nthe result. The problem is that LLMs are designed to generate text but not to assign labels to text\nand treat classi\ufb01cation problems as text generation proble ms. As a result, we need another system\nto map the LLM\u2019s output to the label space (call it label mapping ), that is, we extract \u201cpositive\u201d\nfrom \u201cThe polarity of the text can be classi\ufb01ed as positive\u201d. This is trivial in most cases because\nwe can identify label words via simple heuristics. But occas ionally, LLMs may not express the\nclassi\ufb01cation results using these label words. In this case , the problem becomes more complicated,\nas we need some way to map the generated text or words to prede\ufb01 ned label words.\nOne method to induce output labels from LLMs is to reframe the problem as a cloze task. For\nexample, the following shows a cloze-like prompt for polari ty classi\ufb01cation.\nAnalyze thepolarityofthefollowingtextandclassifyitaspositive, negative, or\nneutral.\nText:\nThe service at the restaurant was slower than expected, whic h was a bit frustrat-\ning.\nThe polarity of the text is positive\n\n--- Page 114 ---\n\n3.1 General Prompt Design 107\nWe can use LLMs to complete the text and \ufb01ll the blank with the m ost appropriate word. Ide-\nally, we wish the \ufb01lled word would be positive ,negative , orneutral . However, LLMs are not\nguaranteed to generate these label words. One method to addr ess this problem is to constrain the\nprediction",
    "f6fc66525be5696b": " the generated text or words to prede\ufb01 ned label words.\nOne method to induce output labels from LLMs is to reframe the problem as a cloze task. For\nexample, the following shows a cloze-like prompt for polari ty classi\ufb01cation.\nAnalyze thepolarityofthefollowingtextandclassifyitaspositive, negative, or\nneutral.\nText:\nThe service at the restaurant was slower than expected, whic h was a bit frustrat-\ning.\nThe polarity of the text is positive\n\n--- Page 114 ---\n\n3.1 General Prompt Design 107\nWe can use LLMs to complete the text and \ufb01ll the blank with the m ost appropriate word. Ide-\nally, we wish the \ufb01lled word would be positive ,negative , orneutral . However, LLMs are not\nguaranteed to generate these label words. One method to addr ess this problem is to constrain the\nprediction to the set of label words and select the one with th e highest probability. Then, the output\nlabel is given by\nlabel = arg max\ny\u2208YPr(y|x) (3.1)\nwhereydenotes the word \ufb01lled in the blank, and Ydenotes the set of label words\n{positive,negative,neutral }.\nAnother method of using LLMs to generate labels is to constra in the output with prompts. For\nexample, we can prompt LLMs to predict within a controlled se t of words. Here is an example.\nAnalyze thepolarityofthefollowingtextandclassifyitaspositive, negative, or\nneutral.\nText:\nThe service at the restaurant was slower than expected, whic h was a bit frustrat-\ning.\nWhat is the polarity of the text?\nJust answer: positive, negative, or neutral.\nPositive\nSentiment analysis is a common NLP problem that has probably been well understood by\nLLMs through pre-training or \ufb01ne-tuning. Thus we can prompt LLMs using simple instructions\nto perform the task. However, for new classi\ufb01cation problem s, it may be necessary to provide\nadditional details about the task, such as the classi\ufb01catio n standards, so that the LLMs can perform\ncorrectly. To do this, we can add a more detailed description of the task and/or demonstrate\nclassi\ufb01cation examples in the prompts. To illustrate, cons ider the following example.\n\n--- Page 115 ---\n\n108 Prompting\nAnalyze thepolarityofthefollowingtextandclassifyitaspositive, negative, or\nneutral.Here\u2019s what each category represents:\nPositive: This indicates thatthetextconveys apositiveemotionorattitude. For\nexample,texts express inghappiness, satisfaction, excitement, oradmirationare\nconsidered positive.\nNegative: This refers toatextthatexpresses anegativeemotion orattitude. It\nencompasses feelings ofsadness, anger, frustration, orcriticism.\nNeutral:Neutralsentiment isused todescribe texts thatdonotexhibit clear posi-\ntiveornegativeemotions butinstead conveyinformational, factual, orindifferent\ntones.\nText:\nThe service at the restaurant was slower than expected, whic h was a bit frustrat-\ning.\nWhat is the polarity of the text?\nPositive\nWhile it seems straightforward to use LLMs for classi\ufb01catio n problems, there are still issues\nthat have not been well addressed. For example, when dealing with a large number of categories,\nit remains challenging to effectively prompt LLMs. Note tha t if we face a very dif\ufb01cult classi\ufb01ca-\ntion problem and have a certain amount of labeled data, \ufb01ne-t uning LLMs or adopting \u201cBERT +\nclassi\ufb01er\u201d-like architectures is also desirable.\n3.1.4.2 Information Extraction\nMany NLP problems can be regarded as information extraction problems, involving the identi\ufb01-\ncation or extraction of speci\ufb01c pieces of information from u nstructured text. This information can\ninclude named entities, relationships, events, and other r elevant data points. The goal of infor-\nmation extraction is to transform raw data into a format that can be easily analyzed and used in\nvarious downstream applications.\nAs information extraction covers a wide range of problems, w e cannot discuss them all here.\nInstead, we start with the task of named entity recognition \u2014 a task that has long been a concern\nin NLP. Named entity recognition is a process that detects an d classi\ufb01es key information in text\ninto speci\ufb01c groups. These key pieces of information, known as named entities, typically include\nproper names and are",
    "e52f7580d723a062": "classi\ufb01er\u201d-like architectures is also desirable.\n3.1.4.2 Information Extraction\nMany NLP problems can be regarded as information extraction problems, involving the identi\ufb01-\ncation or extraction of speci\ufb01c pieces of information from u nstructured text. This information can\ninclude named entities, relationships, events, and other r elevant data points. The goal of infor-\nmation extraction is to transform raw data into a format that can be easily analyzed and used in\nvarious downstream applications.\nAs information extraction covers a wide range of problems, w e cannot discuss them all here.\nInstead, we start with the task of named entity recognition \u2014 a task that has long been a concern\nin NLP. Named entity recognition is a process that detects an d classi\ufb01es key information in text\ninto speci\ufb01c groups. These key pieces of information, known as named entities, typically include\nproper names and are categorized into distinct classes such as people, locations, organizations,\ndates, monetary values, and percentages. Consider \ufb01rst a si mple example of extracting person\nnames from a text3.\n3The text is from https://edition.cnn.com/travel\n\n--- Page 116 ---\n\n3.1 General Prompt Design 109\nIdentifyallpersonnames intheprovided text.\nText:\nIs the UK really doing that badly or have travel trends shifte d, post-pandemic?\nFor Tom Jenkins, CEO of the European Tourism Organisation, i t\u2019s the latter. \u201cI\nthink the UK is doing perfectly well but we\u2019ll see more people going to Europe,\u201d\nhe says of 2024, adding that there\u2019s \u201callegedly a slight plat eau of American de-\nmand for the UK.\u201d\nThe person name identi\ufb01ed in the provided text is: Tom Jenkin s\nThen, we can extract all named entities.\nIdentifyandclassifyallnamed entitiesintheprovided textintocategories such\naspersonnames, locations, dates, andorganizations. Listeach entitywith itstype\nononeline.\nText:\nIs the UK really doing that badly or have travel trends shifte d, post-pandemic?\nFor Tom Jenkins, CEO of the European Tourism Organisation, i t\u2019s the latter. \u201cI\nthink the UK is doing perfectly well but we\u2019ll see more people going to Europe,\u201d\nhe says of 2024, adding that there\u2019s \u201callegedly a slight plat eau of American de-\nmand for the UK.\u201d\n1. Tom Jenkins - Person Name\n2. European Tourism Organisation - Organization\n3. UK - Location\n4. Europe - Location\n5. 2024 - Date\nGiven these identi\ufb01ed named entities, we can further proces s the text using other information\nextraction tasks. For example, we can identify the relation ships between these named entities (call\nitrelation extraction ). The corresponding prompt is shown as follows.\n\n--- Page 117 ---\n\n110 Prompting\nGiven atextandalistofnamed entitiesidenti\ufb01edwithin it,analyze anddescribe\ntherelationships between these entities. Explain how each entityiscontextually\nrelated toothers.\nText:\nIs the UK really doing that badly or have travel trends shifte d, post-pandemic?\nFor Tom Jenkins, CEO of the European Tourism Organisation, i t\u2019s the latter. \u201cI\nthink the UK is doing perfectly well but we\u2019ll see more people going to Europe,\u201d\nhe says of 2024, adding that there\u2019s \u201callegedly a slight plat eau of American de-\nmand for the UK.\u201d\nNamed Entity:\nTom Jenkins, European Tourism Organisation, UK, Europe, 20 24\nTom Jenkins -European Tourism Organisation: Tom Jenkins istheCEO ofthe\nEuropean Tourism Organisation. This indicates aprofessional association where\nTom Jenkins likely hasasigni\ufb01cant roleindirecting theorganization\u2019s activities,\nespecially those related totourism inEurope.\n...\nIf LLMs have been \ufb01ne-tuned with instruction following for i nformation extraction tasks, it\nis relatively easy to perform various information extracti on tasks. For example, the following is a\nprompt template for information extraction.\nYouwillbeprovided with atext. Your taskisto{\u2217task-description \u2217}\nText: {\u2217text\u2217}\nHere task-description is the instruction for the task. Examples of the instruction include\nextract the keywords from the text.\nextract the key events from the text.\ndetect the coreference and anaphoric links between entitie s from the text.\nand so on. Of course, we can provide a more detailed descripti on of the task to prompt LLMs\nmore accurately.\n3.1.4.3 Text Generation\nIn NLP",
    "861abae649aa2a09": "\nTom Jenkins likely hasasigni\ufb01cant roleindirecting theorganization\u2019s activities,\nespecially those related totourism inEurope.\n...\nIf LLMs have been \ufb01ne-tuned with instruction following for i nformation extraction tasks, it\nis relatively easy to perform various information extracti on tasks. For example, the following is a\nprompt template for information extraction.\nYouwillbeprovided with atext. Your taskisto{\u2217task-description \u2217}\nText: {\u2217text\u2217}\nHere task-description is the instruction for the task. Examples of the instruction include\nextract the keywords from the text.\nextract the key events from the text.\ndetect the coreference and anaphoric links between entitie s from the text.\nand so on. Of course, we can provide a more detailed descripti on of the task to prompt LLMs\nmore accurately.\n3.1.4.3 Text Generation\nIn NLP, the term text generation is used broadly to cover a wide range of different problems, m any\nof which focus on generating text based on given text. These p roblems can be roughly categorized\ninto two classes: 1) continual writing based on the input tex t (call it text completion ), and 2)\ntransformation of the input text into another text (call it text transformation ). Here, we illustrate\nthe prompts for the former and consider the prompts for the la tter in the following discussion.\n\n--- Page 118 ---\n\n3.1 General Prompt Design 111\nA simple example of text completion is continuing to write a s tory starting with a few given\nsentences.\nYou areawriter who isgreat atwritingstories. You areprovided with afew\nsentences tostart with. Please continue thestory from theprovided sentences.\nStart of the story:\nOne sunny afternoon, Lily discovered a tiny, golden key hidd en under a rock in\nher backyard. Curious, she picked it up and wondered what it m ight unlock. Just\nthen, her cat, Whiskers, began to meow excitedly, leading he r towards the old\noak tree at the edge of the garden.\nLily followed Whiskers, herheart pounding with excitement and atouch of\nnervousness. Theoldoaktreehadalways seemed mysterious toher,itsgnarled\nbranches casting long shadows even onthebrightest days. Asthey reached the\ntree, Whiskers darted ahead andstarted pawing atsomething buried beneath a\npileofautumn leaves ...\nLikewise, we can create a conversation given the conversati onal context.\nYou areprovided with aconversationbetween atourist andataxidriver. Please\ncomplete theconversation.\nTourist: Could you take me to the downtown museum, please?\nTaxi Driver: Of course! First time in the city?\nTourist: Yes, any must-see places around?\nTaxi Driver: De\ufb01nitely check out the botanical gardens afte r the museum.\nThey\u2019re nearby and quite beautiful.\nTourist: Great, thanks forthetip!\nTaxi Driver: Noproblem. Enjoy your visit!\nAnother application of text completion is generating text b ased on user requirements. The\nrequirements can include style, tone, length, and any speci \ufb01c content that the text should contain.\nFor example, users may need a formal report, a creative story , or a piece of programming code.\nHere is a prompt for generating a \ufb01ve-character regulated Ch inese poem.\n\n--- Page 119 ---\n\n112 Prompting\nYouareapoet. Please write atraditional Chinese poem. Here istherequire ment.\nTheme: Spring\u2019s rejuvenation\nStructure: Five-character regulated poem\nEmotion: Convey a sense of happiness and renewal through viv id spring imagery.\n\u6625\u610f\u76ce\u7136\n\u7fe0\u67f3\u98ce\u4e2d\u821e\uff0c\n\u7ea2\u82b1\u96e8\u540e\u65b0\u3002\n\u5c71\u660e\u6c34\u53c8\u7eff\uff0c\n\u5929\u5bbd\u9e1f\u81ea\u9891\u3002\nIf the LLM is trained to generate language and code, we can pro mpt it to perform code com-\npletion tasks. Here is an example.\nPlease write aPython functiontocalculatetheaverageofalistofnumbers.\ndefcalculate_average(numbers):\nifnumbers: #Check ifthelistisnotempty\nreturn sum(numbers) /len(numbers)\nelse:\nreturn 0#Return 0ifthelistisempty\n3.1.4.4 Text Transformation\nTypical text transformation tasks include machine transla tion, summarization, text style transfer,\nand so on. The prompts used to perform these tasks are not sign i\ufb01cantly different from the ones\npresented previously. Here, we present a few examples to sho wcase these prompts.\nTrans latethefollowingtext",
    "be314a178c9b0e52": "\ufffd\u7136\n\u7fe0\u67f3\u98ce\u4e2d\u821e\uff0c\n\u7ea2\u82b1\u96e8\u540e\u65b0\u3002\n\u5c71\u660e\u6c34\u53c8\u7eff\uff0c\n\u5929\u5bbd\u9e1f\u81ea\u9891\u3002\nIf the LLM is trained to generate language and code, we can pro mpt it to perform code com-\npletion tasks. Here is an example.\nPlease write aPython functiontocalculatetheaverageofalistofnumbers.\ndefcalculate_average(numbers):\nifnumbers: #Check ifthelistisnotempty\nreturn sum(numbers) /len(numbers)\nelse:\nreturn 0#Return 0ifthelistisempty\n3.1.4.4 Text Transformation\nTypical text transformation tasks include machine transla tion, summarization, text style transfer,\nand so on. The prompts used to perform these tasks are not sign i\ufb01cantly different from the ones\npresented previously. Here, we present a few examples to sho wcase these prompts.\nTrans latethefollowingtextfrom English toSpan ish.\nText:\nThe quick brown fox jumps over the lazy dog.\nEl r\u00e1pido zorro marr\u00f3n salta sobre el perro perezoso.\n\n--- Page 120 ---\n\n3.1 General Prompt Design 113\nSummarizethefollowingthisarticleinnomore than 50words:\nArticle:\nIn recent years, urban areas have been facing signi\ufb01cant cha llenges related to sus-\ntainability and environmental management. Cities are expe riencing higher rates\nof pollution, increased traf\ufb01c congestion, and greater dem ands on infrastructure.\nThis growth has led to numerous environmental issues, inclu ding elevated lev-\nels of air and water pollution, increased waste production, and strained public\nservices ...\nUrban areas aregrappling with sustainability challenges, such asrising pollution,\ntraf\ufb01c congestion, andinfrastructure demands ...\nRewrite thistextinaformaltone.\nText:\nHey, what\u2019s up? Long time no see!\nHello, how have youbeen? Ithasbeen quite some time since welastmet!\n3.1.4.5 Question Answering\nThe question-answering format is inherently simple. For a g iven question, there is an answer that\ncorresponds to it. For example, in open-domain question ans wering, we expect the system to\nreturn an answer in response to a user-submitted question. P rompt templates for general-purpose\nquestion answering can be:\n{\u2217question \u2217}\nQuestion: {\u2217question \u2217}\nAnswer:\nQuestion answering is important in NLP because many problem s can be framed as question-\nanswering tasks. In particular, many recent reasoning task s are de\ufb01ned in the form of question\nanswering. For example, in the MMLU benchmark [ Hendrycks et al. ,2021 ], each example con-\nsists of a multiple-choice question, and LLMs are required t o select the correct answer. See the\nfollowing for an example prompt for answering a question in t his dataset.\n\n--- Page 121 ---\n\n114 Prompting\n(Z,\u2217)is a group with a\u2217b=a+b+ 1for alla,binZ. The inverse of ais\n(A) 0\n(B)\u22122\n(C)a\u22122\n(D) (2 +a)\u2217 \u22121\nD\nAnother widely-used benchmark is the GSM8K dataset [ Cobbe et al. ,2021 ]. It consists of\nthousands of grade school math word problems. For each probl em, we use LLMs to return a\nsolution in natural language. For example, the following is a prompt used for solving a problem\nin GSM8K.\nDEMO Q: Jerry\u2019s two daughters play softball on different teams. T hey each have\n8 games this season. Each team practices 4 hours for every gam e they\nplay. If each game lasts for 2 hours, how many hours will Jerry spend at\nthe \ufb01eld watching his daughters play and practice altogethe r?\nA: Jerry will spend 8games * 2hours per game = \u226a8\u22172 = 16 \u226b16\nhours watching one daughter play her games. He will spend 16\u22172 =\u226a\n16\u22172 = 32 \u226b32hours watching both daughters play their games. He\nwill spend 8games * 4hours of practice = \u226a8\u22174 = 32 \u226b32hours\nwatching one daughter practice. He will spend 32\u22172 =\u226a32\u22172 = 64 \u226b\n64hours watching both daughters practice. He will spend a tota l of32\nhours watching games + 64hours watching practice =\u226a32 + 64 =\n96\u226b96hours",
    "26e8ee7843084147": " for every gam e they\nplay. If each game lasts for 2 hours, how many hours will Jerry spend at\nthe \ufb01eld watching his daughters play and practice altogethe r?\nA: Jerry will spend 8games * 2hours per game = \u226a8\u22172 = 16 \u226b16\nhours watching one daughter play her games. He will spend 16\u22172 =\u226a\n16\u22172 = 32 \u226b32hours watching both daughters play their games. He\nwill spend 8games * 4hours of practice = \u226a8\u22174 = 32 \u226b32hours\nwatching one daughter practice. He will spend 32\u22172 =\u226a32\u22172 = 64 \u226b\n64hours watching both daughters practice. He will spend a tota l of32\nhours watching games + 64hours watching practice =\u226a32 + 64 =\n96\u226b96hours. #### 96\nQ: Mary bought six apples from the store. From the apples she b ought,\nfor each that Mary ate, she planted two trees from the remaini ng ones.\nHow many apples did Mary eat?\nA: She planted eight trees. This means she used half of that am ount of\napples, which is 8trees/2trees/apple =\u226a8/2 = 4 \u226b4apples. That\nmeans that she planted four of the six apples she bought, leav ing only 6\napples \u22124apples =\u226a6\u22124 = 2 \u226b2apples to be eaten. #### 2\nUSER Q: Boris has 100 apples. Beck has 23 fewer apples than Boris. I f Boris\ngives Beck 10 apples, how many fewer apples does Beck have tha n Boris\nnow?\nA:Boris starts with 100 apples and gives Beck 10apples, leaving\nhim with 100 apples \u221210apples =\u226a100\u221210 = 90 \u226b90apples.\nInitially, Beck has23fewer apples than Boris, giving him a\nstarting count of100\u221223 =\u226a100\u221223 = 77 \u226b77apples. After\nreceiving 10apples from Boris, Beck\u2019s total comes to77apples +10\napples =\u226a77 + 10 = 87 \u226b87apples. Consequently, Beck now has\n90\u221287 =\u226a90\u221287 = 3 \u226b3fewer apples than Boris. #### 3\n\n--- Page 122 ---\n\n3.2 Advanced Prompting Methods 115\nHere a few-shot prompt is adopted. The LLM learns from these d emonstrations of problem-\nsolution pairs not only the way of problem-solving but also t he way of formatting the output. For\nexample, the \ufb01nal result of calculation follows the #### tok en, and \u226a...\u226bannotates the detailed\ncalculation steps (called calculation annotation )4.\n3.2 Advanced Prompting Methods\nSo far in this chapter, we have introduced the basic concepts related to LLM prompting and pre-\nsented a number of prompts for NLP tasks. We now consider seve ral techniques for enhancing the\neffectiveness of prompting.\n3.2.1 Chain of Thought\nWe have encountered the concept of chain of thought (CoT ) several times in this chapter and\nprevious ones [ Wei et al. ,2022c ;Chowdhery et al. ,2022 ]. CoT methods provide a simple way\nto prompt LLMs to generate step-by-step reasoning for compl ex problems, thereby approaching\ntasks in a more human-like manner. Rather than coming to a con clusion directly, the CoT methods\ninstruct LLMs to generate reasoning steps or to learn from de monstrations of detailed reasoning\nprocesses provided in the prompts. To illustrate CoT, we con sider the problem of algebraic calcu-\nlation, as commonly described in the literature. Suppose we are given a algebraic problem\nCalculate the average of the numbers 2, 4, and 6.\nWe can consider it as the question and prompt an LLM to answer i t.\nQ: Please calculate the average of the numbers 2,4, and 9.\nA: The answer is 6.\nIt seems dif\ufb01cult for the LLM to directly give a correct answe r. A simple improvement is to\nadd demonstrations of similar problems in the prompt, and th us the LLM can learn from these\ndemonstrations.\nQ: Please calculate the average of the numbers 1,3,5, and 7.\nA: The answer is 4.\nQ: Please calculate the average of the numbers 2,4, and 9",
    "c457f8f6cbfab4d2": "ations of detailed reasoning\nprocesses provided in the prompts. To illustrate CoT, we con sider the problem of algebraic calcu-\nlation, as commonly described in the literature. Suppose we are given a algebraic problem\nCalculate the average of the numbers 2, 4, and 6.\nWe can consider it as the question and prompt an LLM to answer i t.\nQ: Please calculate the average of the numbers 2,4, and 9.\nA: The answer is 6.\nIt seems dif\ufb01cult for the LLM to directly give a correct answe r. A simple improvement is to\nadd demonstrations of similar problems in the prompt, and th us the LLM can learn from these\ndemonstrations.\nQ: Please calculate the average of the numbers 1,3,5, and 7.\nA: The answer is 4.\nQ: Please calculate the average of the numbers 2,4, and 9.\nA: The answer is 7.\nThe problem here is that, although we have shown a similar que stion-answer pair, it remains\ndif\ufb01cult for the LLM to reason out the correct answer. In CoT, not only can LLMs learn from the\n4During prediction, a calculator is used when we see \u226a...\u226b. More speci\ufb01cally, once the LLM encounters \u201c=\u201d\nin a\u226a...\u226b, then the calculator calculates the expression on the left- hand side of \u201c=\u201d. This method helps reduce the\ncalculation errors made by LLMs.\n\n--- Page 123 ---\n\n116 Prompting\ncorrespondence between questions and answers but they may g ain more from detailed problem-\nsolving steps that used to derive the answers. To do this, we c an incorporate some reasoning steps\ninto the prompt to obtain a CoT prompt.\nQ: Please calculate the mean square of the numbers 1, 3, 5, and 7.\nA:Calculatethesquare ofeach number:12= 1,32= 9,52= 25 ,and72= 49 .\nSum thesquares, 1 + 9 + 25 + 49 = 84 .There are4numbers intotal.Divide\nthesum bythenumberofitems, 84/4 = 21 . The answer is 21.\nQ: Please calculate the average of the numbers 2,4, and 9.\nA:Calculate2 + 4 + 9 ,which equals 15.There arethree numbers. Divide the\ntotalsum bythecount, resultingin15/3 = 5 . The answer is 5.\nHere we highlight the reasoning steps in green. By providing a detailed reasoning process for\na similar problem, the LLM learns to reason and thereby gener ates a problem-solving path that\nleads to the correct answer.\nThere are several bene\ufb01ts of using CoT prompting. First, CoT allows LLMs to decom-\npose complex problems into smaller, sequential reasoning s teps. This somewhat mirrors human\nproblem-solving behaviors, making it particularly effect ive for tasks requiring detailed, multi-step\nreasoning. Second, CoT makes the reasoning process more tra nsparent and interpretable. Since all\nreasoning steps are visible, we can understand and interpre t how a conclusion was reached. Third,\nif users can follow the logic behind the reasoning process, t hey will be more likely to trust the pre-\ndictions of an LLM. This is particularly important when appl ying LLMs in \ufb01elds like medicine,\neducation, and \ufb01nance. Fourth, CoT is an in-context learnin g approach, and thus, it is applicable\nto most well-trained, off-the-shelf LLMs. Moreover, CoT pr ovides ef\ufb01cient ways to adapt LLMs\nto different types of problems. It can even inspire more crea tive solutions by exploring various\nalternative reasoning paths, which might not be obvious whe n arriving at a conclusion directly.\nThe method described above requires providing one or more ex amples of CoT reasoning, typ-\nically called the few-shot CoT method. By contrast, the zero -shot CoT method does not require\nsuch examples. It instead prompts LLMs to reason step-by-st ep by incorporating speci\ufb01c instruc-\ntions in prompts. For example, below is a zero-shot CoT promp t.\nQ: Please calculate the average of the numbers 2,4, and 9.\nA:Let\u2019s think step-by-step.\nWe have three numbers: 2,4,and 9.Add these numbers together,\n2 + 4 + 9 = 15 .Determine how many numbers there are, which inthiscase\n",
    "cb65b90676bfb2cf": "\ufb01cient ways to adapt LLMs\nto different types of problems. It can even inspire more crea tive solutions by exploring various\nalternative reasoning paths, which might not be obvious whe n arriving at a conclusion directly.\nThe method described above requires providing one or more ex amples of CoT reasoning, typ-\nically called the few-shot CoT method. By contrast, the zero -shot CoT method does not require\nsuch examples. It instead prompts LLMs to reason step-by-st ep by incorporating speci\ufb01c instruc-\ntions in prompts. For example, below is a zero-shot CoT promp t.\nQ: Please calculate the average of the numbers 2,4, and 9.\nA:Let\u2019s think step-by-step.\nWe have three numbers: 2,4,and 9.Add these numbers together,\n2 + 4 + 9 = 15 .Determine how many numbers there are, which inthiscase\nisthree. The average iscalculated bydividing thetotal sum bythenumber of\nelements. Completing thedivision gives 15/3 = 5 .Sotheanswer is5.\nFollowing the instruction \u201cLet\u2019s think step by step\u201d, the LL M is prompted to generate detailed\nreasoning steps. As discussed in Kojima et al. [2022 ]\u2019s work, prompting with such instructions\nmay result in LLMs generating only the reasoning steps witho ut a clear conclusion. In this case,\na second round of prompting can be used to extract the answer f rom these reasoning steps. For\nexample, Kojima et al. [2022 ] create a second prompt which combines both the input and out put\n\n--- Page 124 ---\n\n3.2 Advanced Prompting Methods 117\nin the \ufb01rst round of prompting. Using this combined input, th e LLM can continue its reasoning\nprocess and then generate the correct answer. Furthermore, it is possible to prompt LLMs to\nreason using instructions other than \u201cLet\u2019s think step by st ep\u201d, such as \u201cLet\u2019s think logically\u201d and\n\u201cPlease show me your thinking steps \ufb01rst\u201d.\nWhile we have illustrated CoT methods using an algebraic rea soning problem, these methods\ncan be applied to a variety of different problems. Typical pr oblem-solving scenarios for CoT\ninclude mathematical reasoning, logical reasoning, commo nsense reasoning, symbolic reasoning,\ncode generation, and so on. See Figure 3.1for more examples of applying CoT in various tasks.\nCoT today is one of the most active \ufb01elds of prompt engineerin g. This has not only led to im-\nproved performance for LLM prompting but has opened the door to a wide range of methods for\nstudying and verifying reasoning capabilities of LLMs. Alt hough we have focused on the basic\nidea of CoT in this section, it can be improved in several ways . For example, we can consider the\nreasoning process as a problem of searching through many pos sible paths, each of which may con-\nsist of multiple intermediate states (i.e., reasoning step s). In general, we wish the search space to\nbe well-de\ufb01ned and suf\ufb01ciently large, so that we are more lik ely to \ufb01nd the optimal result. For this\nreason, an area of current LLM research is aimed at designing better structures for representing\nreasoning processes, allowing LLMs to tackle more complex r easoning challenges. These struc-\ntures include tree-based structures [ Yao et al. ,2024 ], graph-based structures [ Besta et al. ,2024 ],\nand so on. By using these compact representations of reasoni ng paths, LLMs can explore a wider\nrange of decision-making paths, analogous to System 2 think ing5. Another line of research fo-\ncuses on prompting LLMs with multi-round interactions. Thi s involves decomposing complex\nproblems into sub-problems, verifying and re\ufb01ning model ou tputs, employing model ensembling,\nand so on. Note that these methods and the issues involved are not limited to CoT. In fact, they\nare often used as more general approaches to improving LLMs, while CoT can be seen as a way\nto test the capabilities of LLMs. We will see discussions of s ome of these issues in the following\nsubsections.\nBefore leaving our discussion of CoT, we should consider its practical limitations. One of them\nis the need for detailed, multi-step reasoning demonstrati ons in few-shot CoT scenarios, which\nmay be dif\ufb01cult to obtain, either automatically or manually . Also, there is no standard method for\nbreaking down complex problems into simpler problem-solvi ng steps.",
    "38911e37980be790": " ing5. Another line of research fo-\ncuses on prompting LLMs with multi-round interactions. Thi s involves decomposing complex\nproblems into sub-problems, verifying and re\ufb01ning model ou tputs, employing model ensembling,\nand so on. Note that these methods and the issues involved are not limited to CoT. In fact, they\nare often used as more general approaches to improving LLMs, while CoT can be seen as a way\nto test the capabilities of LLMs. We will see discussions of s ome of these issues in the following\nsubsections.\nBefore leaving our discussion of CoT, we should consider its practical limitations. One of them\nis the need for detailed, multi-step reasoning demonstrati ons in few-shot CoT scenarios, which\nmay be dif\ufb01cult to obtain, either automatically or manually . Also, there is no standard method for\nbreaking down complex problems into simpler problem-solvi ng steps. This often heavily depends\non the user\u2019s experience. In addition, errors in intermedia te steps can also affect the accuracy of\nthe \ufb01nal conclusion. For further discussion on the pros and c ons of CoT, the interested reader can\nrefer to recent surveys on this topic [ Chu et al. ,2023 ;Yu et al. ,2023 ;Zhang et al. ,2023a ].\n3.2.2 Problem Decomposition\nWe have seen that LLMs can bene\ufb01t from solving a complex probl em by breaking it down into\nsimpler problem-solving tasks. Such an approach can be seen as an example of a broader paradigm\nknown as problem decomposition , which has been extensively explored and discussed in psy-\nchology and computer science. From the psychological persp ective, complex problem-solving\nrefers to a process of addressing a problem using knowledge t hat helps overcome the barriers of\n5System 1 and System 2 thinking, as described by Kahneman [2011 ], represent two different modes of cognitive\nprocessing. System 1 is fast, automatic, intuitive, and emo tional. This mode of thinking operates effortlessly and\nquickly, and is often what guides our daily decisions, judgm ents, and impressions. System 2 is slow, deliberate, and\nanalytical. It is activated when we need to perform complex c omputations.\n\n--- Page 125 ---\n\n118 Prompting\nQ: What home entertainment equipment re-\nquires cable?\nAnswer Choices: (a) radio shack (b) substa-\ntion (c) television (d) cabinet\nA:The answer must require cable. Ofthe\nabove choices, only televisionrequires cable.\nSo the answer is (c).\nQ: What type of person typically contracts\nillness?\nAnswer Choices: (a) hospital (b) head (c)\nsick person (d) elderly person (e) doctor\u2019s of-\n\ufb01ce\nA: Theanswer must beapersonsuscepti -\nbletoillness. Ofthechoices, \"elderly per\nson\" iscorrectbecause they typically have\nweaker immune systems.\nSo the answer is (d).Q: Do hamsters provide food for any ani-\nmals?\nA:Ham sters areprey animals. Prey arefood\nforpreda tors. Thus, hamsters provide food\nforsome animals. So the answer is yes.\nQ: Can you use Microsoft Of\ufb01ce without in-\nternet?\nA:Yes, youcanuseMicrosoft Of\ufb01ceappli-\ncations ,although features likecloud stor-\nageandonlinecollaborationrequire inter-\nnet.So the answer is yes.\n(a) CSQA (b) StrategyQA\nQ: Complete the rest of the sequence, mak-\ning sure that the parentheses are closed prop-\nerly. Input: [{\nA:Let\u2019s think stepbystep.\n0: empty stack\n1:[; stack: [\n2:{; stack: [{\nSo the answer is }].\nQ: Complete the rest of the sequence, mak-\ning sure that the parentheses are closed prop-\nerly. Input:<[ [\nA:Let\u2019s think stepbystep.\n0: empty stack\n1:<; stack:<\n2:[; stack:<[\n3:[; stack:<[ [\nSo the answer is ] ]>.Q: Take the last letters of the words in \"Elon\nMusk\" and concatenate them.\nA:The lastletterof\"Elon\" is\"n\". The last\nletterof\"Musk\" is\"k\".Concatenatingthem\nis\"nk\". The answer is nk.\nQ: Take the last letters of each words in \"Re-\nnata Mariela Mona Kristin\" and concatenate\nthem.\nA:Thelastletterof\"Renata\" is\"a\".The\nlastletterof",
    "e01f0144c0656587": " stepbystep.\n0: empty stack\n1:[; stack: [\n2:{; stack: [{\nSo the answer is }].\nQ: Complete the rest of the sequence, mak-\ning sure that the parentheses are closed prop-\nerly. Input:<[ [\nA:Let\u2019s think stepbystep.\n0: empty stack\n1:<; stack:<\n2:[; stack:<[\n3:[; stack:<[ [\nSo the answer is ] ]>.Q: Take the last letters of the words in \"Elon\nMusk\" and concatenate them.\nA:The lastletterof\"Elon\" is\"n\". The last\nletterof\"Musk\" is\"k\".Concatenatingthem\nis\"nk\". The answer is nk.\nQ: Take the last letters of each words in \"Re-\nnata Mariela Mona Kristin\" and concatenate\nthem.\nA:Thelastletterof\"Renata\" is\"a\".The\nlastletterof\"Mariela\" is\"a\".Thelastlet-\nterof\"Mona\" is\"a\"\". Thelastletterof\n\"Kristin\" is\"n\".Concatenatingthem is\n\"aaan\". The answer is aaan.\n(c) Dyck languages (d) Last Letter Concatenation\nFig. 3.1: CoT in four different reasoning tasks, including CSQA, Stra tegyQA, Dyck languages, and Last Letter Con-\ncatenation. The CoT parts are highlighted in green.\nthe problem6. There are generally no standard or clear paths to a solution for a complex prob-\nlem. However, it is often advantageous to employ strategies that decompose the problem, thereby\nmaking it easier to tackle the corresponding sub-problems w ith less effort. For example, consider\nwriting a blog about the risks of AI. If we simply prompt an LLM with the instruction \u201cPlease\nwrite a blog about the risks of AI\u201d, the LLM may generate a blog with arbitrary structures and\n6A relatively formal de\ufb01nition can be found in Frensch and Funke [2014 ]\u2019s book: complex problem-solving occurs\nto overcome barriers between a given state and a desired goal state by means of behavioral and/or cognitive, multi-step\nactivities .\n\n--- Page 126 ---\n\n3.2 Advanced Prompting Methods 119\nwriting styles. A better method, instead, could be to outlin e the blog and provide more detailed\ninformation about each section. Consider the following pro mpt\nYou are a blog writer. Please follow the provided outline bel ow to write a blog\nabout the risks of AI.\n\u2022 Introduction\nIntroduce AI, its relevance, and the importance of understa nding its risks for youth.\n\u2022 Privacy Concerns\nDiscuss how AI might compromise personal privacy through in teractions online.\n\u2022 Misinformation\nExplore AI\u2019s role in spreading misinformation and in\ufb02uenci ng young people\u2019s deci-\nsions.\n\u2022 Cyberbullying\nHighlight how AI tools can be utilized in cyberbullying and t he impact on mental\nhealth.\n\u2022 Tips for Safe AI Use\nOffer guidelines for responsible AI usage and promote criti cal thinking.\n\u2022 Conclusion\nRecap main points and encourage proactive engagement with A I ethics.\nHere we give the title and major points for each section. Then , the LLM can use this structure to\nbreak down the writing task by \ufb01lling in content for these sec tions. Note that the way to structure\nthe blog can be provided by humans or even generated automati cally. For example, we can use\nthe LLM to \ufb01rst generate the outline, and then ask it to follow this outline to complete the writing.\nIn computer science, decomposing complex problems is a comm only used strategy in software\nand hardware system design. A well-known example is the divi de-and-conquer paradigm, which\nis often used to design algorithms for computation problems that can be reduced to simpler, more\nmanageable problems. For example, consider a problem of det ermining whether a document\ndiscusses the risks of AI. We can instruct the LLM with the fol lowing prompt.\nYou are provided with a text. Please determine whether it dis cusses the risks of\nAI.\n{\u2217document \u2217}\nIf the document is long, the computation will be expensive. A lternatively, we can divide\nthe document into relatively short segments and perform the same task on each segment. These\nsegments can be processed in parallel to further reduce the c omputational cost. Next, we determine\n\n--- Page 127 ---\n\n120 Prompting\nthe relevancy of each segment to the topic of AI risks. The \ufb01na l output is then generated using\nanother prompt.\nYour task is to determine whether a text discusses the risks o f AI.",
    "666d31179b8f04de": "quer paradigm, which\nis often used to design algorithms for computation problems that can be reduced to simpler, more\nmanageable problems. For example, consider a problem of det ermining whether a document\ndiscusses the risks of AI. We can instruct the LLM with the fol lowing prompt.\nYou are provided with a text. Please determine whether it dis cusses the risks of\nAI.\n{\u2217document \u2217}\nIf the document is long, the computation will be expensive. A lternatively, we can divide\nthe document into relatively short segments and perform the same task on each segment. These\nsegments can be processed in parallel to further reduce the c omputational cost. Next, we determine\n\n--- Page 127 ---\n\n120 Prompting\nthe relevancy of each segment to the topic of AI risks. The \ufb01na l output is then generated using\nanother prompt.\nYour task is to determine whether a text discusses the risks o f AI. This text has\nbeen divided into segments, and you have obtained the releva ncy of each segment\nto the topic of AI risks. Based on this, please provide your \ufb01n al result.\nSegment 1: {\u2217relevancy-to-the-topic1 \u2217}\nSegment 2: {\u2217relevancy-to-the-topic2 \u2217}\nSegment 3: {\u2217relevancy-to-the-topic3 \u2217}\n...\nNow let us return to a more general discussion of problem deco mposition in prompting. While\nproblem decomposition can be applied to various NLP problem s, it has been more extensively\ndiscussed and tested in reasoning tasks recently. For compl ex reasoning tasks, we often need\na multi-step reasoning path to reach a correct conclusion. W e can use LLMs to achieve this in\nthree different ways. First, LLMs can directly reach the con clusion. In other words, they can\npredict without explicit reasoning processes, and there is a hidden and uninterpretable reasoning\nmechanism. Second, LLMs are prompted to generate a multi-st ep reasoning path that leads to the\nconclusion, like CoT. However, we run LLMs just once, and all intermediate steps in reasoning\nare generated in a single prediction. Third, we break down th e original problem into a number of\nsub-problems, which are either addressed in separate runs o f LLMs or tackled using other systems.\nHere we focus our attention on the third approach, which is cl osely related to problem decompo-\nsition. Note, however, that a more comprehensive discussio n could cover all these approaches,\nwhile the \ufb01rst two have been discussed to some extent in this c hapter.\nA general framework for problem decomposition involves two elements.\n\u2022Sub-problem Generation . This involves decomposing the input problem into a number o f\nsub-problems.\n\u2022Sub-problem Solving . This involves solving each sub-problem and deriving inter mediate\nand \ufb01nal conclusions through reasoning.\nThese two issues can be modeled in different ways, leading to various problem decomposition\nmethods. One approach is to treat them as separate steps in a t wo-step process. For example,\nconsider the blog writing task described at the beginning of this subsection. In the \ufb01rst step, we\ndecompose the entire problem into sub-problems all at once ( i.e., outline the blog). In the second\nstep, we solve the sub-problems either sequentially or in an other order (i.e., \ufb01ll in content for\neach section as needed). The \ufb01nal output of this process comb ines the results from solving each\nsub-problem. While this method is simple and straightforwa rd, it assumes that the problem is\ncompositional, making it more suitable for tasks like writi ng and code generation.\nHowever, many real-world problems require complex reasoni ng. One key characteristic of\nthese problems is that the reasoning steps may not be \ufb01xed. Th e reasoning path can vary for\ndifferent problems, and each step of reasoning may depend on the outcomes of prior steps. In\n\n--- Page 128 ---\n\n3.2 Advanced Prompting Methods 121\nsuch cases, it is undesirable to use \ufb01xed sub-problem genera tion in advance. Instead, sub-problems\nshould be generated dynamically based on the input problem, and, if possible, generated on the\n\ufb02y during the reasoning process. This makes problem decompo sition more challenging compared\nwith designing divide-and-conquer algorithms. Ideally, w e would like to jointly design both the\nsystems for sub-problem generation and sub-problem solvin g. But a more practical and widely\nused approach is to adopt separate models for these tasks. A s traightforward way to achieve this\nis to adapt an",
    "93bd2b9b7ba5006e": "-world problems require complex reasoni ng. One key characteristic of\nthese problems is that the reasoning steps may not be \ufb01xed. Th e reasoning path can vary for\ndifferent problems, and each step of reasoning may depend on the outcomes of prior steps. In\n\n--- Page 128 ---\n\n3.2 Advanced Prompting Methods 121\nsuch cases, it is undesirable to use \ufb01xed sub-problem genera tion in advance. Instead, sub-problems\nshould be generated dynamically based on the input problem, and, if possible, generated on the\n\ufb02y during the reasoning process. This makes problem decompo sition more challenging compared\nwith designing divide-and-conquer algorithms. Ideally, w e would like to jointly design both the\nsystems for sub-problem generation and sub-problem solvin g. But a more practical and widely\nused approach is to adopt separate models for these tasks. A s traightforward way to achieve this\nis to adapt an LLM for these tasks by either prompting or tunin g the model.\nHere we consider a method based on the above idea, called least-to-most prompting [Zhou et al. ,\n2023b ]. The motivation for this method arises from the challenges of solving dif\ufb01cult reasoning\nproblems \u2014 those that cannot be addressed by simply generali zing from a few examples. For\nthese problems, a more effective problem-solving strategy is to follow a progressive sequence of\nsub-problems that systematically lead to the conclusion. M ore speci\ufb01cally, in the least-to-most\nprompting method, sub-problem generation is performed by p rompting an LLM with instructions\nand/or demonstrations. For example, below is a 2-shot promp t for sub-problem generation in\nleast-to-most prompting.\nTASK Your task is to decompose a problem into several sub-problem s. You will\nbe given a few examples to illustrate how to achieve this.\nDEMO Q: In a community, 5% of the population are infants, 15% are ch ildren,\n40% are adults, and 40% are seniors. Which group makes up the l argest\nportion of the population?\nA: To answer the question \u201cWhich group makes up the largest po rtion of the\npopulation?\u201d, we need to know: \u201c How many percent areinfants?\u201d, \u201c How\nmany percent arechildren?\u201d, \u201c How many percent areadults?\u201d, \u201c How many\npercent areseniors?\u201d.\nQ: Alice, Bob, and Charlie brought beads for their group proj ect in their\ncraft class. Alice has twice as many beads as Bob, and Bob has \ufb01 ve times\nas many beads as Charlie. If Charlie has 6 beads, how many bead s can they\nuse for their craft project?\nA: To answer the question \u201cHow many beads can they use for thei r craft\nproject?\u201d, we need to know: \u201c How many beads does Bob have?\u201d, \u201c How\nmany beads does Alicehave?\u201d.\nUSER Q: The environmental study conducted from 2015 to 2020 revea led that the\naverage temperature in the region increased by 2.3 degrees C elsius. What\nwas the duration of the environmental study?\nA: To answer the question \u201c What wasthedurationoftheenvironmental\nstudy?\u201d, we need to know: \u201c When didtheenvironmentalstudy start?\u201d,\n\u201cWhen didtheenvironmentalstudy end?\u201d.\nBy learning from the examples, the LLM can generate two sub-p roblems for answering the\nnew problem \u201cWhat was the duration of the environmental stud y?\u201d (highlighted in blue and\norange). Given these sub-problems, we solve them sequentia lly. For each sub-problem, we take\nall previously-generated QA pairs as context, and then prod uce the answer. For the example above,\n\n--- Page 129 ---\n\n122 Prompting\nwe need to answer the \ufb01rst sub-problem by prompting the LLM, l ike this\nThe environmental study conducted from 2015 to 2020 reveale d that\nthe average temperature in the region increased by 2.3 degre es Celsius.\nSUB-PROB1 Q:When didtheenvironmentalstudy start?\nA: The environmental study started in 2015.\nOnce we have the answer to the \ufb01rst sub-problem, we proceed to the second one. This time,\nwe include both the \ufb01rst sub-problem and its corresponding a nswer in the input.\nThe environmental study conducted from 2015 to 2020 reveale d that\nthe average temperature in the region increased by 2.3 degre es Celsius.\nSUB-PROB1 Q:When didtheenvironmentalstudy start?\nA: The environmental",
    "3fe9a7d210d62ef1": "ce the answer. For the example above,\n\n--- Page 129 ---\n\n122 Prompting\nwe need to answer the \ufb01rst sub-problem by prompting the LLM, l ike this\nThe environmental study conducted from 2015 to 2020 reveale d that\nthe average temperature in the region increased by 2.3 degre es Celsius.\nSUB-PROB1 Q:When didtheenvironmentalstudy start?\nA: The environmental study started in 2015.\nOnce we have the answer to the \ufb01rst sub-problem, we proceed to the second one. This time,\nwe include both the \ufb01rst sub-problem and its corresponding a nswer in the input.\nThe environmental study conducted from 2015 to 2020 reveale d that\nthe average temperature in the region increased by 2.3 degre es Celsius.\nSUB-PROB1 Q:When didtheenvironmentalstudy start?\nA: The environmental study started in 2015.\nSUB-PROB2 Q:When didtheenvironmentalstudy end?\nA: The environmental study started in 2020.\nFinally, we use the LLM to solve the original problem given th e answers to all the sub-\nproblems.\nThe environmental study conducted from 2015 to 2020 reveale d that\nthe average temperature in the region increased by 2.3 degre es Celsius.\nSUB-PROB1 Q:When didtheenvironmentalstudy start?\nA: The environmental study started in 2015.\nSUB-PROB2 Q:When didtheenvironmentalstudy end?\nA: The environmental study started in 2020.\nFINAL Q:What wasthedurationoftheenvironmentalstudy?\nA: The duration of the environmental study was 5years .\nThe least-to-most method offers a basic approach to prompti ng LLMs to generate and solve\nsub-problems separately. We can improve it in several ways. One simple improvement is to apply\nvarious advanced prompting techniques, which do not requir e changes to the problem decom-\nposition framework. For example, we can incorporate CoT int o the prompting to enhance the\nreasoning performance of sub-problem generation and solvi ng.\nAnother improvement is to explore methods for better decomp osing problems and organizing\nproblem-solving paths. To describe these approaches, we wi ll use the symbol p0to denote the\n\n--- Page 130 ---\n\n3.2 Advanced Prompting Methods 123\ninput problem, and use the symbols {p1,...,p n}to denote the sub-problems corresponding to p0.\nFor least-to-most prompting, we decompose p0into{p1,...,p n}, given by\n{p1,...,p n}=G(p0) (3.2)\nwhereG(\u00b7)denotes the function of sub-problem generation. Then, we so lve the sub-problems\n{p1,...,p n}sequentially, resulting in a sequence of answers {a1,...,a n}. For answering the i-th\nsub-problem pi, we include both the original problem p0and all previously-seen problem-answer\npairs in the context for prediction. The answer aiis given by\nai=Si(pi,{p0,p<i,a<i}) (3.3)\nwherep<i={p1,...,p i\u22121}anda<i={a1,...,a i\u22121}.Si(\u00b7)denotes the function that solves the\nsub-problem pigiven the context {p0,p<i,a<i}. The last step is to generate the answer to the\noriginal problem p0, which can be expressed in a similar manner to Eq. ( 3.3).\na0=S0(p0,{p\u2264n,a\u2264n}) (3.4)\nOne way to re\ufb01ne this model is to modify the G(\u00b7)function so that the model can dynamically\ngenerate answers. Instead of generating all sub-problems a t one time, we can generate each of\nthem during problem-solving [ Dua et al. ,2022 ]. To do this, we can replace Eq. ( 3.2) with\npi=Gi(p0,{p<i,a<i}) (3.5)\nHence we obtain a sub-problem generation model that operate s in a step-by-step manner. At each\nstepi, we \ufb01rst generate the sub-problem piby prompting an LLM with the original problem p0\nand the problem-solving history {p<i,a<i}. We then generate the answer aifor this sub-problem\nusing the same or a different LLM, based on the same contextua l information (see Eq. ( 3.3)). This\nmethod effectively expands the reasoning capacity of LLM",
    "7b2f045f08d0fcaf": "\ufb01ne this model is to modify the G(\u00b7)function so that the model can dynamically\ngenerate answers. Instead of generating all sub-problems a t one time, we can generate each of\nthem during problem-solving [ Dua et al. ,2022 ]. To do this, we can replace Eq. ( 3.2) with\npi=Gi(p0,{p<i,a<i}) (3.5)\nHence we obtain a sub-problem generation model that operate s in a step-by-step manner. At each\nstepi, we \ufb01rst generate the sub-problem piby prompting an LLM with the original problem p0\nand the problem-solving history {p<i,a<i}. We then generate the answer aifor this sub-problem\nusing the same or a different LLM, based on the same contextua l information (see Eq. ( 3.3)). This\nmethod effectively expands the reasoning capacity of LLMs b y allowing them to dynamically\ngenerate and solve sub-problems in intermediate reasoning steps. As a result, the reasoning paths\nare not \ufb01xed in advance, and the models can choose and adapt th eir reasoning strategies during\nproblem-solving.\nAnother way to improve the above model is to focus on developi ng better sub-problem solvers.\nIn our previous discussion, we restricted Si(\u00b7)to LLMs that are prompted to solve the sub-problem\npi. In fact, we can expand this function to any system that is cap able of addressing the sub-\nproblem. For example, Si(\u00b7)could make calls to IR systems, thereby allowing us to access a\nbroader range of data for problem-solving. Another example is usingSi(\u00b7)as a calculator to\naccurately compute results in mathematical problem-solvi ng. If the sub-problem piis complex\nand requires multiple intermediate problem-solving steps , it is also possible to further decompose\npiinto smaller sub-problems. For example, Si(\u00b7)can be de\ufb01ned as a recursive program that\ngenerates and solves sub-problems. This incorporates recu rsion into problem-solving and allows\nus to address problems by iteratively decomposing them. As a result, we can de\ufb01ne a hierarchical\nstructure for problem-solving [ Khot et al. ,2023 ].\nIf we generalize the above formulation a bit further, we can c onsider it as a reinforcement\nlearning problem. A typical method is to model a problem-sol ving process as a decision making\nprocess. In each step of this process, an action is taken base d on the current state. These actions\n\n--- Page 131 ---\n\n124 Prompting\ncan include all functions for sub-problem generation and so lving (i.e.,Gi(\u00b7)andSi(\u00b7)). Thus,\nthe action sequence corresponds to a problem-solving path. Since the discussion of reinforcement\nlearning problems is beyond the scope of this chapter, we ski p the precise description of this\nlearning task. Nevertheless, developing an agent or contro ller to determine when and how to\ngenerate and solve a sub-problem is also a natural choice.\nIn NLP, problem decomposition is related to a long line of res earch on multi-hop question\nanswering [ Mavi et al. ,2024 ]. This task requires the system to gather and combine inform ation\nfrom multiple pieces of text to provide an accurate answer to a complex question. For example,\nto answer the question \u201cWhat is the capital of the country whe re Albert Einstein was born?\u201d, we\nneed to know \u201cWhere Albert Einstein was born?\u201d and \u201cWhat\u2019s th e capital of Germany?\u201d. Earlier\nwork in this area and related ones has investigated the issue of problem decomposition, though the\nmethods might not be based on LLMs. For example, a popular met hod is to develop an additional\nneural model to generate simpler questions that address dif ferent aspects of the original question\n[Andreas et al. ,2016 ;Talmor and Berant ,2018 ;Min et al. ,2019 ]. This question generator can\ncreate questions in a batch or sequential manner.\nBroadly speaking, problem decomposition is also related to the compositionality issue in NLP\n[Drozdov et al. ,2022 ;Press et al. ,2023 ]. For example, in semantic parsing, we map natural lan-\nguage sentences into structured meaning representations b y breaking them down into constituent\nparts and understanding the sentences based on the meanings of these parts and the rules used to\ncombine them. In early studies of this \ufb01eld, highly composit ional sentences were considered easier\nfor testing systems, as it is relatively straightforward to decompose such sentences and compose\nthe meanings of their parts. However, the task becomes",
    "ec5914f81d4f31e1": " met hod is to develop an additional\nneural model to generate simpler questions that address dif ferent aspects of the original question\n[Andreas et al. ,2016 ;Talmor and Berant ,2018 ;Min et al. ,2019 ]. This question generator can\ncreate questions in a batch or sequential manner.\nBroadly speaking, problem decomposition is also related to the compositionality issue in NLP\n[Drozdov et al. ,2022 ;Press et al. ,2023 ]. For example, in semantic parsing, we map natural lan-\nguage sentences into structured meaning representations b y breaking them down into constituent\nparts and understanding the sentences based on the meanings of these parts and the rules used to\ncombine them. In early studies of this \ufb01eld, highly composit ional sentences were considered easier\nfor testing systems, as it is relatively straightforward to decompose such sentences and compose\nthe meanings of their parts. However, the task becomes much m ore dif\ufb01cult when more gener-\nalization is required for modeling compositionality in new data. In this case, we want systems\nto have improved abilities of compositional generalization . In more recent research on LLMs,\nthis issue has been frequently discussed in compositional r easoning tasks, such as SCAN7, as it\nis considered an important aspect of testing the language un derstanding and reasoning abilities\nof LLMs. This also presents new tasks for developing and exam ining problem decomposition\nmethods.\nIn LLMs, one interesting application of problem decomposit ion is tool use. In some cases,\nit is necessary to integrate external tools into LLMs to acce ss accurate data not available during\ntraining or \ufb01ne-tuning. For example, LLMs can integrate wit h APIs to fetch real-time data such\nas weather updates, stock market prices, or news feeds, enab ling them to provide up-to-date re-\nsponses to user queries. When using tools, LLM predictions m ight include markers that indicate\nwhere and how to call external APIs. This requires decomposi ng the problem into sub-problems,\nwith some handled by the LLMs and others by external tools. Mo re detailed discussions on this\ntopic will be presented in Section 3.2.5 .\n3.2.3 Self-re\ufb01nement\nIn many cases, predictions of LLMs can be inaccurate or incor rect. Given that current LLMs can\nperform tasks like re\ufb01nement and correction, it makes sense to explore methods for these models\nto self-re\ufb01ne their outputs. Self-re\ufb01nement is a common phe nomenon in human psychological\n7The SCAN tasks (Simpli\ufb01ed versions of the CommAI Navigation tasks) are designed to evaluate the ability of\nLLMs to perform compositional generalization [ Lake and Baroni ,2018 ]. They involve translating natural language\ncommands into a sequence of actions. For example, a command \u201c jump opposite left and walk thrice\u201d can be translated\ninto the action sequence \u201cLTURN LTURN JUMP WALK WALK WALK\u201d.\n\n--- Page 132 ---\n\n3.2 Advanced Prompting Methods 125\nactivities and daily behavior. For example, when designing a product, a designer might \ufb01rst create\na basic prototype, then re\ufb01ne the design after evaluation an d testing to enhance user experience\nand functionality. The re\ufb01nement can be iterated several ti mes until the design is satisfactory. The\nidea of predict-then-re\ufb01ne can also be found in NLP. One earl y example is Brill\u2019s tagger [ Brill,\n1992 ], where an initial POS tagging result of a sentence can be ite ratively re\ufb01ned using a rule-\nbased system. In the era of deep learning, a good deal of work o n sequence-to-sequence problems,\nsuch as grammar correction and text rewriting, can also be se en as examples on this theme.\nWe can prompt LLMs to do self-re\ufb01nement. Consider a simple ex ample where we \ufb01rst use\nan LLM to translate a text from Chinese to English and then use the same model to re\ufb01ne the\ntranslation. Here is how we can prompt the model to translate :\nTranslate the following text from Chinese to English:\nChinese:\n\u4e00\u7cfb\u5217\u8003\u53e4\u53d1\u73b0\u5960\u5b9a\u7ea2\u5c71\u6587\u5316\u5728\u4e2d\u534e\u6587\u660e\u8d77\u6e90\u7814\u7a76\u4e2d\u7684\u91cd\u8981\u5730\u4f4d\u3002\u6700\u65b0\u516c\n\u5e03\u7684\u7814\u7a76\u6210\u679c\u8ba4\u4e3a\uff0c\u5927\u7ea6\u4ece\u8ddd\u4eca5800\u5e74\u5f00\u59cb\uff0c\u4e2d\u534e\u5927\u5730\u4e0a\u5404\u4e2a\u533a\u57df\u76f8\u7ee7\u51fa\n\u73b0\u8f83\u4e3a\u660e\u663e\u7684\u793e\u4f1a\u5206\u5316\uff0c\u8fdb\u5165\u6587\u660e\u8d77",
    "138e887253c810bf": " deal of work o n sequence-to-sequence problems,\nsuch as grammar correction and text rewriting, can also be se en as examples on this theme.\nWe can prompt LLMs to do self-re\ufb01nement. Consider a simple ex ample where we \ufb01rst use\nan LLM to translate a text from Chinese to English and then use the same model to re\ufb01ne the\ntranslation. Here is how we can prompt the model to translate :\nTranslate the following text from Chinese to English:\nChinese:\n\u4e00\u7cfb\u5217\u8003\u53e4\u53d1\u73b0\u5960\u5b9a\u7ea2\u5c71\u6587\u5316\u5728\u4e2d\u534e\u6587\u660e\u8d77\u6e90\u7814\u7a76\u4e2d\u7684\u91cd\u8981\u5730\u4f4d\u3002\u6700\u65b0\u516c\n\u5e03\u7684\u7814\u7a76\u6210\u679c\u8ba4\u4e3a\uff0c\u5927\u7ea6\u4ece\u8ddd\u4eca5800\u5e74\u5f00\u59cb\uff0c\u4e2d\u534e\u5927\u5730\u4e0a\u5404\u4e2a\u533a\u57df\u76f8\u7ee7\u51fa\n\u73b0\u8f83\u4e3a\u660e\u663e\u7684\u793e\u4f1a\u5206\u5316\uff0c\u8fdb\u5165\u6587\u660e\u8d77\u6e90\u7684\u52a0\u901f\u9636\u6bb5\u3002\nEnglish:\nAseries ofdiscoveries have cemented thesigni\ufb01cant role oftheHongshan\nculture instudies ontheorigins ofChinese civilization. The latest research\n\ufb01ndings suggests that, starting from around 5800 years ago, many regions\nacross China began toexperience noticeable social differentiations, entering an\naccelerated phase intheorigin ofcivilization.\nThen, we can prompt it to re\ufb01ne the translation:\nNiuTrans\n\n--- Page 133 ---\n\n126 Prompting\nYou are provided with a Chinese text along with its English tr anslation. Please\nre\ufb01ne the translation to improve its accuracy and \ufb02uency if n ecessary.\nChinese:\n\u4e00\u7cfb\u5217\u8003\u53e4\u53d1\u73b0\u5960\u5b9a\u7ea2\u5c71\u6587\u5316\u5728\u4e2d\u534e\u6587\u660e\u8d77\u6e90\u7814\u7a76\u4e2d\u7684\u91cd\u8981\u5730\u4f4d\u3002\u6700\u65b0\u516c\n\u5e03\u7684\u7814\u7a76\u6210\u679c\u8ba4\u4e3a\uff0c\u5927\u7ea6\u4ece\u8ddd\u4eca5800\u5e74\u5f00\u59cb\uff0c\u4e2d\u534e\u5927\u5730\u4e0a\u5404\u4e2a\u533a\u57df\u76f8\u7ee7\u51fa\n\u73b0\u8f83\u4e3a\u660e\u663e\u7684\u793e\u4f1a\u5206\u5316\uff0c\u8fdb\u5165\u6587\u660e\u8d77\u6e90\u7684\u52a0\u901f\u9636\u6bb5\u3002\nEnglish:\nA series of discoveries have cemented the signi\ufb01cant role of the Hongshan cul-\nture in studies on the origins of Chinese civilization. The l atest research \ufb01ndings\nsuggests that, starting from around 5800 years ago, many reg ions across China\nbegan to experience noticeable social differentiations, e ntering an accelerated\nphase in the origin of civilization.\nRe\ufb01nement:\nA series of archaeological discoveries have cemented the signi\ufb01cant role\nof the Hongshan culture in studies on the origins of Chinese c ivilization.\nThe latest research \ufb01ndings suggest that, starting from around 5800 years\nago, various regions across China began to experience noticeable soc ial\ndifferentiations, entering an accelerated phase in the ori gin of civilization.\nHere modi\ufb01cations to the input translation are highlighted in green. This process can be iterative:\nat each step, we use the translation from the previous step as input, and prompt the LLM to produce\na potentially better translation.\nIn the above example, the re\ufb01nement of the initial model outp ut was achieved by prompting the\nLLM with a simple instruction like \u201cPlease re\ufb01ne it!\u201d. Howev er, the re\ufb01nement is based solely on\nthe instruction-following ability of the LLM, and there is n o guidance or supervision on how and\nwhere to improve the model output. A more effective approach would be to consider feedback on\nspeci\ufb01c aspects that need re\ufb01nement. For example, we can pro mpt the LLM with \u201cPlease correct\nall the grammatical errors in the translation\u201d, so that the m odel can focus more on grammatical\nerror correction during re\ufb01nement.\nA general framework of self-re\ufb01nement with LLMs involves th ree steps [ Madaan et al. ,2024 ].\n\u2022Prediction . We use an LLM to produce the initial model output.\n\u2022Feedback Collection . We obtain feedback on the model output.\n\u2022Re\ufb01nement . We use the LLM to re\ufb01ne the model output based on the feedback .\nThe last two steps can be repeated multiple times, which lead s to an iterative self-re\ufb01nement\nprocess. In this framework, receiving accurate and detaile d feedback is critical as it guides the sub-\nsequent re\ufb01nement step. In general, feedback can be obtaine d in various ways [ Fernandes et al. ,\n2023 ;Pan et al. ,2024 ]. We can manually examine the model\u2019s",
    "90d9629e1022454c": "\nall the grammatical errors in the translation\u201d, so that the m odel can focus more on grammatical\nerror correction during re\ufb01nement.\nA general framework of self-re\ufb01nement with LLMs involves th ree steps [ Madaan et al. ,2024 ].\n\u2022Prediction . We use an LLM to produce the initial model output.\n\u2022Feedback Collection . We obtain feedback on the model output.\n\u2022Re\ufb01nement . We use the LLM to re\ufb01ne the model output based on the feedback .\nThe last two steps can be repeated multiple times, which lead s to an iterative self-re\ufb01nement\nprocess. In this framework, receiving accurate and detaile d feedback is critical as it guides the sub-\nsequent re\ufb01nement step. In general, feedback can be obtaine d in various ways [ Fernandes et al. ,\n2023 ;Pan et al. ,2024 ]. We can manually examine the model\u2019s predictions to identi fy issues. Also,\nwe can use a feedback model to automatically generate feedba ck. For instance, a reward model\ntrained on some labeled data can help evaluate the model outp ut and return a score indicating its\nquality.\n\n--- Page 134 ---\n\n3.2 Advanced Prompting Methods 127\nSince the focus of this chapter is on LLM prompting, we consid er use LLMs to provide feed-\nback. To illustrate, let us consider an example of generatin g good responses to user questions:\n\u2022 First, we prompt an LLM to answer the input question, as usua l.\nGenerate a response to the question: \u201cWhat are some major env ironmental\nconcerns today?\u201d\nResponse:\nMajor environmental concerns today include climate change, airandwater\npollution, deforestation, lossofbiodiversity, andocean degradation.\n\u2022 Then, using the following prompt, we prompt the same LLM aga in to get feedback on the\ngenerated response.\nYou will be provided with a question and its corresponding re sponse. Eval-\nuate the accuracy and depth of this response, and identify an y factual in-\naccuracies or areas where additional details could enhance understanding.\nQuestion:\nWhat are some major environmental concerns today?\nResponse:\nMajor environmental concerns today include climate change , air and water\npollution, deforestation, loss of biodiversity, and ocean degradation.\nFeedback:\nThe response provided isaccurate and highlights key environmental\nconcerns. However, itwould bene\ufb01t from more speci\ufb01c and detailed\ndescriptions ofthese concerns. Here aresuggestions forimprovement:\n1.Include speci\ufb01c examples ofenvironmental issues.\n2.Add information about major pollutants.\n3.Specify theregions most affected.\n\u2022 Given the feedback the LLM produces, we prompt it to re\ufb01ne th e response.\n\n--- Page 135 ---\n\n128 Prompting\nYou will be provided with a question, its corresponding resp onse, and\nsome feedback on this response.\nQuestion:\nWhat are some major environmental concerns today?\nResponse:\nMajor environmental concerns today include climate change , air and water\npollution, deforestation, loss of biodiversity, and ocean degradation.\nFeedback:\nThe response provided is accurate and highlights key enviro nmental con-\ncerns. However, it would bene\ufb01t from more speci\ufb01c and detail ed descrip-\ntions of these concerns. Here are suggestions for improveme nt:\n1. Include speci\ufb01c examples of environmental issues.\n2. Add information about major pollutants.\n3. Specify the regions most affected.\nNow re\ufb01ne the response based on the provided feedback.\nRe\ufb01ned Response:\nKey environmental concerns include climate change, evidenced byrising\nglobal temperatures and severe weather patterns; signi\ufb01cant airand\nwater pollution inurban andindustrial areas; rampant deforestation in\nregions liketheAmazon, causing biodiversity loss; andocean degradation,\nhighlighted bycoral reefbleaching andwidespread over\ufb01shing.\nIdeally, if a strong LLM is adopted, we would like to have it pe rform all three steps without\nextra training. On the other hand, if we have enough labeled d ata for the task of interest, we can\nenhance the performance of the LLM using supervised learnin g. For example, we can \ufb01ne-tune\nthe LLM to better adapt it to re\ufb01nement tasks, or alternative ly, use task-speci\ufb01c models, which\nmay not necessarily be based on LLMs [ Welleck et al. ,2023 ;Schick et al. ,2023 ]. In a broader\nsense, improving LLMs for self-re\ufb01nement tasks can be seen a s an alignment issue. For example,\nit has been found that some",
    "13694fa6a3f41d2f": "\nregions liketheAmazon, causing biodiversity loss; andocean degradation,\nhighlighted bycoral reefbleaching andwidespread over\ufb01shing.\nIdeally, if a strong LLM is adopted, we would like to have it pe rform all three steps without\nextra training. On the other hand, if we have enough labeled d ata for the task of interest, we can\nenhance the performance of the LLM using supervised learnin g. For example, we can \ufb01ne-tune\nthe LLM to better adapt it to re\ufb01nement tasks, or alternative ly, use task-speci\ufb01c models, which\nmay not necessarily be based on LLMs [ Welleck et al. ,2023 ;Schick et al. ,2023 ]. In a broader\nsense, improving LLMs for self-re\ufb01nement tasks can be seen a s an alignment issue. For example,\nit has been found that some self-correction abilities can be activated through RLHF [ Ganguli et al. ,\n2023 ]. However, discussing these issues is beyond the scope of th is chapter. Further discussion\ncan be found in Chapter 4.\nIn LLMs, self-re\ufb01nement is related to several concepts that reveal the psychological aspects\nof these models, such as the ability to self-re\ufb02ect. A view is that if LLMs are capable of self-\nre\ufb02ection, their predictions can become more accurate and e ven possess self-correcting capabili-\nties. This self-re\ufb02ection can be activated in various ways, for example, by prompting these LLMs\nto engage in more in-depth and careful thinking, or by provid ing examples from which the models\ncan learn and re\ufb02ect. To illustrate, we consider here the deliberate-then-generate (DTG ) method\npresented in Li et al. [2023a ]\u2019s work, where LLMs are prompted to deliberate. In DTG, we ar e\ngiven an initial model output which may contain errors. LLMs are then prompted to identify the\nerror types of this model output and provide an improved outp ut. Below is a template of DTG\nprompting for Chinse-to-English translation tasks.\n\n--- Page 136 ---\n\n3.2 Advanced Prompting Methods 129\nGiven the Chinese sentence: {\u2217source \u2217}\nThe English translation is: {\u2217target \u2217}\nPlease \ufb01rst detect the type of error, and then re\ufb01ne the trans lation.\nError Type:\nWe aim to \ufb01rst predict the error type (red), and then produce a re\ufb01ned translation (blue). This\nprocess of deliberation is guided by the instruction \u201cPleas e \ufb01rst detect the type of error, and then\nre\ufb01ne the translation\u201d. It encourages LLMs to initially eng age in thoughtful analysis and then give\nbetter results. Since error type prediction and re\ufb01nement a re performed in a single run of LLMs,\nthis method incorporates both steps of feedback and re\ufb01neme nt into one process.\nIn the above prompts, we assume that the LLM we use is able to re view the input translation\nand correctly identify its error types. However, this raise s new dif\ufb01culties as the model may not\nbe good at \ufb01nding errors in translations. This will in turn re sult in extra \ufb01ne-tuning or prompt-\ning engineering efforts. So a simpler method is to reduce the burden of error identi\ufb01cation and\nuse LLMs for deliberation only. To do this, we can replace the input translation with a random\ntranslation and assign a default error type. An example of su ch a prompt is shown below.\nGiven the Chinese sentence:\n\u4e00\u7cfb\u5217\u8003\u53e4\u53d1\u73b0\u5960\u5b9a\u7ea2\u5c71\u6587\u5316\u5728\u4e2d\u534e\u6587\u660e\u8d77\u6e90\u7814\u7a76\u4e2d\u7684\u91cd\u8981\u5730\u4f4d\u3002\nThe English translation is:\nAvarietyofinnovativetechniques have rede\ufb01ned theimportance ofmodernart\nincontemporary cultural studies.\nPlease \ufb01rst detect the type of error, and then re\ufb01ne the trans lation.\nError Type: IncorrectTrans lation\nIn this example, the input translation is not generated by LL Ms but is instead randomly sam-\npled from the dataset. So it is simply an incorrect translati on for the source sentence, and we can\nset the error type accordingly. The LLMs then generate a new t ranslation by taking both the source\nsentence and the incorrect translation as input. The design of this prompt can also be considered as\nactivating the learning capabilities of LLMs through \u201cnega t",
    "1e0d6d459bcc5fbb": " Chinese sentence:\n\u4e00\u7cfb\u5217\u8003\u53e4\u53d1\u73b0\u5960\u5b9a\u7ea2\u5c71\u6587\u5316\u5728\u4e2d\u534e\u6587\u660e\u8d77\u6e90\u7814\u7a76\u4e2d\u7684\u91cd\u8981\u5730\u4f4d\u3002\nThe English translation is:\nAvarietyofinnovativetechniques have rede\ufb01ned theimportance ofmodernart\nincontemporary cultural studies.\nPlease \ufb01rst detect the type of error, and then re\ufb01ne the trans lation.\nError Type: IncorrectTrans lation\nIn this example, the input translation is not generated by LL Ms but is instead randomly sam-\npled from the dataset. So it is simply an incorrect translati on for the source sentence, and we can\nset the error type accordingly. The LLMs then generate a new t ranslation by taking both the source\nsentence and the incorrect translation as input. The design of this prompt can also be considered as\nactivating the learning capabilities of LLMs through \u201cnega tive evidence\u201d [ Marcus ,1993 ], thereby\nenabling them to re\ufb02ect and produce better outcomes through contrastive analysis. Nevertheless,\nthis method does not rely on any feedback and can enhance the p erformance of a single LLM\nprediction via simple prompting.\nNote that while DTG is non-iterative, iterative learning an d re\ufb01nement are commonly used\nin NLP. An advantage of these iterative approaches are that t hey mimics human learning and\nproblem-solving, where continuous feedback and adjustmen ts lead to progressively improved out-\ncomes. Iterative methods can be applied to a range of LLM prom pting problems. For example, in\nproblem decomposition, one can incorporate new sub-proble ms and their solutions into the con-\ntext at each step, and thus LLMs can progressively approach t he solution of the original problem.\n\n--- Page 137 ---\n\n130 Prompting\nOn the other hand, iterative methods raise several issues th at are absent in non-iterative meth-\nods, for example, errors in earlier steps may negatively imp act subsequent problem-solving, and\ndetermining when to stop iterating often requires addition al engineering effort.\n3.2.4 Ensembling\nModel ensembling for text generation has been extensively d iscussed in the NLP literature. The\nidea is to combine the predictions of two or more models to gen erate a better prediction. This\ntechnique can be directly applicable to LLMs. For example, w e can collect a set of LLMs and run\neach of them on the same input. The \ufb01nal output is a combined pr ediction from these models.\nFor LLM prompting, it is also possible to improve performanc e by combining predictions\nbased on different prompts. Suppose we have an LLM and a colle ction of prompts that address\nthe same task. We can run this LLM with each of the prompts and t hen combine the predictions.\nFor example, below are three different prompt templates for text simpli\ufb01cation.\nMake this text simpler.\n{\u2217text\u2217}\nCondense and simplify this text.\n{\u2217text\u2217}\nRewrite for easy reading.\n{\u2217text\u2217}\nEach of these prompts will lead to a different prediction, an d we can consider all three predictions\nto generate the \ufb01nal one.\nFormally, let {x1,...,xK}beKprompts for performing the same task. Given an LLM Pr(\u00b7|\u00b7),\nwe can \ufb01nd the best prediction for each xiusing \u02c6yi= arg maxyiPr(yi|xi). These predictions\ncan be combined to form a \u201cnew\u201d prediction:\n\u02c6y= Combine(\u02c6 y1,...,\u02c6yK) (3.6)\nHere Combine( \u00b7)is the combination model, which can be designed in several di fferent ways. For\nexample, we can select the best prediction by voting or by ide ntifying the one that overlaps the\nmost with others. Another method for model combination is to perform model averaging during\ntoken prediction. Let \u02c6yjbe the predicted token at the j-th step for model combination. The\n\n--- Page 138 ---\n\n3.2 Advanced Prompting Methods 131\nprobability of predicting \u02c6yjis given by\n\u02c6yj= arg max\nyjK\u2211\nk=1log Pr(yj|xk,\u02c6y1,...,\u02c6yj\u22121) (3.7)\nIn ensembling for LLM prompting, it is generally advantageo us to use diverse prompts so that\nthe combination can capture a broader range of potential res ponses. This practice is common in\nensemble learning, as diversity helps average out biases an d errors that may be speci\ufb01c to any\nsingle",
    "f24556a89203c8d0": "ent ways. For\nexample, we can select the best prediction by voting or by ide ntifying the one that overlaps the\nmost with others. Another method for model combination is to perform model averaging during\ntoken prediction. Let \u02c6yjbe the predicted token at the j-th step for model combination. The\n\n--- Page 138 ---\n\n3.2 Advanced Prompting Methods 131\nprobability of predicting \u02c6yjis given by\n\u02c6yj= arg max\nyjK\u2211\nk=1log Pr(yj|xk,\u02c6y1,...,\u02c6yj\u22121) (3.7)\nIn ensembling for LLM prompting, it is generally advantageo us to use diverse prompts so that\nthe combination can capture a broader range of potential res ponses. This practice is common in\nensemble learning, as diversity helps average out biases an d errors that may be speci\ufb01c to any\nsingle model or con\ufb01guration. From the Bayesian viewpoint, we can treat the prompt xas a latent\nvariable, given the problem of interest, p. This allows the predictive distribution of ygivenpto\nbe written as the distribution Pr(y|x)marginalized over all possible prompts\nPr(y|p) =\u222b\nPr(y|x) Pr(x|p)dx (3.8)\nThe integral computes the total probability of yby considering all possible values of x, weighted\nby their likelihoods given p. Here Pr(y|x)is given by the LLM, and Pr(x|p)is the prior distri-\nbution of prompts for the problem. This is a good model becaus e the integral effectively accounts\nfor the uncertainty in the choice of x, ensuring that the \ufb01nal predictive distribution Pr(y|p)is\nrobust and encompasses all potential variations and biases in the prompts. However, computing\nthis integral directly can be computationally infeasible d ue to the potentially in\ufb01nite space of x.\nOne approach to addressing this issue is to employ methods li ke Monte Carlo sampling, which\napproximate the integral using a manageable, \ufb01nite number o f prompts.\nWhile the Bayesian treatment is mathematically well-de\ufb01ne d, it is common practice in NLP\nto assume a non-informative or uniform prior and focus inste ad on constructing a set of diverse\nprompts. Consequently, the output can be computed using a st raightforward combination model,\nas described in Eq. ( 3.6). The issue of creating high-quality, diverse prompts has b een studied in\nCoT and other in-context learning areas. Most of the researc h focuses on incorporating a variety\nof demonstration examples across different prompts. Here, we list some of these methods.\n\u2022 Given a problem, we manually create a number of demonstrati ons and use different ones\nfor different prompts.\n\u2022 Given a problem, we use LLMs to automatically generate demo nstrations and prompts.\n\u2022 Given a prompt, we create different prompts by changing the order of demonstrations in the\nprompt.\n\u2022 Given a prompt, we use LLMs to generate a number of similar pr ompts.\n\u2022 Given a prompt, we transform it into other forms, e.g., tran slating it into other languages.\nOf course, in practice, we can combine these methods to achie ve greater diversity. An underly-\ning assumption here is that diverse prompts can lead to diver se model outputs. This is particularly\nthe case when the problem we deal with is relatively new and di f\ufb01cult. For stronger and more ro-\nbust LLMs, the variance in output for similar prompts might n ot be large. In this case, the bene\ufb01t\nof involving multiple prompts can be modest.\n\n--- Page 139 ---\n\n132 Prompting\nIn addition to providing diverse prompts for LLMs, another a pproach is to make use of the\ninherent variance in the outputs of LLMs. One simple way to ge nerate multiple outputs is to\nsample outputs from the hypothesis space. This is straightf orward for LLMs because their search\nalgorithms are naturally suited to producing multiple resu lts. For example, if beam search is\nused, we can easily collect all the complete hypotheses in th e beam to generate multiple output\ncandidates. Also, we can modify the search algorithms to imp lement more powerful sampling\nmethods and explore a wider range of high-quality hypothese s.\nThe next question is how to combine diverse model prediction s. This leads us back to the\nmodel combination issue as described in Eq. ( 3.6). Here we consider the self-consistency method,\nwhich outputs not the prediction with the highest probabili ty, but rather the one that best aligns\n",
    "64478d7bb8dd2381": " to providing diverse prompts for LLMs, another a pproach is to make use of the\ninherent variance in the outputs of LLMs. One simple way to ge nerate multiple outputs is to\nsample outputs from the hypothesis space. This is straightf orward for LLMs because their search\nalgorithms are naturally suited to producing multiple resu lts. For example, if beam search is\nused, we can easily collect all the complete hypotheses in th e beam to generate multiple output\ncandidates. Also, we can modify the search algorithms to imp lement more powerful sampling\nmethods and explore a wider range of high-quality hypothese s.\nThe next question is how to combine diverse model prediction s. This leads us back to the\nmodel combination issue as described in Eq. ( 3.6). Here we consider the self-consistency method,\nwhich outputs not the prediction with the highest probabili ty, but rather the one that best aligns\nwith other predictions [ Wang et al. ,2022a ;2023b ]. First, an LLM is prompted with CoT as\nusual and generates multiple reasoning paths by sampling. T hen, the frequency of each answer is\ncounted across these reasoning paths. The \ufb01nal output is the answer that has the highest count.\nBelow shows three different predictions by prompting an LLM with CoT.\nPrediction 1 (Correct):\nQ: Three friends play a game where they \ufb02ip a fair coin. Each fr iend \ufb02ips the\ncoin once. What is the probability that exactly one of them \ufb02i ps heads?\nA: Let\u2019s think step by step.\nEach coin \ufb02ipisanindependent event with a50% chance oflanding heads\n(H)and50% tails (T). Consider thepossible outcomes forthree \ufb02ips: HHH,\nHHT, HTH, HTT, THH, THT, TTH, TTT. Outofthese, only HHH, HHT, HTH,\nandTHH arerelevant. Weareonly interested inscenarios with exactly one\nhead: HTT, THT, andTTH. There are3such outcomes outof8total, giving a\nprobability of3/8.\nPrediction 2 (Correct):\nQ: Three friends play a game where they \ufb02ip a fair coin. Each fr iend \ufb02ips the\ncoin once. What is the probability that exactly one of them \ufb02i ps heads?\nA: Let\u2019s think step by step.\nUsing abinomial probability formula, wherep(success orheads) is0.5,and\neach ofthe3trials (\ufb02ips) isindependent, welook forexactly one success\n(head). The formula forexactlyksuccesses inntrials isgiven bythe\nbinomial coef\ufb01cient formula(n\nk)pk(1\u2212p)n\u2212k.Plugging inthevalues, weget(3\n1)(0.5)1(0.5)3\u22121= 3\u00b70.5\u00b70.25 = 0.375.\nPrediction 3 (Wrong):\n\n--- Page 140 ---\n\n3.2 Advanced Prompting Methods 133\nQ: Three friends play a game where they \ufb02ip a fair coin. Each fr iend \ufb02ips the\ncoin once. What is the probability that exactly one of them \ufb02i ps heads?\nA: Let\u2019s think step by step.\nSince each coin \ufb02iphasa50% chance oflanding heads anda50% chance of\nlanding tails, andwearelooking forthecase where only oneofthethree \ufb02ips\nresults inheads, wemight initially think thisissimply amatter ofpicking which\n\ufb02ipistheheads. There arethree \ufb02ips, soonemight incorrectly assume thatthe\nchance ofpicking onespeci\ufb01c outcome likethiswould be1outof3.Thus, they\nmight conclude thattheprobability ofexactly onehead is1/3 = 33.3%.\nPredictions 1 and 2 correctly identify the three cases where exactly one head is \ufb02ipped, both\nobtaining a probability of 37.5%. The reasoning in Predicti on 3 fails to account for the total num-\nber of outcomes possible with three coin \ufb02ips, thus giving a w rong answer of 33.3%. Therefore,\nwe select 37.5% as the \ufb01nal answer because it is the consensus .\nSelf-consistency provides a criterion for determining the best prediction in a pool of can-\ndidates. Since the prompt and the model are \ufb01xed in this metho d, it is",
    "e7e560da217b236e": " There arethree \ufb02ips, soonemight incorrectly assume thatthe\nchance ofpicking onespeci\ufb01c outcome likethiswould be1outof3.Thus, they\nmight conclude thattheprobability ofexactly onehead is1/3 = 33.3%.\nPredictions 1 and 2 correctly identify the three cases where exactly one head is \ufb02ipped, both\nobtaining a probability of 37.5%. The reasoning in Predicti on 3 fails to account for the total num-\nber of outcomes possible with three coin \ufb02ips, thus giving a w rong answer of 33.3%. Therefore,\nwe select 37.5% as the \ufb01nal answer because it is the consensus .\nSelf-consistency provides a criterion for determining the best prediction in a pool of can-\ndidates. Since the prompt and the model are \ufb01xed in this metho d, it is not strictly a prompt\nensembling method. Instead, it can be seen as an instance of o utput ensembling methods, also\nknown as hypothesis selection methods, which have long been explored in NLP, particularly for\ntext generation problems [ Xiao et al. ,2013 ]. In these methods, multiple outputs are generated by\nvarying model architectures or parameters. Each output is t hen assigned a score by some criterion,\nand the outputs are re-ranked based on these scores. There ar e various ways to de\ufb01ne the scoring\nfunction, such as measuring the agreement between an output and others, and using a stronger\nmodel to rescore each output8. Figure 3.2shows a comparison of different ensembling methods\nfor LLMs.\nNow, let us brie\ufb02y review the methods we have discussed so far in this section, such as problem\ndecomposition and self-re\ufb01nement. It is apparent that thes e methods enhance decision-making by\nintroducing more \u201cchoices\u201d into the reasoning process. To s ome extent, they all involve evalu-\nating and providing feedback on the results of LLMs. For exam ple, in self-re\ufb01nement, we need\nto offer suggestions for improving the prediction of LLMs, a nd in output ensembling, we select\nthe optimal output from a pool of candidates. In this sense, t hese methods fall under the broader\ncategory of predict-then-verify approaches, where predic tions are initially made, then veri\ufb01ed and\nre\ufb01ned. The fundamental problem here involves verifying an d evaluating the reasoning results\nor intermediate steps. This issue is somewhat related to the problem of training reward models\nin RLHF, although RLHF addresses a different aspect. In fact , the development of veri\ufb01ers has\nbeen explored and implemented in reasoning with LLMs. Most w ork, rather than developing\nheuristic-based inference-time algorithms, focuses on le arning veri\ufb01ers in a supervised manner. A\nstraightforward method is to train veri\ufb01ers as binary class i\ufb01ers, such as classifying an answer as\n8An interpretation of self-consistency is to view it as a mini mum Bayes risk search process. It searches for the best\noutput by minimizing the Bayes risk. More speci\ufb01cally, a ris k functionR(y,yr)is de\ufb01ned on each pair of outputs\n(denoted by (y,yr)), representing the cost of replacing ywith yr. Given a set of outputs \u2126, the risk of an output\ny\u2208\u2126is given by\nRisk( y) =Eyr\u223cPr(yr|x)R(y,yr)\n=\u2211\nyr\u2208\u2126R(y,yr)\u00b7Pr(yr|x) (3.9)\n\n--- Page 141 ---\n\n134 Prompting\nLLM2\nLLM1LLM2\nPrompt Prediction2\nPrediction1Prediction3Combine/Select\nFinal\nPrediction\n(a) Model Ensembling\nLLM Prompt2\nPrompt1Prompt3\nPrediction2\nPrediction1Prediction3Combine/Select\nFinal\nPrediction\n(b) Prompt Ensembling\nLLM Prompt Prediction2\nPrediction1Prediction3Combine/Select\nFinal\nPredictionSample\n(c) Output Ensembling\nFig. 3.2: Ensembling methods for LLMs. In standard model ensembling ( a), multiple LLMs varying in architectures or\nparameters are used. Each LLM receives the same prompt and pr oduces a prediction. These predictions are combined\nto generate the \ufb01nal prediction. In prompt ensembling (b), w e have one LLM and multiple prompts. The LLM produces\na prediction for each prompt, and these predictions are comb ined as usual",
    "78d6c5f6471e97b7": "--- Page 141 ---\n\n134 Prompting\nLLM2\nLLM1LLM2\nPrompt Prediction2\nPrediction1Prediction3Combine/Select\nFinal\nPrediction\n(a) Model Ensembling\nLLM Prompt2\nPrompt1Prompt3\nPrediction2\nPrediction1Prediction3Combine/Select\nFinal\nPrediction\n(b) Prompt Ensembling\nLLM Prompt Prediction2\nPrediction1Prediction3Combine/Select\nFinal\nPredictionSample\n(c) Output Ensembling\nFig. 3.2: Ensembling methods for LLMs. In standard model ensembling ( a), multiple LLMs varying in architectures or\nparameters are used. Each LLM receives the same prompt and pr oduces a prediction. These predictions are combined\nto generate the \ufb01nal prediction. In prompt ensembling (b), w e have one LLM and multiple prompts. The LLM produces\na prediction for each prompt, and these predictions are comb ined as usual. In output ensembling (c), the LLM samples\nmultiple predictions over the prediction space given a prom pt. It can be seen as a method to boost the performance\nof the LLM itself. Note that these ensembling methods can be c ombined to increase the diversity of predictions. For\nexample, we can use both prompt ensembling and output ensemb ling to obtain more diverse predictions.\ncorrect or incorrect, although these veri\ufb01ers are typicall y used as scoring models. Given a reason-\ning path for a problem, the veri\ufb01ers can be used to score eithe r the entire path (called outcome-\nbased approaches) [ Cobbe et al. ,2021 ], or each individual reasoning step (called process-based\napproaches) [ Uesato et al. ,2022 ;Lightman et al. ,2024 ].\n3.2.5 RAG and Tool Use\nRAG is generally employed when standard LLMs, which rely sol ely on pre-trained knowledge,\nlack accuracy and depth in the generated text. By drawing fro m external databases and documents,\n\n--- Page 142 ---\n\n3.2 Advanced Prompting Methods 135\nRAG can signi\ufb01cantly improve the quality of responses, ensu ring they are both contextually rel-\nevant and factually correct. Such an approach is particular ly useful in scenarios that require high\nfactual accuracy and up-to-date information, such as compl ex question answering.\nThe concept of RAG has been mentioned several times in the pre vious sections and chapters.\nFor completeness, we outline the key steps involved in RAG he re.\n\u2022 We prepare a collection of texts which are treated as an addi tional source of knowledge we\ncan access.\n\u2022 We retrieve relevant texts for a given query.\n\u2022 We input both the retrieved texts and the query into an LLM, w hich is then prompted to\nproduce the \ufb01nal prediction.\nSteps 1 and 2 can be implemented by using an external informat ion retrieval system. For\nexample, we can store the collection of texts in a vector data base and then retrieve the most similar\ntexts through vector-based search techniques. Since infor mation retrieval is not the focus of this\nchapter, we will assume that such systems are available off- the-shelf and use them directly.\nHere we present how to prompt LLMs to make use of retrieved tex ts. To illustrate, consider\nan example of using LLMs to answer the following question.\nWhere will the 2028 Olympics be held?\nWe can simply input this question into an online search engin e. It will then return the relevant\npieces of text found on the internet, for example,\n(Wikipedia)\nThe 2028 Summer Olympics, of\ufb01cially the Games of the XXXIV Ol ympiad and com-\nmonly known as Los Angeles 2028 or LA28, is an upcoming intern ational multi-sport\nevent scheduled to take place from July 14-30, 2028, in the Un ited States. ...\n(The Sporting News)\nIn 2028, Los Angeles will become the third city, following Lo ndon and Paris respectively,\nto host three Olympics after hosting the Summer Games in 1932 and 1984. It will also\nbe the \ufb01rst time the United States has hosted an Olympic Games since the 2002 Winter\nGames in Salt Lake City. ...\n...\nWe can use these retrieved texts as additional context, and p rompt an LLM to generate a\nresponse based on these texts. Below is an example RAG prompt .\n\n--- Page 143 ---\n\n136 Prompting\nYour task is to answer the following question. To help you wit h this, relevant\ntexts are provided. Please base your answer on these texts.\nQuestion:\nWhere will the 2028",
    "17732afe23532edb": " 2028 or LA28, is an upcoming intern ational multi-sport\nevent scheduled to take place from July 14-30, 2028, in the Un ited States. ...\n(The Sporting News)\nIn 2028, Los Angeles will become the third city, following Lo ndon and Paris respectively,\nto host three Olympics after hosting the Summer Games in 1932 and 1984. It will also\nbe the \ufb01rst time the United States has hosted an Olympic Games since the 2002 Winter\nGames in Salt Lake City. ...\n...\nWe can use these retrieved texts as additional context, and p rompt an LLM to generate a\nresponse based on these texts. Below is an example RAG prompt .\n\n--- Page 143 ---\n\n136 Prompting\nYour task is to answer the following question. To help you wit h this, relevant\ntexts are provided. Please base your answer on these texts.\nQuestion:\nWhere will the 2028 Olympics be held?\nRelevant Text 1:\nThe 2028 Summer Olympics, of\ufb01cially the Games of the XXXIV Ol ympiad and\ncommonly known as Los Angeles 2028 or LA28 ...\nRelevant Text 2:\nIn 2028, Los Angeles will become the third city, following Lo ndon and Paris\nrespectively, to host three Olympics after ...\n...\nThe 2028 Olympics will be held in Los Angeles.\nThis prompt assumes that the provided texts are relevant to t he question and expects the LLM\nto generate a faithful response using these texts. However, the information retrieval system may\nsometimes provide irrelevant or incorrect texts, which may lead the LLM to produce an incorrect\nanswer. One straightforward way to address this issue is to i mprove the accuracy of the informa-\ntion retrieval system. Nevertheless, as with most AI system s, errors may still occur. Therefore, it\nis also necessary to enhance the robustness of the LLM, so tha t it can make reasonable predictions\neven when the input is inaccurate. Below is a new prompt that e nables the LLM to be more faith-\nful to the facts, and allows it to choose not to answer questio ns when the information provided is\ninaccurate.\nYour task is to answer the following question. To help you wit h this, relevant\ntexts are provided. Please base your answer on these texts.\nPlease note thatyour answers need tobeasaccurateaspossibleandfaithfulto\nthefacts. Iftheinformationprovided isinsuf\ufb01cient foranaccurateresponse, you\nmay simplyoutput\"No answer!\".\nQuestion:\nWhere will the 2028 Olympics be held?\nRelevant Text 1:\nThe 2024 Summer Olympics, of\ufb01cially the Games of the XXXIII O lympiad and\nbranded as Paris 2024, were an international multi-sport ev ent ...\n...\nNoanswer!\nIn this example, the LLM refuses to answer because the provid ed information is insuf\ufb01cient and\nirrelevant to the question.\nBoth RAG and \ufb01ne-tuning are common methods for adapting LLMs using task-speci\ufb01c data.\nStandard RAG is training-free and can be directly applied to LLMs. To further improve RAG, it\n\n--- Page 144 ---\n\n3.2 Advanced Prompting Methods 137\nis also possible to \ufb01ne-tune LLMs, though this will require s ome training effort. For example, we\ncan \ufb01ne-tune LLMs using human-labelled data to supervise th em in learning to refuse to answer.\nNote that, while the examples shown above seem simple, RAG is not trivial. From the prompt\nengineering perspective, different use cases may require d ifferent prompts, though our somewhat\n\u201cgreedy\u201d goal is to develop a universal prompting strategy t hat can adapt to different tasks. In\nmany cases, we need to control how much we depend on the retrie ved context to make predictions.\nSometimes, LLMs must derive responses strictly from the pro vided texts, while at other times,\nthey may need to generate responses using their pre-trained knowledge if the provided texts are\ninsuf\ufb01cient. There are many aspects of RAG, such as improvem ents to the retrieval systems, that\ncannot be covered in this chapter. Interested readers can re fer to surveys of RAG techniques for\nmore information [ Li et al. ,2022 ;Gao et al. ,2023c ].\nOne reason we discuss RAG here is that it can be broadly regard ed as an instance of the\ngeneral problem decomposition framework (see Section 3.2.2 ). RAG divides problem-solving\ninto two steps. In the \ufb01rst step, we collect relevant and supp orting information",
    "37df10f9a8996d50": " a universal prompting strategy t hat can adapt to different tasks. In\nmany cases, we need to control how much we depend on the retrie ved context to make predictions.\nSometimes, LLMs must derive responses strictly from the pro vided texts, while at other times,\nthey may need to generate responses using their pre-trained knowledge if the provided texts are\ninsuf\ufb01cient. There are many aspects of RAG, such as improvem ents to the retrieval systems, that\ncannot be covered in this chapter. Interested readers can re fer to surveys of RAG techniques for\nmore information [ Li et al. ,2022 ;Gao et al. ,2023c ].\nOne reason we discuss RAG here is that it can be broadly regard ed as an instance of the\ngeneral problem decomposition framework (see Section 3.2.2 ). RAG divides problem-solving\ninto two steps. In the \ufb01rst step, we collect relevant and supp orting information for a given query\nfrom various knowledge sources. In the second step, we use LL Ms to generate responses based\non the collected information. If we extend the concept of pro blem decomposition further, we\nwill \ufb01nd that many tasks requiring the use of external system s or tools can be treated as similar\nproblems. One such example is tool use in LLMs. In many applic ations, LLMs need to employ\nexternal databases, APIs, and even simulation tools to gene rate accurate responses. For example,\nLLMs can access real-time data from \ufb01nancial markets to prov ide up-to-date investment advice or\nintegrate with healthcare databases to offer personalized medical insights. This integration extends\nthe capabilities of LLMs by allowing them to interact with, a nd in some contexts, in\ufb02uence or\ncontrol external systems. Consequently, LLMs function mor e asautonomous agents rather than\nmere text generators [ Franklin and Graesser ,1996 ].\nThe issue of tool use is broad and vast. Here we narrow our disc ussion to tasks that can be fa-\ncilitated by calling external APIs to solve some of the sub-p roblems [ Parisi et al. ,2022 ;Gao et al. ,\n2023b ]. Consider again the example of asking an LLM to answer \u201cWher e will the 2028 Olympics\nbe held?\u201d. Suppose the LLM can access a web search tool. We can then prompt the LLM to\nanswer the question with web search, like this\nYour task is to answer the following question. You may use ext ernal tools, such\nas web search, to assist you.\nQuestion:\nWhere will the 2028 Olympics be held?\nThe information regarding this question is given as follows :\n{tool: web-search, query: \"2028 Olympics\"}\nSo the answer is: Los Angeles\nHere {tool: web-search, query: \"2028 Olympics\"} indicates a request to the web search\nsystem using the query \u201c2028 Olympics\u201d. When the LLM sees thi s string, it executes a web search\nand uses the result to replace the string. Then, in subsequen t steps of prediction, the LLM uses\nthis web search result as context to produce the correct answ er.\nConsider another example where we ask the LLM to solve a mathe matical problem.\n\n--- Page 145 ---\n\n138 Prompting\nProblem:\nA swimming pool needs to be \ufb01lled with water. The pool measure s 10 meters\nin length, 4 meters in width, and 2 meters in depth. Calculate the volume of the\npool in cubic meters and then determine how many liters of wat er are needed to\n\ufb01ll it (considering 1 cubic meter equals 1000 liters).\nSolution:\nTo solve this problem, the LLM needs to \ufb01rst calculate the vol ume of the\npool by using the formula for the volume of a rectangular pris m: Length \u00d7\nWidth \u00d7Depth.Therefore, The volume is 10m\u00d74m\u00d72m={tool:\ncalculator, expres sion: 10*4*2}m3. Next, to \ufb01nd out how\nmany liters of water are needed, the LLM multiplies the volum e in cubic\nmeters by 1000 (since 1 cubic meter equals 1000 liters). Thus ,80\u00d71000\n={tool: calculator, expres sion: 80*1000} liters.\nHere the string {tool: calculator, expression: 10 * 4 * 2} triggers the invocation\nof a mathematical interpreter to calculate the result of the expression. Note that the result (i.e.,\n80) will replace {tool: calculator, expression: 10 * 4 * 2} and can be referred to\nin",
    "c566ccde183f0793": " the vol ume of the\npool by using the formula for the volume of a rectangular pris m: Length \u00d7\nWidth \u00d7Depth.Therefore, The volume is 10m\u00d74m\u00d72m={tool:\ncalculator, expres sion: 10*4*2}m3. Next, to \ufb01nd out how\nmany liters of water are needed, the LLM multiplies the volum e in cubic\nmeters by 1000 (since 1 cubic meter equals 1000 liters). Thus ,80\u00d71000\n={tool: calculator, expres sion: 80*1000} liters.\nHere the string {tool: calculator, expression: 10 * 4 * 2} triggers the invocation\nof a mathematical interpreter to calculate the result of the expression. Note that the result (i.e.,\n80) will replace {tool: calculator, expression: 10 * 4 * 2} and can be referred to\nin the following token predictions. For example, in the last step of problem-solving, 80 is used\ninstead of {tool: calculator, expression: 10 * 4 * 2} .\nA key difference between the tool use examples here and the pr eviously discussed RAG ex-\namples is that in tool use, external functions can be called d uring inference. In contrast, in RAG,\nthe retrieved texts are provided before the prediction proc ess begins. However, from the language\nmodeling perspective, they are actually doing the same thin g: before generating the \ufb01nal result,\nwe use external tools, either manually or automatically, to obtain suf\ufb01cient and relevant context. A\nhigh-level interpretation of these approaches is that they both rely on an \u201cagent\u201d that can determine\nwhere and how to call external functions to generate the cont ext necessary for prediction.\nAn issue with tool use is that the original LLMs are not traine d to generate the necessary\nmarkers for tool use. Therefore, we need to \ufb01ne-tune the LLMs to adapt them for these tasks\n[Schick et al. ,2024 ]. As this chapter focuses on prompting, we will not present t he details of this\n\ufb01ne-tuning process. To put it simply, we \ufb01rst need to annotat e data. For each \ufb01ne-tuning example,\nwe replace parts of the output that require the use of externa l tools with prede\ufb01ned commands\nor markers. Then, we use this labeled data to \ufb01ne-tune the par ameters of the LLM as usual. As\na result, the LLM can gain the ability to generate commands fo r calling external tools. During\ninference, we can execute these tool use commands in the mode l outputs to get assistance from\nexternal tools.\n3.3 Learning to Prompt\nSo far in this chapter, we have considered several basic prom pting strategies and various re\ufb01ne-\nments to them. However, all the prompts we have discussed wer e designed manually. This leads\nto a number of problems: First, designing high-quality prom pts is inherently dif\ufb01cult and requires\nsubstantial manual effort. For example, extensive experim entation with different prompts is often\nneeded to identify the most effective ones. Since different LLMs may respond better to certain\n\n--- Page 146 ---\n\n3.3 Learning to Prompt 139\ntypes of prompts, developing universally effective prompt s can be even more resource-intensive.\nSecond, manual prompt design relies heavily on human expert ise, which can limit the diversity\nof approaches and overlook potentially effective prompts t hat are not immediately obvious to hu-\nmans. Third, prompts created by humans can be complex and red undant, leading to longer inputs\nfor LLMs and higher computational costs.\nIn this section, we discuss techniques for automated prompt ing. These methods aim to auto-\nmatically create, optimize, and represent prompts so that t he downstream tasks can be addressed\nmore effectively and ef\ufb01ciently. In particular, we conside r three issues here.\n\u2022 How can we automate the process of designing and optimizing prompts for LLMs?\n\u2022 Are there other forms of representing prompts beyond strin gs, and how can we learn such\nrepresentations?\n\u2022 How can we make prompts more concise and compact, thereby re ducing their complexity\nand length?\nNote that there are many settings in which we can investigate these issues. For example, we\nmight specify that prompts are developed speci\ufb01cally for a p articular LLM, or that the develop-\nment is independent of the LLM used. These settings can lead t o different methods and application\nscenarios, but these methods may overlap in some ways. In the following discussion, we will cover\nseveral different scenarios and discuss the connections be tween",
    "e7778dcda3ad25a6": " methods aim to auto-\nmatically create, optimize, and represent prompts so that t he downstream tasks can be addressed\nmore effectively and ef\ufb01ciently. In particular, we conside r three issues here.\n\u2022 How can we automate the process of designing and optimizing prompts for LLMs?\n\u2022 Are there other forms of representing prompts beyond strin gs, and how can we learn such\nrepresentations?\n\u2022 How can we make prompts more concise and compact, thereby re ducing their complexity\nand length?\nNote that there are many settings in which we can investigate these issues. For example, we\nmight specify that prompts are developed speci\ufb01cally for a p articular LLM, or that the develop-\nment is independent of the LLM used. These settings can lead t o different methods and application\nscenarios, but these methods may overlap in some ways. In the following discussion, we will cover\nseveral different scenarios and discuss the connections be tween various methods.\n3.3.1 Prompt Optimization\nGiven that prompt design is dif\ufb01cult and labor-intensive, i t is desirable to use machine learning\nmodels to discover the optimal prompt for a speci\ufb01c task (cal l itautomatic prompt design or\nprompt optimization ). This approach can broadly be regarded as an instance of automated ma-\nchine learning (AutoML ), which aims to reduce or eliminate the need for expert-driv en manual\ndesign of machine learning models. Although our focus here i s on the design of prompts, prompts\nthemselves are discrete structures. Therefore, designing prompts is very similar to designing ma-\nchine learning models, such as discrete model architecture s. Perhaps one of the most related \ufb01elds\nisneural architecture search (NAS ), where the most optimal neural networks are identi\ufb01ed by\nexploring a space of possible neural networks [ Zoph and Le ,2016 ;Elsken et al. ,2019 ]. If we con-\nsider prompt optimization as a search process, then we can de scribe a general prompt optimization\nframework involving the following components:\n\u2022Prompt Search Space . This de\ufb01nes all possible prompts that the algorithms can ex plore.\nFor example, one can edit some seed prompts to generate a set o f diverse candidate prompts.\n\u2022Performance Estimation . Once an prompt is chosen, it needs to be evaluated. For examp le,\na straightforward way is to input it to an LLM and measure its p erformance on a validation\nset.\n\u2022Search Strategy . The search process is generally the same as that used in many AI sys-\ntems. At each step, the system explores a set of promising pro mpts in the search space and\n\n--- Page 147 ---\n\n140 Prompting\nevaluates them. This process continues as more prompts are e xplored. The outcome of the\nsearch is the best-performing prompt observed until the sea rch stops.\nThis is a very general framework, and different prompt optim ization systems can vary in their\ndesign of each component. A widely-used approach is to use LL Ms as the basis to develop these\ncomponents. Initially, a few prompts are provided. Then, th e following process is iterated until\na stopping criterion is met: 1) the prompts are evaluated on a validation set; 2) a candidate pool\nis maintained by keeping only the most promising prompts; an d 3) new prompts are created by\nemploying LLMs to infer similar prompts from this candidate pool. One bene\ufb01t of this approach is\nthat it allows us to use off-the-shelf LLMs to perform the tas ks mentioned above without the need\nfor substantial system development. To achieve this, we can prompt or \ufb01ne-tune LLMs to adapt\nthem to these tasks. Here we consider Zhou et al. [2023c ]\u2019s method for illustrating LLM-based\nprompt optimization. It involves the following steps.\n\u2022Initialization . LetCrepresent the pool of the candidate prompts we intend to expl ore. The\n\ufb01rst step is to add initial prompts into C. We can do this in several ways. A simple method is\nto create such prompts by hand for a given task. However, in ma ny cases where humans have\nlimited knowledge about how to write effective prompts for t he task, developing prompts\nbecomes challenging. In these cases, it is desirable to use L LMs to generate prompts.\nFor example, we can directly instruct LLMs to produce prompt s, providing them with a\ndescription of the task.\nYou are given a task to complete using LLMs. Please write a pro mpt to\nguide the LLMs.\n{\u2217task-descripion \u2217}\nThis method is straightforward, but it still requires a huma n-provided description of the",
    "adc1201183398308": "\u2019s method for illustrating LLM-based\nprompt optimization. It involves the following steps.\n\u2022Initialization . LetCrepresent the pool of the candidate prompts we intend to expl ore. The\n\ufb01rst step is to add initial prompts into C. We can do this in several ways. A simple method is\nto create such prompts by hand for a given task. However, in ma ny cases where humans have\nlimited knowledge about how to write effective prompts for t he task, developing prompts\nbecomes challenging. In these cases, it is desirable to use L LMs to generate prompts.\nFor example, we can directly instruct LLMs to produce prompt s, providing them with a\ndescription of the task.\nYou are given a task to complete using LLMs. Please write a pro mpt to\nguide the LLMs.\n{\u2217task-descripion \u2217}\nThis method is straightforward, but it still requires a huma n-provided description of the task.\nAn alternative method is to use LLMs to generate prompts give n examples of the input and\noutput of the task. Here is a prompt template.\nYou are provided with several input-output pairs for a task. Please write\nan instruction for performing this task.\nInput: {\u2217input1 \u2217}Output: {\u2217output1 \u2217}\nInput: {\u2217input2 \u2217}Output: {\u2217output2 \u2217}\n...\nAs such, LLMs can infer the corresponding instruction for th e task from the provided inputs\nand outputs.\n\u2022Evaluation . Once we obtain the candidate pool C, we need to evaluate the prompts in C.\nOne method is to feed each prompt into an LLM and assess the res ults on the downstream\n\n--- Page 148 ---\n\n3.3 Learning to Prompt 141\ntask. For example, we can evaluate the output of the LLM given an input using a pre-de\ufb01ned\nmetric, or alternatively, use the log-likelihood of the out put as a measure of the quality of\nthe prompt.\n\u2022Pruning . IfCcontains a large number of prompts, it is reasonable to prune the unpromising\nprompts within it, thus reducing the computational burden i n subsequent steps. This is a\nstandard pruning problem. Given the evaluation score for ea ch prompt, a simple method is\nto keep only a certain percentage of the prompts and discard t he rest.\n\u2022Expansion . Expansion is a key operation in search algorithms used to ex plore different\nstates in the search space. The expansion operation here can be de\ufb01ned as a function\nC\u2032= Expand( C,f) (3.10)\nwhereC\u2032is the set of new prompts generated from Cusing the model f. If we consider f\nas an LLM, we can perform the expansion operation by instruct ingfto generate new and\nrelevant prompts based on C. Below is an example.\nBelow is a prompt for an LLM. Please provide some new prompts t o per-\nform the same task.\nInput: {\u2217prompt \u2217}\nThen, we replace CwithC\u2032. The steps of evaluation, pruning and expansion can be re-\npeated, and so we can gradually explore a wider range of promp ts.\nIn prompt optimization, the expansion step plays a key role, as it de\ufb01nes how we explore\nthe search space, and our goal is to \ufb01nd optimal results with m inimal effort. One improvement\nto this step is to treat the problem as a paraphrasing task. A s imple method is to apply off-the-\nshelf paraphrasing systems, either based on LLMs or other mo dels, to transform input prompts\ninto semantically equivalent forms [ Jiang et al. ,2020 ]. Alternatively, we can de\ufb01ne speci\ufb01c edit\noperations, such as insertions and modi\ufb01cations, for each t oken. A given prompt can be edited\ninto new prompts by applying these operations [ Prasad et al. ,2023 ]. Also, further evaluation and\npruning can be applied to \ufb01lter out low-quality prompts. In a ddition to framing prompt generation\nas a paraphrasing problem, we can improve the quality of prom pts during expansion by learning\nfrom feedback [ Pryzant et al. ,2023 ]. This approach is somewhat related to the self-re\ufb01nement\nissue discussed in Section 3.2.3 . An LLM can be used to generate feedback on an input prompt,\nwhich is then revised based on this feedback. This feedback- and-revision cycle can be repeated\nmultiple times until the result converges or the desired out come is achieved.\nAnother approach to prompt optimization is to apply classic optimization techniques. For\nexample, the problem can be framed as an evolutionary comput ation",
    "a46cbae89730ca30": " and modi\ufb01cations, for each t oken. A given prompt can be edited\ninto new prompts by applying these operations [ Prasad et al. ,2023 ]. Also, further evaluation and\npruning can be applied to \ufb01lter out low-quality prompts. In a ddition to framing prompt generation\nas a paraphrasing problem, we can improve the quality of prom pts during expansion by learning\nfrom feedback [ Pryzant et al. ,2023 ]. This approach is somewhat related to the self-re\ufb01nement\nissue discussed in Section 3.2.3 . An LLM can be used to generate feedback on an input prompt,\nwhich is then revised based on this feedback. This feedback- and-revision cycle can be repeated\nmultiple times until the result converges or the desired out come is achieved.\nAnother approach to prompt optimization is to apply classic optimization techniques. For\nexample, the problem can be framed as an evolutionary comput ation problem, where prompts\nare treated as candidates that evolve generation by generat ion as the optimization progresses\n[Guo et al. ,2024 ]. Since many powerful optimization algorithms have been de veloped in related\n\ufb01elds, they can be directly applied to this problem.\n\n--- Page 149 ---\n\n142 Prompting\nIn practice, we might be tempted to use existing LLM APIs to im plement the steps described\nabove. Such an approach, however, would be strongly depende nt on the inference and in-context\nlearning abilities of the LLMs. If these LLMs are not strong a nd lack adaptation to the tasks, they\nmay introduce errors into search, for example, generating i ncorrect prompts during expansion. In\nsuch cases, it is preferable to train models that are better s uited to the tasks. One approach in\nthis research direction appeals to reinforcement learning , which has been widely used in solving\ndiscrete decision making and optimization problems. For ex ample, Deng et al. [2022 ] developed\na prompt generator by integrating an FFN-based adaptor into an LLM. The prompt generator is\ntrained as a typical policy network, but only the parameters of the adaptor are updated while the\nremaining parameters of the model are kept unchanged. Durin g training, the reward is obtained by\ntesting the generated prompts using another LLM, similar to the evaluation method as discussed\nabove. Once the training is complete, the prompt generator i s then employed to generate new\nprompts.\nNote that, in our discussion here, prompts are simply seen as sequences of tokens, and the out-\nput of prompt optimization is such a sequence. However, in a s trict sense, prompts have complex\nstructures and include different \ufb01elds such as user input, i nstruction, and demonstration. While\nour discussed approaches are mostly general, much work in pr ompt optimization has focused on\nlearning better instructions for prompting. Speci\ufb01cally, the goal is to generate instructions that\neffectively guide LLMs based on a given task. Of course, the c oncept of prompt optimization\ncan also be extended to learning other parts of prompts. For e xample, there has been substan-\ntial research interest in learning to select or generate dem onstrations in CoT [ Liu et al. ,2022 ;\nRubin et al. ,2022 ;Zhang et al. ,2023b ]. One of the differences between learning instructions\nand learning demonstrations is that generating high-quali ty demonstrations using LLMs is rela-\ntively easy and the focus of learning demonstrations is typi cally on how to sample appropriate\ndemonstrations from a pool of candidates. In contrast, the d if\ufb01culty in learning instructions is\npartly because pre-trained LLMs are not suited to predict th e quality of instructions, and testing\nthese instructions on downstream tasks is computationally expensive. This makes the optimization\nmethods costly to apply, and exploring a wide variety of inst ructions poses signi\ufb01cant challenges.\n3.3.2 Soft Prompts\nAlthough developing natural language prompts, either manu ally or automatically, is a straight-\nforward and widely applied approach, it presents some probl ems. One problem is that natural\nlanguage prompts can be complex and lengthy, resulting in si gni\ufb01cant computational burdens\nwhen processed via LLMs. In many applications, users may nee d to perform a task repeatedly,\nand inputting the same long prompt into the LLMs a large numbe r of times is clearly inef\ufb01cient.\nAnother problem is that while prompts are typically represe nted as discrete token sequences (call\nthem hard prompts ) in regular LLM input, the LLMs encode them as low-dimension al real-\nvalued vectors.",
    "b10a1b34378ce4de": " and testing\nthese instructions on downstream tasks is computationally expensive. This makes the optimization\nmethods costly to apply, and exploring a wide variety of inst ructions poses signi\ufb01cant challenges.\n3.3.2 Soft Prompts\nAlthough developing natural language prompts, either manu ally or automatically, is a straight-\nforward and widely applied approach, it presents some probl ems. One problem is that natural\nlanguage prompts can be complex and lengthy, resulting in si gni\ufb01cant computational burdens\nwhen processed via LLMs. In many applications, users may nee d to perform a task repeatedly,\nand inputting the same long prompt into the LLMs a large numbe r of times is clearly inef\ufb01cient.\nAnother problem is that while prompts are typically represe nted as discrete token sequences (call\nthem hard prompts ) in regular LLM input, the LLMs encode them as low-dimension al real-\nvalued vectors. This raises the question of whether there ar e more compact and ef\ufb01cient ways to\nrepresent prompts.\nIn this subsection, we introduce the concept of soft prompts , which can be viewed as hidden,\ndistributed representations of prompts. When prompting LL Ms, we are concerned with commu-\nnicating tasks or questions to elicit the desired responses . We can de\ufb01ne hard prompts as explicit,\nprede\ufb01ned text sequences that users input directly into LLM s to guide the responses. In contrast,\nwe can think of soft prompts as implicit, adaptable promptin g patterns embedded within LLMs.\nUnlike hard prompts, which are expressed in natural languag e and should be understandable for\n\n--- Page 150 ---\n\n3.3 Learning to Prompt 143\n...Translate this into Chinese . I have a cat . ...Transformer... hjhj+1hj+2hj+3hj+4hj+5hj+6hj+7hj+8hj+9 ...... ... ... ... ... ... ... ... ... ... ... ...\nHard Prompt (Instruction)Soft Prompt\nFig. 3.3: Illustration of hard and soft prompts. Here the hard prompt i s the instruction we input to the LLM for\nperforming the task. The LLM encodes this instruction as usu al, and the intermediate representations corresponding to\nthe instruction can be viewed as some sort of soft prompt.\nhumans, soft prompts are encoded in a format that is more comp rehensible to the model rather\nthan to humans. To illustrate, consider a simple prompt\nTranslate the sentence into Chinese.\nConsider it done!\nHere, the instruction \u201cTranslate the sentence into Chinese \u201d can be seen as a hard prompt, denoted\nby the token sequence c1...c5. By feeding these tokens into an LLM, they are transformed in to a\nsequence of real-valued vectors h1...h5, each corresponding to a token. We can roughly think of\nh1...h5as a soft prompt, as illustrated in Figure 3.3.\nWhile the above example shows that soft prompts can be genera ted by transforming hard\nprompts, there is not necessarily a direct correspondence b etween them. In fact, we do not even\nneed to interpret soft prompts using meaningful text. They a re instead simply hidden states in\nLLMs and can be learned as standard parameters of the models t hrough continuous optimiza-\ntion. Such a treatment allows us to explore prompting method s beyond text. As another bene\ufb01t,\nsoft prompts provide dense, low-dimensional, and learnabl e representations for encoding how\nwe guide LLMs to generate speci\ufb01c outputs. The training and a pplication of these representa-\ntions require signi\ufb01cantly lower computational costs than those required for processing long hard\nprompts. This approach would be of great practical value in L LM inference applications where\nthe same prompt is repeatedly used.\n3.3.2.1 Adapting LLMs with Less Prompting\nOne obvious way to adapt an LLM for a particular task is to simp ly \ufb01ne-tune the model using\nlabeled data. This leads to a variety of LLM alignment method s, such as supervised \ufb01ne-tuning,\nwhich update the model parameters by aligning the responses to given prompts with supervision\nsignals. Fine-tuned LLMs embed task-related information i n model parameters, and thus these\n\n--- Page 151 ---\n\n144 Prompting\nmodels can response correctly when dealing with similar pro mpts with those in \ufb01ne-tuning.\nIf we take this idea further, we can expect LLMs to absorb the k nowledge about prompting\nof a task as much as possible during \ufb01ne-tuning. Consequentl y, the prompting information is\npartially captured in the model",
    "b1894a9b4f4cd94e": " L LM inference applications where\nthe same prompt is repeatedly used.\n3.3.2.1 Adapting LLMs with Less Prompting\nOne obvious way to adapt an LLM for a particular task is to simp ly \ufb01ne-tune the model using\nlabeled data. This leads to a variety of LLM alignment method s, such as supervised \ufb01ne-tuning,\nwhich update the model parameters by aligning the responses to given prompts with supervision\nsignals. Fine-tuned LLMs embed task-related information i n model parameters, and thus these\n\n--- Page 151 ---\n\n144 Prompting\nmodels can response correctly when dealing with similar pro mpts with those in \ufb01ne-tuning.\nIf we take this idea further, we can expect LLMs to absorb the k nowledge about prompting\nof a task as much as possible during \ufb01ne-tuning. Consequentl y, the prompting information is\npartially captured in the model parameters, and the \ufb01ne-tun ed LLMs can perform the task with\nless prompting. Here we consider a simple form of prompt, whe re only an instruction (denoted by\nc) and an user input (denoted by z) are included. A prompt can be expressed using the following\ntuple\nx= (c,z) (3.11)\nGiven a set of prompt-response pairs D={(x,y)}, the objective of \ufb01ne-tuning is to minimize\nthe total loss incurred over this set. A popular method is to m inimize the negative log-likelihood\n(i.e., maximize the log-likelihood) with respect to the mod el parameters \u03b8:\n\u02c6\u03b8= arg max\n\u03b8\u2211\n(x,y)\u2208Dlog Pr \u03b8(y|x)\n= arg max\n\u03b8\u2211\n(x,y)\u2208Dlog Pr \u03b8(y|c,z) (3.12)\nwhere Pr\u03b8(\u00b7|\u00b7)is the probability predicted by an LLM with the parameters \u03b89.\nIn general, the instruction in each \ufb01ne-tuning example shou ld follow the guideline of prompt\ndesign, for example, a good instruction should be as clear as possible and provide a detailed\ndescription of the task. However, the method described in th e above equation does not restrict the\ninstruction to any particular form. This \ufb02exibility allows us to instruct LLMs in any way that we\nwant. Consider an example where we intend to instruct LLMs to translate an English sentence\ninto Chinese. Of course, as mentioned earlier in this chapte r, we can prompt LLMs using the\ninstruction\nTranslate the following sentence from English to Chinese.\nIf we want the instruction to be simpler, we may rephrase it in to a simpler form\nTranslate this into Chinese.\nEven, we can de\ufb01ne the instruction as a single phrase\nTranslate!\nWith certain \ufb01ne-tuning effort, we can adapt LLMs to follow a ny of these instructions. From\na ef\ufb01cient prompting perspective, there are computational advantages in simplifying instructions\nin prompting. For example, we can use simple instructions li ke \u201cTranslate!\u201d to perform tasks\nthat would typically require more complex and detailed inst ructions. This can make subsequent\n9In practice, we initialize \u03b8with the parameters obtained from pre-training, and then ad just\u03b8moderately to ensure\nthat the results after \ufb01ne-tuning do not deviate too much fro m the pre-trained results.\n\n--- Page 152 ---\n\n3.3 Learning to Prompt 145\nFull Context User Input + Prt(y|c,z)\nc z yTeacher Model:\nSimpli\ufb01ed Context User Input + Prs(y|c\u2032,z)\nc\u2032 z yStudent Model:Loss\nFig. 3.4: Illustration of context distillation [ Snell et al. ,2022 ]. The teacher model is a standard LLM, which takes both\nthe context and the user input as model input and produces a pr ediction as model output. Then, we simplify the context\n(e.g., simplifying the instruction in prompting) and use th e student model to make predictions based on the simpli\ufb01ed\ncontext and the user input. The student model is trained by mi nimizing the loss between the predictions produced by\nthe two models.\nprompting during inference much easier. On the other hand, \ufb01 ne-tuning LLMs with overly simpli-\n\ufb01ed instructions may be harmful to the generalization of the models. Since simpli\ufb01ed instructions\ncan lead to a loss of information, it is more likely that the LL Ms will over\ufb01t the \ufb01ne-tuning data\nand fail to generalize beyond those instructions. In scenar ios involving both complex and simpl",
    "ac5938bbe6766a2f": " distillation [ Snell et al. ,2022 ]. The teacher model is a standard LLM, which takes both\nthe context and the user input as model input and produces a pr ediction as model output. Then, we simplify the context\n(e.g., simplifying the instruction in prompting) and use th e student model to make predictions based on the simpli\ufb01ed\ncontext and the user input. The student model is trained by mi nimizing the loss between the predictions produced by\nthe two models.\nprompting during inference much easier. On the other hand, \ufb01 ne-tuning LLMs with overly simpli-\n\ufb01ed instructions may be harmful to the generalization of the models. Since simpli\ufb01ed instructions\ncan lead to a loss of information, it is more likely that the LL Ms will over\ufb01t the \ufb01ne-tuning data\nand fail to generalize beyond those instructions. In scenar ios involving both complex and simpli-\n\ufb01ed instructions for \ufb01ne-tuning, this problem is more sever e because the labeled data available for\n\ufb01ne-tuning is usually limited, and accommodating a variety of instructions is costly.\nAn alternative way to adapt LLMs for simpli\ufb01ed instructions is through knowledge distillation.\nAs an example, we consider the context distillation method [ Snell et al. ,2022 ]. The goal of this\nmethod is to learn a student model that can make use of simpli\ufb01 ed instructions from a well-trained\ninstruction-following teacher model. Figure 3.4shows an illustration of this approach. Building\nthe teacher model follows a standard \ufb01ne-tuning process: we \ufb01rst collect a certain amount of\ndata that includes instructions, user inputs, and correct r esponses, and then we continue to train a\npre-trained model with this dataset. For building the stude nt model, we need to construct a new\ndataset D\u2032where each sample is a tuple consisting of an instruction, a c orresponding simpli\ufb01ed\ninstruction, and a user input, denoted by x\u2032= (c,c\u2032,z). Knowledge distillation is performed by\nminimizing a loss function de\ufb01ned on the outputs of the teach er and student models\n\u02c6\u03b8= arg min\n\u03b8\u2211\nx\u2032\u2208D\u2032Loss(Prt(\u00b7|\u00b7),Prs\n\u03b8(\u00b7|\u00b7),x\u2032) (3.13)\nwhere Prt(\u00b7|\u00b7)denotes the pre-trained teacher model, and Prs\n\u03b8(\u00b7|\u00b7)denotes the student model with\nthe parameters \u03b8. To keep the notation simple we will write Loss(Prt(\u00b7|\u00b7),Prs\n\u03b8(\u00b7|\u00b7),x)asLoss\nfor short. A commonly-used loss is the sequence-level loss, which has the basic form:\nLoss =\u2211\nyPrt(y|c,z) log Prs\n\u03b8(y|c\u2032,z) (3.14)\nBut this function is computationally infeasible because it requires summing over an exponen-\ntially large number of outputs. A variant of this method is to train the student model using outputs\ngenerated by the teacher model. For each sample, we use the te acher model to produce an output\n\n--- Page 153 ---\n\n146 Prompting\n\u02c6y= arg maxylog Prt(y|c,z). Then we consider \u02c6yas the target for learning, and the loss function\nis given by\nLoss = log Prs\n\u03b8(\u02c6y|c\u2032,z) (3.15)\nAlternatively, we can minimize the distances between the pr obability distributions outputted\nby the two models [ Askell et al. ,2021 ]. For example, the loss function can be de\ufb01ned as the KL\ndivergence between the two output distributions\nLoss = KL(Pt||Ps\n\u03b8) (3.16)\nwhere\nPt= Prt(\u00b7|c,z) (3.17)\nPs\n\u03b8= Prs\n\u03b8(\u00b7|c\u2032,z) (3.18)\nAlthough we have restricted ourselves to knowledge distill ation for instructions, the approaches\ndiscussed here are general. By learning from the outputs of t he teacher model, the knowledge in\nprompting can be distilled into the parameters of the studen t model. Therefore, the distilled model\ncan be considered as encoding some sort of soft prompt. This m ethod can be applied to many\nother problems in prompt learning, such as compressing long contexts and learning soft prompts\nas speci\ufb01c components of LLMs.\n3.3.2.2 Learning Soft Prompts for Parameter-ef\ufb01cient Fine",
    "80087bedd8d4d88b": " can be de\ufb01ned as the KL\ndivergence between the two output distributions\nLoss = KL(Pt||Ps\n\u03b8) (3.16)\nwhere\nPt= Prt(\u00b7|c,z) (3.17)\nPs\n\u03b8= Prs\n\u03b8(\u00b7|c\u2032,z) (3.18)\nAlthough we have restricted ourselves to knowledge distill ation for instructions, the approaches\ndiscussed here are general. By learning from the outputs of t he teacher model, the knowledge in\nprompting can be distilled into the parameters of the studen t model. Therefore, the distilled model\ncan be considered as encoding some sort of soft prompt. This m ethod can be applied to many\nother problems in prompt learning, such as compressing long contexts and learning soft prompts\nas speci\ufb01c components of LLMs.\n3.3.2.2 Learning Soft Prompts for Parameter-ef\ufb01cient Fine -tuning\nUpdating all parameters is a common method for adapting LLMs to tasks of interest. Although\n\ufb01ne-tuning is considered computationally cheaper than pre -training, it is still costly to apply in\npractice. This issue motivates the development of paramete r-ef\ufb01cient \ufb01ne-tuning methods, which\naim to minimize the number of parameters that need to be updat ed.\nOne approach, known as pre\ufb01x \ufb01ne-tuning , is to append a series of trainable vectors, or\npre\ufb01xes, at the beginning of the input of each Transformer la yer [ Li and Liang ,2021 ]. These\npre\ufb01xes can be thought of as soft prompts that serve as additi onal context to guide the behavior\nof the model under speci\ufb01c tasks. During \ufb01ne-tuning, we need only to learn the pre\ufb01xes for\nembedding task-speci\ufb01c knowledge. Thus, this method is ef\ufb01 cient because it only modi\ufb01es a\nsmall part of the model rather than adjusting the entire set o f model parameters.\nSpeci\ufb01cally, let the input of a layer at depth lbe denoted by Hl=hl\n0hl\n1...hl\nm. The output of\nthe layer can be expressed as\nHl+1= Layer( Hl) (3.19)\nIn pre\ufb01x \ufb01ne-tuning, we extend the sequence hl\n0hl\n1...hl\nmby adding a few vectors at beginning,\nwhich we denote as pl\n0pl\n1...pl\nn. Hence Hlcan be written in the form\nHl=pl\n0pl\n1...pl\nn\ued19\ued18\ued17\ued1a\ntrainablehl\n0hl\n1...hl\nm\ued19\ued18\ued17\ued1a\nprevious layer output(3.20)\n\n--- Page 154 ---\n\n3.3 Learning to Prompt 147\nThe output of the layer is the last m+ 1representations.\nHl+1= Layer( Hl)[\u2212m\u22121 :]\n=hl+1\n0hl+1\n1...hl+1\nm (3.21)\nwhere [\u2212m\u22121 :]denotes the slicing operation that extracts the last m+1elements of a sequence.\nGiven Hl+1, the input of the next layer can be expressed in the same form o f Eq. ( 3.20):\nHl+1=pl+1\n0pl+1\n1...pl+1\nnHl+1\n=pl+1\n0pl+1\n1...pl+1\nnhl+1\n0hl+1\n1...hl+1\nm (3.22)\nHere each pi\u2208Rdcan be seen as a learnable parameter. During training, pl\n0pl\n1...pl\nnare trained\nas usual, and the parameters of the original Transformer mod el are kept \ufb01xed.\nFigure 3.5shows an illustration of pre\ufb01x \ufb01ne-tuning for a translation task. Here, only the pre\ufb01x\nvectors pl\n0andpl\n1are updated by receiving the error gradients from the output (i.e., the Chinese\ntranslation). By adjusting these vectors for the translati on task, the model adapts accordingly. This\nmakes pl\n0andpl\n1serve as prompts which activate the LLM to perform the task wi thout needing\nexplicit input prompts like \u201cTranslate the following sente nce from English to Chinese\u201d. At test\ntime, we prepend the optimized pl\n0andpl\n1to the layer, and the LLM will then translate the input",
    "00966679a49d78ca": " each pi\u2208Rdcan be seen as a learnable parameter. During training, pl\n0pl\n1...pl\nnare trained\nas usual, and the parameters of the original Transformer mod el are kept \ufb01xed.\nFigure 3.5shows an illustration of pre\ufb01x \ufb01ne-tuning for a translation task. Here, only the pre\ufb01x\nvectors pl\n0andpl\n1are updated by receiving the error gradients from the output (i.e., the Chinese\ntranslation). By adjusting these vectors for the translati on task, the model adapts accordingly. This\nmakes pl\n0andpl\n1serve as prompts which activate the LLM to perform the task wi thout needing\nexplicit input prompts like \u201cTranslate the following sente nce from English to Chinese\u201d. At test\ntime, we prepend the optimized pl\n0andpl\n1to the layer, and the LLM will then translate the input\nsentence. Note that pre\ufb01x \ufb01ne-tuning introduces additiona lL\u00d7n\u00d7dparameters, where Lis the\nnumber of layers, nis the number of pre\ufb01xes, and dis the dimensionality of each pre\ufb01x. However,\nthis number is much smaller compared to the total number of pa rameters in the LLM, making the\n\ufb01ne-tuning process highly ef\ufb01cient.\nWhile pre\ufb01x \ufb01ne-tuning is simple, it still requires modi\ufb01ca tions to LLMs. Alternatively, sep-\narating soft prompts from the LLMs allows us to preserve the o riginal model architecture, making\nit more ef\ufb01cient for deployment across different tasks with out the need to adjust the core model.\nOne such method is prompt tuning [ Lester et al. ,2021 ]. Like pre\ufb01x \ufb01ne-tuning, prompt tuning\nincorporates trainable vectors so that LLMs can adapt to giv en tasks by adjusting these vectors.\nHowever, prompt tuning differs in that it modi\ufb01es only the em bedding layer.\nRecall that in LLMs each input token ziis represented by an embedding ei. These embeddings\nare generally learned through a token embedding model and ar e then used as the real inputs to the\nLLMs, replacing the symbolically represented tokens. In pr ompt tuning, a number of pseudo\nembeddings p0...pnare added at the beginning of the token embedding sequence. S o the actual\ninput to the LLMs can be expressed as\np0p1...pn\ued19\ued18\ued17\ued1a\ntrainablee0e1...em\ued19\ued18\ued17\ued1a\ntoken embeddings\nNote that a pseudo embedding needs not to correspond to any to ken in natural language. Instead\nthese embeddings can be seen as \u201csoft prompt embeddings\u201d tha t serve to condition the LLMs.\nBy training soft prompt embeddings on task-speci\ufb01c data, th ey learn to interact adaptively with\nthe token embeddings e0...emand guide the behavior of LLMs. Since prompt tuning does not\nchange the underlying parameters of pre-trained LLMs, it is considered a lightweight and ef\ufb01cient\nmethod of \ufb01ne-tuning, improving task-speci\ufb01c performance while maintaining their generalization\ncapabilities. See Figure 3.6for an illustration of prompt tuning.\n\n--- Page 155 ---\n\n148 Prompting\npl\n0 pl\n1 hl\n0 hl\n1 hl\n2 hl\n3 hl\n4Layer l\npl\u22121\n0 pl\u22121\n1 hl\u22121\n0 hl\u22121\n1 hl\u22121\n3 hl\u22121\n4 hl\u22121\n5Layer l\u22121pl+1\n0 pl+1\n1 hl+1\n0 hl+1\n1 hl+1\n3 hl+1\n4 hl+1\n5Layer l+ 1\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7Loss Loss\n\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7\nLook out !\u5c0f\u5fc3 !trainable pre\ufb01xes\nUser Input LLM Prediction Soft Prompt\nFig. 3.5: Illustration of pre\ufb01x \ufb01ne-tuning for a translation task (Lo ok out! \u2192\u5c0f\u5fc3!). For each layer, we add two\npre\ufb01xes pl\n0andpl\n1at the beginning. The LLM is trained to minimize the loss on th e predictions given the input.\nDuring this process, only the pre\ufb01xes are optimized while th e rest of the parameters remain \ufb01xed. Therefore, the model\ncan adapt to the given task in a very ef\ufb01cient manner",
    "d0b6b4fe5f4d6ddd": " hl+1\n0 hl+1\n1 hl+1\n3 hl+1\n4 hl+1\n5Layer l+ 1\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7Loss Loss\n\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7\nLook out !\u5c0f\u5fc3 !trainable pre\ufb01xes\nUser Input LLM Prediction Soft Prompt\nFig. 3.5: Illustration of pre\ufb01x \ufb01ne-tuning for a translation task (Lo ok out! \u2192\u5c0f\u5fc3!). For each layer, we add two\npre\ufb01xes pl\n0andpl\n1at the beginning. The LLM is trained to minimize the loss on th e predictions given the input.\nDuring this process, only the pre\ufb01xes are optimized while th e rest of the parameters remain \ufb01xed. Therefore, the model\ncan adapt to the given task in a very ef\ufb01cient manner. At infer ence time, the LLM works with optimized pre\ufb01xes, and\ncan perform the task without the need of explicit hard prompt s.\nSince p0p1...pnis itself a sequence, we can employ sequence models to better represent\nit. For example, a Transformer model can encode this sequenc e, and the resulting representation\ncan then be used as the input to the LLM. In other words, we can d evelop an additional model\nfor encoding soft prompts. Another way to improve prompting is by combining soft and hard\nprompts, thereby taking advantage of both types [ Liu et al. ,2023b ]. In the embedding sequence,\nwe can arrange or intersperse these prompts. This would resu lt in different prompt patterns. For\nexample, a simple pattern that uses both two types of prompt i s\np0p1\u00b7 \u00b7 \u00b7 pnq0q1\u00b7 \u00b7 \u00b7 qm\u2032e0 e1\u00b7 \u00b7 \u00b7 em\nc0c1\u00b7 \u00b7 \u00b7cm\u2032z0z1\u00b7 \u00b7 \u00b7zmSoft Prompt Hard Prompt User Input and Response\nwherec0...cm\u2032denotes the hard prompt and q0...qm\u2032denotes the corresponding embedding se-\nquence.\nHere we have considered methods for inserting soft prompts i n LLMs. But we skip the details\nof training these soft prompts and assume that the reader is f amiliar with the standard supervised\nlearning process, that is, maximizing the likelihood of the correct model output given the model\n\n--- Page 156 ---\n\n3.3 Learning to Prompt 149\nLayerl\u22121LayerlLayerl+ 1\n\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7\np0 p1 e0 e1 e2 e3 e4\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7Loss Loss\nLook out !\u5c0f\u5fc3 !trainable prompt\nembeddings\nUser Input LLM Prediction Soft Prompt\nFig. 3.6: Illustration of prompt tuning for a translation task (Look o ut!\u2192\u5c0f\u5fc3!). Instead of using \ufb01xed textual\nprompts, soft prompts are learnable embeddings that are add ed at the beginning of the embedding sequence. During\n\ufb01ne-tuning, only these prompt embeddings are optimized to e f\ufb01ciently adapt the LLM to the given task. Once opti-\nmized, the prompt embeddings are used to instruct the LLM to p erform the task as new data arrives.\ninput. In fact, learning soft prompts can be related to many i ssues in LLM \ufb01ne-tuning. For exam-\nple, if we consider it as a context compression problem, we ca n apply the knowledge distillation\nmethods described previously. In Mu et al. [2024 ]\u2019s work, prompts are compressed and repre-\nsented as a few pseudo tokens, which are appended to each inpu t sequence. The embeddings of\nthese pseudo tokens are optimized to mimic the predictions o f a standard-prompted model. In\nother words, the prompting knowledge is distilled from a tea cher model into the pseudo tokens.\nBroadly speaking, many parameter-ef\ufb01cient \ufb01ne-tuning met hods can be thought of as learning\nsome sort of soft prompt [ Lialin et al. ,2023 ]. When we \ufb01ne-tune a part of an LLM for a task, this\nprocess can essentially be seen as injecting task-related p rompting information into that speci\ufb01c\npart of the model. Another widely-used approach to paramete r-ef\ufb01cient \ufb01ne-tuning is to add an\nadaptor layer between the existing model layers. This appro ach allows us to \ufb01ne-tune only the\n",
    "b41eab3f2f73a7e3": " are compressed and repre-\nsented as a few pseudo tokens, which are appended to each inpu t sequence. The embeddings of\nthese pseudo tokens are optimized to mimic the predictions o f a standard-prompted model. In\nother words, the prompting knowledge is distilled from a tea cher model into the pseudo tokens.\nBroadly speaking, many parameter-ef\ufb01cient \ufb01ne-tuning met hods can be thought of as learning\nsome sort of soft prompt [ Lialin et al. ,2023 ]. When we \ufb01ne-tune a part of an LLM for a task, this\nprocess can essentially be seen as injecting task-related p rompting information into that speci\ufb01c\npart of the model. Another widely-used approach to paramete r-ef\ufb01cient \ufb01ne-tuning is to add an\nadaptor layer between the existing model layers. This appro ach allows us to \ufb01ne-tune only the\nadaptor layer on speci\ufb01c tasks without altering the underly ing architecture or retraining the entire\nmodel. In this sense, adaptor layers can be viewed as soft pro mpts that encode prompting and task-\nrelated information and interact with the original LLM to he lp it adapt. To summarize, Figure 3.7\nshows a comparison of different methods of using soft prompt s in LLMs.\n3.3.2.3 Learning Soft Prompts with Compression\nAnother approach to learning soft prompts is from the perspe ctive of compression. As a simple\nexample, consider the problem of approximating a long conte xt using a continuous representation\n[Wingate et al. ,2022 ]. Suppose we have a user input zand its context c(such as long instructions\nand demonstrations). Now we want to develop a compressed rep resentation of the context, denoted\n\n--- Page 157 ---\n\n150 Prompting\nLLM\n(a) Soft Prompts as Pre\ufb01xesLLM\n(b) Soft Prompts as Inputs (Embeddings)\nLLMLayer\n(c) Fine-tuning Parts of the ModelLLM\nAdaptor\n(d) Fine-tuning the Adaptor\nFig. 3.7: Illustrations of using soft prompts in LLMs. Here tunable so ft prompts are shown in blue, and components\nwhose parameters are \ufb01xed during \ufb01ne-tuning are shown in gra y. In sub-\ufb01gure (a), soft prompts are pre\ufb01xes appended\nto each layer of the LLM. In sub-\ufb01gure (b), soft prompts are us ed as input embeddings for the LLM. In sub-\ufb01gures (c)\nand (d), soft prompts are broadly treated as components of th e model that are \ufb01ne-tuned for task adaptation.\nby\u03c3, such that the prediction based on zand\u03c3is as close as possible to the prediction based on z\nandc. This goal can be expressed in the form\n\u02c6\u03c3= arg min\n\u03c3s(\u02c6y,\u02c6y\u03c3) (3.23)\nwhere \u02c6y= arg maxyPr(y|c,z)and\u02c6y\u03c3= arg maxy\u03c3Pr(y|\u03c3,z)are the LLM predictions given\nthe full context and the compressed context, respectively. The function s(\u00b7,\u00b7)typically represents\na loss or similarity measure, aiming to minimize the differe nce in predictions between the two\ncontext representations.\nOne general framework for achieving this is knowledge disti llation, where \u02c6yand\u02c6y\u03c3can be\nseen as the predictions of the teacher model and the student m odel, respectively. This formal-\nization links our discussion to the context distillation pr oblem discussed earlier. The training\nobjective can be obtained by analogy with Eqs. ( 3.15) and ( 3.16). For example, a simple training\nobjective is given by\n\u02c6\u03c3= arg max\n\u03c3log Pr(\u02c6 y|\u03c3,z) (3.24)\nAlternatively, we can minimize the KL divergence between th e output distributions, giving\n\u02c6\u03c3= arg min\n\u03c3KL(Pr( \u00b7|c,z)||Pr(\u00b7|\u03c3,z)) (3.25)\nThe difference with the models in Eqs. ( 3.15) and ( 3.16) is that here the compressed context is\nrepresented as real-valued vectors (call them prompt embeddings ), rather than as normal tokens.\nBy applying the above methods, we distill the context from th e token sequence cinto the embed-\ndings\u03c3. Note that the teacher model Pr(\u00b7|c,z)and the student model Pr(\u00b7|\u03c3,z)may not share\nthe same architecture or model settings. In",
    "2fd6e4e168228288": " ( 3.15) and ( 3.16). For example, a simple training\nobjective is given by\n\u02c6\u03c3= arg max\n\u03c3log Pr(\u02c6 y|\u03c3,z) (3.24)\nAlternatively, we can minimize the KL divergence between th e output distributions, giving\n\u02c6\u03c3= arg min\n\u03c3KL(Pr( \u00b7|c,z)||Pr(\u00b7|\u03c3,z)) (3.25)\nThe difference with the models in Eqs. ( 3.15) and ( 3.16) is that here the compressed context is\nrepresented as real-valued vectors (call them prompt embeddings ), rather than as normal tokens.\nBy applying the above methods, we distill the context from th e token sequence cinto the embed-\ndings\u03c3. Note that the teacher model Pr(\u00b7|c,z)and the student model Pr(\u00b7|\u03c3,z)may not share\nthe same architecture or model settings. In practice, we gen erally wish for the teacher model to be\n\n--- Page 158 ---\n\n3.3 Learning to Prompt 151\n\u03c3<i\n1\u03c3<i\n2 ei\n1 ei\n2 ei\n3 ei\n4 \u27e8e1\u27e9 \u27e8e2\u27e9\nzi\n1zi\n2zi\n3zi\n4 \u27e8g1\u27e9 \u27e8 g2\u27e9Transformer Layersh<i\n1 h<i\n1 h1 h2 h3 h4 \u03c3<i+1\n1\u03c3<i+1\n2\nSoft Prompts\nat Stepi\u22121Soft Prompts\nat the Current Step\nFig. 3.8: Illustration of compressing a context segment into soft pro mpts (\u03ba= 2 andmi= 4). The input to the\nLLM includes the soft prompts from the previous step ( \u03c3<i\n1and\u03c3<i\n2), the tokens of the segment ( z1,z2,z3,andz4),\nand the summary tokens ( \u27e8g1\u27e9and\u27e8g2\u27e9). Given these, the LLM operates as usual. We then extract the outputs at the\nlast Transformer layer that correspond to the summary token s. These outputs can be viewed as the soft prompts that\naccumulated up to this segment.\nstronger, while the student model should be smaller and more ef\ufb01cient.\nWhile compressing full context into continuous representa tions is a straightforward approach\nto learning soft prompts, it requires a teacher model that ca n deal with long input sequences. In\nmany cases, however, the context is so long that applying an L LM is too costly or infeasible.\nModeling long input sequences can fall under the broad famil y of ef\ufb01cient methods for long-\ncontext LLMs. Many techniques have been developed to addres s this issue. For example, one can\nuse a \ufb01xed-size KV cache to store the past information at each step during inference. Ef\ufb01cient\nTransformer architectures and long-context LLMs have been intensively discussed in this book.\nFor more detailed discussions of these topics, interested r eaders can refer to Chapter 2.\nThere are also methods speci\ufb01cally designed to compress lon g context into soft prompts. Here\nwe consider Chevalier et al. [2023 ]\u2019s method as an example. The basic idea is that we learn\nsoft prompts gradually by accumulating the \ufb01xed-size conte xt representation over the context\nsequence. Given a long context, we \ufb01rst divide it into a numbe r of segments z1,...,zK. We\nthen process these segments in sequence, each time generati ng a representation of the context we\nhave processed so far, denoted by \u03c3<i+1. To do this, a few summary tokens \u27e8g1\u27e9,...,\u27e8g\u03ba\u27e9are\nintroduced. At each step, we take a segment zi=zi\n1...zi\nmi, along with the previous context rep-\nresentation\u03c3<iand the summary tokens \u27e8g1\u27e9,...,\u27e8g\u03ba\u27e9as input, and use an LLM to produce the\ncorresponding hidden representation sequence at the last T ransformer layer. An example of this\nprocess is illustrated in Figure 3.8.\nHere\u03c3<iis essentially a memory. The model operates in an RNN fashion . Each time we take\na segment and update this memory by encoding both the previou s memory state and the segment.\nTherefore, the \u03c3<iproduced at the last segment is a representation of the entir e context sequence.\nThe Transformer model for learning these representations c an be a standard LLM but we need to\n\ufb01ne-tune it to adapt to this context representation task.\nNote that here we simply consider prompt andcontext as similar terms, even though they are\nnot the same. Although we are",
    "fd960eeb7b9b9927": " a segment zi=zi\n1...zi\nmi, along with the previous context rep-\nresentation\u03c3<iand the summary tokens \u27e8g1\u27e9,...,\u27e8g\u03ba\u27e9as input, and use an LLM to produce the\ncorresponding hidden representation sequence at the last T ransformer layer. An example of this\nprocess is illustrated in Figure 3.8.\nHere\u03c3<iis essentially a memory. The model operates in an RNN fashion . Each time we take\na segment and update this memory by encoding both the previou s memory state and the segment.\nTherefore, the \u03c3<iproduced at the last segment is a representation of the entir e context sequence.\nThe Transformer model for learning these representations c an be a standard LLM but we need to\n\ufb01ne-tune it to adapt to this context representation task.\nNote that here we simply consider prompt andcontext as similar terms, even though they are\nnot the same. Although we are somewhat \u201cmisusing\u201d the concep tprompt , we can often view it as\na type of context. From this perspective, the methods discus sed here can be applied to general text\ncompression problems.\n\n--- Page 159 ---\n\n152 Prompting\n3.3.3 Prompt Length Reduction\nWhile soft prompts provide dense, hidden representations, they are not directly interpretable. The\nlack of interpretability can be a signi\ufb01cant barrier for use rs trying to understand how their inputs\nin\ufb02uence LLM outputs. Moreover, although soft prompts are e f\ufb01cient for \ufb01ne-tuning and de-\nployment, they are in\ufb02exible and do not allow for easy adjust ments without extensive \ufb01ne-tuning\nor modi\ufb01cation. This in\ufb02exibility can limit their utility i n dynamic environments where prompt\nchanges are frequently needed.\nOne alternative way to develop ef\ufb01cient prompts is to simpli fy the text used for prompting.\nFor example, below is a prompt for answering questions on hea lthcare and \ufb01nance.\nThe task involves developing a language model capable of und erstanding and\nresponding to user inquiries across various domains, with a particular emphasis\non healthcare and \ufb01nance. Considering the broad range of pot ential queries,\nfrom the speci\ufb01cs of medical diagnoses to the nuances of \ufb01nan cial regulations,\nthe model must ensure a comprehensive understanding and acc urate responses.\nQuestion:\nWhat are the best practices for using arti\ufb01cial intelligenc e in diagnosing cardio-\nvascular diseases?\nWe can simplify the task description by deleting the unimpor tant parts.\nThe task involves developing a language model capable of und erstanding\nand responding to user inquiries across various domains, with aparticular\nemphasis on healthcare and \ufb01nance. Considering thebroad range ofpotential\nqueries, from thespeci\ufb01cs ofmedical diagnoses tothenuances of\ufb01nancial\nregulations, The model must ensure a comprehensive understanding and ac-\ncurate responses.\nWe can also paraphrase it as a shorter text.\nThe task involves developing a language model focused on hea lthcare and\n\ufb01nance, capable of understanding and accurately respondin g to a wide range\nof user inquiries.\nThis problem can be viewed as a classic NLP issue \u2014 text simpli \ufb01cation. So the methods\nused can be general and not restricted to the problem of simpl ifying prompts. There are many\nways to achieve this. One simple method is to de\ufb01ne some heuri stics and identify redundant\nwords that can be eliminated without losing essential infor mation. For example, we can examine\neach token in a sequence in terms of its contribution to the ov erall meaning and remove those that\nprovide minimal value [ Li et al. ,2023c ;Jiang et al. ,2023b ]. Another method involves framing\nthe problem as a sequence-to-sequence task. With labeled da ta for text simpli\ufb01cation, we can\ntrain an encoder-decoder model to transform each input text into its simpli\ufb01ed form. In addition,\n\n--- Page 160 ---\n\n3.4 Summary 153\ngiven that many LLMs have been \ufb01ne-tuned and aligned to perfo rm text simpli\ufb01cation tasks, it is\nstraightforward to use these models to simplify prompts. Fo r example, we can prompt an LLM to\nsimplify a text under certain constraints, such as limiting the length of the simpli\ufb01ed text.\n3.4 Summary\nIn this chapter, we have discussed a variety of issues relate d to LLM prompting. Our discussion\nhas focused mainly on two aspects:\n\u2022 How to design basic prompts to guide the predictions of L",
    "0572eee68659caa5": " ,2023c ;Jiang et al. ,2023b ]. Another method involves framing\nthe problem as a sequence-to-sequence task. With labeled da ta for text simpli\ufb01cation, we can\ntrain an encoder-decoder model to transform each input text into its simpli\ufb01ed form. In addition,\n\n--- Page 160 ---\n\n3.4 Summary 153\ngiven that many LLMs have been \ufb01ne-tuned and aligned to perfo rm text simpli\ufb01cation tasks, it is\nstraightforward to use these models to simplify prompts. Fo r example, we can prompt an LLM to\nsimplify a text under certain constraints, such as limiting the length of the simpli\ufb01ed text.\n3.4 Summary\nIn this chapter, we have discussed a variety of issues relate d to LLM prompting. Our discussion\nhas focused mainly on two aspects:\n\u2022 How to design basic prompts to guide the predictions of LLMs and re\ufb01ne these prompts for\nmore effective and ef\ufb01cient problem-solving?\n\u2022 How to automate the design and representation of prompts?\nSolutions to these issues involve both general prompt desig ns and more advanced techniques, such\nas CoT and prompt learning, which have been explored extensi vely in recent research.\nIn NLP, prompting can be viewed as a technology that has evolv ed along with LLMs, and\nin a sense, it has opened the door to the practical applicatio n of these models in an impressive\nrange of problem domains. In fact, if we expand the concept of prompts to some extent, it can be\ntraced back to the early days of machine learning and NLP. For example, many NLP systems use\nhand-crafted features and templates to \u201cprompt\u201d speci\ufb01c ta sks. Imagine developing a feature to\nindicate whether a text is formal or informal. We can feed thi s feature into a machine translation\nsystem to condition the translation on the type of the input t ext.\nThe widespread use of the modern concept of prompts began wit h the rise of large pre-trained\nmodels in the \ufb01eld of NLP. Initially, these models, such as BE RT, were adapted to speci\ufb01c down-\nstream tasks mainly through \ufb01ne-tuning. However, research ers soon discovered that by designing\nspeci\ufb01c \"prompts\" \u2014 adding certain words or sentences to the input \u2014 the models could be\ntriggered to respond to speci\ufb01c tasks without extensive \ufb01ne -tuning. This motivated the NLP com-\nmunity to develop and apply universal foundation models tha t can be prompted to address various\ntasks without changing the underlying architecture and the pre-training procedure.\nPrompting approaches were \ufb01rst experimented with smaller m odels and later demonstrated\nimpressive capabilities with large models like GPT-3, whic h could generate high-quality text in\nresponse to simple prompts across various tasks. As prompti ng technology evolved, prompt en-\ngineering emerged as a critical area of research. As discuss ed in this chapter, it broadly involves\ndesigning effective prompts to maximize model performance , encompassing both hand-crafted\nand automatically generated prompts. More recent research has explored how to enhance the\neffectiveness of prompting through techniques like few-sh ot learning, zero-shot learning, and\nCoT reasoning, enabling LLMs to work effectively across a wi de range of scenarios. A gen-\neral discussion of prompting can be very broad, and we cannot cover all details in this chapter.\nFor more advanced techniques of prompting, the reader can re fer to recent surveys. Topics in-\nclude in-context learning [ Li,2023 ;Dong et al. ,2022 ], CoT [ Chu et al. ,2023 ;Yu et al. ,2023 ;\nZhang et al. ,2023a ], ef\ufb01cient prompting [ Chang et al. ,2024 ], and general prompt engineering\n[Liu et al. ,2023c ;Chen et al. ,2023a ].\n\n--- Page 161 ---\n\n154 Prompting\nNote that although we would ideally like to develop general p rompting methods without ad-\njusting model architectures and parameters, the results of prompting generally depend heavily\non the quality and size of the given LLMs. For stronger models , such as commercialized online\nLLMs, simple prompts may be suf\ufb01cient to instruct these mode ls to perform tasks correctly. In\nthis case, prompt engineering is relatively easy, though we still need certain efforts to make LLMs\nwork properly. By contrast, if the LLMs are not powerful enou gh, we may need to carefully design\nthe prompts to achieve the desired results. In many cases, \ufb01n e-tuning",
    "c30fa9a5779a4356": " et al. ,2023a ], ef\ufb01cient prompting [ Chang et al. ,2024 ], and general prompt engineering\n[Liu et al. ,2023c ;Chen et al. ,2023a ].\n\n--- Page 161 ---\n\n154 Prompting\nNote that although we would ideally like to develop general p rompting methods without ad-\njusting model architectures and parameters, the results of prompting generally depend heavily\non the quality and size of the given LLMs. For stronger models , such as commercialized online\nLLMs, simple prompts may be suf\ufb01cient to instruct these mode ls to perform tasks correctly. In\nthis case, prompt engineering is relatively easy, though we still need certain efforts to make LLMs\nwork properly. By contrast, if the LLMs are not powerful enou gh, we may need to carefully design\nthe prompts to achieve the desired results. In many cases, \ufb01n e-tuning is still necessary to adapt\nthe models to sophisticated prompting strategies.\n\n--- Page 162 ---\n\nCHAPTER 4\nAlignment\nAlignment is not a new concept in NLP, but its meaning varies a cross different domains and over\ntime. In traditional NLP, the term alignment typically refers to the tasks that link corresponding\nelements in two sets, such as aligning words between a Chines e sentence and an English sentence.\nAs LLMs become increasingly important in NLP research, this term is more broadly used to refer\nto aligning model outputs with human expectations. The prob lem that alignment addresses is\nthat the output of a model may not align with the speci\ufb01c goals or contexts intended by users.\nFor example, pre-trained LLMs may not be able to follow user i nstructions because they were\nnot trained to do so. Another example is that LLMs may generat e harmful content or perpetuate\nbiases inherent in their training data. This poses new chall enges in ensuring that LLM outputs are\nnot only accurate and relevant, but also ethically sound and non-discriminatory.\nSimply pre-training LLMs can result in a variety of alignmen t problems. Our ultimate goal\nis to resolve or mitigate all these problems to ensure LLMs ar e both accurate and safe. There\nis an interesting issue here: since large language models ar e trained on vast amounts of data,\nwe have reason to believe that if we have suf\ufb01cient data cover ing a variety of tasks and aligned\nwith human preferences, pre-training could make LLMs accur ate and safe enough, perhaps even\neliminating the need for alignment. However, the reality is that it is nearly impossible to gather\ndata that encompasses all tasks or adequately represents hu man preferences. This makes it dif\ufb01cult\nto achieve model alignment through pre-training alone, or a t least, at this stage, alignment remains\na very necessary and critical step in the development of LLMs .\nIn this chapter, we will focus on alignment methods for LLMs. We will begin by discussing the\ngeneral alignment tasks. Then we will consider two widely-u sed approaches, known as instruc-\ntion alignment andhuman preference alignment , respectively. The former resorts to supervised\n\ufb01ne-tuning techniques and guides the LLMs to generate outpu ts that adhere closely to user instruc-\ntions. On the other hand, the latter typically relies on rein forcement learning techniques, where\nthe LLMs are trained based on feedback from humans. While the se methods are motivated by\ndifferent goals, they are commonly used together to develop well-aligned LLMs.\n4.1 An Overview of LLM Alignment\nAlignment can be achieved in several different ways. We need different methods for LLM align-\nment because this problem is itself complicated and multifa ceted, requiring a blend of technical\nconsiderations. Here we consider three widely-used approa ches to aligning LLMs.\nThe \ufb01rst approach is to \ufb01ne-tune LLMs with labeled data. This approach is straightforward\nas it simply extends the pre-existing training of a pre-trai ned LLM to adapt it to speci\ufb01c tasks.\nAn example of this is supervised \ufb01ne-tuning (SFT), in which the LLM is further trained on a\ndataset comprising task-speci\ufb01c instructions paired with their expected outputs. The SFT dataset\nis generally much smaller compared to the original training set, but this data is highly specialized.\nThe result of SFT is that the LLM can learn to execute tasks bas ed on user instructions. For\nexample, by \ufb01ne-tuning the LLM with a set of question-answer pairs, the model can respond to\nspeci\ufb01c questions, even if not directly covered in the SFT da taset.",
    "6054ad72af016f5a": " Here we consider three widely-used approa ches to aligning LLMs.\nThe \ufb01rst approach is to \ufb01ne-tune LLMs with labeled data. This approach is straightforward\nas it simply extends the pre-existing training of a pre-trai ned LLM to adapt it to speci\ufb01c tasks.\nAn example of this is supervised \ufb01ne-tuning (SFT), in which the LLM is further trained on a\ndataset comprising task-speci\ufb01c instructions paired with their expected outputs. The SFT dataset\nis generally much smaller compared to the original training set, but this data is highly specialized.\nThe result of SFT is that the LLM can learn to execute tasks bas ed on user instructions. For\nexample, by \ufb01ne-tuning the LLM with a set of question-answer pairs, the model can respond to\nspeci\ufb01c questions, even if not directly covered in the SFT da taset. This method proves particularly\n155\n\n--- Page 163 ---\n\n156 Alignment\nPre-training PromptingInstruction\nAlignment\n(e.g., SFT)Human Preference\nAlignment\n(e.g., RLHF)Pre-training\nStageAlignment\nStage\nTraining & Fine-tuning Inference\nFig. 4.1: Schematic illustration of the pre-train-then-align metho d for developing LLMs. In the pre-training stage, we\ntrain an LLM on vast amounts of data using next token predicti on. Then, in the alignment stage, we align the LLM\nto user instructions, intents, and preferences. This inclu des instruction alignment, human preference alignment, an d\nprompting.\nuseful when it is relatively easy to describe the input-outp ut relationships and straightforward to\nannotate the data.\nThe second approach is to \ufb01ne-tune LLMs using reward models. One dif\ufb01culty in alignment\nis that human values and expectations are complex and hard to describe. In many cases, even for\nhumans themselves, articulating what is ethically correct or culturally appropriate can be challeng-\ning. As a result, collecting or annotating \ufb01ne-tuning data i s not as straightforward as it is with SFT.\nMoreover, aligning LLMs is not just a task of \ufb01tting data, or i n other words, the limited samples\nannotated by humans are often insuf\ufb01cient to comprehensive ly describe these behaviors. What we\nreally need here is to teach the model how to determine which o utputs are more in line with human\npreferences, for example, we not only want the outputs to be t echnically accurate but also to align\nwith human expectations and values. One idea is to develop a r eward model analogous to a human\nexpert. This reward model would work by rewarding the LLM whe never it generates responses\nthat align more closely with human preferences, much like ho w a teacher provides feedback to a\nstudent. To obtain such a reward model, we can train a scoring function from human preference\ndata. The trained reward model is then used as a guide to adjus t and re\ufb01ne the LLM. This frames\nthe LLM alignment task as a reinforcement learning task. The resulting methods, such as rein-\nforcement learning from human feedback (RLHF ), have been demonstrated to be particularly\nsuccessful in adapting LLMs to follow the subtleties of huma n behavior and social norms.\nThe third approach is to perform alignment during inference rather than during training or\n\ufb01ne-tuning. From this perspective, prompting in LLMs can al so be seen as a form of alignment,\nbut it does not involve training or \ufb01ne-funing. So we can dyna mically adapt an LLM to various\ntasks at minimal cost. Another method to do alignment at infe rence time is to rescore the outputs\nof an LLM. For example, we could develop a scoring system to si mulate human feedback on the\noutputs of the LLM (like a reward model) and prioritize those that receive more positive feedback.\nThe three methods mentioned above are typically used in sequ ence once the pre-training is\ncomplete: we \ufb01rst perform SFT, then RLHF, and then prompt the LLM in some way during\ninference. This roughly divides the development of LLMs int o two stages \u2014 the pre-training stage\nand the alignment stage. Figure 4.1shows an illustration of this. Since prompting techniques h ave\nbeen intensively discussed in the previous chapter, we will focus on \ufb01ne-tuning-based alignment\nmethods in the rest of this chapter.\n\n--- Page 164 ---\n\n4.2 Instruction Alignment 157\n4.2 Instruction Alignment\nOne feature of LLMs is that they can follow the",
    "318685fe9a76ae1b": "ce time is to rescore the outputs\nof an LLM. For example, we could develop a scoring system to si mulate human feedback on the\noutputs of the LLM (like a reward model) and prioritize those that receive more positive feedback.\nThe three methods mentioned above are typically used in sequ ence once the pre-training is\ncomplete: we \ufb01rst perform SFT, then RLHF, and then prompt the LLM in some way during\ninference. This roughly divides the development of LLMs int o two stages \u2014 the pre-training stage\nand the alignment stage. Figure 4.1shows an illustration of this. Since prompting techniques h ave\nbeen intensively discussed in the previous chapter, we will focus on \ufb01ne-tuning-based alignment\nmethods in the rest of this chapter.\n\n--- Page 164 ---\n\n4.2 Instruction Alignment 157\n4.2 Instruction Alignment\nOne feature of LLMs is that they can follow the prompts provid ed by users to perform various\ntasks. In many applications, a prompt consists of a simple in struction and user input, and we want\nthe LLM to follow this instruction to perform the task correc tly. This ability of LLMs is also\ncalled the instruction-following ability. For example, be low is a prompt where we want the LLM\nto extract key points and provide a concise summary for a leng thy article.\nInstruction Summarize this text in three sentences.\nInput Daylight Savings Time (DST) - the process of moving clocks fo rward\nby one hour in the summer - was started in Germany in 1916 ...\nOutput\nThis task requires the LLM to understand the instruction \u201cSu mmarize this text in three sentences\u201d\nand perform the summarization accordingly. However, LLMs a re typically trained for next-token\nprediction rather than for generating outputs that follow i nstructions. Applying a pre-trained LLM\nto the above example would likely result in the model continu ing to write the input article instead\nof summarizing the main points. The goal of instruction alig nment (or instruction \ufb01ne-tuning ) is\nto tune the LLM to accurately respond to user instructions an d intentions. The rest of this section\nwill discuss some issues related to instruction alignment, including \ufb01ne-tuning LLMs to follow\ninstructions, generating or collecting instruction data, and generalizing instruction alignment.\n4.2.1 Supervised Fine-tuning\nOne straightforward approach to adapting LLMs to follow ins tructions is to \ufb01ne-tune these models\nusing annotated input-output pairs [ Ouyang et al. ,2022 ;Wei et al. ,2022a ]. Unlike standard lan-\nguage model training, here we do not wish to maximize the prob ability of generating a complete\nsequence, but rather maximizing the probability of generat ing the rest of the sequence given its\npre\ufb01x. This approach makes instruction \ufb01ne-tuning a bit dif ferent from pre-training. The SFT\ndata is a collection of such input-output pairs (denoted by S), where each output is the correct\nresponse for the corresponding input instruction. For exam ple, below is an SFT dataset\nx(instruction + user input) y(output)\nSummarize the following article. {\u2217summary \u2217}\nArticle: In recent years, solar energy has seen\nunprecedented growth, becoming the fastest-growing ...\nExtract the main \ufb01nancial \ufb01gures from the following Revenue: $10 million,\nearnings report. Pro\ufb01t Margin: 15%\nReport: The company reported a revenue of $10 million\nin the \ufb01rst quarter with a pro\ufb01t margin of 15% ...\nClassify the following email as spam or not spam. Spam\nText: Congratulations! You\u2019ve won a $500 gift card.\nClick here to claim now.\nProvide a solution to the following technical issue. First, check for ...\nIssue: my computer is running slow and often freezes.\n\n--- Page 165 ---\n\n158 Alignment\nwhere the instructions are highlighted. This dataset conta ins instructions and the corresponding\noutputs for several different NLP problems, and so we can \ufb01ne -tune an LLM to handle multiple\ntasks simultaneously.\nLetx=x0...xmbe an input sequence (e.g., instruction + user input) and y=y1...ynbe the\ncorresponding output sequence. In SFT, we aim to maximize th e probability of the output ygiven\nthe input x. Consider an LLM with pre-trained parameters \u02c6\u03b8. The \ufb01ne-tuning objective can then\nbe formulated as:\n\u02dc\u03b8= arg max\n\u02c6\u03b8+\u2211\n(x,y)\u2208Dlog Pr \u02c6\u03b8+(y|x) (4",
    "b96222c82af990bc": " gift card.\nClick here to claim now.\nProvide a solution to the following technical issue. First, check for ...\nIssue: my computer is running slow and often freezes.\n\n--- Page 165 ---\n\n158 Alignment\nwhere the instructions are highlighted. This dataset conta ins instructions and the corresponding\noutputs for several different NLP problems, and so we can \ufb01ne -tune an LLM to handle multiple\ntasks simultaneously.\nLetx=x0...xmbe an input sequence (e.g., instruction + user input) and y=y1...ynbe the\ncorresponding output sequence. In SFT, we aim to maximize th e probability of the output ygiven\nthe input x. Consider an LLM with pre-trained parameters \u02c6\u03b8. The \ufb01ne-tuning objective can then\nbe formulated as:\n\u02dc\u03b8= arg max\n\u02c6\u03b8+\u2211\n(x,y)\u2208Dlog Pr \u02c6\u03b8+(y|x) (4.1)\nwhere \u02dc\u03b8denotes the parameters optimized via \ufb01ne-tuning, and \u02c6\u03b8+represents an adjustment to \u02c6\u03b8.\nHere we will omit the superscript +and use\u03b8to represent \u02c6\u03b8+to keep the notation uncluttered. But\nthe reader should keep in mind that the \ufb01ne-tuning starts fro m the pre-trained parameters rather\nthan randomly initialized parameters.\nThe objective function log Pr \u03b8(yi|x,y<i)is computed by summing the log-probabilities of\nthe tokens in y, conditional on the input xand all the previous tokens y<i:\nlog Pr \u03b8(y|x) =n\u2211\ni=1log Pr \u03b8(yi|x,y<i) (4.2)\nThis formulation is equivalent to minimizing the cross-ent ropy loss.\nNote that minimizing the conditional log-probability log Pr \u03b8(y|x)is not a standard language\nmodel training problem. If we concatenate xandyas a single sequence, a more general form of\nlanguage modeling is based on the joint log-probability log Pr \u03b8(x,y), that is, we minimize the\nloss over all tokens of the sequence seqx,y= [x,y]. We can write the probability of this sequence\nusing the chain rule\nlog Pr \u03b8(seqx,y) = log Pr \u03b8(x,y)\n= log Pr \u03b8(x)\ued19\ued18\ued17\ued1a\nset to 0+ log Pr \u03b8(y|x)\ued19\ued18\ued17\ued1a\nloss computation(4.3)\nThere are two terms on the right-hand side of the equation. We can simply set the \ufb01rst term\nlog Pr \u03b8(x)to 0, focusing solely on the second term log Pr \u03b8(y|x)for loss computation. As a\nresult, the training can be implemented using standard LLMs . For the sequence seqx,y, we \ufb01rst\nrun the forward pass as usual. Then, during the backward pass , we force the loss corresponding\ntoxto be zero. Figure 4.2shows an illustration of this process.\nBy taking log Pr \u03b8(seqx,y)as the objective function, we can describe SFT using a regula r form\nof language model training:\n\u02dc\u03b8= arg max\n\u03b8\u2211\n(x,y)\u2208Dlog Pr \u03b8(seqx,y) (4.4)\nThe problem we considered above is fundamentally a single-round prediction problem, where\nthe LLM generates a response based on a single input without a ny further interaction or feedback\nfrom the user. The input is processed, and the output is gener ated in one go. This is typical in\n\n--- Page 166 ---\n\n4.2 Instruction Alignment 159\nx0x1x2x3y1x1x2x3y1y2Input Output\n(a) Forward Passx0x1x2x3y1x1x2x3y1y2Loss = 0 Loss \u0338= 0\n(b) Backward Pass\nFig. 4.2: Illustration of supervised \ufb01ne-tuning for LLMs. We concate nate the input and the output into a single se-\nquence. During the forward pass, we run the LLM as usual. Duri ng the backward pass, we compute the loss only for\nthe output part and simply set the loss for the input part to 0.\nscenarios where a single question is asked, and a single answ er is provided, with no follow-up\nquestions or clari\ufb01cations. However, in practice, we somet imes have to handle multi-round pre-\ndiction problems, for example, an LLM engages",
    "426b6f525963971c": "x3y1x1x2x3y1y2Input Output\n(a) Forward Passx0x1x2x3y1x1x2x3y1y2Loss = 0 Loss \u0338= 0\n(b) Backward Pass\nFig. 4.2: Illustration of supervised \ufb01ne-tuning for LLMs. We concate nate the input and the output into a single se-\nquence. During the forward pass, we run the LLM as usual. Duri ng the backward pass, we compute the loss only for\nthe output part and simply set the loss for the input part to 0.\nscenarios where a single question is asked, and a single answ er is provided, with no follow-up\nquestions or clari\ufb01cations. However, in practice, we somet imes have to handle multi-round pre-\ndiction problems, for example, an LLM engages in a dialogue o ver multiple turns. In this setting,\nthe LLM not only generates responses based on the initial inp ut but also incorporates subsequent\ninputs that might re\ufb01ne or expand on earlier interactions. F or example, we can use the LLM to act\nas a healthcare assistance chatbot and have a conversation w ith the user, like this\nUser I\u2019ve been feeling very tired lately.\nChatbot I\u2019msorry tohear that. Besides feeling tired, have younoticed any\nother symptoms?\nUser Yes, I\u2019m also experiencing headaches frequently.\nChatbot How long have these symptoms been going on?\nUser About a week now.\nChatbot Itmight begood tocheck inwith ahealthcare professional. Would\nyoulikehelp setting upanappointment?\nUser Yes, please. Can it be after work hours?\nChatbot Sure, Icanarrange that. There areslots available next Wednesday\nandThursday after 5PM. Which dayworks better foryou?\n...\nIn this task, there are several rounds of conversation, each involving the generation of a re-\nsponse based on the user\u2019s request or question and the conver sational history. Suppose we have\nKrounds of conversation, denoted by {x1,y1,x2,y2,...,xK,yK}. Here xkandykdenote the\nuser request and the response, respectively, for each round k. The log-probability of generating\nthe response can be written as log Pr \u03b8(yk|x1,y1,...,xk). Our goal is then to maximize the sum\nof these log-probabilities\n\u02dc\u03b8= arg max\n\u03b8K\u2211\nk=1log Pr \u03b8(yk|x1,y1,...,xk) (4.5)\n\n--- Page 167 ---\n\n160 Alignment\nA straightforward implementation of this involves calcula ting the conditional probability for\neachk. However, it requires running the LLM Ktimes, each time with an increased conversa-\ntional history to make predictions. A more ef\ufb01cient method i s to perform loss computation of all\nresponses in a single run of the LLM. To do this, we represent t he conversation as a sequence\nseqx1,y1,...,xK,yK= [x1,y1,...,xK,yK](orseqfor short). The log-probability of this sequence\nis given by\nlog Pr \u03b8(seq) = log Pr \u03b8(x1,y1,...,xK,yK)\n= log Pr \u03b8(x1)\ued19\ued18\ued17\ued1a\nset to 0+ log Pr \u03b8(y1|x1)\ued19\ued18\ued17\ued1a\nloss computation+\u00b7 \u00b7 \u00b7+\nlog Pr \u03b8(xK|x1,y1,...,yK\u22121)\ued19 \ued18\ued17 \ued1a\nset to 0+\nlog Pr \u03b8(yK|x1,y1,...,xK)\ued19\ued18\ued17 \ued1a\nloss computation(4.6)\nThe trick here is that we ignore the loss for generating user i nputs, as illustrated in Figure 4.3.\nHence we only compute the probabilities of generating the re sponses given their conversational\nhistories, in other words, the value on the right-hand side o f Eq. ( 4.6) is actually equal to the value\non the right-hand side of Eq. ( 4.5). As with Eq. ( 4.4), the training of this multi-round prediction\nmodel can be achieved by maximizing the log likelihood over a training dataset D:\n\u02dc\u03b8= arg max\n\u03b8\u2211\nseq\u2208Dlog Pr \u03b8(seq) (4.7)\nWhile",
    "16f5880119b957bf": "\ued18\ued17 \ued1a\nset to 0+\nlog Pr \u03b8(yK|x1,y1,...,xK)\ued19\ued18\ued17 \ued1a\nloss computation(4.6)\nThe trick here is that we ignore the loss for generating user i nputs, as illustrated in Figure 4.3.\nHence we only compute the probabilities of generating the re sponses given their conversational\nhistories, in other words, the value on the right-hand side o f Eq. ( 4.6) is actually equal to the value\non the right-hand side of Eq. ( 4.5). As with Eq. ( 4.4), the training of this multi-round prediction\nmodel can be achieved by maximizing the log likelihood over a training dataset D:\n\u02dc\u03b8= arg max\n\u03b8\u2211\nseq\u2208Dlog Pr \u03b8(seq) (4.7)\nWhile implementing the SFT methods introduced above seems t rivial as they are fundamen-\ntally the same as regular language model training, there are still issues that need to be considered\nin practice. For example,\n\u2022 SFT requires labeled data. This makes SFT quite different f rom pre-training, where raw text\nis used as training data and is readily available. As in other supervised machine learning\nproblems, data annotation and selection in SFT are not simpl e tasks. In general, we wish\nto develop SFT data that is both substantial in quantity and h igh in quality, and this data\nshould be highly relevant to the tasks the LLM will perform. O n the other hand, there is\na need to \ufb01ne-tune LLMs with less data to minimize computatio nal and data construction\ncosts. Often, the quality of LLMs is highly dependent on the d ata used in SFT. Thus, such\ndata must be carefully developed and examined. As we will see in later subsections, SFT\ncan be more ef\ufb01cient and effective through more advanced tec hniques for data construction.\n\u2022 SFT is still computationally expensive for LLMs due to thei r big size. As a result, main-\ntaining and updating such models is resource-intensive. Fo r example, applying gradient up-\ndates to billions of parameters within an LLM requires signi \ufb01cant computational power and\nmemory. This often requires high-performance computing en vironments, which are costly\nto operate. To address these challenges, various optimizat ion strategies, such as pruning,\nquantization, and the use of more ef\ufb01cient training algorit hms, have been explored. In par-\nticular, there has been signi\ufb01cant interest in parameter-e f\ufb01cient \ufb01ne-tuning methods which\nare designed to maintain state-of-the-art performance wit hout the need for extensive compu-\ntational resources. We have seen in Chapter 3 that applying t echniques like soft prompts can\n\n--- Page 168 ---\n\n4.2 Instruction Alignment 161\nUser: I\u2019ve been feeling very tired lately.\nChatbot: I\u2019m sorry to hear that. Besides feeling tired,\nhave you noticed any other symptoms?\nUser: Yes, I\u2019m also experiencing headaches frequently.\nChatbot: How long have these symptoms been going on?\n...x1 y1x2 y2 \u00b7 \u00b7 \u00b7Pr\u03b8(x1) Pr\u03b8(y1|x1) Pr\u03b8(x2|x1,y1) Pr\u03b8(y2|x1,y1,x2)Loss = 0 Loss \u0338= 0 Loss = 0 Loss \u0338= 0\nFig. 4.3: Illustration of supervised \ufb01ne-tuning for conversational models. Here the LLM acts as a chatbot to respond to\neach request based on the conversational history. The conve rsation progresses by alternating between the user and the\nchatbot. In SFT, we treat the entire conversation as a sequen ce, just like in standard LLMs, but compute the loss only\nfor the responses of the LLM.\nmake the \ufb01ne-tuning process more ef\ufb01cient. For further disc ussion on parameter-ef\ufb01cient\nmethods, the reader can refer to related papers on this issue [Houlsby et al. ,2019 ;Hu et al. ,\n2022 ;Han et al. ,2024 ].\n\u2022 SFT can be regarded as a post-training step following pre-t raining. It is a separate training\nphase designed to preserve the advantages of the initial pre -training while incorporating new\nadjustments. This may seem paradoxical because updating a p re-trained LLM with further\ndata potentially causes the model to forget some of its prior knowledge. Imagine",
    "fb6c68f16c8c9b55": " on the conversational history. The conve rsation progresses by alternating between the user and the\nchatbot. In SFT, we treat the entire conversation as a sequen ce, just like in standard LLMs, but compute the loss only\nfor the responses of the LLM.\nmake the \ufb01ne-tuning process more ef\ufb01cient. For further disc ussion on parameter-ef\ufb01cient\nmethods, the reader can refer to related papers on this issue [Houlsby et al. ,2019 ;Hu et al. ,\n2022 ;Han et al. ,2024 ].\n\u2022 SFT can be regarded as a post-training step following pre-t raining. It is a separate training\nphase designed to preserve the advantages of the initial pre -training while incorporating new\nadjustments. This may seem paradoxical because updating a p re-trained LLM with further\ndata potentially causes the model to forget some of its prior knowledge. Imagine a scenario\nwhere we have a large amount of SFT data and extensively \ufb01ne-t une the LLM. In this\ncase, the LLM could over\ufb01t the data, which in turn may reduce g eneralization performance\nor cause catastrophic forgetting. A common strategy to miti gate this issue is to employ\nregularization and early stopping techniques. Another pra ctical approach is to use a smaller\nlearning rate to gently adjust the weights of the LLM. In addi tion, \ufb01ne-tuning with data from\ndiverse sources and problem domains can also be bene\ufb01cial. N evertheless, in practice, the\nSFT step is often carefully examined and requires substanti al engineering and experimental\nefforts to optimize.\n4.2.2 Fine-tuning Data Acquisition\nFine-tuning data is so important that much recent work in LLM has focused on developing various\ndatasets for instruction \ufb01ne-tuning. As with most work in ma chine learning, there are generally\ntwo approaches to data acquisition \u2014 manual data generation and automatic data generation.\n\n--- Page 169 ---\n\n162 Alignment\n4.2.2.1 Manually Generated Data\nOne straightforward method is to recruit human annotators t o create input-output pairs for the\ntasks of interest. Unlike data annotation in conventional N LP, such as text classi\ufb01cation, where\nannotators simply assign labels to collected texts accordi ng to guidelines, creating \ufb01ne-tuning data\nfor LLMs requires more steps and effort, making it thus more c hallenging. Suppose we want to\nobtain \ufb01ne-tuning data for the English-to-Chinese machine translation task. The \ufb01rst step is to\nwrite a prompt template to describe the task and format the pr oblem clearly. For example,\nInstruction Translate the text from English to Chinese.\nUser Input {\u2217text\u2217}\nOutput {\u2217translation \u2217}\nThen, we collect pairs of source and target texts (i.e., Chin ese texts and the corresponding\ntranslations), and replace the variables {\u2217text\u2217}and{\u2217translation \u2217}to generate the \ufb01ne-tuning\nsamples. For example, given a pair of English and Chinese sen tences\nHow\u2019s the weather today? \u2192\u4eca\u5929\u5929\u6c14\u600e\u4e48\u6837\uff1f\n{\u2217text\u2217} {\u2217 translation \u2217}\nwe can generate a \ufb01ne-tuning sample using the prompt templat e, like this\nInstruction Translate the text from English to Chinese.\nUser Input How\u2019s the weather today?\nOutput\u4eca\u5929\u5929\u6c14\u600e\u4e48\u6837\uff1f\nThat is,\nx=Translate the text from English to Chinese. \\n How\u2019s the weather today?\ny=\u4eca\u5929\u5929\u6c14\u600e\u4e48\u6837\uff1f\nWe can use this (x,y)pair to \ufb01ne-tune the LLM, as described in the previous subsec tion.\nOne dif\ufb01culty here is that there are many, many different way s to write prompt templates\nfor the same task, and different people may produce prompt te mplates with varying qualities\nand complexities. Sometimes, we may write prompt templates with overly complex or verbose\ninstructions. Sometimes, we may not even know exactly what t he target task is and how to de-\nscribe it. A widely-adopted strategy is to create prompt tem plates for existing NLP tasks, given\nthat there have been so many well-established NLP problems a nd benchmarks [ Bach et al. ,2022 ;\nWang et al. ,2022b ;Mishra et al. ,2022 ]. In this case, annotators can be given the original task\ndescription and many examples. Then, they can use their own w ays to express how to prompt the\nLLM to perform the tasks. Note that,",
    "0161c673e9ed81fc": " LLM, as described in the previous subsec tion.\nOne dif\ufb01culty here is that there are many, many different way s to write prompt templates\nfor the same task, and different people may produce prompt te mplates with varying qualities\nand complexities. Sometimes, we may write prompt templates with overly complex or verbose\ninstructions. Sometimes, we may not even know exactly what t he target task is and how to de-\nscribe it. A widely-adopted strategy is to create prompt tem plates for existing NLP tasks, given\nthat there have been so many well-established NLP problems a nd benchmarks [ Bach et al. ,2022 ;\nWang et al. ,2022b ;Mishra et al. ,2022 ]. In this case, annotators can be given the original task\ndescription and many examples. Then, they can use their own w ays to express how to prompt the\nLLM to perform the tasks. Note that, while such a method can ea se the process of creating and\nwriting prompts, we still need annotation frameworks and cr owdsourcing systems to manage the\nwork and conduct quality control. For example, we generally need to design annotation guidelines\n\n--- Page 170 ---\n\n4.2 Instruction Alignment 163\nand a uni\ufb01ed format for writing prompt templates, especiall y when many annotators are contribut-\ning to the same task. One advantage of inducing prompts from e xisting NLP tasks is that, once the\nprompt templates have been developed, it is easy to generate prompts using the annotated samples\nin the original tasks. For example, given a bilingual datase t for English-to-Chinese translation, we\ncan easily create a number of \ufb01ne-tuning examples by \ufb01lling t he slots in the above template with\nthe sentence pairs in this dataset.\nAnother approach is to directly use the naturally existing d ata available on the internet. A\ncommon example is by collecting question-and-answer pairs from QA websites to \ufb01ne-tune LLMs\nfor open-domain QA tasks [ Joshi et al. ,2017 ]. Many benchmarks in QA are built in this way\nbecause there are so many types of questions that it is imposs ible to think of them all by a small\ngroup of people. Instead, using data from those websites can ensure that the LLM \ufb01ne-tuning data\nis at a good or acceptable level in terms of quantity and quali ty.\nIn addition to employing existing resources, another strai ghtforward way to develop a \ufb01ne-\ntuning dataset is to crowdsource the data. A simple approach is to allow users to input any ques-\ntion, after which responses are either manually given or aut omatically generated by an LLM and\nthen manually annotated and corrected. It is thus possible t o capture real user behavior and conse-\nquently gather inputs and outputs for a large number of \u201cnew\u201d problems that traditional NLP tasks\ndo not cover.\nAn issue related to the construction of the \ufb01ne-tuning datas ets is that we usually want the\ndata to be as diverse as possible. Many studies have found tha t increasing the diversity of \ufb01ne-\ntuning data can improve the robustness and generalization a bility of LLMs. For this reason, there\nhas been considerable interest in involving more diverse pr ompts and tasks in LLM \ufb01ne-tunining\ndatasets. We will give further discussions on the generaliz ation of \ufb01ne-tuning in Section 4.2.4 .\n4.2.2.2 Automatically Generated Data\nOne limitation of manual data generation is that the quality and diversity largely depend on human\nexperience and creativity. Therefore, if we want LLMs to han dle a broad range of tasks, that\nis, to effectively execute any instruction, relying on huma n-annotated data for LLM \ufb01ne-tuning\nis often inef\ufb01cient. Moreover, the coverage of such data can be limited, and the data may even\ncontain biases introduced by the annotators themselves. An alternative approach is to generate data\nautomatically. For example, we can collect a number of quest ions through crowdsourcing, and\nemploy a well-tuned LLM to generate answers to the questions . These question-answer pairs are\nthen used as \ufb01ne-tuning samples as usual. This method, thoug h very simple, has been extensively\napplied to generate large-scale \ufb01ne-tuning data for LLMs.\nThe above way of producing synthetic \ufb01ne-tuning data is simi lar to those used in data aug-\nmentation for NLP. If we have an LLM, we can produce a predicti on in response to any input.\nRepeating this process for different inputs allows us to cre ate a",
    "a3fcfc1fed1c1dad": "a n-annotated data for LLM \ufb01ne-tuning\nis often inef\ufb01cient. Moreover, the coverage of such data can be limited, and the data may even\ncontain biases introduced by the annotators themselves. An alternative approach is to generate data\nautomatically. For example, we can collect a number of quest ions through crowdsourcing, and\nemploy a well-tuned LLM to generate answers to the questions . These question-answer pairs are\nthen used as \ufb01ne-tuning samples as usual. This method, thoug h very simple, has been extensively\napplied to generate large-scale \ufb01ne-tuning data for LLMs.\nThe above way of producing synthetic \ufb01ne-tuning data is simi lar to those used in data aug-\nmentation for NLP. If we have an LLM, we can produce a predicti on in response to any input.\nRepeating this process for different inputs allows us to cre ate a suf\ufb01cient number of \ufb01ne-tuning\nsamples. Such a method is particularly useful for \ufb01ne-tunin g new LLMs using a well-tuned LLM.\nHowever, one disadvantage of this approach is that it relies on human-crafted or collected inputs\nfor data generation, which may turn out to be inappropriate f or generalizing LLMs. In many LLM\napplications, a signi\ufb01cant challenge arises from the broad range of users\u2019 questions and requests,\nmany of which are not covered in existing NLP tasks and datase ts. In these cases, it becomes\nnecessary to generate not only the predictions but also the i nputs themselves.\nHere we consider self-instruct as an example to illustrate how to generate LLM \ufb01ne-tuning\n\n--- Page 171 ---\n\n164 Alignment\nInitializationInitialize the task pool with a number of instructions\nand corresponding input-output samples.\nSample 1: (Instruction ,User -input ,Output)\nSample 2: (Instruction ,User -input ,Output)\n\u00b7 \u00b7 \u00b7Task Pool\nSamplingDraw a few instructions from the pool\nInstruction a\nInstruction b\nInstruction cTask Poolsampling\nInstruction\nGenerationPrompt the LLM to generate a new instruction based on\nthe drawn instructions.\nYou are provided several different instructions for perfor ming\nsome tasks. Please generate an instruction based on these.\nTask 1: Instruction a\nTask 2: Instruction b\nTask 3: Instruction c\nNew Task: Instruction new\nSample\nGenerationGiven the newly-generated instruction and a few\ninput-output samples, generate a new sample.\nYou are provided with a set of input-output samples tasks,\neach composed of an instruction, a user input, and an output.\nPlease generate a new sample based on these.\nSample 1: Samplea\nSample 2: Sampleb\nNew Sample: Instruction new User -inputnew Outputnew\nFilteringFilter out invalid and low-quality samples.\nAdd the remaining samples into the pool.\nFig. 4.4: Illustration of self-instruct [ Wang et al. ,2023b ]. This method maintains a pool of instructions and correspo nd-\ning input-output samples. Initially, the pool contains a nu mber of hand-crafted instructions and samples. Each time,\nwe draw a few instructions from the pool. An LLM is then prompt ed to generate new instructions and samples based\non those drawn. Finally, the newly-generated instructions and samples are \ufb01ltered and added to the pool.\nsamples [ Wang et al. ,2023d ;Honovich et al. ,2023 ]. The idea is that we can prompt an LLM to\ncreate a new instruction by learning from other instruction s. Given this instruction, the LLM can\nthen \ufb01ll in other \ufb01elds (such as the user input) and produce th e predictions. Figure 4.4shows a\nschematic illustration of self-instruct. Here we give a bri ef outline of the key steps involved.\n\u2022 The self-instruct algorithm maintains a pool of tasks. Ini tially it contains a number of seed\nhand-crafted tasks, each with an instruction and input-out put sample. As the algorithm\nproceeds, LLM-generated instructions and samples will be a dded to this pool.\n\n--- Page 172 ---\n\n4.2 Instruction Alignment 165\n\u2022 At each step, a small number of instructions are drawn from t he instruction pool. For ex-\nample, we can randomly select a few human-written instructi ons and a few LLM-generated\ninstructions to ensure diversity.\n\u2022 The selected instructions are then used as demonstration e xamples. Thus, the LLM can\nin-context learn from these examples and produce a new instr uction. Below is an example\ntemplate for prompting the LLM.\nYou are provided several different instructions for perfor ming some tasks.\n",
    "20f154411d0897d0": "\nschematic illustration of self-instruct. Here we give a bri ef outline of the key steps involved.\n\u2022 The self-instruct algorithm maintains a pool of tasks. Ini tially it contains a number of seed\nhand-crafted tasks, each with an instruction and input-out put sample. As the algorithm\nproceeds, LLM-generated instructions and samples will be a dded to this pool.\n\n--- Page 172 ---\n\n4.2 Instruction Alignment 165\n\u2022 At each step, a small number of instructions are drawn from t he instruction pool. For ex-\nample, we can randomly select a few human-written instructi ons and a few LLM-generated\ninstructions to ensure diversity.\n\u2022 The selected instructions are then used as demonstration e xamples. Thus, the LLM can\nin-context learn from these examples and produce a new instr uction. Below is an example\ntemplate for prompting the LLM.\nYou are provided several different instructions for perfor ming some tasks.\nPlease generate an instruction based on these.\nTask 1: {instruction1 }\nTask 2: {instruction2 }\nTask 3: {instruction3 }\nTask 4: {instruction4 }\nNew Task:\n\u2022 Given the generated instruction, the LLM is then prompted t o complete the sample by \ufb01lling\nin the remaining input \ufb01elds and generating the correspondi ng output. Below is a prompt\ntemplate.\nYou are provided with a set of input-output samples, each com posed of\nan instruction, a user input, and an output. Please generate a new sample\nbased on these.\nSample 1: {instruction1 }\nInput: {user-input1 }\nOutput: {output1 }\nSample 2: {instruction2 }\nInput: {user-input2 }\nOutput: {output2 }\nNew Sample: {new-instruction }\n\u2022 This newly-generated sample is examined by some heuristic rules (such as \ufb01ltering out\nsamples or instructions that are similar to those already in the pool). If it passes, the sample\nand instruction are added to the pool.\nThis generation process can be repeated many times to obtain a suf\ufb01cient number of \ufb01ne-\ntuning samples. Note that, above, we just show simple prompt templates for generating instruction\nand \ufb01ne-tuning samples. Of course, we can develop better tem plates to generate more diverse and\naccurate instruction and \ufb01ne-tuning samples. For example, for certain tasks like text classi\ufb01cation,\n\n--- Page 173 ---\n\n166 Alignment\nthe LLM may tend to produce biased predictions, for example, most generated samples belong to\na single class. In such cases, we can adjust the order of gener ation of different \ufb01elds. More\nspeci\ufb01cally, we can specify the output (i.e., the class) wit h some prior, and prompt the LLM\nto generate user input given both the instruction and the out put. This method resembles input\ninversion , where the LLM generates the input based on the speci\ufb01ed outp ut [Longpre et al. ,2023 ].\nUsing LLM-generated instructions and \ufb01ne-tuning samples h as been a common method for\ndeveloping LLMs, especially given that manually developin g such data is so expensive that most\nresearch groups cannot afford it. In several well-tuned LLM s, their \ufb01ne-tuning datasets include\na certain amount of synthetic data, which has proved useful [ Ouyang et al. ,2022 ;Taori et al. ,\n2023 ;Chiang et al. ,2023 ]. There have been further studies on synthetic data generat ion for LLM\n\ufb01ne-tuning. For example, one can generate more diverse inst ructions by introducing evolutionary\nalgorithms [ Xu et al. ,2024 ], or use synthetic data as supervision signals in a more adva nced \ufb01ne-\ntuning process [ Chen et al. ,2024b ]. More recently, there has also been considerable interest in\nusing synthetic data in the pre-training stage [ Gunasekar et al. ,2023 ;Allal et al. ,2024 ].\nIn many applications, a real-world scenario is that, given a task, we can collect or annotate a\nrelatively small amount of \ufb01ne-tuning data, for example, we can recruit experts to create questions\nfor QA tasks in a speci\ufb01c domain. But the quantity and diversi ty of this data are in general not\nsuf\ufb01cient. In this case, we can use self-instruct technique s to generate more diverse question-\nanswer pairs, and thus augment the \ufb01ne-tuning data. This pro vides a way of bootstrapping the\nLLM starting from a seed set of \ufffd",
    "42f3a791c9377566": " synthetic data as supervision signals in a more adva nced \ufb01ne-\ntuning process [ Chen et al. ,2024b ]. More recently, there has also been considerable interest in\nusing synthetic data in the pre-training stage [ Gunasekar et al. ,2023 ;Allal et al. ,2024 ].\nIn many applications, a real-world scenario is that, given a task, we can collect or annotate a\nrelatively small amount of \ufb01ne-tuning data, for example, we can recruit experts to create questions\nfor QA tasks in a speci\ufb01c domain. But the quantity and diversi ty of this data are in general not\nsuf\ufb01cient. In this case, we can use self-instruct technique s to generate more diverse question-\nanswer pairs, and thus augment the \ufb01ne-tuning data. This pro vides a way of bootstrapping the\nLLM starting from a seed set of \ufb01ne-tuning samples. Note that using self-generated data is a com-\nmon practice and has long been applied in NLP. For example, th is approach has been successfully\nused in parsing and machine translation [ Charniak ,1997 ;Sennrich et al. ,2016 ].\n4.2.3 Fine-tuning with Less Data\nWith the increasing prominence of instruction \ufb01ne-tuning, there has been a surge in demand for\nlarge-scale, high-quality \ufb01ne-tuning data. For example, t he FLAN \ufb01ne-tuning dataset, which\nis compiled from 1,836 tasks, contains 15 million samples [ Longpre et al. ,2023 ]. Fine-tuning\nLLMs with such large datasets is typically a computationall y expensive task, especially given\nthat updating the large number of parameters in LLMs is resou rce-intensive. One approach for\nmitigating this issue is to explore ef\ufb01cient model training methods, for example, one can use\nparameter-ef\ufb01cient methods to update only a small portion o f the model. However, many \ufb01ne-\ntuning datasets contain a large amount of synthetic data, wh ere errors and biases are still inevitable.\nAnother approach to ef\ufb01cient \ufb01ne-tuning is to consider only the most relevant and impactful\nexamples for \ufb01ne-tuning. We can thus reduce the amount of dat a that needs to be processed while\nstill maintaining the quality of the model updates. There ar e several methods to achieve this. For\nexample, Zhou et al. [2023a ] built an instruction-following dataset containing only 1 ,000 sam-\nples by carefully crafting the prompts and collecting sampl es from a variety of NLP tasks. They\nshowed that the LLaMa 65B model \ufb01ne-tuned with this dataset c ould be competitive with or even\nbetter than models with much more \ufb01ne-tuning effort. This su ggests that LLMs can be adapted\nto respond to diverse tasks without necessarily needing \ufb01ne -tuning on all types of instruction-\nfollowing data. Chen et al. [2024a ] developed a system based on the GPT-3.5 model to assess\nthe quality of each instruction-following sample. Therefo re, they could select high-quality sam-\nples from existing datasets, showing better \ufb01ne-tuning per formance with fewer \ufb01ne-tuning sam-\nples. Researchers have also developed methods to either sel ect or \ufb01lter out data using heuristics\n\n--- Page 174 ---\n\n4.2 Instruction Alignment 167\n[Zhao et al. ,2024 ;Ge et al. ,2024 ], or to prioritize data that more signi\ufb01cantly in\ufb02uences th e \ufb01ne-\ntuning process [ Xia et al. ,2024 ]. In fact, most of these methods can be seen as instances of la rger\nfamilies of data selection and \ufb01ltering methods. And it is of ten the case that using higher quality\n(but maybe less) data is bene\ufb01cial for training NLP models.\nThe discoveries in instruction \ufb01ne-tuning somewhat differ from traditional views in NLP: the\nability of models to handle complex problems can be activate d with a small amount of annotated\ndata, rather than requiring massive amounts of supervised d ata for extensive training. One pos-\nsible explanation is that the ability of generating correct responses given instructions has been\nlearned during pre-training, but such instruction-respon se mappings are not with high probabil-\nities during inference. Fine-tuning can slightly adjust th e models to get them to follow instruc-\ntions, requiring signi\ufb01cantly less training effort than pr e-training.",
    "bed1930b523cb428": "tuning process [ Xia et al. ,2024 ]. In fact, most of these methods can be seen as instances of la rger\nfamilies of data selection and \ufb01ltering methods. And it is of ten the case that using higher quality\n(but maybe less) data is bene\ufb01cial for training NLP models.\nThe discoveries in instruction \ufb01ne-tuning somewhat differ from traditional views in NLP: the\nability of models to handle complex problems can be activate d with a small amount of annotated\ndata, rather than requiring massive amounts of supervised d ata for extensive training. One pos-\nsible explanation is that the ability of generating correct responses given instructions has been\nlearned during pre-training, but such instruction-respon se mappings are not with high probabil-\nities during inference. Fine-tuning can slightly adjust th e models to get them to follow instruc-\ntions, requiring signi\ufb01cantly less training effort than pr e-training. This is closely related to what\nis known as the super\ufb01cial alignment hypothesis , which suggests that learning primarily occurs\nduring pre-training, and the subsequent \ufb01ne-tuning or alig nment phase does not signi\ufb01cantly con-\ntribute to the underlying knowledge base of an LLM [ Zhou et al. ,2023a ]. Since the core abilities\nand knowledge of the model are already established from pre- training, effective \ufb01ne-tuning for\nalignment with user needs can be achieved with relatively sm all training \ufb01ne-tuning effort. This\nimplies the possibility of \ufb01ne-tuning LLMs with very little data. In another direction, it may not\nbe necessary to restrict \ufb01ne-tuning to paired instruction- response data. For example, Hewitt et al.\n[2024 ] found that instruction-following can be implicitly achie ved by \ufb01ne-tuning LLMs only on\nresponses, without corresponding instructions.\nA concept related to the discussion here is sample ef\ufb01ciency . A machine learning method is\ncalled sample ef\ufb01cient if it can learn effectively from a small number of training ex amples. In this\nsense, instruction \ufb01ne-tuning is sample ef\ufb01cient compared with pre-training. From the perspective\nof machine learning, sample-ef\ufb01cient methods can be seen as ef\ufb01cient ways to sample the space\nof data, and are advantageous as they make optimal use of scar ce data. Therefore, sampling-based\nlearning techniques, such as many reinforcement learning a lgorithms, can bene\ufb01t from these sam-\nple ef\ufb01cient approaches. For example, in human preference a lignment, we can either ef\ufb01ciently\nsample preference data via reward models [ Liu et al. ,2024b ] or improve sampling ef\ufb01ciency in\npolicy learning [ Wang et al. ,2024 ].\n4.2.4 Instruction Generalization\nIn many machine learning and NLP problems, training a model t o generalize is a fundamental\ngoal. For example, in text classi\ufb01cation, we expect our mode l to correctly classify new texts that\nwere not seen during training. However, generalization pos es additional challenges in instruction\n\ufb01ne-tuning. We expect instruction-\ufb01ne-tuned LLMs to not on ly generate appropriate responses for\ndifferent inputs within a task but also to accurately perfor m various tasks as described by different\ninstructions. To illustrate this issue, consider an LLM Pr(y|c,z), where cis an instruction, z\nis a user input, and yis the corresponding model output (i.e., the response). Sup pose that the\nperformance of this model is evaluated in terms of a metric, w ritten as Performance(Pr( y|c,z))\norP(c,z,y)for short. Informally, when we say this model can generalize within a given task\n(indicated by the instruction c\u2217), we mean that there may be a value \u01ebsuch that the average\nperformance on new inputs is above this value:\n1\n|Z|\u2211\nz\u2032\u2208ZP(c\u2217,z\u2032,y\u2032)>\u01eb (4.8)\n\n--- Page 175 ---\n\n168 Alignment\nwhere Zis the set of new inputs, and z\u2032andy\u2032are an input in this set and the corresponding\noutput, respectively.\nLikewise, we can say that this model can generalize across ta sks if the average performance\nover all instruction-input pairs is above some \u01eb:\n1\n|D|\u2211\n(c\u2032,z\u2032)\u2208DP(c\u2032,z\u2032,y\u2032)>\u01eb (4.9)\nwhere Dis the set of new instruction-input pairs.\nHere, we need to deal with variations in two",
    "efa94e6b91ab13ec": ", when we say this model can generalize within a given task\n(indicated by the instruction c\u2217), we mean that there may be a value \u01ebsuch that the average\nperformance on new inputs is above this value:\n1\n|Z|\u2211\nz\u2032\u2208ZP(c\u2217,z\u2032,y\u2032)>\u01eb (4.8)\n\n--- Page 175 ---\n\n168 Alignment\nwhere Zis the set of new inputs, and z\u2032andy\u2032are an input in this set and the corresponding\noutput, respectively.\nLikewise, we can say that this model can generalize across ta sks if the average performance\nover all instruction-input pairs is above some \u01eb:\n1\n|D|\u2211\n(c\u2032,z\u2032)\u2208DP(c\u2032,z\u2032,y\u2032)>\u01eb (4.9)\nwhere Dis the set of new instruction-input pairs.\nHere, we need to deal with variations in two dimensions: inst ruction and user input. This\nmakes the generalization problem very complex, because, in tuitively, a model needs to learn from\na vast number of tasks and different input-output pairs asso ciated with each task to achieve good\ngeneralization. As we have discussed several times in this b ook, achieving such generalization\nincurs much lower cost than pre-training. In general, \ufb01ne-t uning LLMs with instruction-response\ndata to some extent can lead to models yielding instruction f ollowing on new tasks. Nevertheless,\nit is typically believed that certain efforts are still need ed to adapt LLMs to make them understand\nand execute instructions broadly.\nOne way to generalize instruction \ufb01ne-tuning is to increase the diversity of the \ufb01ne-tuning\ndata. In earlier studies on instruction \ufb01ne-tuning, resear chers developed many datasets, covering a\nwide variety of NLP tasks and different instructions for eac h task [ Wang et al. ,2022b ;Sanh et al. ,\n2022 ;Longpre et al. ,2023 ]. By transforming these tasks into a uni\ufb01ed format, one can \ufb01 ne-\ntune an LLM with a suf\ufb01ciently large number of samples, for ex ample, there have been several\ninstruction \ufb01ne-tuning datasets that involve over 100 NLP t asks and 1M samples. However, these\nearly datasets mostly focus on existing academic problems, but not those that users want to deal\nwith in real-world applications. Much recent work has shift ed focus to addressing new and more\npractical problems. For example, there has been considerab le interest in constructing datasets\nthat contain large and complicated demonstrations and resp onses from SOTA models to real user\nqueries [ Wang et al. ,2023c ;Teknium ,2023 ].\nPerhaps the use of large and diverse \ufb01ne-tuning datasets has its origins in attempts to scale\nLLMs in different dimensions. Indeed, scaling laws have bee n used broadly to motivate the de-\nvelopment of a wide range of different instruction-\ufb01ne-tun ed LLMs. And it is reasonable to scale\ninstruction \ufb01ne-tuning to make an LLM follow broad instruct ions. From the perspective of LLM\nalignment, however, scaling instruction \ufb01ne-tuning might not be ef\ufb01cient to achieve generaliza-\ntion.\nOne problem is that instruction \ufb01ne-tuning relies on superv ised learning that learns to gener-\nalize and perform tasks based on instruction-response mapp ings. However, such an approach does\nnot capture subtle or complex human preferences (e.g., tone , style, or subjective quality) because\nthese are hard to encode as explicit instruction-response d ata. Moreover, the generalization per-\nformance is bounded by the diversity and quality of the instr uction-response dataset. Given these\nlimitations, we would instead like to employ preference mod els as an additional \ufb01ne-tuning step\nfollowing instruction \ufb01ne-tuning, so the LLMs can generali ze further (see Section 4.3).\nAnother view is that some instruction-response mappings ma y already be learned during pre-\ntraining, and so the pre-trained LLMs have encoded such mapp ings. However, since we often do\nnot know exactly what data is used in the pre-training, it is h ard to judge whether we need to learn\nsuch mappings in the \ufb01ne-tuning. A related question is wheth er out-of-distribution generalization\n\n--- Page 176 ---\n\n4.2 Instruction Alignment 169\nis primarily achieved during pre-training or \ufb01ne-tuning. W hile directly answering this question is\nbeyond the scope of this chapter, it has been shown that pre-t raining",
    "927df59dca750b33": " quality of the instr uction-response dataset. Given these\nlimitations, we would instead like to employ preference mod els as an additional \ufb01ne-tuning step\nfollowing instruction \ufb01ne-tuning, so the LLMs can generali ze further (see Section 4.3).\nAnother view is that some instruction-response mappings ma y already be learned during pre-\ntraining, and so the pre-trained LLMs have encoded such mapp ings. However, since we often do\nnot know exactly what data is used in the pre-training, it is h ard to judge whether we need to learn\nsuch mappings in the \ufb01ne-tuning. A related question is wheth er out-of-distribution generalization\n\n--- Page 176 ---\n\n4.2 Instruction Alignment 169\nis primarily achieved during pre-training or \ufb01ne-tuning. W hile directly answering this question is\nbeyond the scope of this chapter, it has been shown that pre-t raining on large and diverse datasets\nis effective in improving out-of-distribution performanc e [Hendrycks et al. ,2020 ;Radford et al. ,\n2021 ;Gunasekar et al. ,2023 ]. This raises an interesting problem: if an LLM has been well pre-\ntrained at scale, \ufb01ne-tuning may not be as essential for out- of-distribution generalization, since the\nmodel may have already encountered suf\ufb01cient distribution al variation. This prompts researchers\nto \ufb01ne-tune LLMs with modest effort or to explore new methods to achieve instruction-following.\nAs discussed in the previous sub-section, for example, inst ruction following can be yielded by\n\ufb01ne-tuning on a small number of carefully selected instruct ion-response pairs [ Zhou et al. ,2023a ],\nor even by using methods that are not explicitly designed to d o so [ Kung and Peng ,2023 ].\nThe above discussion provides two different strategies: on e requires scaling up \ufb01ne-tuning\ndatasets for larger diversity, the other requires small but necessary \ufb01ne-tuning datasets for ef\ufb01cient\nLLM adaptation. However, in practice, involving diverse in structions often helps. In many cases,\nwe need to adapt our LLM for speci\ufb01c purposes. But the LLM, whi ch has possibly encoded broad\ninstruction-following mappings during pre-training, mig ht tend to behave as a general-purpose\ninstruction executor even with modest \ufb01ne-tuning. An inter esting phenomenon is that when \ufb01ne-\ntuning on math data, the resulting LLM might not specialize i n math outputs. Instead, this model\nmight respond normally to general instructions, for exampl e, it could generate poetry if instructed\nto do so [ Hewitt ,2024 ]. This is not a bad thing, but it shows that LLMs may not easily change their\nnature of following general instructions. In this case, add itional adaptations with more diverse\ndata may help adjust the way the LLM follows instructions, pa rticularly for those tasks we aim to\naddress.\n4.2.5 Using Weak Models to Improve Strong Models\nSo far we have explored a variety of instruction \ufb01ne-tuning m ethods based on labeled data. One\nof the limitations of many such methods is that they require t he data to be annotated by humans or\ngenerated by strong LLMs, which can provide accurate superv ision signals in \ufb01ne-tuning. How-\never, in many cases, the LLM we have in hand is already strong ( or at least is advantageous in\nspeci\ufb01c aspects of problem solving), and thus it is not easy t o \ufb01nd a superior model for supervi-\nsion. Even for human experts, when the problem becomes compl ex, providing correct and detailed\nanswers might be dif\ufb01cult, or sometimes infeasible. For exa mple, when faced with an extremely\nlong document, the experts would \ufb01nd it challenging to ident ify any inconsistencies, subtle biases,\nor missing key points without conducting an exhaustive and t ime-consuming review.\nOne may ask at this point: can we use weak LLMs to supervise str ong LLMs? This seems\nto be a signi\ufb01cant challenge, but it may re\ufb02ect a future scena rio where we need to supervise AI\nsystems that are smarter than humans or any other AI systems [ Burns et al. ,2023b ]. The problem\nof using smaller, less complex models to improve the trainin g of larger, more complex models\nis also called the weak-to-strong generalization problem. While there have not been mature\napproaches to weak-to-strong generalization, using small er models to assist stronger models has\nindeed",
    "393349cb0aa2258f": "answers might be dif\ufb01cult, or sometimes infeasible. For exa mple, when faced with an extremely\nlong document, the experts would \ufb01nd it challenging to ident ify any inconsistencies, subtle biases,\nor missing key points without conducting an exhaustive and t ime-consuming review.\nOne may ask at this point: can we use weak LLMs to supervise str ong LLMs? This seems\nto be a signi\ufb01cant challenge, but it may re\ufb02ect a future scena rio where we need to supervise AI\nsystems that are smarter than humans or any other AI systems [ Burns et al. ,2023b ]. The problem\nof using smaller, less complex models to improve the trainin g of larger, more complex models\nis also called the weak-to-strong generalization problem. While there have not been mature\napproaches to weak-to-strong generalization, using small er models to assist stronger models has\nindeed proven useful in several areas of LLMs.\nFor instruction \ufb01ne-tuning, one of the simplest ways of appl ying weak LLMs is to use these\nmodels to generate synthetic \ufb01ne-tuning data. Suppose we ha ve a collection of inputs X, where\neach input includes an instruction and a user input if necess ary. For each x\u2208X, we use a weak\nLLM Prw(\u00b7)to generate a prediction \u02c6y= arg maxyPrw(y|x). Then, the strong LLM Prs\n\u03b8(\u00b7)can\n\n--- Page 177 ---\n\n170 Alignment\nbe trained on these generated predictions (see Eq. ( 4.1)):\n\u02dc\u03b8= arg max\n\u03b8\u2211\nx\u2208Xlog Prs\n\u03b8(\u02c6y|x) (4.10)\nwhere\u03b8is the model parameters.\nThe above form transforms the \ufb01ne-tuning problem into a know ledge distillation problem, in\nother words, we distill knowledge from the weak model to the s trong model. Consequently, we\ncan employ various knowledge distillation methods to achie ve this goal. However, explaining\nweak-to-strong \ufb01ne-tuning from the perspective of knowled ge distillation is not straightforward.\nA major concern is that the strong model may merely imitate or over\ufb01t the errors of the weak\nmodel and fail to generalize. For example, the \ufb01ne-tuned str ong model still cannot solve dif\ufb01cult\nproblems that the weak model cannot accurately predict. For tunately, preliminary experiments in\nthis line of research have shown positive and promising resu lts. For example, Burns et al. [2023a ]\nfound that \ufb01ne-tuning the strong pre-trained GPT-4 model wi th GPT-2-level supervision could\nimprove generalization across several NLP tasks. To measur e how the weak model improves the\ngeneralization of the strong model, we de\ufb01ne the following t erms:\n\u2022Weak Performance (Pweak). This is the test-set performance of the weak model, which\ncan be regarded as the baseline performance.\n\u2022Weak-to-strong Performance (Pweak \u2192strong ). This is the test-set performance of the strong\nmodel that is \ufb01ne-tuned with the weak model.\n\u2022Strong Ceiling Performance (Pceiling ). This is the test-set performance of the strong model\nthat is \ufb01ne-tuned with ground truth data. For example, we \ufb01ne -tune the strong model with\nhuman-annotated predictions and take the resulting model a s a ceiling.\nThen, the performance gap recovered (PGR ) can be de\ufb01ned as\nPGR = max{\n0,Pweak \u2192strong \u2212Pweak\nPceiling \u2212Pweak}\n(4.11)\nThis metric measures how much of the performance gap between the ceiling model and the\nweak model can be recovered by the weak-to-strong model. A PG R of 1 indicates that the weak-\nto-strong \ufb01ne-tuning can completely closes the performanc e gap, whereas a PGR of 0 indicates\nno improvement. In Burns et al. [2023a ]\u2019s work, it is shown that PGR can be around 0.8on 22\nNLP classi\ufb01cation tasks. It should be noted that, while the p otential of weak-to-strong \ufb01ne-tuning\nis promising, achieving substantial weak-to-strong gener alization remains a challenging goal that\nneeds further investigation [ Aschenbrenner ,2024 ].\nFine-tuning LLMs with weak supervision is just one choice fo r using small models to improve\nlarge models. Although this section primarily focuses on \ufb01n e-tuning LLMs, we also mention\nother methods here to give",
    "1719d2aa5bced58e": " gap between the ceiling model and the\nweak model can be recovered by the weak-to-strong model. A PG R of 1 indicates that the weak-\nto-strong \ufb01ne-tuning can completely closes the performanc e gap, whereas a PGR of 0 indicates\nno improvement. In Burns et al. [2023a ]\u2019s work, it is shown that PGR can be around 0.8on 22\nNLP classi\ufb01cation tasks. It should be noted that, while the p otential of weak-to-strong \ufb01ne-tuning\nis promising, achieving substantial weak-to-strong gener alization remains a challenging goal that\nneeds further investigation [ Aschenbrenner ,2024 ].\nFine-tuning LLMs with weak supervision is just one choice fo r using small models to improve\nlarge models. Although this section primarily focuses on \ufb01n e-tuning LLMs, we also mention\nother methods here to give a more complete discussion (see Fi gure 4.5for illustrations of these\nmethods).\n\u2022 Instead of using small models to generate synthetic data, i t is also straightforward to in-\ncorporate knowledge distillation loss based on these model s. For example, a simple loss\n\n--- Page 178 ---\n\n4.2 Instruction Alignment 171\nLarge Model\nx\u02c6yCompute Loss & Train\nDataset=\u21d2 x\u02c6ySmall Model\nInput Predict\n(a) Fine-tuning on data generated by a small model\n(weak-to-strong generalization)Large Model\nxyCompute Loss & Train\nDataset=\u21d2 x ySmall ModelKD LossLM Loss\n(b) Fine-tuning with KD Loss from a small model\n(weak-to-strong generalization)\nLarge Model\nxyCompute Loss & Train\nDataset=\u21d2 x yDataset\nSmall ModelData\nSelection\n(c) Data selection with a small modelSmall Model 2 Small Model 1 Small Model 3\nx x xCombination Modely\n(d) Ensemble of multiple small models\nLarge Model\nxy2\nSmall Model\n(e) Cascading (at inference time)xy1Step 2\n(expensive)Step 1\n(cheap)If Step 1 is not satisfactory, go to Step 2\nFig. 4.5: Illustrations of using small models to improve large models in LLMs. One approach involves using smaller\nmodels for the \ufb01ne-tuning or pre-training of larger models. This includes generating synthetic data (a), incorporatin g\nauxiliary loss (b), and selecting appropriate data (c). Ano ther approach involves combining small models and large\nmodels. This includes learning a strong model by aggregatin g multiple small models (d), and cascading small models\nwith large models (e).\nfunction that measures the difference between the small and large models can be de\ufb01ned as:\nLoss kd= KL(Prw(\u00b7|x)||Prs\n\u03b8(\u00b7|x)) (4.12)\nThen, we can add this loss to the original loss of language mod eling, and yield the following\ntraining objective\n\u02dc\u03b8= arg max\n\u03b8\u2211\n(x,y)\u2208Dlog Prs\n\u03b8(y|x)\u2212\u03bb\u00b7Loss kd (4.13)\n\n--- Page 179 ---\n\n172 Alignment\nwhere Dis the set of input and output pairs, and \u03bbis the coef\ufb01cient of the interpolation. This\nmethod can be employed in either the pre-training or \ufb01ne-tun ing phase. We can adjust \u03bbto\ncontrol how much the small model in\ufb02uences the training. For example, we can gradually\ndecrease\u03bbto make the training rely more on the original language model ing loss as the\nlarge model becomes more capable.\n\u2022 Another approach to involving small models in LLM pre-trai ning and \ufb01ne-tuning is to use\nthem to do data selection or \ufb01ltering. Given a sequence, we ca n compute the likelihood\nor cross-entropy using a small model. These quantities can t hen be used as criteria for\nselecting or \ufb01ltering data. For example, sequences with low likelihood or high cross-entropy\nmight be excluded from the training set, as they are less alig ned with the small model\u2019s\nlearned distribution. Conversely, sequences with high lik elihood or low cross-entropy can\nbe prioritized, ensuring that the training focuses on more r elevant or high-quality data.\n\u2022 Ensemble learning is a simple and effective way to build a st rong model by combining mul-\ntiple weak models. Applying this technique to LLMs is straig htforward. We can aggregate\ndistributions predicted by multiple small models or specia lized submodels, and derive the\n\ufffd",
    "b10543288ff134cc": "-trai ning and \ufb01ne-tuning is to use\nthem to do data selection or \ufb01ltering. Given a sequence, we ca n compute the likelihood\nor cross-entropy using a small model. These quantities can t hen be used as criteria for\nselecting or \ufb01ltering data. For example, sequences with low likelihood or high cross-entropy\nmight be excluded from the training set, as they are less alig ned with the small model\u2019s\nlearned distribution. Conversely, sequences with high lik elihood or low cross-entropy can\nbe prioritized, ensuring that the training focuses on more r elevant or high-quality data.\n\u2022 Ensemble learning is a simple and effective way to build a st rong model by combining mul-\ntiple weak models. Applying this technique to LLMs is straig htforward. We can aggregate\ndistributions predicted by multiple small models or specia lized submodels, and derive the\n\ufb01nal prediction from the aggregated results. This aggregat ion can be done using methods\nsuch as majority voting, weighted averaging, or stacking.\n\u2022 Small models can also be employed at inference time to impro ve overall ef\ufb01ciency. Suppose\nwe have a large model that is slow but more accurate, and a smal l model that is fast but\nless accurate. In model cascading, the small model \ufb01rst proc esses the input data, quickly\ngenerating preliminary results. If these results meet cert ain pre-de\ufb01ned criteria, they can be\ndirectly used. However, if the initial results are not suf\ufb01c iently good, the input is then passed\nto the larger, more accurate model to produce a better result . This approach signi\ufb01cantly\nreduces computational costs and latency, as the small model can effectively handle many\ninputs without access to the large model.\n4.3 Human Preference Alignment: RLHF\nSo far in this chapter, we have focused on \ufb01ne-tuning LLMs usi ng input-output paired labeled data.\nThis approach allows us to adapt LLMs for instruction-follo wing via supervised learning. In many\napplications, however, LLMs are required not only to follow instructions but also to act in ways\nthat are more aligned with human values and preferences. Con sider a scenario where a user asks an\nLLM how to hack into a computer system. If the LLM is not approp riately aligned, it may respond\nby providing details on how to perform this illegal activity . Instead, a more desirable response\nmight be to advise the user against engaging in illegal activ ities and offer a general overview of\nthe consequences of such actions. The dif\ufb01culty in achievin g this is that the ethical nuances and\ncontextual considerations required for an LLM to respond ap propriately in such scenarios are not\nalways straightforward to encode into a \ufb01ne-tuning dataset . What\u2019s even more challenging is that,\noften, humans themselves cannot precisely express their ow n preferences.\nIn this section, we discuss an alternative LLM \ufb01ne-tuning me thod, called reinforcement learn-\ning from human feedback or RLHF for short [ Christiano et al. ,2017 ;Stiennon et al. ,2020 ]. The\nbasic idea behind RLHF is that LLMs can learn from comparison s of model outputs using reward\n\n--- Page 180 ---\n\n4.3 Human Preference Alignment: RLHF 173\nLLM\nxy\nPredicted Token Distributions\n(ntoken distributions)Gold-standard Predictions\n(none-hot distributions)Objective (MLE):\nmax Pr( y|x)\nwhere\nx: input\ny: gold-standard output\n(a) Supervised \ufb01ne-tuning (maximizing the prediction prob ability given the input)\nLLM\nxy1y2Generate multiple\noutputs via sampling\nPrediction y2Prediction y1Objective (RL Loss Minimization):\nminL(x,{y1,y2},r)Reward Model Human preference datatrain\nwhere\nL(\u00b7): loss function\nr(\u00b7): reward model\n(b) Reinforcement Learning from Human Feedback\nFig. 4.6: Supervised \ufb01ne-tuning vs. reinforcement learning from hum an feedback. In supervised \ufb01ne-tuning, we\noptimize the LLM by maximizing the probability of the predic tion given the input. In reinforcement learning from\nhuman feedback, we \ufb01rst train a reward model on human prefere nce data (on each pair of predictions, evaluators are\nasked to choose which one they prefer). Then, we use this rewa rd model to supervise the LLM during \ufb01ne-tuning.\nmodels (see Figure 4.6). To do this, we can recruit human experts who indicate their preferences\nbetween",
    "f7714dae01ad1569": " multiple\noutputs via sampling\nPrediction y2Prediction y1Objective (RL Loss Minimization):\nminL(x,{y1,y2},r)Reward Model Human preference datatrain\nwhere\nL(\u00b7): loss function\nr(\u00b7): reward model\n(b) Reinforcement Learning from Human Feedback\nFig. 4.6: Supervised \ufb01ne-tuning vs. reinforcement learning from hum an feedback. In supervised \ufb01ne-tuning, we\noptimize the LLM by maximizing the probability of the predic tion given the input. In reinforcement learning from\nhuman feedback, we \ufb01rst train a reward model on human prefere nce data (on each pair of predictions, evaluators are\nasked to choose which one they prefer). Then, we use this rewa rd model to supervise the LLM during \ufb01ne-tuning.\nmodels (see Figure 4.6). To do this, we can recruit human experts who indicate their preferences\nbetween pairs of outputs generated by the LLM. This preferen ce data is used to train a reward\nmodel that can predict the perceived quality of LLM outputs. Once trained, the reward model\nprovides feedback by assigning scores to new outputs that th e LLM generates in response to the\ninputs. The LLM uses these scores to update its parameters th rough reinforcement learning algo-\nrithms. In the rest of this section, we will \ufb01rst introduce th e basic knowledge of reinforcement\nlearning to facilitate the discussion, and then discuss met hods for training reward models and\naligning LLMs with these models.\n4.3.1 Basics of Reinforcement Learning\nWe begin by looking at some basic concepts of reinforcement l earning. Note that the notation used\nhere slightly differs from that used in the previous section s and chapters because we want to make\nour description more consistent with those in the reinforce ment learning literature. Nevertheless,\nwe will show how this notation corresponds to the language mo deling notation. The reader who\nis already familiar with reinforcement learning technique s may skip or skim this subsection.\nA general reinforcement learning framework describes how a n agent interacts with a dynamic\nenvironment. This interaction is modeled as a sequence of ac tions taken by the agent in response\nto the state of the environment. At each time step, the agent o bserves the current state, chooses an\naction based on its policy, performs the action, and then rec eives feedback from the environment\nin the form of a reward and a new state. This sequence of observ e-act-receive feedback is repeated\n\n--- Page 181 ---\n\n174 Alignment\nuntil the agent achieves its goal.\nA reinforcement learning system involves several componen ts\n\u2022Agent . This is the learner or decision-maker in reinforcement lea rning. In the context of\nLLMs, it can be seen as the LLM itself.\n\u2022Environment . This includes everything external to the agent with which t he agent interacts.\nBut the environment in LLMs is less about a physical or virtua l space and more about the\nframework within which the agent (e.g., an LLM) receives fee dback and learns.\n\u2022State (s). A state represents the current situation of the environme nt. Given a sequence of\ntokens for language modeling, a state at a time step can be vie wed as the tokens we observed\nso far, that is, the context tokens we take to predict the next token. For example, we can\nde\ufb01ne (x,y<t)as the state when predicting the next token at the time step t.\n\u2022Action (a). Actions represent possible decisions the agent can make. We can see them as\npossible predicted tokens in the vocabulary.\n\u2022Reward (R). The reward is the feedback from the environment that evalu ates the success\nof an action. For example, r(s,a,s\u2032)denotes the reward the agent receives for taking the\nactionaat the statesand moving to the next state s\u2032. If the state-action sequence is given,\nwe can denote the reward at the time step tasrt=r(st,at,st+1). Also note that if the\ndecision-making process is deterministic, we can omit st+1because it is can be determined\nbystandat. In such cases, we can use r(st,at)as shorthand for r(st,at,st+1).\n\u2022Policy (\u03c0). For an LLM, a policy is de\ufb01ned as the probability distribut ion over the tokens\nthat the LLM predicts, given the preceding context tokens. F ormally, this can be expressed\nas\n\u03c0(a|s) = Pr(yt|x,y<t) (4.14)\nwhereacorresponds to the token y, andscorresponds to the context (x,y<t). Figure 4.7",
    "5fd942c44646a671": "denotes the reward the agent receives for taking the\nactionaat the statesand moving to the next state s\u2032. If the state-action sequence is given,\nwe can denote the reward at the time step tasrt=r(st,at,st+1). Also note that if the\ndecision-making process is deterministic, we can omit st+1because it is can be determined\nbystandat. In such cases, we can use r(st,at)as shorthand for r(st,at,st+1).\n\u2022Policy (\u03c0). For an LLM, a policy is de\ufb01ned as the probability distribut ion over the tokens\nthat the LLM predicts, given the preceding context tokens. F ormally, this can be expressed\nas\n\u03c0(a|s) = Pr(yt|x,y<t) (4.14)\nwhereacorresponds to the token y, andscorresponds to the context (x,y<t). Figure 4.7\nillustrates how an LLM can be treated as a policy in the reinfo rcement learning framework.\n\u2022Value Function (VandQ). A state-value function (or value function, for short) assesses\nthe expected discounted return (i.e., accumulated rewards ) for an agent starting from a par-\nticular state sand following a speci\ufb01c policy \u03c0. It is de\ufb01ned as:\nV(s) =E[\nr(s0,a0,s1) +\u03b3r(s1,a1,s2) +\u03b32r(s2,a2,s3) +\u00b7 \u00b7 \u00b7\u23d0\u23d0s0=s,\u03c0]\n=E[\nr0+\u03b3r1+\u03b32r2+\u00b7 \u00b7 \u00b7\u23d0\u23d0s0=s,\u03c0]\n=E[\u221e\u2211\nt=0\u03b3trt\u23d0\u23d0s0=s,\u03c0]\n(4.15)\nwhere\u03b3\u2208[0,1]is the discount factor that adjusts the importance of future rewards,s0=s\nindicates that the agent starts with the state s, and the expectation Eis performed over all\npossible trajectories (i.e., state-action sequences). Si milarly, an action-value function (or\n\n--- Page 182 ---\n\n4.3 Human Preference Alignment: RLHF 175\nx0 x1 ... xm y1 ... yt\u22121Policy (LLM)y1 y2 ... yt\nState st(xandy<t)Action at\nReward Model\nR(st,at)Value Functions\nV(st)andQ(st,at)Feedback\nFig. 4.7: LLM as policy in reinforcement learning. At each step t, the LLM predicts a token ytgiven the model\ninput xand the previously-generated tokens y<t. This process can be framed as a reinforcement learning prob lem,\nwhereytserves as the action, (x,y<t)as the state, and the predicted distribution Pr(yt|x,y<t)as the policy. Once\nytis predicted, the LLM inputs both (x,y<t)andytto the reward model, which evaluates how effectively the cho sen\ntoken contributes to achieving the desired textual outcome . This evaluation generates reward scores which are used to\ncompute the value functions V(st)andQ(st,at). These functions then provide feedback to the LLM and guide t he\npolicy training.\nQ-value function ) measures the expected return starting from a state staking an action a\nand thereafter following a policy \u03c0, given by\nQ(s,a) =E[\u221e\u2211\nt=0\u03b3trt\u23d0\u23d0s0=s,a0=a,\u03c0]\n(4.16)\nwherea0=aindicates that the action taken at the initial state is a.\nThe goal of reinforcement learning is to learn a policy that m aximizes the cumulative re-\nward (orreturn ) the agent receives over the long run. Given a state-action s equence\u03c4=\n{(s1,a1),...,(sT,aT)}1, the cumulative reward over this sequence can be written as\nR(\u03c4) =T\u2211\nt=1rt (4.17)\nThe expectation of this cumulative reward over a space of sta te-action sequences is given in\nthe form\nJ(\u03b8) =E\n\u03c4\u223cD[\nR(\u03c4)\u23d0\u23d0\u03c0\u03b8]\n=\u2211\n\u03c4\u2208DPr\u03b8(\u03c4)R(\u03c4)\n=\u2211\n\u03c4\u2208DPr\u03b8(\u03c4)T\u2211\nt=1rt (4.18)\n1We assume the state-action sequence begins with s1anda1, rather than s0anda0, to align with the notation\ncommonly used in this chapter, where the prediction ytypically starts from y1",
    "92fb40d7fb2fe5df": " re-\nward (orreturn ) the agent receives over the long run. Given a state-action s equence\u03c4=\n{(s1,a1),...,(sT,aT)}1, the cumulative reward over this sequence can be written as\nR(\u03c4) =T\u2211\nt=1rt (4.17)\nThe expectation of this cumulative reward over a space of sta te-action sequences is given in\nthe form\nJ(\u03b8) =E\n\u03c4\u223cD[\nR(\u03c4)\u23d0\u23d0\u03c0\u03b8]\n=\u2211\n\u03c4\u2208DPr\u03b8(\u03c4)R(\u03c4)\n=\u2211\n\u03c4\u2208DPr\u03b8(\u03c4)T\u2211\nt=1rt (4.18)\n1We assume the state-action sequence begins with s1anda1, rather than s0anda0, to align with the notation\ncommonly used in this chapter, where the prediction ytypically starts from y1. Of course, it is also common to denote\na state-action sequence as {(s0,a0),...,(sT,aT)}or{(s0,a0),...,(sT\u22121,aT\u22121)}in the literature. But this variation\nin notation does not affect the discussion of the models pres ented here.\n\n--- Page 183 ---\n\n176 Alignment\nwhere\u03c4\u223c D indicates that \u03c4is drawn from the state-action sequence space D, and the subscript\n\u03b8indicates the parameters of the policy. J(\u03b8)is also called the performance function .\nThen the training objective is to maximize J(\u03b8):\n\u02dc\u03b8= arg max\n\u03b8J(\u03b8) (4.19)\nNow, we have a simple reinforcement learning approach: 1) we sample a number of state-\naction sequences; then, 2) we evaluate each sequence using t he performance function; then, 3) we\nupdate the model to maximize this performance function. If w e take Eq. ( 4.18) and use gradient\ndescent to optimize the policy, this approach would constit utes a form of policy gradient methods\n[Williams ,1992 ].\nNote that in many NLP problems, such as machine translation, rewards are typically sparse.\nFor instance, a reward is only received at the end of a complet e sentence. This means that rt= 0\nfor allt < T , andrtis non-zero only when t=T. Ideally, one might prefer feedback to\nbe immediate and frequent (dense), and thus the training of t he policy can be easier and more\nef\ufb01cient. While several methods have been proposed to addre ss sparse rewards, such as reward\nshaping, we will continue in our discussion to assume a spars e reward setup, where the reward is\navailable only upon completing the prediction.\nThe model described in Eqs. ( 4.17-4.19) establishes a basic form of reinforcement learning,\nand many variants and improvements of this model have been de veloped. Before showing those\nmore sophisticated models, let us take a moment to interpret the objective function J(\u03b8)from the\nperspective of policy gradient. In gradient descent, we nee d to compute the gradient of J(\u03b8)with\nrespect to\u03b8:\n\u2202J(\u03b8)\n\u2202\u03b8=\u2202\u2211\n\u03c4\u2208DPr\u03b8(\u03c4)R(\u03c4)\n\u2202\u03b8\n=\u2211\n\u03c4\u2208D\u2202Pr\u03b8(\u03c4)\n\u2202\u03b8R(\u03c4)\n=\u2211\n\u03c4\u2208DPr\u03b8(\u03c4)\u2202Pr\u03b8(\u03c4)/\u2202\u03b8\nPr\u03b8(\u03c4)R(\u03c4)\n=\u2211\n\u03c4\u2208DPr\u03b8(\u03c4)\u2202log Pr \u03b8(\u03c4)\n\u2202\u03b8R(\u03c4) (4.20)\nIn some cases, we will assume that every sequence in Dis equally probable (i.e., Pr\u03b8(\u03c4) =\n1/|D|). In this case we can simplify Eq. ( 4.20) and need only consider the terms\u2202log Pr \u03b8(\u03c4)\n\u2202\u03b8and\nR(\u03c4):\n\u2202J(\u03b8)\n\u2202\u03b8=1\nm\u2211\n\u03c4\u2208D\u2202log Pr \u03b8(\u03c4)\n\u2202\u03b8R(\u03c4) (4.21)\nOne advantage of this result is that R(\u03c4)does not need to be differentiable, which means that we\ncan use any type of reward function in reinforcement learnin g.\nBy treating the generation of the sequence \u03c4as a Markov decision process, we can further\n\n--- Page 184 ---\n\n4.3 Human Preference Alignment: RLHF 177\nderive\u2202log Pr \u03b8(\u03c4)\n\u2202\u03b8, and obtain:\n",
    "8ee988cfba969d31": "In some cases, we will assume that every sequence in Dis equally probable (i.e., Pr\u03b8(\u03c4) =\n1/|D|). In this case we can simplify Eq. ( 4.20) and need only consider the terms\u2202log Pr \u03b8(\u03c4)\n\u2202\u03b8and\nR(\u03c4):\n\u2202J(\u03b8)\n\u2202\u03b8=1\nm\u2211\n\u03c4\u2208D\u2202log Pr \u03b8(\u03c4)\n\u2202\u03b8R(\u03c4) (4.21)\nOne advantage of this result is that R(\u03c4)does not need to be differentiable, which means that we\ncan use any type of reward function in reinforcement learnin g.\nBy treating the generation of the sequence \u03c4as a Markov decision process, we can further\n\n--- Page 184 ---\n\n4.3 Human Preference Alignment: RLHF 177\nderive\u2202log Pr \u03b8(\u03c4)\n\u2202\u03b8, and obtain:\n\u2202log Pr \u03b8(\u03c4)\n\u2202\u03b8=\u2202\n\u2202\u03b8logT\u220f\nt=1\u03c0\u03b8(at|st) Pr(st+1|st,at)\n=\u2202\n\u2202\u03b8T\u2211\nt=1log\u03c0\u03b8(at|st)\ued19\ued18\ued17\ued1a\npolicy+\u2202\n\u2202\u03b8T\u2211\nt=1log Pr(st+1|st,at)\ued19\ued18\ued17\ued1a\ndynamics(4.22)\nwhere the gradient is decomposed into two parts: the policy g radient and the dynamics gradient.\nThe policy component, log\u03c0\u03b8(at|st), determines the log-probability of taking action atgiven\nstatest, and it is parameterized by \u03b8. The dynamics component, log Pr(st+1|st,at), represents\nthe log-probability of transitioning to state st+1from statestafter taking action at. In typical\nreinforcement learning settings, the dynamics are not dire ctly in\ufb02uenced by the policy parameters\n\u03b8, and thus, their derivatives are often zero. In this case, th erefore, Eq. ( 4.22) can be simpli\ufb01ed to:\n\u2202log Pr \u03b8(\u03c4)\n\u2202\u03b8=\u2202\n\u2202\u03b8T\u2211\nt=1log\u03c0\u03b8(at|st) (4.23)\nIn other words, we only concentrate on optimizing the policy without concerning ourselves with\nthe underlying dynamics.\nSubstituting Eq. ( 4.23) into Eq. ( 4.21), and expanding R(\u03c4), we then obtain\n\u2202J(\u03b8)\n\u2202\u03b8=1\n|D|\u2211\n\u03c4\u2208D\u2202\n\u2202\u03b8(T\u2211\nt=1log\u03c0\u03b8(at|st)T\u2211\nt=1rt)\n(4.24)\nWhile this policy gradient approach is straightforward, it suffers from the problem that the\nvariance of the estimated gradients can be very high, making the learning process noisy and inef-\n\ufb01cient. One reason for this high variance problem is that rew ards can vary greatly across different\nsteps or scenarios. Imagine that in a sequence of action deci sions, the reward model tends to assign\nsmall rewards to good actions (e.g., Rt= 2) and large penalties to poor actions (e.g., Rt=\u221250).\nSuch varying reward scales for good and poor actions can resu lt in a very low total reward for the\nentire sequence, even if it includes good actions.\nOne simple method for reducing the variance of the gradient i s to set a baseline band subtract\nit from\u2211T\nt=1rt, resulting in\u2211T\nt=1rt\u2212b.2Here, the baseline can be interpreted as a reference\npoint. By centering the rewards around this baseline, we rem ove systematic biases in the reward\nsignal, making the updates more stable and less sensitive to extreme \ufb02uctuations in individual\nrewards.\n2In fact, the use of a baseline bdoes not change the variance of the total rewards\u2211T\nt=1rt. However, it is important\nto note that while introducing a baseline does not alter the o verall variance of the rewards, it helps reduce the variance\nof the gradient estimates. This is because subtracting the b aseline from the total rewards effectively reduces \ufb02uctuat ions\naround their mean, which makes the gradient estimates more s table. In general, the operation\u2211T\nt=1rt\u2212bcenters the\nrewards around zero (e.g., bis de\ufb01ned as the expected value of\u2211T\n",
    "a1f7e5626d631961": "1rt\u2212b.2Here, the baseline can be interpreted as a reference\npoint. By centering the rewards around this baseline, we rem ove systematic biases in the reward\nsignal, making the updates more stable and less sensitive to extreme \ufb02uctuations in individual\nrewards.\n2In fact, the use of a baseline bdoes not change the variance of the total rewards\u2211T\nt=1rt. However, it is important\nto note that while introducing a baseline does not alter the o verall variance of the rewards, it helps reduce the variance\nof the gradient estimates. This is because subtracting the b aseline from the total rewards effectively reduces \ufb02uctuat ions\naround their mean, which makes the gradient estimates more s table. In general, the operation\u2211T\nt=1rt\u2212bcenters the\nrewards around zero (e.g., bis de\ufb01ned as the expected value of\u2211T\nt=1rt), which can lead to reduced variance in the\nproduct\u2211T\nt=1log\u03c0\u03b8(at|st)(\u2211T\nt=1rt\u2212b).\n\n--- Page 185 ---\n\n178 Alignment\nThis policy gradient model with a baseline can be given by\n\u2202J(\u03b8)\n\u2202\u03b8=1\n|D|\u2211\n\u03c4\u2208D\u2202\n\u2202\u03b8(T\u2211\nt=1log\u03c0\u03b8(at|st))( T\u2211\nt=1rt\u2212b)\n=1\n|D|\u2211\n\u03c4\u2208D\u2202\n\u2202\u03b8[T\u2211\nt=1log\u03c0\u03b8(at|st)(T\u2211\nk=1rk\u2212b)]\n=1\n|D|\u2211\n\u03c4\u2208D\u2202\n\u2202\u03b8[T\u2211\nt=1log\u03c0\u03b8(at|st)(t\u22121\u2211\nk=1rk+T\u2211\nk=trk\u2212b)]\n(4.25)\nHere we write\u2211T\nk=1rkas the sum of two terms\u2211t\u22121\nk=1rkand\u2211T\nk=trkto distinguish between the\nrewards accrued before and after the action at time step t. Note that in Markov decision processes,\nthe future is independent of the past given the present. Ther efore, the action taken at time step t\ncannot in\ufb02uence the rewards received before t, or in other words, the rewards prior to tare already\n\u201c\ufb01xed\u201d by the time the action at tis chosen. The term\u2211t\u22121\nk=1rkdoes not contribute to the gradient\nand can be omitted, leading to a simpli\ufb01ed version of Eq. ( 4.25)\n\u2202J(\u03b8)\n\u2202\u03b8=1\n|D|\u2211\n\u03c4\u2208D\u2202\n\u2202\u03b8[T\u2211\nt=1log\u03c0\u03b8(at|st)(T\u2211\nk=trk\u2212b)]\n(4.26)\nAlso note that removing\u2211T\nk=trkcan further reduce the variance of the gradient.\nThere are many ways to de\ufb01ne the baseline b. Here we consider the value function of the state\nst, that is, the estimated value of being in state st:V(st) =E(rt+rt+1+\u00b7 \u00b7 \u00b7+rT). Hence we\nhave\nA(st,at) =T\u2211\nk=trk\u2212b\n=T\u2211\nk=trk\u2212V(st) (4.27)\nwhere\u2211T\nk=trkrepresents the actual return received, and V(st)represents the expected return.\nA(st,at)(orAtfor short) is called the advantage at time step t, which quanti\ufb01es the relative\nbene\ufb01t of the action atcompared to the expected value of following the policy from t he statest\nonward.\nBy using the advantage function A(st,at), the gradient of J(\u03b8)can be written in the form\n\u2202J(\u03b8)\n\u2202\u03b8=1\n|D|\u2211\n\u03c4\u2208D\u2202\n\u2202\u03b8(T\u2211\nt=1log\u03c0\u03b8(at|st)A(st,at))\n(4.28)\nThis optimization objective corresponds to the advantage actor-critic (A2C ) method in re-\ninforcement learning [ Mnih et al. ,2016 ]. In this method, the actor aims at learning a policy. It\nupdates the policy parameters using Eq. ( 4.28) to help focus more on actions that are likely to\nimprove performance. The critic, on the other hand, updates",
    "853c47e06a7c9f4e": " time step t, which quanti\ufb01es the relative\nbene\ufb01t of the action atcompared to the expected value of following the policy from t he statest\nonward.\nBy using the advantage function A(st,at), the gradient of J(\u03b8)can be written in the form\n\u2202J(\u03b8)\n\u2202\u03b8=1\n|D|\u2211\n\u03c4\u2208D\u2202\n\u2202\u03b8(T\u2211\nt=1log\u03c0\u03b8(at|st)A(st,at))\n(4.28)\nThis optimization objective corresponds to the advantage actor-critic (A2C ) method in re-\ninforcement learning [ Mnih et al. ,2016 ]. In this method, the actor aims at learning a policy. It\nupdates the policy parameters using Eq. ( 4.28) to help focus more on actions that are likely to\nimprove performance. The critic, on the other hand, updates its estimation of the value function,\nwhich is used to calculate the advantage function A(st,at), thus serving as the evaluator of the\n\n--- Page 186 ---\n\n4.3 Human Preference Alignment: RLHF 179\npolicy being learned by the actor.\nIn the A2C method, A(st,at)is typically expressed as the difference of the action-valu e func-\ntionQ(st,at)and the state-value function V(st)\nA(st,at) =Q(st,at)\u2212V(st) (4.29)\nAt \ufb01rst glance, this model may seem challenging to develop be cause it requires two separate sub-\nmodels to calculate Q(st,at)andV(st)respectively. Fortunately, considering that Q(st,at)can\nbe de\ufb01ned as the return rt+V(st+1), we can rewrite Eq. ( 4.29) as\nA(st,at) =rt+V(st+1)\u2212V(st) (4.30)\nor alternatively, introduce the discount factor \u03b3to obtain a more general form\nA(st,at) =rt+\u03b3V(st+1)\u2212V(st) (4.31)\nA(st,at) =rt+\u03b3V(st+1)\u2212V(st)is also called the temporal difference (TD) error. What\nwe need is to train a critic network for the value function V(st), and then use it to compute the\nadvantage function3.\nUp to this point, we have spent considerable space discussin g the basics of reinforcement\nlearning, especially on how to derive the optimization obje ctive for the A2C method. However,\nreinforcement learning is a vast \ufb01eld, and many technical de tails cannot be covered here. The in-\nterested reader can refer to reinforcement learning books f or more details [ Sutton and Barto ,2018 ;\nSzepesv\u00e1ri ,2010 ]. Nevertheless, we now have the necessary knowledge to furt her discuss RLHF.\nIn the subsequent subsections, we will return to the discuss ion on LLM alignment, demonstrating\nhow to use the A2C method for aligning with human preferences .\n4.3.2 Training Reward Models\nWe have shown that reward models play a very important role in the general reinforcement learn-\ning framework and form the basis for computing value functio ns. We now consider the problem\nof training these reward models.\nIn RLHF, a reward model is a neural network that maps a pair of i nput and output token\nsequences to a scalar. Given an input xand an output y, the reward can be expressed as\nr= Reward( x,y) (4.33)\nwhere Reward( \u00b7)is the reward model. rcan be interpreted as a measure of how well the output y\naligns with the desired behavior given the input x. As discussed in the previous subsection, both x\n3The training loss for the value network (or critic network) i n A2C is generally formulated as the mean squared\nerror between the computed return rt+\u03b3V(st+1)and the predicted state value V(st). Suppose that the value network\nis parameterized by \u03c9. The loss function is given by\nLv(\u03c9) =1\nM\u2211(\nrt+\u03b3V\u03c9(st+1)\u2212V\u03c9(st))2(4.32)\nwhereMis the number of training samples, for example, for a sequenc e ofTtokens, we can set M=T.\n\n--- Page 187 ---\n\n180 Alignment\nx0x1x2 \u00b7 \u00b7 \u00b7xmy1y2 \u00b7 \u00b7 \u00b7yn\n(Last Token \u27e8EOS \u27e9)hx0hx1hx2 \u00b7 \u00b7 \u00b7 hxm hy1hy2 \u00b7 \u00b7 \u00b7 hlast\nTransformer Decoder (LLM)Representation\nat Each PositionReward (Scalar",
    "e91cb5d73a707d5d": " subsection, both x\n3The training loss for the value network (or critic network) i n A2C is generally formulated as the mean squared\nerror between the computed return rt+\u03b3V(st+1)and the predicted state value V(st). Suppose that the value network\nis parameterized by \u03c9. The loss function is given by\nLv(\u03c9) =1\nM\u2211(\nrt+\u03b3V\u03c9(st+1)\u2212V\u03c9(st))2(4.32)\nwhereMis the number of training samples, for example, for a sequenc e ofTtokens, we can set M=T.\n\n--- Page 187 ---\n\n180 Alignment\nx0x1x2 \u00b7 \u00b7 \u00b7xmy1y2 \u00b7 \u00b7 \u00b7yn\n(Last Token \u27e8EOS \u27e9)hx0hx1hx2 \u00b7 \u00b7 \u00b7 hxm hy1hy2 \u00b7 \u00b7 \u00b7 hlast\nTransformer Decoder (LLM)Representation\nat Each PositionReward (Scalar)\nWr Linear Map\nFig. 4.8: Architecture of the reward model based on Transformer. The m ain component of this model is still an LLM.\nWe use the Transformer decoder as the sequence representati on model. We extract the representation of the last position\nof the decoder as the representation of the entire sequence [x,y]. We then map this representation to a scalar through a\nlinear transformation, which serves as the reward score for [x,y].\nandyare assumed to complete texts. This means that the reward mod el evaluates the relationship\nbetween inputs and outputs that provide full semantic conte nt. For example, when applying the\nreward model, it assigns a value of 0 (or another predetermin ed value) at each position tin the\noutput sequence y=y1...yn. Only at the \ufb01nal position, when t=n, does the reward model\ngenerate the actual reward score. To keep the notation unclu ttered, we will use r(x,y)to denote\nthe reward model Reward( x,y)from here on.\nThere are many ways to implement the reward model. One simple approach is to build the\nreward model based on a pre-trained LLM. More speci\ufb01cally, w e can concatenate xandyto form\na single token sequence seqx,y= [x,y]. We run a pre-trained LLM on this sequence, as usual,\nand at each position, we obtain a representation from the top -most Transformer layer. Then, we\ntake the representation at the last position (denoted by hlast) and map it to a scalar via linear\ntransformation:\nr(x,y) = hlastWr (4.34)\nwhere hlastis ad-dimensional vector, and Wris ad\u00d71linear mapping matrix. This architecture\nof the reward model is illustrated in Figure 4.8.\nTo train the reward model, the \ufb01rst step is to collect human fe edback on a set of generated\noutputs. Given an input x, we use the LLM to produce multiple candidate outputs {y1,...,yN}.\nHuman feedback can be obtained in several ways:\n\u2022Pairwise Comparison (Pairwise Ranking ). Given two different outputs, human experts\nselect which one is better.\n\u2022Rating . Human experts provide a score or rating to each output. This score is often a\ncontinuous or discrete numerical value, such as a score on a s cale (e.g., 1-5 stars, or 1-10\npoints). In some cases, the rating might be binary, indicati ng a \u201cyes/no\u201d or \u201cpositive/nega-\ntive\u201d preference.\n\n--- Page 188 ---\n\n4.3 Human Preference Alignment: RLHF 181\n\u2022Listwise Ranking . Human experts are asked to rank or order the given set of poss ible\noutputs.\nHere we consider pairwise comparison feedback as it is one of the simplest and most com-\nmon forms of human feedback used in RLHF. In this setting, eac h time, two outputs (ya,yb)are\nrandonly drawn from the candidate pool {y1,...,yN}. Human experts are then presented with\nthese pairs and asked to decide which output they prefer base d on speci\ufb01c criteria, such as clarity,\nrelevance, and accuracy. The human feedback can be encoded a s a binary label, ya\u227bybfor a\npreference for ya, and yb\u227byafor a preference for yb.\nOne simple and widely used model for describing such pairwis e comparisons is the Bradley-\nTerry model [Bradley and Terry ,1952 ]. It is a probabilistic model that estimates the probabilit y\nthat one item is preferred over another. Adapting this model to the notation used here, we can\nwrite the probability that yais preferred over ybin the form\nPr(y",
    "218447eb64eab097": " and most com-\nmon forms of human feedback used in RLHF. In this setting, eac h time, two outputs (ya,yb)are\nrandonly drawn from the candidate pool {y1,...,yN}. Human experts are then presented with\nthese pairs and asked to decide which output they prefer base d on speci\ufb01c criteria, such as clarity,\nrelevance, and accuracy. The human feedback can be encoded a s a binary label, ya\u227bybfor a\npreference for ya, and yb\u227byafor a preference for yb.\nOne simple and widely used model for describing such pairwis e comparisons is the Bradley-\nTerry model [Bradley and Terry ,1952 ]. It is a probabilistic model that estimates the probabilit y\nthat one item is preferred over another. Adapting this model to the notation used here, we can\nwrite the probability that yais preferred over ybin the form\nPr(ya\u227byb|x) =er(x,ya)\ner(x,ya)+er(x,yb)\n=er(x,ya)\u2212r(x,yb)\ner(x,ya)\u2212r(x,yb)+ 1\n= Sigmoid( r(x,ya)\u2212r(x,yb)) (4.35)\nWhen training the reward model, we want to maximize this pref erence probability. A loss\nfunction based on the Bradley-Terry model is given by\nLr(\u03c6) = \u2212E(x,ya,yb)\u223cD r[log Pr \u03c6(ya\u227byb|x)](4.36)\nwhere (x,ya,yb)is drawn from a human-annotated dataset Drconsisting of preference pairs of\noutputs and their corresponding inputs. \u03c6represents the parameters of the reward model, which\nincludes both the parameters of the Transformer decoder and the linear mapping matrix Wr. In\npractice, assuming (x,ya,yb)is uniformly sampled from Dr, we can replace the expectation with\na summation\nLr(\u03c6) = \u22121\n|Dr|\u2211\n(x,ya,yb)\u2208Drlog Pr \u03c6(ya\u227byb|x) (4.37)\nThe goal of training the reward model is to \ufb01nd the optimal par ameters \u02c6\u03c6that minimize this\nloss function, given by\n\u02c6\u03c6= arg min\n\u03c6Lr(\u03c6) (4.38)\nSince the reward model itself is also an LLM, we can directly r euse the Transformer training\nprocedure to optimize the reward model. The difference from training a standard LLM is that we\nonly need to replace the cross-entropy loss with the pairwis e comparison loss as described in Eq.\n(4.37). After the training of the reward model, we can apply the tra ined reward model r\u02c6\u03c6(\u00b7)to\nsupervise the target LLM for alignment.\nIt is worth noting that although we train the reward model to p erform pairwise ranking, we\napply it to score each input-output pair independently duri ng the alignment process. The pairwise\n\n--- Page 189 ---\n\n182 Alignment\nranking objective ensures that the reward model is sensitiv e to subtle differences between outputs,\nbut we rely on the continuous scores produced by the reward mo del to guide the optimization of\nthe LLM. An advantage of this approach is that we can choose fr om or combine various ranking\nloss functions, and still apply the resulting reward models in the same way as we have done in this\nsubsection. This consistency ensures a uni\ufb01ed framework fo r aligning the LLM, regardless of the\nspeci\ufb01c ranking loss used during reward model training.\n4.3.3 Training LLMs\nHaving obtained the reward model, we then train the policy (i .e., the LLM) via the A2C method.\nRecall from Section 4.3.1 that a state-action sequence or trajectory \u03c4can be evaluated by the utility\nfunction\nU(\u03c4;\u03b8) =T\u2211\nt=1log\u03c0\u03b8(at|st)A(st,at) (4.39)\nwhereA(st,at)is the advantage of taking the action atgiven the state st. An estimate of A(st,at)\nis de\ufb01ned as the TD error rt+\u03b3V(st+1)\u2212V(st), where the value function V(st)is trained with\nthe reward model.\nGiven this utility function, the A2C-based loss function ca n be written in the form\nL(\u03b8) = \u2212E\u03c4\u223cD[U(\u03c4;\u03b8)]\n=\u2212E\u03c4\u223cD[T\u2211\nt=1log\u03c0\u03b8(at|st)A(st,at)](4.40)\nwhere Dis",
    "17970069e7db756e": "LM) via the A2C method.\nRecall from Section 4.3.1 that a state-action sequence or trajectory \u03c4can be evaluated by the utility\nfunction\nU(\u03c4;\u03b8) =T\u2211\nt=1log\u03c0\u03b8(at|st)A(st,at) (4.39)\nwhereA(st,at)is the advantage of taking the action atgiven the state st. An estimate of A(st,at)\nis de\ufb01ned as the TD error rt+\u03b3V(st+1)\u2212V(st), where the value function V(st)is trained with\nthe reward model.\nGiven this utility function, the A2C-based loss function ca n be written in the form\nL(\u03b8) = \u2212E\u03c4\u223cD[U(\u03c4;\u03b8)]\n=\u2212E\u03c4\u223cD[T\u2211\nt=1log\u03c0\u03b8(at|st)A(st,at)](4.40)\nwhere Dis a space of state-action sequences. As usual, the goal of tr aining the policy is to\nminimize this loss function\n\u02dc\u03b8= arg min\n\u03b8L(\u03b8) (4.41)\nIf we map the problem back to the language modeling problem an d adopt the notation from\nLLMs, the loss function can be written as:\nL(\u03b8) = \u2212E(x,y)\u223cD[U(x,y;\u03b8)](4.42)\nwhere\nU(x,y;\u03b8) =T\u2211\nt=1log\u03c0\u03b8(yt|x,y<t)A(x,y<t,yt) (4.43)\nHere\u03c0\u03b8(yt|x,y<t) = Pr \u03b8(yt|x,y<t)is the LLM parameterized by \u03b8.\nIn general, we do not have a human annotated input-output dat asetDin RLHF, but a dataset\ncontaining inputs only. The outputs, in this case, are typic ally the predictions made by the LLM.\n\n--- Page 190 ---\n\n4.3 Human Preference Alignment: RLHF 183\nThe loss function is then de\ufb01ned as\nL(\u03b8) = \u2212Ex\u223cDEy\u223c\u03c0\u03b8(\u00b7|x)[U(x,y;\u03b8)](4.44)\nwhere Ddenotes the input-only dataset, and y\u223c\u03c0\u03b8(\u00b7|x)denotes that the output yis sampled by\nthe policy\u03c0\u03b8(\u00b7|x).\nThe above formulation provides a basic form of the A2C method for LLMs. Improved versions\nof this model are more commonly used in RLHF. In the following discussion, we will still use\nthe reinforcement learning notation to simplify the presen tation and will get back the language\nmodeling notation later.\nOne common improvement of policy gradient methods is to use importance sampling to\nre\ufb01ne the estimation of U(\u03c4;\u03b8). This can be written as\nU(\u03c4;\u03b8) =T\u2211\nt=1\u03c0\u03b8(at|st)\n\u03c0\u03b8ref(at|st)A(st,at) (4.45)\nHere we replace the log-probability log\u03c0\u03b8(at|st)with the ratio\u03c0\u03b8(at|st)\n\u03c0\u03b8ref(at|st).\u03b8refdenotes the pa-\nrameters of the previous policy (such as an initial model fro m which we start the training). So\n\u03c0\u03b8(at|st)\n\u03c0\u03b8ref(at|st), also called the ratio function , can be interpreted as the log-probability ratio between\nthe current policy \u03c0\u03b8and the previous policy \u03c0\u03b8ref(call it the reference policy). By using the\nratio function we reweight the observed rewards based on the likelihood of the actions under the\ncurrent policy versus the reference policy. When\u03c0\u03b8(at|st)\n\u03c0\u03b8ref(at|st)>1, the actionatis more favored by\nthe current policy compared to the reference policy. By cont rast, when\u03c0\u03b8(at|st)\n\u03c0\u03b8ref(at|st)<1, the action\natis less favored by the current policy4.\n4Consider a more general case where we wish to evaluate the pol icy using its expected reward (also see Eq. ( 4.18))\nJ(\u03b8) =E\u03c4\u223c\u03c0\u03b8[\nR(\u03c4)]\n(4.46)\nHere\u03c4\u223c\u03c0\u03b8means that the sequence \u03c4is generated by the policy \u03c0\u03b8. Alternatively, we can write J(\u03b8)in another form\nJ(\u03b8) =E\u03c4\u223c\u03c0\u03b8ref[Pr\u03b8(\u03c4)\nPr\u03b8ref(\u03c4)R(\u03c4)]\n(4.47)\nIt is not dif\ufb01cult to \ufb01nd that the right-hand sides of these eq uations are essentially the same since\nE\u03c4\u223c\u03c0\u03b8ref[\nPr\u03b8",
    "b6cadcb04b065ce1": " actionatis more favored by\nthe current policy compared to the reference policy. By cont rast, when\u03c0\u03b8(at|st)\n\u03c0\u03b8ref(at|st)<1, the action\natis less favored by the current policy4.\n4Consider a more general case where we wish to evaluate the pol icy using its expected reward (also see Eq. ( 4.18))\nJ(\u03b8) =E\u03c4\u223c\u03c0\u03b8[\nR(\u03c4)]\n(4.46)\nHere\u03c4\u223c\u03c0\u03b8means that the sequence \u03c4is generated by the policy \u03c0\u03b8. Alternatively, we can write J(\u03b8)in another form\nJ(\u03b8) =E\u03c4\u223c\u03c0\u03b8ref[Pr\u03b8(\u03c4)\nPr\u03b8ref(\u03c4)R(\u03c4)]\n(4.47)\nIt is not dif\ufb01cult to \ufb01nd that the right-hand sides of these eq uations are essentially the same since\nE\u03c4\u223c\u03c0\u03b8ref[\nPr\u03b8(\u03c4)\nPr\u03b8ref(\u03c4)R(\u03c4)]\n=\u2211\n\u03c4Pr\u03b8ref(\u03c4)Pr\u03b8(\u03c4)\nPr\u03b8ref(\u03c4)R(\u03c4) =\u2211\n\u03c4Pr\u03b8(\u03c4)R(\u03c4) =E\u03c4\u223c\u03c0\u03b8[\nR(\u03c4)]\nNote that this equivalence holds only when the expectation i s performed over the entire sequence space. In practice,\nhowever, we often only sample a relatively small number of se quences using a policy in policy learning. As a result,\nthe sampling method itself matters. Eq. ( 4.47) offers an interesting manner to separate the sampling and r eward\ncomputation processes: we \ufb01rst use a baseline policy (with \u03b8ref) to sample a number of sequences, and then use the\ntarget policy (with \u03b8) to compute the expected reward. In this way, we separate the policy used for collecting the data,\nand the policy used for computing the gradient. This approac h avoids the need to directly sample from the policy we are\nevaluating, which can be bene\ufb01cial in cases where generatin g sequences from the target policy is expensive or dif\ufb01cult.\nIn reinforcement learning, E\u03c4\u223c\u03c0\u03b8ref[\nPr\u03b8(\u03c4)\nPr\u03b8ref(\u03c4)R(\u03c4)]\nis often called a surrogate objective .\nEq. ( 4.47) can also be interpreted from a policy gradient perspective . ForE\u03c4\u223c\u03c0\u03b8ref[\nPr\u03b8(\u03c4)\nPr\u03b8ref(\u03c4)R(\u03c4)]\n, the gradient at\n\u03b8=\u03b8refis given by\n\u2202\n\u2202\u03b8E\u03c4\u223c\u03c0\u03b8ref[Pr\u03b8(\u03c4)\nPr\u03b8ref(\u03c4)R(\u03c4)]\u23d0\u23d0\u23d0\n\u03b8=\u03b8ref=E\u03c4\u223c\u03c0\u03b8ref[\u2202Pr\u03b8(\u03c4)|\u03b8=\u03b8ref\n\u2202\u03b8R(\u03c4)]\n(4.48)\nThe right-hand side is a standard form used in policy gradien t methods, meaning that we compute the direction of\n\n--- Page 191 ---\n\n184 Alignment\nA problem with the model presented in Eq. ( 4.47) (as well as in Eq. ( 4.39)) is that the\nvariance in the gradient estimates is often high, making the learning process unstable. To mitigate\nthis issue, techniques such as clipping are often employed t o bound the importance weights and\nprevent large updates. A clipped version of the utility func tion (also called the clipped surrogate\nobjective function) is given by\nUclip(\u03c4;\u03b8) =T\u2211\nt=1Clip(\u03c0\u03b8(at|st)\n\u03c0\u03b8ref(at|st))\nA(st,at) (4.49)\nClip(\u03c0\u03b8(at|st)\n\u03c0\u03b8ref(at|st))\n= min(\u03c0\u03b8(at|st)\n\u03c0\u03b8ref(at|st),bound(\u03c0\u03b8(at|st)\n\u03c0\u03b8ref(at|st),1\u2212\u01eb,1 +\u01eb))\n(4.50)\nHere the function bound(\u03c0\u03b8(at|st)\n\u03c0\u03b8ref(at|st),1\u2212\u01eb,1 +\u01eb)constrains the ratio function to the range [1\u2212\n\u01eb,1 +\u01eb].\nA further improvement to the above model is to consider trust regions in optimization [ Schulman et al. ,\n2015 ]. In reinforcement learning, a large update to the policy ca n lead to instability, where the\nagent may start performing worse after an update. A reasonab le idea is to optimize the model in\nthe trust region, which refers to a region around the current parameter estimate where the model\nis well-behaved. One approach to incorporating trust regio ns is to impose a constraint on the size\nof the policy",
    "1ffe3733f7742664": "\u03c0\u03b8(at|st)\n\u03c0\u03b8ref(at|st),bound(\u03c0\u03b8(at|st)\n\u03c0\u03b8ref(at|st),1\u2212\u01eb,1 +\u01eb))\n(4.50)\nHere the function bound(\u03c0\u03b8(at|st)\n\u03c0\u03b8ref(at|st),1\u2212\u01eb,1 +\u01eb)constrains the ratio function to the range [1\u2212\n\u01eb,1 +\u01eb].\nA further improvement to the above model is to consider trust regions in optimization [ Schulman et al. ,\n2015 ]. In reinforcement learning, a large update to the policy ca n lead to instability, where the\nagent may start performing worse after an update. A reasonab le idea is to optimize the model in\nthe trust region, which refers to a region around the current parameter estimate where the model\nis well-behaved. One approach to incorporating trust regio ns is to impose a constraint on the size\nof the policy update, ensuring that the current policy does n ot deviate too signi\ufb01cantly from the\nreference policy. This can be achieved by adding a penalty ba sed on some form of divergence\nbetween the current and reference policies to the objective function. A simple form of such a\npenalty is given by the difference in the log-probability of the sequence \u03c4under the current policy\nversus the reference policy:\nPanalty = log \u03c0\u03b8(\u03c4)\u2212log\u03c0\u03b8ref(\u03c4) (4.51)\nIn practice, this penalty can be approximated by considerin g only the policy probabilities and\nignoring the dynamics. This gives\nPenalty =T\u2211\nt=1log\u03c0\u03b8(at|st)\u2212T\u2211\nt=1log\u03c0\u03b8ref(at|st) (4.52)\nBy including this penalty in the optimization objective, we encourage the current policy to remain\nclose to the reference policy, limiting very large updates t hat could destabilize learning.\nWe can incorporate this penalty into the clipped surrogate o bjective function, and obtain\nUppo-clip(\u03c4;\u03b8) =Uclip(\u03c4;\u03b8)\u2212\u03b2Penalty (4.53)\nwhere\u03b2is the weight of the penalty. This training method is called proximal policy optimization\n(PPO ), which is one of the most popular reinforcement learning me thods used in LLMs and many\nother \ufb01elds [ Schulman et al. ,2017 ].\nNow we can write the objective of training LLMs in the form of P PO.\nthe parameter update at the point \u03b8=\u03b8refon the optimization surface.\n\n--- Page 192 ---\n\n4.3 Human Preference Alignment: RLHF 185\nU(x,y;\u03b8) =Uppo-clip(x,y;\u03b8)\u2212\u03b2Penalty (4.54)\nwhere\nUppo-clip(x,y;\u03b8) =T\u2211\nt=1Clip(\u03c0\u03b8(yt|x,y<t)\n\u03c0\u03b8ref(yt|x,y<t))\nA(x,y<t,yt) (4.55)\nPenalty = log Pr \u03b8(y|x)\u2212log Pr \u03b8ref(y|x)\n=T\u2211\nt=1log Pr \u03b8(yt|x,y<t)\u2212T\u2211\nt=1log Pr \u03b8ref(yt|x,y<t) (4.56)\nAlthough the notation here appears a bit tedious, the idea of PPO is simple: we develop an\nobjective by combining the clipped likelihood ratio of the t arget and reference policies with an\nadvantage function, and then impose a penalty that ensures p olicy updates are not too large. The\nPPO-based RLHF is illustrated in Figure 4.9.\nTo summarize, implementing RLHF requires building four mod els, all based on the Trans-\nformer decoder architecture.\n\u2022Reward Model (r\u03c6(\u00b7)where\u03c6denotes the parameters). The reward model learns from\nhuman preference data to predict the reward for each pair of i nput and output token se-\nquences. It is a Transformer decoder followed by a linear lay er that maps a sequence (the\nconcatenation of the input and output) to a real-valued rewa rd score.\n\u2022Value Model orValue Function (V\u03c9(\u00b7)where\u03c9denotes the parameters). The value func-\ntion receives reward scores from the reward model and is trai ned to predict the expected\nsum of rewards that can be obtained starting from a state. It i s generally based on the same\narchitecture as the reward model.\n\u2022Reference Model (\u03c0\u03b8ref(\u00b7) = Pr \u03b8ref(\u00b7)where\u03b8refdenotes the parameters). The reference\nmodel is the baseline LLM that serves as a starting point for p olicy training. In RLHF, it\nrepresents the previous",
    "fccd271726b5dea2": "(\u00b7)where\u03c6denotes the parameters). The reward model learns from\nhuman preference data to predict the reward for each pair of i nput and output token se-\nquences. It is a Transformer decoder followed by a linear lay er that maps a sequence (the\nconcatenation of the input and output) to a real-valued rewa rd score.\n\u2022Value Model orValue Function (V\u03c9(\u00b7)where\u03c9denotes the parameters). The value func-\ntion receives reward scores from the reward model and is trai ned to predict the expected\nsum of rewards that can be obtained starting from a state. It i s generally based on the same\narchitecture as the reward model.\n\u2022Reference Model (\u03c0\u03b8ref(\u00b7) = Pr \u03b8ref(\u00b7)where\u03b8refdenotes the parameters). The reference\nmodel is the baseline LLM that serves as a starting point for p olicy training. In RLHF, it\nrepresents the previous version of the model or a model train ed without human feedback. It\nis used to perform sampling over the space of outputs and cont ribute to the loss computation\nfor policy training.\n\u2022Target Model orPolicy (\u03c0\u03b8(\u00b7) = Pr \u03b8(\u00b7)where\u03b8denotes the parameters). This policy\ngoverns how the LLM decides the most appropriate next token g iven its context. It is trained\nunder the supervision of both the reward model and the value m odel.\nIn practice, these models need to be trained in a certain orde r. First, we need to initialize them\nusing some other models. For example, the reward model and th e value model can be initialized\nwith a pre-trained LLM, while the reference model and the tar get model can be initialized with a\nmodel that has been instruction \ufb01ne-tuned. Note that, at thi s point, the reference model is ready for\nuse and will not be further updated. Second, we need to collec t human preference data and train the\nreward model on this data. Third, both the value model and the policy are trained simultaneously\nusing the reward model. At each position in an output token se quence, we update the value model\n\n--- Page 193 ---\n\n186 Alignment\nReward Model\nTraining\nPolicy TrainingReward Model\nr\u03c6(x,y)To Learn\nLLM Policy\nPr\u03b8(yt|x,y<t)To Learn\nValue Function\nV\u03c9(x,y<t)To Learn\nRef Model\nPr\u03b8old(yt|x,y<t)FixedMinimizing the loss based on\nthe Bradley-Terry model\nmin\n\u03c6\u22121\n|Dr|\u2211\n(x,ya,yb)\u2208D rlog\u03c3(r\u03c6(x,ya)\u2212r\u03c6(x,yb))Human preference data Dr={(x,ya,yb)}\nInput-only data D={x}x0 x1 \u00b7 \u00b7 \u00b7 xm y1 \u00b7 \u00b7 \u00b7 yt\u22121\nState (x,y<t)x1 x2 \u00b7 \u00b7 \u00b7 y1 y2 \u00b7 \u00b7 \u00b7 ytAction yt\n(sampled with Pr\u03b8old)Evaluate the state-action pair using the advantage\nfunction or the TD error (based on the reward\nmodel and the value function)\nLLM Policy\nLLM Policy\nMinimizing the clipped PPO loss\nwith the penalty\nmin\n\u03b8\u2212\u2211\nx\u2208D,y\u223cPr\u03b8old(\u00b7|x)\u2211T\nt=1[\nClip(Pr\u03b8(yt|x,y<t)\nPr\u03b8old(yt|x,y<t))\nAt\u2212\n\u03b2\u00b7(\nlog Pr \u03b8(yt|x,y<t)\u2212\nlog Pr \u03b8old(yt|x,y<t))]Value Function\nMinimizing the MSE between the\ncomputed return and the predicted\nstate value\nmin\n\u03c91\nM\u2211\nx\u2208D\u2211T\nt=1\n[\nrt+\u03b3V\u03c9(x,y<t+1)\u2212V\u03c9(x,y<t)]2\n\u2217\u2217rt=r(x,y<t+1)denotes the reward received as step t.\n\u2217\u2217Atdenotes the advantage at step t, and can be de\ufb01ned as rt+\u03b3V\u03c9(x,y<t+1)\u2212V\u03c9(x,y<t)\nFig. 4.9: Illustration of RLHF. The \ufb01rst step is to collect human prefe rence data and train the reward model using this\ndata. Once the reward model is optimized, along with the refe rence model, we proceed to train both the policy and\nthe value function. At each prediction step, we compute the s um of the PPO-based loss and update the parameters of\nthe policy. This requires access to the reward model, the ref erence model, and the value function",
    "7999205516327413": "D\u2211T\nt=1\n[\nrt+\u03b3V\u03c9(x,y<t+1)\u2212V\u03c9(x,y<t)]2\n\u2217\u2217rt=r(x,y<t+1)denotes the reward received as step t.\n\u2217\u2217Atdenotes the advantage at step t, and can be de\ufb01ned as rt+\u03b3V\u03c9(x,y<t+1)\u2212V\u03c9(x,y<t)\nFig. 4.9: Illustration of RLHF. The \ufb01rst step is to collect human prefe rence data and train the reward model using this\ndata. Once the reward model is optimized, along with the refe rence model, we proceed to train both the policy and\nthe value function. At each prediction step, we compute the s um of the PPO-based loss and update the parameters of\nthe policy. This requires access to the reward model, the ref erence model, and the value function at hand. At the same\ntime, we update the parameters of the value function by minim izing the MSE loss.\nby minimizing the MSE error of value predition, and the polic y is updated by minimizing the PPO\nloss.\n\n--- Page 194 ---\n\n4.4 Improved Human Preference Alignment 187\n4.4 Improved Human Preference Alignment\nIn the previous section, we reviewed the basic concepts of re inforcement learning and the general\nframework of RLHF. In this section, we will discuss some re\ufb01n ements of RLHF and alternative\nmethods to achieve human preference alignment.\n4.4.1 Better Reward Modeling\nIn Section 4.3.2 , we highlighted the task of learning from human preferences as well as the use\nof pairwise ranking loss for training reward models. Here we consider more methods for reward\nmodeling. Our discussion will be relatively general, and si nce the reward model is widely used in\nmany reinforcement learning problems, it will be easy for us to apply the methods discussed here\nto RLHF and related applications.\n4.4.1.1 Supervision Signals\nThe training of reward models can broadly be seen as a ranking problem, where the model learns\nto assign scores to outputs so that their order re\ufb02ects the pr eferences indicated by humans. There\nare several methods to train a reward model from the perspect ive of ranking.\nOne approach is to extend pairwise ranking to listwise ranki ng. For each sample in a dataset,\nwe can use the LLM to generate multiple outputs, and ask human experts to order these outputs.\nFor example, given a set of four outputs {y1,y2,y3,y4}, one possible order of them can be\ny2\u227by3\u227by1\u227by4. A very simple method to model the ordering of the list is to ac cumulate the\npairwise comparison loss. For example, we can de\ufb01ne the list wise loss by accumulating the loss\nover all pairs of outputs:\nLlist=\u2212E(x,Y)\u223cD r[1\nN(N\u22121)\u2211\nya\u2208Y,yb\u2208Y\nya\u0338=yblog Pr( ya\u227byb|x)]\n(4.57)\nwhereYis a list of outputs, and Nis the number of outputs in the list. Pr(ya\u227byb|x)can be\nde\ufb01ned using the Bradley-Terry model, that is, Pr(ya\u227byb|x) = Sigmoid( r(x,ya)\u2212r(x,yb)).\nHere we omit the \u03c6superscript on the Pr(\u00b7)to keep the notation uncluttered.\nAn extension to the Bradley-Terry model for listwise rankin g could involve a ranking mecha-\nnism that takes into account the entire list of outputs rathe r than just pairwise comparisons. One\nsuch model is the Plackett-Luce model , which generalizes the Bradley-Terry model to handle\nmultiple items in a ranking [ Plackett ,1975 ]. In the Plackett-Luce model, for each item in a list,\nwe de\ufb01ne a \u201cworth\u201d for this item that re\ufb02ects its relative str ength of being chosen over other items.\nFor the reward modeling problem here, the worth of yin the listYcan be de\ufb01ned as\n\u03b1(y) = exp( r(x,y)) (4.58)\n\n--- Page 195 ---\n\n188 Alignment\nThen the probability of selecting yfromYis given by\nPr(yis selected |x,Y) =\u03b1(y)\u2211\ny\u2032\u2208Y\u03b1(y\u2032)\n=exp(r(x,y))\u2211\ny\u2032\u2208Yexp(r(x,y\u2032))(4.59)\nSuppose \u02da",
    "8311b5f9531399ff": " just pairwise comparisons. One\nsuch model is the Plackett-Luce model , which generalizes the Bradley-Terry model to handle\nmultiple items in a ranking [ Plackett ,1975 ]. In the Plackett-Luce model, for each item in a list,\nwe de\ufb01ne a \u201cworth\u201d for this item that re\ufb02ects its relative str ength of being chosen over other items.\nFor the reward modeling problem here, the worth of yin the listYcan be de\ufb01ned as\n\u03b1(y) = exp( r(x,y)) (4.58)\n\n--- Page 195 ---\n\n188 Alignment\nThen the probability of selecting yfromYis given by\nPr(yis selected |x,Y) =\u03b1(y)\u2211\ny\u2032\u2208Y\u03b1(y\u2032)\n=exp(r(x,y))\u2211\ny\u2032\u2208Yexp(r(x,y\u2032))(4.59)\nSuppose \u02daYis an ordered list yj1\u227byj2\u227b \u00b7 \u00b7 \u00b7 \u227b yjN. The overall log-probability of this\nordered list can be de\ufb01ned as the sum of the conditional log-p robabilities at each stage of selection,\ngiven by\nlog Pr( \u02daY|x) = log Pr( yj1\u227byj2\u227b \u00b7 \u00b7 \u00b7 \u227b yjN|x)\n= log Pr( yj1|x,{yj1,yj2,...,yjN}) +\nlog Pr( yj2|x,{yj2,...,yjN}) +\n\u00b7 \u00b7 \u00b7+\nlog Pr( yjN|x,{yjN})\n=N\u2211\nk=1log Pr( yjk|x,\u02daY\u2265k) (4.60)\nwhere \u02daY\u2265krepresents the subset of the list of outputs that remain unse lected at the k-th stage, i.e.,\n\u02daY\u2265k={yjk,...,yjN}. Given the log-probability log Pr( \u02daY|x), we can de\ufb01ne the loss function\nbased on the Plackett-Luce model by\nLpl=\u2212E(x,\u02daY)\u223cD r[log Pr( \u02daY|x)](4.61)\nThere are also many other pairwise and listwise methods for m odeling rankings, such as\nRankNet [ Burges et al. ,2005 ] and ListNet [ Cao et al. ,2007 ]. All these methods can be cate-\ngorized into a large family of learning-to-rank approaches , and most of them are applicable to the\nproblem of modeling human preferences. However, discussin g these methods is beyond the scope\nof this chapter. Interested readers can refer to books on thi s topic for more details [ Liu,2009 ;Li,\n2011 ].\nIn addition to pairwise and listwise ranking, using pointwi se methods to train reward models\noffers an alternative way to capture human preferences. Unl ike methods that focus on the relative\nrankings between different outputs, pointwise methods tre at each output independently. For ex-\nample, human experts might assign a score to an individual ou tput, such as a rating on a \ufb01ve-point\nscale. The objective is to adjust the reward model so that its outputs align with these scores. A\nsimple way to achieve pointwise training is through regress ion techniques where the reward of\neach output is treated as a target variable. Let \u03d5(x,y)be the score assigned to ygiven xby\nhumans. Pointwise reward models can be trained by minimizin g a loss function, often based on\nmean squared error or other regression losses, between the p redicted reward r(x,y)and the actual\nhuman feedback \u03d5(x,y). For example, the loss function could be\nLpoint =\u2212E[\u03d5(x,y)\u2212r(x,y)]2(4.62)\nWhile pointwise methods are conceptually simpler and can di rectly guide the reward model to\n\n--- Page 196 ---\n\n4.4 Improved Human Preference Alignment 189\npredict scores, they might not always be the best choice in RL HF. A problem is that these methods\nmay struggle with high variance in human feedback, especial ly when different experts provide\ninconsistent scores for similar outputs. Because they focu s on \ufb01tting to absolute scores rather than\nrelative differences, inconsistencies in scoring can lead to poor model performance. Moreover,\n\ufb01tting to speci\ufb01c scored outputs might discourage generali zation, particularly given that training\ndata is often very limited in RLHF. In contrast, methods that consider relative preferences can\npromote the learning of more generalized patterns of",
    "c19d420ce46bf814": " feedback \u03d5(x,y). For example, the loss function could be\nLpoint =\u2212E[\u03d5(x,y)\u2212r(x,y)]2(4.62)\nWhile pointwise methods are conceptually simpler and can di rectly guide the reward model to\n\n--- Page 196 ---\n\n4.4 Improved Human Preference Alignment 189\npredict scores, they might not always be the best choice in RL HF. A problem is that these methods\nmay struggle with high variance in human feedback, especial ly when different experts provide\ninconsistent scores for similar outputs. Because they focu s on \ufb01tting to absolute scores rather than\nrelative differences, inconsistencies in scoring can lead to poor model performance. Moreover,\n\ufb01tting to speci\ufb01c scored outputs might discourage generali zation, particularly given that training\ndata is often very limited in RLHF. In contrast, methods that consider relative preferences can\npromote the learning of more generalized patterns of succes s and failure. Nevertheless, there are\nscenarios where pointwise methods might still be suitable. For example, in tasks where training\ndata is abundant and the costs of obtaining accurate, consis tent annotations are low, pointwise\nmethods can prove effective.\nIn fact, to make the supervision signal for training the rewa rd model more robust, we can also\nintroduce additional regularization terms into training. For example, if we consider the \ufb01rst term\nUppo-clip(x,y;\u03b8)in Eq. ( 4.54) as a type of generalized reward, then the second term (i.e., the\npenalty term) can be viewed as a form of regularization for th e reward model, except that here the\ngoal is to train the policy rather than the reward model. Anot her example is that Eisenstein et al.\n[2023 ] develop a regularization term based on the squared sum of re wards, and add it to the\npairwise comparison loss in RLHF:\nLreg=Lpair+ (\u2212E(x,ya,yb)\u223cD r[r(x,ya) +r(x,yb)]2)\n=\u2212E(x,ya,yb)\u223cD r[log Pr \u03c6(ya\u227byb|x)]\n\u2212E(x,ya,yb)\u223cD r[r(x,ya) +r(x,yb)]2(4.63)\nOptimizing with this regularization term can help mitigate the underdetermination of reward mod-\nels5.\n4.4.1.2 Sparse Rewards vs. Dense Rewards\nAs discussed in Section 4.3, the rewards in RLHF are very sparse: they are observed only a t the\nend of sequences, rather than continuously throughout the g eneration process. Dealing with sparse\nrewards has long been a concern in reinforcement learning, a nd has been one of the challenges in\nmany practical applications. For example, in robotics, it o ften needs to shape the reward function\nto ease optimization rather than relying solely on end-of-s equence rewards. Various methods\nhave been developed to address this issue. One common approa ch is reward shaping, where the\noriginal function is modi\ufb01ed to include intermediate rewar ds, thereby providing more immediate\nfeedback. Also, one can adopt curriculum learning to sequen tially structure tasks in a way that the\ncomplexity gradually increases. This can help models to mas ter simpler tasks \ufb01rst, which prepares\nthem for more complex challenges as their skills develop. Th ere are many such methods that can\nmitigate the impact of sparse rewards, such as Monte Carlo me thods and intrinsic motivation. Most\nof these methods are general and the discussion of them can be found in the broader literature on\nreinforcement learning, such as Sutton and Barto [2018 ]\u2019s book.\nAlthough we do not discuss methods for mitigating sparse rew ards in detail here, an interesting\nquestion arises: why are sparse rewards so successful in RLH F? Recall from Section 4.3.1 that\nthe supervision signal received at each time step tis not the reward for the current action, but\n5A model is called underdetermined if there are multiple alte rnative sets of parameters that can achieve the same\nobjective.\n\n--- Page 197 ---\n\n190 Alignment\nrather some form of the accumulated rewards from tuntil the last time step. Such supervision\nsignals are dense over the sequence, because the reward obta ined at the end of the sequence can\nbe transferred back to that time step, regardless of which ti me step it is. In other words, the sparse\nrewards are transformed into the dense supervision signals . Furthermore, from the perspective of\nreward shaping, Ng et al. [1999 ] show that the reward at tcan be de\ufb01ned as\nr",
    "38b3e1293c27b1e8": " mitigating sparse rew ards in detail here, an interesting\nquestion arises: why are sparse rewards so successful in RLH F? Recall from Section 4.3.1 that\nthe supervision signal received at each time step tis not the reward for the current action, but\n5A model is called underdetermined if there are multiple alte rnative sets of parameters that can achieve the same\nobjective.\n\n--- Page 197 ---\n\n190 Alignment\nrather some form of the accumulated rewards from tuntil the last time step. Such supervision\nsignals are dense over the sequence, because the reward obta ined at the end of the sequence can\nbe transferred back to that time step, regardless of which ti me step it is. In other words, the sparse\nrewards are transformed into the dense supervision signals . Furthermore, from the perspective of\nreward shaping, Ng et al. [1999 ] show that the reward at tcan be de\ufb01ned as\nr\u2032(st,at,st+1) =r(st,at,st+1) +f(st,at,st+1) (4.64)\nwherer\u2032(\u00b7)is the transformed reward function, r(\u00b7)is the original reward function, and f(\u00b7)is\nthe shaping reward function. To ensure the optimality of the policy under the transformed reward\nfunction, the shaping reward function can be given in the for m\nf(st,at,st+1) =\u03b3\u03a6(st+1)\u2212\u03a6(st) (4.65)\nwhere \u03a6(s)is called the potential value of the state s. If we de\ufb01ne \u03a6(s)as the common value\nfunction as in Eq. ( 4.15) and substitute Eq. ( 4.65) into Eq. ( 4.64), we obtain\nr\u2032(st,at,st+1) =r(st,at,st+1) +\u03b3V(st+1)\u2212V(st) (4.66)\nIt is interesting to see that this function is exactly the sam e as the advantage function used in PPO.\nThis relates advantage-based methods to reward shaping: th e advantage is essentially a shaped\nreward.\nOn the other hand, one of the reasons for adopting end-of-seq uence rewards lies in the nature\nof the RLHF tasks. Unlike traditional reinforcement learni ng environments where the agent in-\nteracts with a dynamic environment, RLHF tasks often involv e complex decision-making based\non linguistic or other high-level cognitive processes. The se processes do not lend themselves eas-\nily to frequent and meaningful intermediate rewards becaus e the quality and appropriateness of\nthe actions can only be fully evaluated after observing thei r impact in the larger context of the\nentire sequence or task. In this case, the reward signals bas ed on human feedback, though very\nsparse, are typically very informative and accurate. Conse quently, this sparsity, together with the\nhigh informativeness and accuracy of the human feedback, ca n make the learning both robust and\nef\ufb01cient.\n4.4.1.3 Fine-grained Rewards\nFor many applications, our objective will be more complex th an merely evaluating an entire text.\nFor example, in sentiment analysis, we often do not just dete rmine the sentiment of a text, but need\nto analyze the sentiment in more detail by associating it wit h speci\ufb01c aspects of a topic discussed\nin the text. Consider the sentence \"The camera of the phone is excellent, but the battery life is\ndisappointing.\" In this example, we would need to separatel y analyze the sentiments expressed\nabout the camera and the battery. Such analysis, known as asp ect-based sentiment analysis, helps\nprovide a \ufb01ner-grained understanding of the customer revie w compared to general sentiment anal-\nysis.\nFor the problem of reward modeling, we often need to model dif ferent parts of a sequence as\nwell. A simple and straightforward way to do this is to divide a sequence into different segments\nand then compute the reward for each segment [ Wu et al. ,2023 ]. Suppose that an output token\n\n--- Page 198 ---\n\n4.4 Improved Human Preference Alignment 191\nsequence ycan be divided into nssegments {\u00afy1,...,\u00afyns}by some criterion. We can use the\nreward model to evaluate each of these segments. By taking x,yand\u00afykas input to the reward\nmodel, the reward score for the k-th segment is given by\nrk=r(x,y,\u00afyk) (4.67)\nThen the reward score for the entire output sequence is given by\nr(x,y) =ns\u2211\nk=1r(x,y,\u00afyk) (4.68)\nHerer",
    "2b555c6848895566": " general sentiment anal-\nysis.\nFor the problem of reward modeling, we often need to model dif ferent parts of a sequence as\nwell. A simple and straightforward way to do this is to divide a sequence into different segments\nand then compute the reward for each segment [ Wu et al. ,2023 ]. Suppose that an output token\n\n--- Page 198 ---\n\n4.4 Improved Human Preference Alignment 191\nsequence ycan be divided into nssegments {\u00afy1,...,\u00afyns}by some criterion. We can use the\nreward model to evaluate each of these segments. By taking x,yand\u00afykas input to the reward\nmodel, the reward score for the k-th segment is given by\nrk=r(x,y,\u00afyk) (4.67)\nThen the reward score for the entire output sequence is given by\nr(x,y) =ns\u2211\nk=1r(x,y,\u00afyk) (4.68)\nHerer(x,y)can be used to train the policy as usual.\nA problem with this model is that training reward models at th e segment level is not as straight-\nforward as learning from human preferences on entire texts, as it is dif\ufb01cult to obtain segment-level\nhuman preference data. For rating-like problems (e.g., we r ate a segment according to its level\nof misinformation), one simple approach is to assign a ratin g score to each segment and train the\nreward model using pointwise methods. For example, we can us e a strong LLM to rate the se-\nquences \u00afy1...\u00afyk\u22121and\u00afy1...\u00afyk, and obtain the scores s(\u00afy1...\u00afyk\u22121)ands(\u00afy1...\u00afyk). We can then\nde\ufb01ne the score of the segment \u00afykas the difference between s(\u00afy1...\u00afyk)ands(\u00afy1...\u00afyk\u22121)\ns(\u00afyk) =s(\u00afy1...\u00afyk)\u2212s(\u00afy1...\u00afyk\u22121) (4.69)\nUsing these segment-level scores, we can train the reward mo del with a regression loss func-\ntion\nLrating =\u2212E\u00afyk[s(\u00afyk)\u2212r(x,y,\u00afyk)]2(4.70)\nSometimes, alignment can be treated as a classi\ufb01cation prob lem, for example, we assess\nwhether a segment has ethical issues. In this case, the segme nt can be labeled as ethical or\nunethical, either by humans or using additional classi\ufb01ers . Given the label of the segment, we\ncan train the reward model using some classi\ufb01cation loss fun ction. For example, suppose that\nr(x,y,\u00afyk) = 1 if the segment is classi\ufb01ed as unethical, and r(x,y,\u00afyk) =\u22121otherwise6. The\nhinge loss of training binary classi\ufb01cation models is given by\nLhinge = max(0,1\u2212r(x,y,\u00afyk)\u00b7\u02c6r) (4.71)\nwhere \u02c6r\u2208 {1,\u22121}denotes the ground truth label.\nThe remaining issue here is how to split yinto segments. One approach is to de\ufb01ne a \ufb01xed-\nlength segmentation, where yis divided into equal-length chunks. However, this may not a lways\nbe ideal, as the content of the sequence may not align well wit h \ufb01xed boundaries. An alternative\napproach is to segment ybased on speci\ufb01c linguistic or semantic cues, such as senten ce bound-\naries, topic shifts, or other meaningful structures in the t ext. Such a segmentation can be achieved\nby using linguistic segmentation systems or prompting LLMs to identify natural breaks in the se-\nquence. Another approach is to use dynamic segmentation met hods based on the complexity of\n6To allow the reward model to output categories, we can replac e the linear layer described in Section 4.3.2 with a\nSoftmax layer.\n\n--- Page 199 ---\n\n192 Alignment\nthe sequence. For example, segments could be de\ufb01ned where th ere is a signi\ufb01cant change in the\nreward score, which might correspond to shifts in the task be ing modeled.\n4.4.1.4 Combination of Reward Models\nA reward model can be viewed as a proxy for the environment. Si nce the true environment is often\ntoo complex or unknown, developing a perfect proxy for the en vironment is generally not possible.\nAs a result, over-aligning LLMs with this imperfect proxy mi ght lead to decreased performance,\nknown as the overoptimization problem [Stiennon et al.",
    "daac6181c47adf31": " identify natural breaks in the se-\nquence. Another approach is to use dynamic segmentation met hods based on the complexity of\n6To allow the reward model to output categories, we can replac e the linear layer described in Section 4.3.2 with a\nSoftmax layer.\n\n--- Page 199 ---\n\n192 Alignment\nthe sequence. For example, segments could be de\ufb01ned where th ere is a signi\ufb01cant change in the\nreward score, which might correspond to shifts in the task be ing modeled.\n4.4.1.4 Combination of Reward Models\nA reward model can be viewed as a proxy for the environment. Si nce the true environment is often\ntoo complex or unknown, developing a perfect proxy for the en vironment is generally not possible.\nAs a result, over-aligning LLMs with this imperfect proxy mi ght lead to decreased performance,\nknown as the overoptimization problem [Stiennon et al. ,2020 ;Gao et al. ,2023a ]7. We can also\nexplain this through Goodhart\u2019s law, which states: when a measure becomes a target, it ceases to\nbe a good measure [Goodhart ,1984 ].\nAddressing the overoptimization problem is not easy, and th ere is no mature solution yet. The\nideal approach might be to develop an oracle reward model tha t can perfectly capture the true\nobjectives of the task and prevent the agent from \u201ctricking\u201d . However, creating such a model is\nextremely dif\ufb01cult due to the complexity of the real-world e nvironment, as well as the challenge\nof de\ufb01ning all the relevant factors that contribute to the de sired outcome. Instead, a more practical\napproach is to combine multiple reward models, thereby alle viating the misalignment between\nthe training objective and the true objective that arises fr om using a single, speci\ufb01c reward model\n[Coste et al. ,2024 ].\nGiven a set of reward models, combining them is straightforw ard, and in some cases, we can\nsimply treat this problem as an ensemble learning problem. A simple yet common approach is to\naverage the outputs of these models to obtain a more precise r eward estimation:\nrcombine =1\nNK\u2211\nk=1wk\u00b7rk(x,y) (4.72)\nwhererk(\u00b7)is thek-th reward model in the ensemble, wkis the weight of rk(\u00b7), andKis the\nnumber of reward models. This combined reward can then be use d to supervise the training of\na policy. In fact, there are many ways to combine different mo dels, for example, one can make\npredictions using Bayesian model averaging or develop a fus ion network to learn to combine the\npredictions from different models. Alternatively, one can frame this task as a multi-objective\noptimization problem, and use multiple reward models to tra in the policy simultaneously. These\nmethods have been intensively discussed in the literature o n optimization and machine learning\n[Miettinen ,1999 ;Bishop ,2006 ].\nIn addition to model combination methods, another importan t issue is how to collect or con-\nstruct multiple different reward models. One of the simples t approaches is to employ ensemble\nlearning techniques, such as developing diverse reward mod els from different subsets of a given\ndataset or from various data sources. For RLHF, it is also pos sible to construct reward models\nbased on considerations of different aspects of alignment. For example, we can develop a reward\nmodel to evaluate the factual accuracy of the output and anot her reward model to evaluate the\n7This problem is also called reward hacking orreward gaming [Krakovna et al. ,2020 ;Skalse et al. ,2022 ;\nPan et al. ,2022 ], which refers to the phenomenon where the agent attempts to trick the reward model but fails to\nalign its actions with the true intended objectives of the ta sk. Imagine a student who is assigned homework and is re-\nwarded with points or praise for completing it. The student m ight then \ufb01nd ways to \ufb01nish the homework with minimal\neffort to maximize the reward, such as copying and pasting so lutions from the internet or previous assignments, rather\nthan solving the problems themselves.\n\n--- Page 200 ---\n\n4.4 Improved Human Preference Alignment 193\nya\u227bybPreference\nData\nReward ModelValue Function\nPolicytraining with MLETraining\nwith PPO\n(a) RLHF (PPO)\nya\u227bybPreference\nData\nPolicytraining with MLE\n(b) DPO\nFig. 4.10: Standard RLHF (PPO) vs. DPO. In RLHF, the",
    "d4439b723d5110f6": " et al. ,2022 ], which refers to the phenomenon where the agent attempts to trick the reward model but fails to\nalign its actions with the true intended objectives of the ta sk. Imagine a student who is assigned homework and is re-\nwarded with points or praise for completing it. The student m ight then \ufb01nd ways to \ufb01nish the homework with minimal\neffort to maximize the reward, such as copying and pasting so lutions from the internet or previous assignments, rather\nthan solving the problems themselves.\n\n--- Page 200 ---\n\n4.4 Improved Human Preference Alignment 193\nya\u227bybPreference\nData\nReward ModelValue Function\nPolicytraining with MLETraining\nwith PPO\n(a) RLHF (PPO)\nya\u227bybPreference\nData\nPolicytraining with MLE\n(b) DPO\nFig. 4.10: Standard RLHF (PPO) vs. DPO. In RLHF, the human preference da ta is used to train a reward model, which\nis then employed in training the policy as well as the value fu nction. In DPO, the use of human preference data is more\ndirect, and the policy is trained on this data without the nee d for reward model training.\ncompleteness of the output. These two models are complement ary to each other, and can be com-\nbined to improve the overall evaluation of the output. Anoth er approach is to employ different\noff-the-shelf LLMs as reward models. This approach is simpl e and practical, as there have been\na lot of well-developed LLMs and we just need to use them with n o or little modi\ufb01cation. An\ninteresting issue, though not closely related to the discus sion here, arises: can an LLM that aligns\nwith other LLMs outperform those LLMs? Probably not at \ufb01rst g lance. In part, this is because\nthe target LLM merely imitates other LLMs based on limited su pervision and thus cannot capture\nwell the nuances of the behaviors of these supervisors. Howe ver, given the strong generalization\nability of LLMs, this approach can, in fact, be quite bene\ufb01ci al. For example, using open-sourced\nor commercial LLMs as reward models has demonstrated strong performance in aligning LLMs,\neven achieving state-of-the-art results on several popula r tasks [ Lambert et al. ,2024 ].\n4.4.2 Direct Preference Optimization\nAlthough learning reward models is a standard step in reinfo rcement learning, it makes the entire\ntraining process much more complex than supervised trainin g. Training a reliable reward model\nis itself not an easy task, and a poorly trained reward model c an greatly affect the outcome of\npolicy learning. We now consider an alternative alignment m ethod, called direct preference op-\ntimization (DPO ), which simpli\ufb01es the training framework by eliminating th e need to explicitly\nmodel rewards [ Rafailov et al. ,2024 ]. This method directly optimizes the policy based on user\npreferences, rather than developing a separate reward mode l. As a result, we can achieve human\npreference alignment in a supervised learning-like fashio n. Figure 4.10 shows a comparison of\nthe standard RLHF method and the DPO method.\nBefore deriving the DPO objective, let us \ufb01rst review the obj ective of policy training used in\nRLHF. As discussed in Section 4.3.3 , the policy is typically trained by optimizing a loss functi on\n\n--- Page 201 ---\n\n194 Alignment\nwith a penalty term. The DPO method assumes a simple loss func tion where the quality of the\noutput ygiven the input xis evaluated by the reward model r(x,y). The training objective is thus\ngiven by\n\u02dc\u03b8= arg min\n\u03b8Ex\u223cDEy\u223c\u03c0\u03b8(\u00b7|x)[\u2212r(x,y)\ued19\ued18\ued17\ued1a\nloss+\u03b2(log\u03c0\u03b8(y|x)\u2212log\u03c0\u03b8ref(y|x))\ued19 \ued18\ued17 \ued1a\npenalty](4.73)\nNote that in this optimization problem, only the term \u03c0\u03b8(y|x)depends on the target policy \u03c0\u03b8(\u00b7).\nBoth the reward model r(x,y)and the reference model \u03c0\u03b8ref(y|x)are assumed to be \ufb01xed given\nxandy. This is a strong assumption compared with PPO, but as will be shown later, it simpli\ufb01es\nthe problem and crucial for deriving the DPO objective.\nSince\u03b8is the variable we want to optimize, we rearrange the right-h and",
    "afed922cc0ce4195": " model r(x,y). The training objective is thus\ngiven by\n\u02dc\u03b8= arg min\n\u03b8Ex\u223cDEy\u223c\u03c0\u03b8(\u00b7|x)[\u2212r(x,y)\ued19\ued18\ued17\ued1a\nloss+\u03b2(log\u03c0\u03b8(y|x)\u2212log\u03c0\u03b8ref(y|x))\ued19 \ued18\ued17 \ued1a\npenalty](4.73)\nNote that in this optimization problem, only the term \u03c0\u03b8(y|x)depends on the target policy \u03c0\u03b8(\u00b7).\nBoth the reward model r(x,y)and the reference model \u03c0\u03b8ref(y|x)are assumed to be \ufb01xed given\nxandy. This is a strong assumption compared with PPO, but as will be shown later, it simpli\ufb01es\nthe problem and crucial for deriving the DPO objective.\nSince\u03b8is the variable we want to optimize, we rearrange the right-h and side of Eq. ( 4.73) to\nisolate\u03c0\u03b8(y|x)as an independent term:\n\u02dc\u03b8= arg min\n\u03b8Ex\u223cDEy\u223c\u03c0\u03b8(\u00b7|x)[\u03b2log\u03c0\u03b8(y|x)\u2212\u03b2log\u03c0\u03b8ref(y|x)\u2212r(x,y)]\n= arg min\n\u03b8Ex\u223cDEy\u223c\u03c0\u03b8(\u00b7|x)[log\u03c0\u03b8(y|x)\u2212(log\u03c0\u03b8ref(y|x) +1\n\u03b2r(x,y))]\n= arg min\n\u03b8Ex\u223cDEy\u223c\u03c0\u03b8(\u00b7|x)[log\u03c0\u03b8(y|x)\ued19\ued18\ued17\ued1a\ndependent on \u03b8\u2212log\u03c0\u03b8ref(y|x) exp(1\n\u03b2r(x,y))\n\ued19 \ued18\ued17 \ued1a\nnot dependent on \u03b8](4.74)\nThis equation de\ufb01nes the objective function as the differen ce between the log-probability dis-\ntribution function of yand another function of y. This form of the objective function seems not\n\u201cideal\u201d, as we usually prefer to see the difference between t wo distributions, so that we can in-\nterpret this difference as some kind of divergence between t he distributions. A simple idea is\nto convert the second term (i.e., log\u03c0\u03b8ref(y|x) exp(1\n\u03b2r(x,y))) into a log-probability distribution\nover the domain of y. If we treat \u03c0\u03b8ref(y|x) exp(1\n\u03b2r(x,y))as an unnormalized probability of y,\nwe can convert it into a normalized probability by dividing i t by a normalization factor:\nZ(x) =\u2211\ny\u03c0\u03b8ref(y|x) exp(1\n\u03b2r(x,y))(4.75)\nHence we can de\ufb01ne a probability distribution by\n\u03c0\u2217(y|x) =\u03c0\u03b8ref(y|x) exp(1\n\u03b2r(x,y))\nZ(x)(4.76)\n\n--- Page 202 ---\n\n4.4 Improved Human Preference Alignment 195\nWe then rewrite Eq. ( 4.74) as\n\u02dc\u03b8= arg min\n\u03b8Ex\u223cDEy\u223c\u03c0\u03b8(\u00b7|x)[\nlog\u03c0\u03b8(y|x)\u2212log\u03c0\u03b8ref(y|x) exp(1\n\u03b2r(x,y))\nZ(x)\n\u2212logZ(x)]\n= arg min\n\u03b8Ex\u223cDEy\u223c\u03c0\u03b8(\u00b7|x)[\nlog\u03c0\u03b8(y|x)\u2212log\u03c0\u2217(y|x)\u2212logZ(x)]\n= arg min\n\u03b8Ex\u223cD[\nEy\u223c\u03c0\u03b8(\u00b7|x)[\nlog\u03c0\u03b8(y|x)\u2212log\u03c0\u2217(y|x)]\n\u2212Ey\u223c\u03c0\u03b8(\u00b7|x)[logZ(x)]]\n= arg min\n\u03b8Ex\u223cD[\nKL(\u03c0\u03b8(\u00b7|x)||\u03c0\u2217(\u00b7|x))\n\ued19\ued18\ued17\ued1a\nKL divergence\u2212logZ(x)\ued19\ued18\ued17\ued1a\nconstant wrt. \u03b8]\n(4.77)\nSince logZ(x)is independent of \u03b8, it does not affect the result of the arg min\u03b8operation,\nand can be removed from the objective. Now we obtain a new trai ning objective which \ufb01nds the\noptimal policy \u03c0\u03b8by minimizing the KL divergence between \u03c0\u03b8(\u00b7|x)and\u03c0\u2217(\u00b7|x)\n\u02dc\u03b8= arg min\n\u03b8Ex\u223cD[\nKL(\u03c0\u03b8(\u00b7|x)||\u03c0\u2217(\u00b7|x))]\n(4.78)\nClearly, the solution to this optimization problem is given by\n\u03c0\u03b8(y|x)",
    "fc87838b1ae5f138": ")]]\n= arg min\n\u03b8Ex\u223cD[\nKL(\u03c0\u03b8(\u00b7|x)||\u03c0\u2217(\u00b7|x))\n\ued19\ued18\ued17\ued1a\nKL divergence\u2212logZ(x)\ued19\ued18\ued17\ued1a\nconstant wrt. \u03b8]\n(4.77)\nSince logZ(x)is independent of \u03b8, it does not affect the result of the arg min\u03b8operation,\nand can be removed from the objective. Now we obtain a new trai ning objective which \ufb01nds the\noptimal policy \u03c0\u03b8by minimizing the KL divergence between \u03c0\u03b8(\u00b7|x)and\u03c0\u2217(\u00b7|x)\n\u02dc\u03b8= arg min\n\u03b8Ex\u223cD[\nKL(\u03c0\u03b8(\u00b7|x)||\u03c0\u2217(\u00b7|x))]\n(4.78)\nClearly, the solution to this optimization problem is given by\n\u03c0\u03b8(y|x) =\u03c0\u2217(y|x)\n=\u03c0\u03b8ref(y|x) exp(1\n\u03b2r(x,y))\nZ(x) (4.79)\nGiven this equation, we can express the reward r(x,y)using the target model \u03c0\u03b8(y|x), the\nreference model \u03c0\u03b8ref(y|x), and the normalization factor Z(x):\nr(x,y) =\u03b2(\nlog\u03c0\u03b8(y|x)\n\u03c0\u03b8ref(y|x)+ logZ(x))\n(4.80)\nThis is interesting because we initially seek to learn the po licy\u03c0\u03b8(\u00b7)using the reward model\nr(x,y), but eventually obtain a representation of the reward model based on the policy. Given the\nreward model de\ufb01ned in Eq. ( 4.80), we can apply it to the Bradley-Terry model to calculate the\npreference probability (also see Section 4.3.2 ):\nPr\u03b8(ya\u227byb|x) = Sigmoid( r(x,ya)\u2212r(x,yb))\n= Sigmoid(\n\u03b2(\nlog\u03c0\u03b8(ya|x)\n\u03c0\u03b8ref(ya|x)+ logZ(x))\n\u2212\n\u03b2(\nlog\u03c0\u03b8(yb|x)\n\u03c0\u03b8ref(yb|x)+ logZ(x)))\n= Sigmoid(\n\u03b2log\u03c0\u03b8(ya|x)\n\u03c0\u03b8ref(ya|x)\u2212\u03b2log\u03c0\u03b8(yb|x)\n\u03c0\u03b8ref(yb|x))\n(4.81)\n\n--- Page 203 ---\n\n196 Alignment\nThis formula is elegant because it converts the difference i n rewards into the difference in\nratio functions, and we do not need to calculate the value of Z(x). A direct result is that we no\nlonger need a reward model, but only need the target policy an d reference model to calculate the\nprobability of preferences. Finally, we can train the targe t policy by minimizing the following\nDPO loss function\nLdpo(\u03b8) = \u2212E(x,ya,yb)\u223cD r[log Pr \u03b8(ya\u227byb|x)](4.82)\nThe form of this loss function is very similar to that used in t raining reward models in RLHF (see\nEq. ( 4.36)). But it should be noted that the loss function here depends on the parameters of the\npolicy (i.e.,\u03b8) rather than the parameters of the reward model (i.e., \u03c6).\nThe main advantage of DPO lies in its simplicity and ef\ufb01cienc y. The DPO objective is very\nstraightforward \u2014 it directly optimizes for preference-ba sed feedback, rather than relying on sep-\narately developed reward models. Moreover, DPO is generall y more sample-ef\ufb01cient, as it learns\nfrom a \ufb01xed dataset without the need for the computationally expensive sampling process used\nin PPO. This makes DPO a popular method for human preference a lignment, especially when\ndeveloping and applying reward models via reinforcement le arning is challenging.\nDPO can broadly be viewed as an of\ufb02ine reinforcement learning method, where the training\ndata is pre-collected and \ufb01xed, and there is no exploration. In contrast, online reinforcement learn-\ning methods like PPO, which require exploring new states thr ough interaction with the environ-\nment (using the reward model as a proxy), also have their uniq ue advantages. One of the bene\ufb01ts\nof online reinforcement learning is that it allows the agent to continuously adapt to changes in\nthe environment by learning from real-time feedback. This m eans that, unlike of\ufb02ine methods,\nonline methods are not constrained by the static nature of pr e-collected data and can discover\nnew problem-solving strategies. In addition, exploration can help the agent cover a wider range of\nstate-action pairs, thus improving generalization. This c ould be",
    "b661401b1d2a54ca": " lignment, especially when\ndeveloping and applying reward models via reinforcement le arning is challenging.\nDPO can broadly be viewed as an of\ufb02ine reinforcement learning method, where the training\ndata is pre-collected and \ufb01xed, and there is no exploration. In contrast, online reinforcement learn-\ning methods like PPO, which require exploring new states thr ough interaction with the environ-\nment (using the reward model as a proxy), also have their uniq ue advantages. One of the bene\ufb01ts\nof online reinforcement learning is that it allows the agent to continuously adapt to changes in\nthe environment by learning from real-time feedback. This m eans that, unlike of\ufb02ine methods,\nonline methods are not constrained by the static nature of pr e-collected data and can discover\nnew problem-solving strategies. In addition, exploration can help the agent cover a wider range of\nstate-action pairs, thus improving generalization. This c ould be an important advantage for LLMs,\nas generalization is considered a critical aspect in applyi ng such large models.\n4.4.3 Automatic Preference Data Generation\nAlthough learning from human preferences is an effective an d popular method for aligning LLMs,\nannotating preference data is costly. Using human feedback does not only faces the problem of\nlimited scalability, but it may also introduce bias because human feedback is inherently subjective.\nAs a result, one can turn to AI feedback methods to address the se scalability and consistency issues\nwithout the limitations associated with human annotators.\nAs with data generation for instruction \ufb01ne-tuning, genera ting preference data using LLMs is\nstraightforward. Given a set of inputs, we \ufb01rst use an LLM to g enerate pairs of outputs. Then, we\nprompt the LLM to label the preference between each pair of ou tputs, along with its corresponding\ninput. Below is an example of prompting the LLM to generate a p reference label for a pair of\nconsumer service responses.\n\n--- Page 204 ---\n\n4.4 Improved Human Preference Alignment 197\nConsider a customer service scenario where a customer poses a request. You\nwill review two responses to this request. Please indicate w hich response is\npreferred. Note that a good response should be courteous, cl ear, and concise. It\nshould address the customer\u2019s concern directly, provide he lpful information or a\nsolution, and maintain a positive tone.\nRequest:\nHello, I noticed that my order hasn\u2019t arrived yet, though it w as scheduled to\narrive several days ago. Could you please update me on its sta tus? Thank you!\nResponse A:\nI\u2019m very sorry for the delay and understand how disappointin g this can be. We\u2019re\ndoing our best to sort this out quickly for you.\nResponse B:\nHey, stuff happens! Your package will get there when it gets t here, no need to\nstress.\nResponse A is preferred.\nOnce we collect such preference labels, we can use them, alon g with the output pair and input,\nto train the reward model. Of course, we can consider demonst rating a few examples or using\nadvanced prompting techniques, such as CoT, to improve labe ling performance. For example, we\ncan include in the prompt an example showing how and why one of the two responses is preferred\nbased on a CoT rationale.\nIn addition to preference labels, we can also obtain the prob ability associated with each label\n[Lee et al. ,2023 ]. A simple method is to extract the probabilities for the lab el tokens, such as \u201cA\u201d\nand \u201cB\u201d, from the probabilities output by the LLM. We can then use the Softmax function or other\nnormalization techniques to re-normalize these probabili ties into a distribution over the labels.\nThese probabilities of preferred labels can serve as pointw ise supervision signals for training the\nreward model, as discussed in Section 4.4.1 .\nFor data generation, although it is easy to scale up, it is oft en necessary to ensure the data is\naccurate and diverse. Here, the data quality and diversity i ssues involve not only the labeling of\npreferences but also the inputs and outputs of the model. The refore, we often need to use a variety\nof techniques to obtain large-scale, high-quality data. Fo r example, one can generate diverse\nmodel outputs and annotations by using different LLMs, prom pts, in-context demonstrations, and\nso on [ Cui et al. ,2024 ].Dubois et al. [2024 ] report that the variability in pairwise preference data\nis important for training LLMs from either human or AI feedba ck.\nWhile learning from AI feedback is highly scalable and gener ally objective, this method is\nmore suited to well-de\ufb01ned tasks where objective perform",
    "3b4b0b063b3f82f5": " training the\nreward model, as discussed in Section 4.4.1 .\nFor data generation, although it is easy to scale up, it is oft en necessary to ensure the data is\naccurate and diverse. Here, the data quality and diversity i ssues involve not only the labeling of\npreferences but also the inputs and outputs of the model. The refore, we often need to use a variety\nof techniques to obtain large-scale, high-quality data. Fo r example, one can generate diverse\nmodel outputs and annotations by using different LLMs, prom pts, in-context demonstrations, and\nso on [ Cui et al. ,2024 ].Dubois et al. [2024 ] report that the variability in pairwise preference data\nis important for training LLMs from either human or AI feedba ck.\nWhile learning from AI feedback is highly scalable and gener ally objective, this method is\nmore suited to well-de\ufb01ned tasks where objective performan ce metrics are available. By contrast,\nlearning from human feedback is more advantageous when alig ning AI systems with human val-\nues, preferences, and complex real-world tasks that requir e understanding of subtle or subjective\ncontext. These methods can be combined to train LLMs that ben e\ufb01t from both human insights\nand the scalability of AI feedback.\n\n--- Page 205 ---\n\n198 Alignment\n4.4.4 Step-by-step Alignment\nSo far, our discussion of alignment has primarily focused on the use of reward models for evalu-\nating entire input-output sequence pairs. These methods ca n be easily adapted to scenarios where\nthe correctness of an output can be examined by checking whet her the desired result is included.\nFor example, in the task of calculating a mathematical expre ssion, a reward model can provide\npositive feedback if the answer is correct, and negative fee dback if the answer is wrong. How-\never, in many problems that require complex reasoning, simp ly examining the correctness of the\n\ufb01nal result is insuf\ufb01cient for learning. Imagine a student w ho is only given the \ufb01nal answer to\na challenging math problem. Knowing whether the \ufb01nal answer is right or wrong does not help\nthe student \ufb01gure out where they went wrong and how to calcula te the correct answer. A better\napproach would be to guide the student with a step-by-step br eakdown of the problem-solving\nprocess and encourage understanding of the underlying conc epts and logic behind these steps.\nIn Chapter 3, we studied CoT methods to prompt LLMs to explici tly write out intermediate\nsteps or the reasoning process needed to reach a conclusion o r solve a problem. We saw that\nbreaking down a problem into smaller parts could make it easi er to understand the solution path\nand increase the accuracy of the output. These methods can be naturally extended to the alignment\nof LLMs, that is, we supervise the model during the intermedi ate steps of reasoning. Consider a\nreasoning task where an LLM produces a sequence of reasoning steps y={\u00afy1,...,\u00afyns}for the\ngiven input. The result of the reasoning is assumed to be incl uded in the last step \u00afyns, and can\nbe easily veri\ufb01ed. For this reasoning problem, Uesato et al. [2022 ] categorize LLM \ufb01ne-tuning\napproaches into two classes:\n\u2022Outcome-based Approaches . Supervision occurs only when the end result is veri\ufb01ed. Thi s\nis a standard method for learning from human feedback we have discussed in this chapter.\nFor example, the LLM is optimized to maximize some form of the rewardr(x,y).\n\u2022Process-based Approaches . Supervision is involved in all intermediate steps in addit ion to\nthe last step. To do this, we need to develop a model to give a su pervision signal at each\nstep, and develop loss functions that can make use of such sup ervision signals.\nFigure 4.11 shows two LLM outputs for an example math problem. Although t he LLM gives\nthe correct \ufb01nal answer in both cases, it makes mistakes duri ng the problem-solving process in the\nsecond output. Outcome-based approaches overlook these mi stakes and give positive feedback for\nthe entire solution. By contrast, process-based approache s can take these mistakes into account\nand provide additional guidance on the detailed reasoning s teps.\nAn important issue for process-based approaches is that we n eed to get step-level feedback\nduring a (potentially) long reasoning path. We can collect o r generate reasoning paths correspond-\ning to problems from existing datasets. Human experts then a nnotate each step in these paths for\ncorrectness. These annotations can be used to directly",
    "4ed8f42d9a7fb3d5": " To do this, we need to develop a model to give a su pervision signal at each\nstep, and develop loss functions that can make use of such sup ervision signals.\nFigure 4.11 shows two LLM outputs for an example math problem. Although t he LLM gives\nthe correct \ufb01nal answer in both cases, it makes mistakes duri ng the problem-solving process in the\nsecond output. Outcome-based approaches overlook these mi stakes and give positive feedback for\nthe entire solution. By contrast, process-based approache s can take these mistakes into account\nand provide additional guidance on the detailed reasoning s teps.\nAn important issue for process-based approaches is that we n eed to get step-level feedback\nduring a (potentially) long reasoning path. We can collect o r generate reasoning paths correspond-\ning to problems from existing datasets. Human experts then a nnotate each step in these paths for\ncorrectness. These annotations can be used to directly trai n LLMS or as rewards in reward mod-\neling. However, in practice, richer annotations are often i ntroduced [ Lightman et al. ,2024 ]. In\naddition to the correct andincorrect labels, a step can also be labeled as neutral to indicate that\nwhile the step may be technically correct, it might still be p roblematic within the overall reason-\ning process. Furthermore, to improve the ef\ufb01ciency of data a nnotation, techniques such as active\nlearning can be employed. Identifying obvious errors usual ly does not signi\ufb01cantly contribute to\n\n--- Page 206 ---\n\n4.4 Improved Human Preference Alignment 199\nQuestion (from The International Junior Math Olympiad):\nJessica has a lot of apps on her phone. The number of apps she ha s is divisible by 5. If she\ndownloaded 10 more apps, she would still have less than 50 app s. What is the maximum\noriginal number of apps Jessica could have?\nResponse 1 :\nxmust be a multiple of 5, i.e.,\nx\u22610 (mod 5) .x\u00afy1:\nAfter downloading 10apps, the total\ntotal number of apps should still be less\nthan50:x+ 10<50. Solving this\ninequality:x<40.x\u00afy2:\nSincexmust be a multiple of 5and\nless than 40, the largest value of xis35.x\u00afy3:\nVerify the result. If x= 35 , then after\ndownloading 10 apps, Jessica would\nhave: 35 + 10 = 45 which satisfy\n45<50.x\u00afy4:\nThe maximum original number of\napps is 35.x\u00afy5:\ncorrectResponse 2 :\nThe number of apps is divisible by 5,\nsoxmust be a multiple of 5.x\u00afy1:\nIf she downloads 10 more apps, her\nnew total will be x+ 10 , and this must\nbe less than 50. Therefore, x+ 10<50.\nSimplifying this gives x<40.x\u00afy2:\nxmust be less than or equal to\nthe largest multiple of 5, which is 40.x\u00afy3:\nButx+ 10 should not be more\nthan or equal to 50. So we need to\nsubtract 5from 40.x\u00afy4:\nTherefore, the \ufb01nal result is 35. x \u00afy5:\ncorrectproblematic\nproblematic\nFig. 4.11: Two LLM responses to a math problem. In response 1, both the \ufb01n al result and all the reasoning steps are\ncorrect. In response 2, the \ufb01nal result is correct, but there are mistakes in the reasoning process (highlighted in red).\nFor outcome-based approaches, both responses are consider ed correct. For process-based approaches, the mistakes in\nresponse 2 can be considered in reward modeling.\nlearning from reasoning mistakes. Instead, annotating ste ps that the model con\ufb01dently considers\ncorrect but are actually problematic is often more effectiv e.\nGiven a set of step-level annotated reasoning paths and corr esponding inputs, we can train\na reward model to provide feedback for supervising policy le arning. The reward model can be\ntreated as a classi\ufb01cation model, and so its architecture ca n be a Transformer decoder with a\nSoftmax layer stacked on top. At step k, the reward model takes both the problem description\n(denoted by x) and the reasoning steps generated so far (denoted by \u00afy\u2264k) as input and outputs\na probability distribution over the label set {correct,incorrect }or{correct,incorrect,neutral }.\nThen the learned reward model is used to",
    "ad058fe2a50641f0": " responses are consider ed correct. For process-based approaches, the mistakes in\nresponse 2 can be considered in reward modeling.\nlearning from reasoning mistakes. Instead, annotating ste ps that the model con\ufb01dently considers\ncorrect but are actually problematic is often more effectiv e.\nGiven a set of step-level annotated reasoning paths and corr esponding inputs, we can train\na reward model to provide feedback for supervising policy le arning. The reward model can be\ntreated as a classi\ufb01cation model, and so its architecture ca n be a Transformer decoder with a\nSoftmax layer stacked on top. At step k, the reward model takes both the problem description\n(denoted by x) and the reasoning steps generated so far (denoted by \u00afy\u2264k) as input and outputs\na probability distribution over the label set {correct,incorrect }or{correct,incorrect,neutral }.\nThen the learned reward model is used to evaluate reasoning p aths by assessing the correctness of\neach step. A simple method to model correctness is to count th e number of steps that are classi\ufb01ed\nascorrect , given by\nr(x,y) =ns\u2211\nk=1\u03b4(correct,C(x,\u00afy\u2264k)) (4.83)\nwhereC(x,\u00afy\u2264k)denotes the label with the maximum probability. We can also u se log-probabilities\n\n--- Page 207 ---\n\n200 Alignment\nof classi\ufb01cation to de\ufb01ne the reward of the entire path\nr(x,y) =ns\u2211\nk=1log Pr( correct |x,\u00afy\u2264k) (4.84)\nwhere Pr(correct |x,\u00afy\u2264k)denotes the probability of the correct label generated by the reward\nmodel. The reward score r(x,y)can then be used to train the policy in RLHF as usual.\nWhile we restrict our discussion to math problems, the appro aches described here are general\nand can be applied to a wide variety of tasks that involve mult i-step reasoning and decision-\nmaking. Moreover, we can consider various aspects when asse ssing the quality of a step, rather\nthan just its correctness. For example, in dialogue systems , responses must not only be accurate\nbut also contextually appropriate across multiple turns of conversation. If a model provides a\ncorrect response but fails to maintain coherence in the cont ext of the ongoing dialogue, step-\nlevel feedback could help the model identify and correct suc h discrepancies. Also note that the\nprocess-based approaches are related to the \ufb01ne-grained re ward modeling approaches discussed\nin Section 4.4.1.3 . All these approaches essentially aim to provide more detai led supervision to\nLLMs by breaking their outputs into smaller, more manageabl e steps. However, process-based\nfeedback focuses more on evaluating the correctness of a ste p based on its preceding steps, while\nthe approaches in Section 4.4.1.3 emphasize evaluating each step independently.\nThe idea of aligning LLMs step by step has great application p otential, especially considering\nthe recent shift towards more complex reasoning tasks in the use of LLMs. For example, both\nthe GPT-o1 and GPT-o3 models are designed with more advanced reasoning techniques (such\nas long internal CoT) to solve challenging problems like sci enti\ufb01c and mathematical reasoning\n[OpenAI ,2024 ]. These tasks often rely on long and complex reasoning paths , and therefore, it\nseems essential to introduce detailed supervision signals in the reasoning process. Moreover, from\na practical perspective, effective supervision on long rea soning paths not only improves reasoning\nperformance, but it also helps the model eliminate redundan t or unnecessary reasoning steps,\nthereby reducing reasoning complexity and improving ef\ufb01ci ency.\n4.4.5 Inference-time Alignment\nIn this section we explored a variety of methods to align mode ls with human preferences and an-\nnotations. However, one of the signi\ufb01cant limitations of ma ny such methods is that LLMs must\nbe \ufb01ne-tuned. For RLHF and its variants, training LLMs with r eward models can be computa-\ntionally expensive and unstable, leading to increased comp lexity and costs when applying these\napproaches. In this case, we can consider aligning models at inference time, thus avoiding the\nadditional complexity and effort involved.\nOne simple way to achieve inference-time alignment is to use the reward model to select\nthe best one from Nalternative outputs generated by the LLM, a method known as Best-of-N\nsampling (BoN sampling ). We can consider BoN sampling as a form of reranking. In fact ,\nreranking methods have",
    "aaa1ae056816a6b5": " and improving ef\ufb01ci ency.\n4.4.5 Inference-time Alignment\nIn this section we explored a variety of methods to align mode ls with human preferences and an-\nnotations. However, one of the signi\ufb01cant limitations of ma ny such methods is that LLMs must\nbe \ufb01ne-tuned. For RLHF and its variants, training LLMs with r eward models can be computa-\ntionally expensive and unstable, leading to increased comp lexity and costs when applying these\napproaches. In this case, we can consider aligning models at inference time, thus avoiding the\nadditional complexity and effort involved.\nOne simple way to achieve inference-time alignment is to use the reward model to select\nthe best one from Nalternative outputs generated by the LLM, a method known as Best-of-N\nsampling (BoN sampling ). We can consider BoN sampling as a form of reranking. In fact ,\nreranking methods have been widely used in NLP tasks, such as machine translation, for a long\ntime. They are typically applied in situations where traini ng complex models is costly. In such\ncases, directly reranking the outputs allows for the incorp oration of these complex models at a\nvery low cost8.\n8Reranking methods can also help us explore what are known as m odel errors and search errors, although these\n\n--- Page 208 ---\n\n4.5 Summary 201\nIn the BoN sampling process, the LLM takes the input sequence xand generates Ndifferent\noutput sequences {\u02c6y1,...,\u02c6yN}:\n{\u02c6y1,...,\u02c6yN}= argTopN\ny[Pr(y|x)] (4.85)\nwhere the argTopN operation returns the top- Noutputs that maximize the function Pr(y|x).\nThese outputs can be generated in a variety of ways, dependin g on the search algorithm used by\nthe model (e.g., sampling or beam search). Once the N-best output candidates are generated, the\nreward model is used to evaluate and select the best one:\n\u02c6ybest = max {r(x,\u02c6y1),...,r (x,\u02c6yN)} (4.86)\nIt is worth noting that the result of BoN sampling is also in\ufb02u enced by the diversity of the\nN-best list. This is a common issue with most reranking method s. Typically, we wish the N-best\noutput candidates to have relatively high quality but be suf \ufb01ciently different from each other. In\nmany text generation systems, the N-best outputs are very similar, often differing by just one\nor two words. The diversity issue is even more challenging in LLMs, as the N-best outputs\ngenerated by an LLM can be different in their wordings, yet th eir semantic meanings are often\nquite similar. In practice, one can adjust the model hyperpa rameters and/or adopt different LLMs\nto generate more diverse output candidates for reranking. N evertheless, as with many practical\nsystems, we need to make a trade-off between selecting high- quality candidates and ensuring\nsuf\ufb01cient variation in the generated outputs.\nBoN sampling can be used for training LLMs as well. A closely r elated method is rejection\nsampling . In this method, we \ufb01rst select the \u201cbest\u201d outputs from the N-best lists via the reward\nmodel, and then take these selected outputs to \ufb01ne-tune the L LM. In this way, we can introduce\nhuman preferences into the training of LLMs via a much simple r approach compared to RLHF.\nMany LLMs have adopted rejection sampling for \ufb01ne-tuning [ Nakano et al. ,2021 ;Touvron et al. ,\n2023b ].\n4.5 Summary\nIn this chapter, we have explored a range of techniques for al igning LLMs. In particular, we\nhave discussed \ufb01ne-tuning methods that enable LLMs to follo w instructions and align them with\nhuman preferences. One of the bene\ufb01ts of \ufb01ne-tuning LLMs is c omputation ef\ufb01ciency. Unlike\npre-training based on large-scale neural network optimiza tion, \ufb01ne-tuning is a post-training step\nand so is less computationally expensive. Moreover, it is be tter suited to address problems that are\nnot easily solved in pre-training, such as human value align ment. The widespread attention to the\nalignment issue has also led to a surge of research papers on t his topic, which has posed challenges\nin writing this chapter, as it is dif\ufb01cult to cover all the lat est techniques. However, we have tried\nto provide",
    "a432fba38cbbfa0a": "b ].\n4.5 Summary\nIn this chapter, we have explored a range of techniques for al igning LLMs. In particular, we\nhave discussed \ufb01ne-tuning methods that enable LLMs to follo w instructions and align them with\nhuman preferences. One of the bene\ufb01ts of \ufb01ne-tuning LLMs is c omputation ef\ufb01ciency. Unlike\npre-training based on large-scale neural network optimiza tion, \ufb01ne-tuning is a post-training step\nand so is less computationally expensive. Moreover, it is be tter suited to address problems that are\nnot easily solved in pre-training, such as human value align ment. The widespread attention to the\nalignment issue has also led to a surge of research papers on t his topic, which has posed challenges\nin writing this chapter, as it is dif\ufb01cult to cover all the lat est techniques. However, we have tried\nto provide a relatively detailed introduction to the fundam ental approaches to alignment, such as\nissues are not often discussed in the context of LLMs. For exa mple, suppose we have an old model and a new, more\npowerful model. We can use the new model to select the best out put from the N-best list of the old model as the oracle\noutput. The performance difference between the oracle outp ut and the top-1 output of the original N-best list re\ufb02ects\nthe performance gain brought by the new model. If the perform ance gain is signi\ufb01cant, we can say that the old model\nhas more model errors. If the gain is small, it may indicate th at the issue lies in search errors, as the best candidates\nwere not found.\n\n--- Page 209 ---\n\n202 Alignment\ninstruction \ufb01ne-tuning and RLHF.\nWhile we have focused on LLM alignment techniques in this cha pter, the term AI alignment\nis a wide-ranging concept. It generally refers to the proces s of ensuring that the behavior of an AI\nsystem aligns with human values, goals, and expectations. T he idea of AI alignment can be traced\nback to the early days of AI. A widely cited description of AI a lignment comes from an article by\nthe mathematician and computer scientist Norbert Wiener [ Wiener ,1960 ]. The quote is as follows\nIf we use, to achieve our purposes, a mechanical agency with w hose operation\nwe cannot ef\ufb01ciently interfere ... we had better be quite sur e that the purpose\nput into the machine is the purpose which we really desire.\nAt that time, AI alignment was a distant concern for research ers. But today, it greatly in\ufb02u-\nences the design of various AI systems. For example, in robot ics, alignment is critical to ensur-\ning that autonomous robots safely interact with humans and t heir environments. In autonomous\ndriving, cars must not only follow traf\ufb01c laws but also make c omplex, real-time decisions that\nprioritize human safety, avoid accidents, and navigate eth ical dilemmas.\nIn current AI research, alignment is usually achieved by dev eloping a surrogate objective that\nis analogous to the real goal and steering the AI system towar ds this objective. However, designing\nthe objective of AI alignment is very dif\ufb01cult. One reason is that human values are diverse and\noften context-dependent, making it dif\ufb01cult to distill the m into a single, universally applicable\nobjective function. Also, the complexity of real-world env ironments, where values and goals often\ncon\ufb02ict or evolve over time, further complicates alignment efforts. Even if we could de\ufb01ne an\nappropriate objective, AI systems may \ufb01nd unintended ways t o achieve it, leading to \u201cmisaligned\u201d\noutcomes that still technically satisfy the objective but i n a harmful or counterproductive way.\nThese challenges have motivated and are motivating AI resea rch towards more aligned sys-\ntems, either through developing new mechanisms for perceiv ing the world or more ef\ufb01cient and\ngeneralizable methods to adapt these systems to given tasks . More importantly, as AI systems\nbecome more powerful and intelligent, especially given tha t recent advances in LLMs have shown\nremarkable capabilities in dealing with many challenging p roblems, the need for AI alignment\nhas become more urgent. Researchers have started to be conce rned with AI safety and warn the\ncommunity that they need to develop and release AI systems wi th great caution to prevent these\nsystems from being misaligned [ Russell ,2019 ;Bengio et al. ,2024 ].\n\n--- Page 210 ---\n\nBibliography\n[Ainslie et al., 2020] Joshua Ainslie, Santiago",
    "ba30681b895c70f0": "aligned\u201d\noutcomes that still technically satisfy the objective but i n a harmful or counterproductive way.\nThese challenges have motivated and are motivating AI resea rch towards more aligned sys-\ntems, either through developing new mechanisms for perceiv ing the world or more ef\ufb01cient and\ngeneralizable methods to adapt these systems to given tasks . More importantly, as AI systems\nbecome more powerful and intelligent, especially given tha t recent advances in LLMs have shown\nremarkable capabilities in dealing with many challenging p roblems, the need for AI alignment\nhas become more urgent. Researchers have started to be conce rned with AI safety and warn the\ncommunity that they need to develop and release AI systems wi th great caution to prevent these\nsystems from being misaligned [ Russell ,2019 ;Bengio et al. ,2024 ].\n\n--- Page 210 ---\n\nBibliography\n[Ainslie et al., 2020] Joshua Ainslie, Santiago Ontanon, Ch ris Alberti, Vaclav Cvicek, Zachary Fisher,\nPhilip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, and Li Yang. Etc: Encoding long and\nstructured inputs in transformers. In Proceedings of the 2020 Conference on Empirical Methods in\nNatural Language Processing (EMNLP) , pages 268\u2013284, 2020.\n[Ainslie et al., 2023] Joshua Ainslie, James Lee-Thorp, Mic hiel de Jong, Yury Zemlyanskiy, Federico\nLebron, and Sumit Sanghai. Gqa: Training generalized multi -query transformer models from multi-\nhead checkpoints. In Proceedings of the 2023 Conference on Empirical Methods in N atural Language\nProcessing , pages 4895\u20134901, 2023.\n[Aky\u00fcrek et al., 2023] Ekin Aky\u00fcrek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou.\nWhat learning algorithm is in-context learning? investiga tions with linear models. In Proceedings of\nThe Eleventh International Conference on Learning Represe ntations , 2023.\n[Alabdulmohsin et al., 2022] Ibrahim M Alabdulmohsin, Behn am Neyshabur, and Xiaohua Zhai. Revisit-\ning neural scaling laws in language and vision. Advances in Neural Information Processing Systems , 35:\n22300\u201322312, 2022.\n[Allal et al., 2024] Loubna Ben Allal, Anton Lozhkov, and Dan iel van Strien. cosmopedia: how to create\nlarge-scale synthetic data for pre-training. https://huggingface.co/blog/cosmopedia , 2024.\n[Almazrouei et al., 2023] Ebtesam Almazrouei, Hamza Alobei dli, Abdulaziz Alshamsi, Alessandro Cap-\npelli, Ruxandra Cojocaru, M\u00e9rouane Debbah, \u00c9tienne Gof\ufb01ne t, Daniel Hesslow, Julien Launay, Quentin\nMalartic, Daniele Mazzotta, Badreddine Noune, Baptiste Pa nnier, and Guilherme Penedo. The falcon\nseries of open language models. arXiv preprint arXiv:2311.16867 , 2023.\n[Andreas et al., 2016] Jacob Andreas, Marcus Rohrbach, Trev or Darrell, and Dan Klein. Neural module\nnetworks. In Proceedings of the IEEE conference on computer vision and pa ttern recognition , pages\n39\u201348, 2016.\n[Arjovsky et al., 2016] Martin Arjovsky, Amar Shah, and Yosh ua Bengio. Unitary evolution recurrent\nneural networks. In International conference on machine learning , pages 1120\u20131128, 2016.\n[Aschenbrenner, 2024] Leopold Aschenbrenner. Situationa l awareness: The decade ahead, 2024. URL\nhttps://situational-awareness.ai/ .\n[Askell et al., 2021] Amanda Askell, Yuntao Bai, Anna Chen, D awn Drain, Deep Ganguli, Tom Henighan,\nAndy Jones, Nicholas Joseph, Benjamin Mann, Nova DasSarma, Nelson Elhage, Zac Hat\ufb01eld-Dodds,\nDanny Hernandez, Jackson Kernion, Kamal Ndousse, Catherin e Olsson, Dario Amodei, Tom B. Brown,\nJack Clark, Sam McCandlish, Chris Olah, and Jared Kaplan. A g eneral language assistant as",
    "1f562b4ed986346f": ", Amar Shah, and Yosh ua Bengio. Unitary evolution recurrent\nneural networks. In International conference on machine learning , pages 1120\u20131128, 2016.\n[Aschenbrenner, 2024] Leopold Aschenbrenner. Situationa l awareness: The decade ahead, 2024. URL\nhttps://situational-awareness.ai/ .\n[Askell et al., 2021] Amanda Askell, Yuntao Bai, Anna Chen, D awn Drain, Deep Ganguli, Tom Henighan,\nAndy Jones, Nicholas Joseph, Benjamin Mann, Nova DasSarma, Nelson Elhage, Zac Hat\ufb01eld-Dodds,\nDanny Hernandez, Jackson Kernion, Kamal Ndousse, Catherin e Olsson, Dario Amodei, Tom B. Brown,\nJack Clark, Sam McCandlish, Chris Olah, and Jared Kaplan. A g eneral language assistant as a laboratory\nfor alignment. arXiv preprint arXiv:2112.00861 , 2021.\n[Bach et al., 2022] Stephen H. Bach, Victor Sanh, Zheng Xin Yo ng, Albert Webson, Colin Raffel, Nihal V .\nNayak, Abheesht Sharma, Taewoon Kim, M. Saiful Bari, Thibau lt F\u00e9vry, Zaid Alyafeai, Manan Dey,\nAndrea Santilli, Zhiqing Sun, Srulik Ben-David, Canwen Xu, Gunjan Chhablani, Han Wang, Jason Alan\nFries, Maged Saeed AlShaibani, Shanya Sharma, Urmish Thakk er, Khalid Almubarak, Xiangru Tang,\nDragomir R. Radev, Mike Tian-Jian Jiang, and Alexander M. Ru sh. Promptsource: An integrated de-\nvelopment environment and repository for natural language prompts. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Linguistics: System Demonstrations , pages 93\u2013104, 2022.\n[Bengio et al., 2003] Yoshua Bengio, R\u00e9jean Ducharme, Pasca l Vincent, and Christian Jauvin. A neural\nprobabilistic language model. Journal of Machine Learning Research , 3:1137\u20131155, 2003.\n[Bengio et al., 2006] Yoshua Bengio, Pascal Lamblin, Dan Pop ovici, and Hugo Larochelle. Greedy layer-\nwise training of deep networks. Advances in neural information processing systems , 19, 2006.\n[Bengio et al., 2024] Yoshua Bengio, Geoffrey Hinton, Andre w Yao, Dawn Song, Pieter Abbeel, Trevor\nDarrell, Yuval Noah Harari, Ya-Qin Zhang, Lan Xue, Shai Shal ev-Shwartz, Gillian K. Had\ufb01eld, Jeff\n203\n\n--- Page 211 ---\n\n204 Alignment\nClune, Tegan Maharaj, Frank Hutter, Atilim Gunes Baydin, Sh eila A. McIlraith, Qiqi Gao, Ashwin\nAcharya, David Krueger, Anca Dragan, Philip Torr, Stuart Ru ssell, Daniel Kahneman, Jan Markus\nBrauner, and S\u00f6ren Mindermann. Managing extreme ai risks am id rapid progress. Science , 384(6698):\n842\u2013845, 2024.\n[Bentivogli and Giampiccolo, 2011] Luisa Bentivogli and Da nilo Giampiccolo. Pascal recognizing textual\nentailment challenge (rte-7) at tac 2011. https://tac.nist.gov/2011/RTE/ , 2011.\n[Besta et al., 2024] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski,\nLukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Nie wiadomski, Piotr Nyczyk, and Torsten\nHoe\ufb02er. Graph of thoughts: Solving elaborate problems with large language models. In Proceedings of\nthe AAAI Conference on Arti\ufb01cial Intelligence , volume 38, pages 17682\u201317690, 2024.\n[Biderman et al., 2021] Stella Biderman, Sid Black, Charles Foster, Leo Gao, Eric Hallahan, Horace He,\nBen Wang, and Phil Wang. Rotary embeddings: A relative revol ution. https://blog.eleuther.ai/\nrotary-embed",
    "2505fe61af47a477": ".gov/2011/RTE/ , 2011.\n[Besta et al., 2024] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski,\nLukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Nie wiadomski, Piotr Nyczyk, and Torsten\nHoe\ufb02er. Graph of thoughts: Solving elaborate problems with large language models. In Proceedings of\nthe AAAI Conference on Arti\ufb01cial Intelligence , volume 38, pages 17682\u201317690, 2024.\n[Biderman et al., 2021] Stella Biderman, Sid Black, Charles Foster, Leo Gao, Eric Hallahan, Horace He,\nBen Wang, and Phil Wang. Rotary embeddings: A relative revol ution. https://blog.eleuther.ai/\nrotary-embeddings/ , 2021.\n[Bishop, 2006] Christopher M. Bishop. Pattern Recognition and Machine Learning . Springer, 2006.\n[Blum and Mitchell, 1998] Avrim Blum and Tom Mitchell. Combi ning labeled and unlabeled data with\nco-training. In Proceedings of the eleventh annual conference on Computati onal learning theory , pages\n92\u2013100, 1998.\n[Bradley and Terry, 1952] Ralph Allan Bradley and Milton E. T erry. Rank analysis of incomplete block\ndesigns: I. the method of paired comparisons. Biometrika , 39(3/4):324\u2013345, 1952.\n[Brandon et al., 2024] William Brandon, Mayank Mishra, Anir uddha Nrusimha, Rameswar Panda, and\nJonathan Ragan Kelly. Reducing transformer key-value cach e size with cross-layer attention. arXiv\npreprint arXiv:2405.12981 , 2024.\n[Brill, 1992] Eric Brill. A simple rule-based part of speech tagger. In Speech and Natural Language:\nProceedings of a Workshop Held at Harriman, New York, Februa ry 23-26, 1992 , 1992.\n[Brown et al., 1993] Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer.\nThe mathematics of statistical machine translation: Param eter estimation. Computational Linguistics ,\n19(2):263\u2013311, 1993.\n[Brown et al., 2020] Tom Brown, Benjamin Mann, Nick Ryder, Me lanie Subbiah, Jared D Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry , Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child , Aditya Ramesh, Daniel Ziegler, Jeffrey\nWu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mat eusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish, Alec Radfo rd, Ilya Sutskever, and Dario Amodei.\nLanguage models are few-shot learners. Advances in neural information processing systems , 33:1877\u2013\n1901, 2020.\n[Bubeck et al., 2023] S\u00e9bastien Bubeck, Varun Chandrasekar an, Ronen Eldan, Johannes Gehrke, Eric\nHorvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott M. Lundberg, Harsha Nori, Hamid\nPalangi, Marco T\u00falio Ribeiro, and Yi Zhang. Sparks of arti\ufb01c ial general intelligence: Early experiments\nwith gpt-4. arXiv preprint arXiv:2303.12712 , 2023.\n[Bulatov et al., 2022] Aydar Bulatov, Yury Kuratov, and Mikh ail Burtsev. Recurrent memory transformer.\nAdvances in Neural Information Processing Systems , 35:11079\u201311091, 2022.\n[Burges et al., 2005] Chris Burges, Tal Shaked, Erin Renshaw , Ari Lazier, Matt Deeds, Nicole Hamilton,\nand Greg Hullender. Learning to rank using gradient descent . InProceedings of the 22nd international\nconference on Machine learning , pages 89\u201396, 2005.\n[Burns et al., 2023] Collin Burns",
    "3ce641ed14b3a247": "id\nPalangi, Marco T\u00falio Ribeiro, and Yi Zhang. Sparks of arti\ufb01c ial general intelligence: Early experiments\nwith gpt-4. arXiv preprint arXiv:2303.12712 , 2023.\n[Bulatov et al., 2022] Aydar Bulatov, Yury Kuratov, and Mikh ail Burtsev. Recurrent memory transformer.\nAdvances in Neural Information Processing Systems , 35:11079\u201311091, 2022.\n[Burges et al., 2005] Chris Burges, Tal Shaked, Erin Renshaw , Ari Lazier, Matt Deeds, Nicole Hamilton,\nand Greg Hullender. Learning to rank using gradient descent . InProceedings of the 22nd international\nconference on Machine learning , pages 89\u201396, 2005.\n[Burns et al., 2023] Collin Burns, Pavel Izmailov, Jan Hendr ik Kirchner, Bowen Baker, Leo Gao, Leopold\nAschenbrenner, Yining Chen, Adrien Ecoffet, Manas Jogleka r, Jan Leike, Ilya Sutskever, and Jeff Wu.\nWeak-to-strong generalization: Eliciting strong capabil ities with weak supervision. arXiv preprint\narXiv:2312.09390 , 2023a.\n\n--- Page 212 ---\n\n4.5 Summary 205\n[Burns et al., 2023] Collin Burns, Jan Leike, Leopold Aschen brenner, Jeffrey Wu, Pavel Izmailov, Leo\nGao, Bowen Baker, and Jan Hendrik Kirchner. Weak-to-strong generalization, 2023b. URL https://\nhttps://openai.com/index/weak-to-strong-generalizat ion.\n[Caballero et al., 2023] Ethan Caballero, Kshitij Gupta, Ir ina Rish, and David Krueger. Broken neural\nscaling laws. In ICLR 2023 Workshop on Mathematical and Empirical Understan ding of Foundation\nModels , 2023.\n[Cao et al., 2007] Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Ts ai, and Hang Li. Learning to rank: from\npairwise approach to listwise approach. In Proceedings of the 24th international conference on Machin e\nlearning , pages 129\u2013136, 2007.\n[Chang et al., 2024] Kaiyan Chang, Songcheng Xu, Chenglong W ang, Yingfeng Luo, Tong Xiao, and\nJingbo Zhu. Ef\ufb01cient prompting methods for large language m odels: A survey. arXiv preprint\narXiv:2404.01077 , 2024.\n[Charniak, 1997] Eugene Charniak. Statistical parsing wit h a context-free grammar and word statistics.\nAAAI/IAAI , 2005(598-603):18, 1997.\n[Chen et al., 2023] Banghao Chen, Zhaofeng Zhang, Nicolas La ngren\u00e9, and Shengxin Zhu. Unleashing\nthe potential of prompt engineering in large language model s: a comprehensive review. arXiv preprint\narXiv:2310.14735 , 2023a.\n[Chen et al., 2023] Lichang Chen, Shiyang Li, Jun Yan, Hai Wan g, Kalpa Gunaratna, Vikas Yadav, Zheng\nTang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, and Hongxi a Jin. Alpagasus: Training a better alpaca\nwith fewer data. arXiv preprint arXiv:2307.08701 , 2023b.\n[Chen et al., 2024] Lichang Chen, Shiyang Li, Jun Yan, Hai Wan g, Kalpa Gunaratna, Vikas Yadav, Zheng\nTang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, and Hongxi a Jin. Alpagasus: Training a better alpaca\nwith fewer data. In The Twelfth International Conference on Learning Represen tations , 2024a.\n[Chen et al., 2023] Shouyuan Chen, Sherman Wong, Liangjian C hen, and Yuandong Tian. Extending\ncontext window of large language models via positional inte rpolation. arXiv preprint arXiv:2306.15595 ,\n2023c.\n[Chen et al., 2020] Tianlong",
    "8948c9adc939738d": "with fewer data. arXiv preprint arXiv:2307.08701 , 2023b.\n[Chen et al., 2024] Lichang Chen, Shiyang Li, Jun Yan, Hai Wan g, Kalpa Gunaratna, Vikas Yadav, Zheng\nTang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, and Hongxi a Jin. Alpagasus: Training a better alpaca\nwith fewer data. In The Twelfth International Conference on Learning Represen tations , 2024a.\n[Chen et al., 2023] Shouyuan Chen, Sherman Wong, Liangjian C hen, and Yuandong Tian. Extending\ncontext window of large language models via positional inte rpolation. arXiv preprint arXiv:2306.15595 ,\n2023c.\n[Chen et al., 2020] Tianlong Chen, Jonathan Frankle, Shiyu C hang, Sijia Liu, Yang Zhang, Zhangyang\nWang, and Michael Carbin. The lottery ticket hypothesis for pre-trained bert networks. Advances in\nneural information processing systems , 33:15834\u201315846, 2020.\n[Chen et al., 2024] Zixiang Chen, Yihe Deng, Huizhuo Yuan, Ka ixuan Ji, and Quanquan Gu. Self-play \ufb01ne-\ntuning converts weak language models to strong language mod els. arXiv preprint arXiv:2401.01335 ,\n2024b.\n[Chevalier et al., 2023] Alexis Chevalier, Alexander Wetti g, Anirudh Ajith, and Danqi Chen. Adapting\nlanguage models to compress contexts. In Proceedings of the 2023 Conference on Empirical Methods\nin Natural Language Processing , pages 3829\u20133846, 2023.\n[Chi et al., 2022] Ta-Chung Chi, Ting-Han Fan, Peter J Ramadg e, and Alexander Rudnicky. Kerple:\nKernelized relative positional embedding for length extra polation. Advances in Neural Information Pro-\ncessing Systems , 35:8386\u20138399, 2022.\n[Chi et al., 2023] Ta-Chung Chi, Ting-Han Fan, Alexander Rud nicky, and Peter Ramadge. Dissecting\ntransformer length extrapolation via the lens of receptive \ufb01eld analysis. In Proceedings of the 61st Annual\nMeeting of the Association for Computational Linguistics ( Volume 1: Long Papers) , pages 13522\u201313537,\n2023.\n[Chiang et al., 2023] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Yin g Sheng, Zhanghao Wu, Hao Zhang, Lian-\nmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzale z, Ion Stoica, and Eric P. Xing. Vicuna:\nAn open-source chatbot impressing gpt-4 with 90%* chatgpt q uality, March 2023. URL https://\nlmsys.org/blog/2023-03-30-vicuna/ .\n[Chowdhery et al., 2022] Aakanksha Chowdhery, Sharan Naran g, Jacob Devlin, Maarten Bosma, Gaurav\n\n--- Page 213 ---\n\n206 Alignment\nMishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker\nSchuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abh ishek Rao, Parker Barnes, Yi Tay, Noam\nShazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hu tchinson, Reiner Pope, James Bradbury,\nJacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, To ju Duke, Anselm Levskaya, Sanjay Ghe-\nmawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Veda nt Misra, Kevin Robinson, Liam Fedus,\nDenny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Bar ret Zoph, Alexander Spiridonov, Ryan\nSepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andr ew M. Dai, Thanumalayan Sankara-\nnarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica More ira, Rewon Child, Oleksandr Polozov,\nKatherine Lee, Zongwei Zhou,",
    "70a3b4b98c58ff3a": "odkumar Prabhakaran, Emily Reif, Nan Du, Ben Hu tchinson, Reiner Pope, James Bradbury,\nJacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, To ju Duke, Anselm Levskaya, Sanjay Ghe-\nmawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Veda nt Misra, Kevin Robinson, Liam Fedus,\nDenny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Bar ret Zoph, Alexander Spiridonov, Ryan\nSepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andr ew M. Dai, Thanumalayan Sankara-\nnarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica More ira, Rewon Child, Oleksandr Polozov,\nKatherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Ma rk Diaz, Orhan Firat, Michele Catasta,\nJason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, S lav Petrov, and Noah Fiedel. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint arXiv:2204.02311 , 2022.\n[Christiano et al., 2017] Paul F Christiano, Jan Leike, Tom B rown, Miljan Martic, Shane Legg, and Dario\nAmodei. Deep reinforcement learning from human preference s.Advances in neural information pro-\ncessing systems , 30, 2017.\n[Chu et al., 2023] Zheng Chu, Jingchang Chen, Qianglong Chen , Weijiang Yu, Tao He, Haotian Wang,\nWeihua Peng, Ming Liu, Bing Qin, and Ting Liu. A survey of chai n of thought reasoning: Advances,\nfrontiers and future. arXiv preprint arXiv:2309.15402 , 2023.\n[Chung et al., 2022] Hyung Won Chung, Le Hou, S. Longpre, Barr et Zoph, Yi Tay, William Fedus,\nEric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu,\nZhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery , Dasha Valter, Sharan Narang, Gau-\nrav Mishra, Adams Wei Yu, Vincent Zhao, Yanping Huang, Andre w M. Dai, Hongkun Yu, Slav Petrov,\nEd Huai hsin Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denn y Zhou, Quoc V . Le, and Jason Wei.\nScaling instruction-\ufb01netuned language models. arXiv preprint arXiv:2210.11416 , 2022.\n[Clark et al., 2019] Kevin Clark, Minh-Thang Luong, Quoc V Le , and Christopher D Manning. Electra:\nPre-training text encoders as discriminators rather than g enerators. In Proceedings of International\nConference on Learning Representations , 2019.\n[Cobbe et al., 2021] Karl Cobbe, Vineet Kosaraju, Mohammad B avarian, Mark Chen, Heewoo Jun, Lukasz\nKaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Rei ichiro Nakano, Christopher Hesse, and John\nSchulman. Training veri\ufb01ers to solve math word problems. arXiv preprint arXiv:2110.14168 , 2021.\n[Conneau et al., 2020] Alexis Conneau, Kartikay Khandelwal , Naman Goyal, Vishrav Chaudhary, Guil-\nlaume Wenzek, Francisco Guzm\u00e1n, \u00c9douard Grave, Myle Ott, Lu ke Zettlemoyer, and Veselin Stoyanov.\nUnsupervised cross-lingual representation learning at sc ale. In Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics , pages 8440\u20138451, 2020.\n[Coste et al., 2024] Thomas Coste, Usman Anwar, Robert Kirk, and David Krueger. Reward model ensem-\nbles help mitigate overoptimization. In The Twelfth International Conference on Learning Represen ta-\ntions , 2024.\n[Cui et al., 2024] Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Bingxiang He, Wei Zhu, Yuan\nNi, Gu",
    "227e14e1fa5cc96d": "ikay Khandelwal , Naman Goyal, Vishrav Chaudhary, Guil-\nlaume Wenzek, Francisco Guzm\u00e1n, \u00c9douard Grave, Myle Ott, Lu ke Zettlemoyer, and Veselin Stoyanov.\nUnsupervised cross-lingual representation learning at sc ale. In Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics , pages 8440\u20138451, 2020.\n[Coste et al., 2024] Thomas Coste, Usman Anwar, Robert Kirk, and David Krueger. Reward model ensem-\nbles help mitigate overoptimization. In The Twelfth International Conference on Learning Represen ta-\ntions , 2024.\n[Cui et al., 2024] Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Bingxiang He, Wei Zhu, Yuan\nNi, Guotong Xie, Ruobing Xie, Yankai Lin, Zhiyuan Liu, and Ma osong Sun. ULTRAFEEDBACK:\nBoosting language models with scaled AI feedback. In Proceedings of the 41st International Conference\non Machine Learning , volume 235, pages 9722\u20139744, 2024.\n[Dai et al., 2023] Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shu ming Ma, Zhifang Sui, and Furu Wei.\nWhy can gpt learn in-context? language models secretly perf orm gradient descent as meta-optimizers.\nInFindings of the Association for Computational Linguistics : ACL 2023 , pages 4005\u20134019, 2023.\n[Dai et al., 2019] Zihang Dai, Zhilin Yang, Yiming Yang, Jaim e G Carbonell, Quoc Le, and Ruslan\nSalakhutdinov. Transformer-xl: Attentive language model s beyond a \ufb01xed-length context. In Proceed-\nings of the 57th Annual Meeting of the Association for Comput ational Linguistics , pages 2978\u20132988,\n2019.\n[Dao et al., 2022] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast\nand memory-ef\ufb01cient exact attention with io-awareness. Advances in Neural Information Processing\n\n--- Page 214 ---\n\n4.5 Summary 207\nSystems , 35:16344\u201316359, 2022.\n[Dehghani et al., 2018] Mostafa Dehghani, Stephan Gouws, Or iol Vinyals, Jakob Uszkoreit, and \u0141ukasz\nKaiser. Universal transformers. arXiv preprint arXiv:1807.03819 , 2018.\n[Deletang et al., 2024] Gregoire Deletang, Anian Ruoss, Pau l-Ambroise Duquenne, Elliot Catt, Tim\nGenewein, Christopher Mattern, Jordi Grau-Moya, Li Kevin W enliang, Matthew Aitchison, Laurent\nOrseau, Marcus Hutter, and Joel Veness. Language modeling i s compression. In The Twelfth Interna-\ntional Conference on Learning Representations , 2024.\n[Deng et al., 2022] Mingkai Deng, Jianyu Wang, Cheng-Ping Hs ieh, Yihan Wang, Han Guo, Tianmin Shu,\nMeng Song, Eric Xing, and Zhiting Hu. Rlprompt: Optimizing d iscrete text prompts with reinforcement\nlearning. In Proceedings of the 2022 Conference on Empirical Methods in N atural Language Processing ,\npages 3369\u20133391, 2022.\n[Devlin et al., 2019] Jacob Devlin, Ming-Wei Chang, Kenton L ee, and Kristina Toutanova. Bert: Pre-\ntraining of deep bidirectional transformers for language u nderstanding. In Proceedings of the 2019\nConference of the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short Papers) , pages 4171\u20134186, 2019.\n[Ding et al., 2024] Yiran Ding, Li Lyna Zhang, Chengruidong Z hang, Yuanyuan Xu, Ning Shang, Jiahang\nXu, Fan Yang, and Mao Yang. Longrope: Extending llm context w indow beyond 2 million tokens. arXiv\npreprint arXiv:2402.13753 , 2024.\n[Dolan and Brockett, ",
    "3c761c50708b44a8": " 3369\u20133391, 2022.\n[Devlin et al., 2019] Jacob Devlin, Ming-Wei Chang, Kenton L ee, and Kristina Toutanova. Bert: Pre-\ntraining of deep bidirectional transformers for language u nderstanding. In Proceedings of the 2019\nConference of the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short Papers) , pages 4171\u20134186, 2019.\n[Ding et al., 2024] Yiran Ding, Li Lyna Zhang, Chengruidong Z hang, Yuanyuan Xu, Ning Shang, Jiahang\nXu, Fan Yang, and Mao Yang. Longrope: Extending llm context w indow beyond 2 million tokens. arXiv\npreprint arXiv:2402.13753 , 2024.\n[Dolan and Brockett, 2005] Bill Dolan and Chris Brockett. Au tomatically constructing a corpus of senten-\ntial paraphrases. In Proceedings of Third International Workshop on Paraphrasi ng (IWP2005) , 2005.\n[Dong et al., 2019] Li Dong, Nan Yang, Wenhui Wang, Furu Wei, X iaodong Liu, Yu Wang, Jianfeng\nGao, Ming Zhou, and Hsiao-Wuen Hon. Uni\ufb01ed language model pr e-training for natural language\nunderstanding and generation. Advances in neural information processing systems , 32, 2019.\n[Dong et al., 2022] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun,\nJingjing Xu, and Zhifang Sui. A survey on in-context learnin g.arXiv preprint arXiv:2301.00234 , 2022.\n[Dong et al., 2021] Yihe Dong, Jean-Baptiste Cordonnier, an d Andreas Loukas. Attention is not all you\nneed: Pure attention loses rank doubly exponentially with d epth. In International Conference on Machine\nLearning , pages 2793\u20132803. PMLR, 2021.\n[Drozdov et al., 2022] Andrew Drozdov, Nathanael Sch\u00e4rli, E kin Aky\u00fcrek, Nathan Scales, Xinying Song,\nXinyun Chen, Olivier Bousquet, and Denny Zhou. Composition al semantic parsing with large language\nmodels. In Proceedings of The Eleventh International Conference on Le arning Representations , 2022.\n[Dua et al., 2022] Dheeru Dua, Shivanshu Gupta, Sameer Singh , and Matt Gardner. Successive prompting\nfor decomposing complex questions. In Proceedings of the 2022 Conference on Empirical Methods in\nNatural Language Processing , pages 1251\u20131265, 2022.\n[Dubey et al., 2024] Abhimanyu Dubey, Abhinav Jauhri, Abhin av Pandey, Abhishek Kadian, Ahmad Al-\nDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang , Angela Fan, et al. The llama 3 herd of\nmodels. arXiv preprint arXiv:2407.21783 , 2024.\n[Dubois et al., 2024] Yann Dubois, Chen Xuechen Li, Rohan Tao ri, Tianyi Zhang, Ishaan Gulrajani, Jimmy\nBa, Carlos Guestrin, Percy S Liang, and Tatsunori B Hashimot o. Alpacafarm: A simulation framework\nfor methods that learn from human feedback. Advances in Neural Information Processing Systems , 36,\n2024.\n[Eisenstein et al., 2023] Jacob Eisenstein, Chirag Nagpal, Alekh Agarwal, Ahmad Beirami, Alex D\u2019Amour,\nDJ Dvijotham, Adam Fisch, Katherine Heller, Stephen Pfohl, Deepak Ramachandran, and Peter Shaw.\nHelping or herding? reward model ensembles mitigate but do n ot eliminate reward hacking. arXiv\npreprint arXiv:2312.09244 , 2023.\n[Elsken et al., 2019] Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search:\nA survey. Journal of Machine Learning Research , 20(55):1\u201321, 2019.\n\n--- Page 215 ---\n\n208 Alignment\n[Erhan et al., 2010] Dumitru",
    "315f52337fb525a9": " methods that learn from human feedback. Advances in Neural Information Processing Systems , 36,\n2024.\n[Eisenstein et al., 2023] Jacob Eisenstein, Chirag Nagpal, Alekh Agarwal, Ahmad Beirami, Alex D\u2019Amour,\nDJ Dvijotham, Adam Fisch, Katherine Heller, Stephen Pfohl, Deepak Ramachandran, and Peter Shaw.\nHelping or herding? reward model ensembles mitigate but do n ot eliminate reward hacking. arXiv\npreprint arXiv:2312.09244 , 2023.\n[Elsken et al., 2019] Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search:\nA survey. Journal of Machine Learning Research , 20(55):1\u201321, 2019.\n\n--- Page 215 ---\n\n208 Alignment\n[Erhan et al., 2010] Dumitru Erhan, Aaron Courville, Yoshua Bengio, and Pascal Vincent. Why does\nunsupervised pre-training help deep learning? In Proceedings of the thirteenth international conference\non arti\ufb01cial intelligence and statistics , pages 201\u2013208, 2010.\n[Fan et al., 2019] Angela Fan, Edouard Grave, and Armand Joul in. Reducing transformer depth on demand\nwith structured dropout. In Proceedings of International Conference on Learning Repre sentations , 2019.\n[Fedus et al., 2022] William Fedus, Barret Zoph, and Noam Sha zeer. Switch transformers: Scaling to\ntrillion parameter models with simple and ef\ufb01cient sparsit y.The Journal of Machine Learning Research ,\n23(1):5232\u20135270, 2022.\n[Fernandes et al., 2023] Patrick Fernandes, Aman Madaan, Em my Liu, Ant\u00f3nio Farinhas, Pedro Henrique\nMartins, Amanda Bertsch, Jos\u00e9 G. C. de Souza, Shuyan Zhou, To ngshuang Wu, Graham Neubig, and\nAndr\u00e9 F. T. Martins. Bridging the gap: A survey on integratin g (human) feedback for natural language\ngeneration. Transactions of the Association for Computational Linguis tics, 11:1643\u20131668, 2023.\n[Franklin and Graesser, 1996] Stan Franklin and Art Graesse r. Is it an agent, or just a program?: A taxon-\nomy for autonomous agents. In International workshop on agent theories, architectures, and languages ,\npages 21\u201335. Springer, 1996.\n[Frensch and Funke, 2014] Peter A Frensch and Joachim Funke. Complex problem solving: The European\nperspective . Psychology Press, 2014.\n[Gale et al., 2019] Trevor Gale, Erich Elsen, and Sara Hooker . The state of sparsity in deep neural networks.\narXiv preprint arXiv:1902.09574 , 2019.\n[Ganguli et al., 2023] Deep Ganguli, Amanda Askell, Nichola s Schiefer, Thomas I. Liao, Kamile Luko-\nsiute, Anna Chen, Anna Goldie, Azalia Mirhoseini, Catherin e Olsson, Danny Hernandez, Dawn Drain,\nDustin Li, Eli Tran-Johnson, Ethan Perez, Jackson Kernion, Jamie Kerr, Jared Mueller, Joshua Landau,\nKamal Ndousse, Karina Nguyen, Liane Lovitt, Michael Sellit to, Nelson Elhage, Noem\u00ed Mercado, Nova\nDasSarma, Oliver Rausch, Robert Lasenby, Robin Larson, Sam Ringer, Sandipan Kundu, Saurav Kada-\nvath, Scott Johnston, Shauna Kravec, Sheer El Showk, Tamera Lanham, Timothy Telleen-Lawton, Tom\nHenighan, Tristan Hume, Yuntao Bai, Zac Hat\ufb01eld-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph,\nSam McCandlish, Tom Brown, Christopher Olah, Jack Clark, Sa muel R. Bowman, and Jared Kaplan.\nThe capacity for moral self-correction in large language mo dels. arXiv preprint arXiv:2302.07459 , 2023.\n[Gao et al., 2023] Leo Gao, John Schulman, and Jacob Hilton. S caling laws for reward model overopti-\nmization. In International Conference on Machine Learning , pages 10835\u201310866. PMLR, 2023a.\n[Gao et al",
    "19138dc2402890d3": " Sandipan Kundu, Saurav Kada-\nvath, Scott Johnston, Shauna Kravec, Sheer El Showk, Tamera Lanham, Timothy Telleen-Lawton, Tom\nHenighan, Tristan Hume, Yuntao Bai, Zac Hat\ufb01eld-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph,\nSam McCandlish, Tom Brown, Christopher Olah, Jack Clark, Sa muel R. Bowman, and Jared Kaplan.\nThe capacity for moral self-correction in large language mo dels. arXiv preprint arXiv:2302.07459 , 2023.\n[Gao et al., 2023] Leo Gao, John Schulman, and Jacob Hilton. S caling laws for reward model overopti-\nmization. In International Conference on Machine Learning , pages 10835\u201310866. PMLR, 2023a.\n[Gao et al., 2023] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alo n, Pengfei Liu, Yiming Yang, Jamie\nCallan, and Graham Neubig. Pal: Program-aided language mod els. In International Conference on\nMachine Learning , pages 10764\u201310799. PMLR, 2023b.\n[Gao et al., 2023] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxian g Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei\nSun, and Haofen Wang. Retrieval-augmented generation for l arge language models: A survey. arXiv\npreprint arXiv:2312.10997 , 2023c.\n[Garg et al., 2022] Shivam Garg, Dimitris Tsipras, Percy S Li ang, and Gregory Valiant. What can trans-\nformers learn in-context? a case study of simple function cl asses. Advances in Neural Information\nProcessing Systems , 35:30583\u201330598, 2022.\n[Ge et al., 2024] Yuan Ge, Yilun Liu, Chi Hu, Weibin Meng, Shim in Tao, Xiaofeng Zhao, Hongxia\nMa, Li Zhang, Boxing Chen, Hao Yang, Bei Li, Tong Xiao, and Jin gbo Zhu. Clustering and rank-\ning: Diversity-preserved instruction selection through e xpert-aligned quality estimation. arXiv preprint\narXiv:2402.18191 , 2024.\n[Gemma Team, 2024] Google DeepMind Gemma Team. Gemma: Open M odels Based on Gemini Re-\nsearch and Technology, 2024.\n[Goodhart, 1984] Charles AE Goodhart. Problems of monetary management: the UK experience . Springer,\n1984.\n\n--- Page 216 ---\n\n4.5 Summary 209\n[Gordon et al., 2021] Mitchell A Gordon, Kevin Duh, and Jared Kaplan. Data and parameter scaling laws\nfor neural machine translation. In Proceedings of the 2021 Conference on Empirical Methods in N atural\nLanguage Processing , pages 5915\u20135922, 2021.\n[Gu and Dao, 2023] Albert Gu and Tri Dao. Mamba: Linear-time s equence modeling with selective state\nspaces. arXiv preprint arXiv:2312.00752 , 2023.\n[Gunasekar et al., 2023] Suriya Gunasekar, Yi Zhang, Jyoti A neja, Caio C\u00e9sar Teodoro Mendes, Allie Del\nGiorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann , Gustavo de Rosa, Olli Saarikivi, Adil\nSalim, Shital Shah, Harkirat Singh Behl, Xin Wang, S\u00e9bastie n Bubeck, Ronen Eldan, Adam Tauman\nKalai, Yin Tat Lee, and Yuanzhi Li. Textbooks are all you need .arXiv preprint arXiv:2306.11644 , 2023.\n[Guo et al., 2024] Qingyan Guo, Rui Wang, Junliang Guo, Bei Li , Kaitao Song, Xu Tan, Guoqing Liu, Jiang\nBian, and Yujiu Yang. Connecting large language models with evolutionary algorithms yields powerful\nprompt optimizers. In The Twelfth International Conference on Learning Represen tations , 2024.\n[Gupta and Berant, 2020] Ankit Gupta and Jonathan Berant. Gm at: Global memory augmentation for\n",
    "98db8abcf5ce2d1d": "avo de Rosa, Olli Saarikivi, Adil\nSalim, Shital Shah, Harkirat Singh Behl, Xin Wang, S\u00e9bastie n Bubeck, Ronen Eldan, Adam Tauman\nKalai, Yin Tat Lee, and Yuanzhi Li. Textbooks are all you need .arXiv preprint arXiv:2306.11644 , 2023.\n[Guo et al., 2024] Qingyan Guo, Rui Wang, Junliang Guo, Bei Li , Kaitao Song, Xu Tan, Guoqing Liu, Jiang\nBian, and Yujiu Yang. Connecting large language models with evolutionary algorithms yields powerful\nprompt optimizers. In The Twelfth International Conference on Learning Represen tations , 2024.\n[Gupta and Berant, 2020] Ankit Gupta and Jonathan Berant. Gm at: Global memory augmentation for\ntransformers. arXiv preprint arXiv:2006.03274 , 2020.\n[Gupta et al., 2021] Ankit Gupta, Guy Dar, Shaya Goodman, Dav id Ciprut, and Jonathan Berant. Memory-\nef\ufb01cient transformers via top-k attention. In Proceedings of the Second Workshop on Simple and Ef\ufb01cient\nNatural Language Processing , pages 39\u201352, 2021.\n[Han et al., 2021] Xu Han, Zhengyan Zhang, Ning Ding, Yuxian G u, Xiao Liu, Yuqi Huo, Jiezhong Qiu,\nLiang Zhang, Wentao Han, Minlie Huang, Qin Jin, Yanyan Lan, Y ang Liu, Zhiyuan Liu, Zhiwu Lu,\nXipeng Qiu, Ruihua Song, Jie Tang, Ji-Rong Wen, Jinhui Yuan, Wayne Xin Zhao, and Jun Zhu. Pre-\ntrained models: Past, present and future. AI Open , 2:225\u2013250, 2021.\n[Han et al., 2024] Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang , and Sai Qian Zhang. Parameter-ef\ufb01cient\n\ufb01ne-tuning for large models: A comprehensive survey. arXiv preprint arXiv:2403.14608 , 2024.\n[Harlap et al., 2018] Aaron Harlap, Deepak Narayanan, Amar P hanishayee, Vivek Seshadri, Nikhil Deva-\nnur, Greg Ganger, and Phil Gibbons. Pipedream: Fast and ef\ufb01c ient pipeline parallel dnn training. arXiv\npreprint arXiv:1806.03377 , 2018.\n[He et al., 2019] Kaiming He, Ross Girshick, and Piotr Doll\u00e1r . Rethinking imagenet pre-training. In\nProceedings of the IEEE/CVF International Conference on Co mputer Vision , pages 4918\u20134927, 2019.\n[He et al., 2021] Pengcheng He, Xiaodong Liu, Jianfeng Gao, a nd Weizhu Chen. Deberta: Decoding-\nenhanced bert with disentangled attention. In Proceedings of International Conference on Learning\nRepresentations , 2021.\n[Hendrycks and Gimpel, 2016] Dan Hendrycks and Kevin Gimpel . Gaussian error linear units (gelus).\narXiv preprint arXiv:1606.08415 , 2016.\n[Hendrycks et al., 2020] Dan Hendrycks, Xiaoyuan Liu, Eric W allace, Adam Dziedzic, Rishabh Krishnan,\nand Dawn Song. Pretrained transformers improve out-of-dis tribution robustness. In Proceedings of the\n58th Annual Meeting of the Association for Computational Li nguistics , pages 2744\u20132751, 2020.\n[Hendrycks et al., 2021] Dan Hendrycks, Collin Burns, Steve n Basart, Andy Zou, Mantas Mazeika, Dawn\nSong, and Jacob Steinhardt. Measuring massive multitask la nguage understanding. In Proceedings of\nInternational Conference on Learning Representations , 2021.\n[Hestness et al., 2017] Joel Hestness, Sharan Narang, Newsh a Ardalani, Gregory Diamos, Heewoo Jun,\nHassan Kianinejad, Md Mostofa Ali Patwary, Yang Yang, and Ya nqi Zhou. Deep learning scaling is\npredictable, empirically. arXiv preprint arXiv",
    "fe0488a7ac522df1": "nan,\nand Dawn Song. Pretrained transformers improve out-of-dis tribution robustness. In Proceedings of the\n58th Annual Meeting of the Association for Computational Li nguistics , pages 2744\u20132751, 2020.\n[Hendrycks et al., 2021] Dan Hendrycks, Collin Burns, Steve n Basart, Andy Zou, Mantas Mazeika, Dawn\nSong, and Jacob Steinhardt. Measuring massive multitask la nguage understanding. In Proceedings of\nInternational Conference on Learning Representations , 2021.\n[Hestness et al., 2017] Joel Hestness, Sharan Narang, Newsh a Ardalani, Gregory Diamos, Heewoo Jun,\nHassan Kianinejad, Md Mostofa Ali Patwary, Yang Yang, and Ya nqi Zhou. Deep learning scaling is\npredictable, empirically. arXiv preprint arXiv:1712.00409 , 2017.\n[Hewitt, 2024] John Hewitt. Instruction following without instruction tuning, 2024. URL https://nlp.\nstanford.edu/~johnhew/instruction-following.html .\n[Hewitt et al., 2024] John Hewitt, Nelson F Liu, Percy Liang, and Christopher D Manning. Instruction\nfollowing without instruction tuning. arXiv preprint arXiv:2409.14254 , 2024.\n\n--- Page 217 ---\n\n210 Alignment\n[Hochreiter and Schmidhuber, 1997] Sepp Hochreiter and J\u00fcr gen Schmidhuber. Long short-term memory.\nNeural computation , 9(8):1735\u20131780, 1997.\n[Hoffmann et al., 2022] Jordan Hoffmann, Sebastian Borgeau d, Arthur Mensch, Elena Buchatskaya, Trevor\nCai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendric ks, Johannes Welbl, Aidan Clark, Tom\nHennigan, Eric Noland, Katie Millican, George van den Dries sche, Bogdan Damoc, Aurelia Guy, Si-\nmon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Orio l Vinyals, and Laurent Sifre. Training\ncompute-optimal large language models. arXiv preprint arXiv:2203.15556 , 2022.\n[Honovich et al., 2023] Or Honovich, Thomas Scialom, Omer Le vy, and Timo Schick. Unnatural instruc-\ntions: Tuning language models with (almost) no human labor. InProceedings of the 61st Annual Meeting\nof the Association for Computational Linguistics (Volume 1 : Long Papers) , pages 14409\u201314428, 2023.\n[Houlsby et al., 2019] Neil Houlsby, Andrei Giurgiu, Stanis law Jastrzebski, Bruna Morrone, Quentin\nDe Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylva in Gelly. Parameter-ef\ufb01cient transfer\nlearning for NLP. In Proceedings of the 36th International Conference on Machin e Learning , pages\n2790\u20132799. PMLR, 2019.\n[Hu et al., 2022] Edward J Hu, yelong shen, Phillip Wallis, Ze yuan Allen-Zhu, Yuanzhi Li, Shean Wang,\nLu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International\nConference on Learning Representations , 2022.\n[Huang, 2009] Liang Huang. Dynamic programming-based sear ch algorithms in NLP. In Proceedings\nof Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the\nAssociation for Computational Linguistics, Companion Vol ume: Tutorial Abstracts , 2009.\n[Huang et al., 2019] Yanping Huang, Youlong Cheng, Ankur Bap na, Orhan Firat, Mia Xu Chen, Dehao\nChen, HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, an d Zhifeng Chen. Gpipe: Ef\ufb01cient\ntraining of giant neural networks using pipeline paralleli sm.Advances in neural information processing\nsystems , 32, 2019.\n[Hutchins et al., 2022] DeLesley Hutchins, Imanol Schlag, Y uhuai Wu, Ethan Dyer, and Behnam\nNeyshabur. Block-recurrent transformers",
    "a3a111944ec9eb14": "-based sear ch algorithms in NLP. In Proceedings\nof Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the\nAssociation for Computational Linguistics, Companion Vol ume: Tutorial Abstracts , 2009.\n[Huang et al., 2019] Yanping Huang, Youlong Cheng, Ankur Bap na, Orhan Firat, Mia Xu Chen, Dehao\nChen, HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, an d Zhifeng Chen. Gpipe: Ef\ufb01cient\ntraining of giant neural networks using pipeline paralleli sm.Advances in neural information processing\nsystems , 32, 2019.\n[Hutchins et al., 2022] DeLesley Hutchins, Imanol Schlag, Y uhuai Wu, Ethan Dyer, and Behnam\nNeyshabur. Block-recurrent transformers. Advances in neural information processing systems , 35:\n33248\u201333261, 2022.\n[Jelinek, 1998] Frederick Jelinek. Statistical methods for speech recognition . MIT Press, 1998.\n[Jiang et al., 2023] Albert Q Jiang, Alexandre Sablayrolles , Arthur Mensch, Chris Bamford, Deven-\ndra Singh Chaplot, Diego de las Casas, Florian Bressand, Gia nna Lengyel, Guillaume Lample, Lucile\nSaulnier, L\u00e9lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril,\nThomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. Mistra l 7b. arXiv preprint arXiv:2310.06825 ,\n2023a.\n[Jiang et al., 2023] Huiqiang Jiang, Qianhui Wu, Chin-Yew Li n, Yuqing Yang, and Lili Qiu. Llmlingua:\nCompressing prompts for accelerated inference of large lan guage models. In Proceedings of the 2023\nConference on Empirical Methods in Natural Language Proces sing, pages 13358\u201313376, 2023b.\n[Jiang et al., 2020] Zhengbao Jiang, Frank F Xu, Jun Araki, an d Graham Neubig. How can we know what\nlanguage models know? Transactions of the Association for Computational Linguis tics, 8:423\u2013438,\n2020.\n[Jiao et al., 2020] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xi n Jiang, Xiao Chen, Linlin Li, Fang Wang,\nand Qun Liu. Tinybert: Distilling bert for natural language understanding. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020 , pages 4163\u20134174, 2020.\n[Joshi et al., 2017] Mandar Joshi, Eunsol Choi, Daniel S Weld , and Luke Zettlemoyer. Triviaqa: A large\nscale distantly supervised challenge dataset for reading c omprehension. In Proceedings of the 55th\nAnnual Meeting of the Association for Computational Lingui stics (Volume 1: Long Papers) , pages 1601\u2013\n1611, 2017.\n[Joshi et al., 2020] Mandar Joshi, Danqi Chen, Yinhan Liu, Da niel S Weld, Luke Zettlemoyer, and Omer\nLevy. Spanbert: Improving pre-training by representing an d predicting spans. Transactions of the\n\n--- Page 218 ---\n\n4.5 Summary 211\nassociation for computational linguistics , 8:64\u201377, 2020.\n[Jurafsky and Martin, 2008] Dan Jurafsky and James H. Martin .Speech and Language Processing (2nd\ned.). Prentice Hall, 2008.\n[Kahneman, 2011] Daniel Kahneman. Thinking, fast and slow . macmillan, 2011.\n[Kaplan et al., 2020] Jared Kaplan, Sam McCandlish, Tom Heni ghan, Tom B Brown, Benjamin Chess, Re-\nwon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Am odei. Scaling laws for neural language\nmodels. arXiv preprint arXiv:2001.08361 , 2020.\n[Katharopoulos et al., 2020] Angelos Katharopoulos, Apoor v Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret.\nTransformers are rnns: Fast autoregressive transformers w ith linear attention. In International",
    "c6fd27f99b4727a7": ".\n[Jurafsky and Martin, 2008] Dan Jurafsky and James H. Martin .Speech and Language Processing (2nd\ned.). Prentice Hall, 2008.\n[Kahneman, 2011] Daniel Kahneman. Thinking, fast and slow . macmillan, 2011.\n[Kaplan et al., 2020] Jared Kaplan, Sam McCandlish, Tom Heni ghan, Tom B Brown, Benjamin Chess, Re-\nwon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Am odei. Scaling laws for neural language\nmodels. arXiv preprint arXiv:2001.08361 , 2020.\n[Katharopoulos et al., 2020] Angelos Katharopoulos, Apoor v Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret.\nTransformers are rnns: Fast autoregressive transformers w ith linear attention. In International conference\non machine learning , pages 5156\u20135165. PMLR, 2020.\n[Khandelwal et al., 2020] Urvashi Khandelwal, Omer Levy, Da n Jurafsky, Luke Zettlemoyer, and Mike\nLewis. Generalization through memorization: Nearest neig hbor language models. In International\nConference on Learning Representations , 2020.\n[Khot et al., 2023] Tushar Khot, Harsh Trivedi, Matthew Finl ayson, Yao Fu, Kyle Richardson, Peter Clark,\nand Ashish Sabharwal. Decomposed prompting: A modular appr oach for solving complex tasks. In\nProceedings of The Eleventh International Conference on Le arning Representations , 2023.\n[Kim et al., 2023] Sehoon Kim, Coleman Hooper, Thanakul Watt anawong, Minwoo Kang, Ruohan\nYan, Hasan Genc, Grace Dinh, Qijing Huang, Kurt Keutzer, Mic hael W. Mahoney, Yakun Sophia\nShao, and Amir Gholami. Full stack optimization of transfor mer inference: a survey. arXiv preprint\narXiv:2302.14017 , 2023.\n[Kirkpatrick et al., 2017] James Kirkpatrick, Razvan Pasca nu, Neil Rabinowitz, Joel Veness, Guillaume\nDesjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago R amalho, Agnieszka Grabska-Barwinska,\nDemis Hassabis, Claudia Clopath, Dharshan Kumaran, and Rai a Hadsell. Overcoming catastrophic\nforgetting in neural networks. Proceedings of the national academy of sciences , 114(13):3521\u20133526,\n2017.\n[Koehn, 2010] Philipp Koehn. Statistical Machine Translation . Cambridge University Press, 2010.\n[Kojima et al., 2022] Takeshi Kojima, Shixiang Shane Gu, Mac hel Reid, Yutaka Matsuo, and Yusuke\nIwasawa. Large language models are zero-shot reasoners. Advances in neural information processing\nsystems , 35:22199\u201322213, 2022.\n[Korthikanti et al., 2023] Vijay Anand Korthikanti, Jared C asper, Sangkug Lym, Lawrence McAfee,\nMichael Andersch, Mohammad Shoeybi, and Bryan Catanzaro. R educing activation recomputation in\nlarge transformer models. Proceedings of Machine Learning and Systems , 5, 2023.\n[Krakovna et al., 2020] Victoria Krakovna, Jonathan Uesato , Vladimir Mikulik, Matthew\nRahtz, Tom Everitt, Ramana Kumar, Zac Kenton, Jan Leike, and Shane Legg. Speci\ufb01-\ncation gaming: the \ufb02ip side of ai ingenuity. https://deepmind.google/discover/blog/\nspecification-gaming-the-flip-side-of-ai-ingenuity , 2020.\n[Kung and Peng, 2023] Po-Nien Kung and Nanyun Peng. Do models really learn to follow instructions?\nan empirical study of instruction tuning. arXiv preprint arXiv:2305.11383 , 2023.\n[Kwon et al., 2023] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Y ing Sheng, Lianmin Zheng, Cody Hao\nYu, Joseph E Gonzalez, Hao Zhang, and Ion Stoica. Ef\ufb01cient me mory management for large language\nmodel serving with pagedattention. arXiv preprint ar",
    "17042f6fb6a8c62e": "z, Tom Everitt, Ramana Kumar, Zac Kenton, Jan Leike, and Shane Legg. Speci\ufb01-\ncation gaming: the \ufb02ip side of ai ingenuity. https://deepmind.google/discover/blog/\nspecification-gaming-the-flip-side-of-ai-ingenuity , 2020.\n[Kung and Peng, 2023] Po-Nien Kung and Nanyun Peng. Do models really learn to follow instructions?\nan empirical study of instruction tuning. arXiv preprint arXiv:2305.11383 , 2023.\n[Kwon et al., 2023] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Y ing Sheng, Lianmin Zheng, Cody Hao\nYu, Joseph E Gonzalez, Hao Zhang, and Ion Stoica. Ef\ufb01cient me mory management for large language\nmodel serving with pagedattention. arXiv preprint arXiv:2309.06180 , 2023.\n[Lake and Baroni, 2018] Brenden Lake and Marco Baroni. Gener alization without systematicity: On\nthe compositional skills of sequence-to-sequence recurre nt networks. In International conference on\nmachine learning , pages 2873\u20132882. PMLR, 2018.\n[Lambert et al., 2024] Nathan Lambert, Valentina Pyatkin, J acob Morrison, LJ Miranda, Bill Yuchen\nLin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Ye jin Choi, Noah A. Smith, and Han-\nnaneh Hajishirzi. Rewardbench: Evaluating reward models f or language modeling. arXiv preprint\n\n--- Page 219 ---\n\n212 Alignment\narXiv:2403.13787 , 2024.\n[Lample and Conneau, 2019] Guillaume Lample and Alexis Conn eau. Cross-lingual language model\npretraining. arXiv preprint arXiv:1901.07291 , 2019.\n[Lan et al., 2020] Zhenzhong Lan, Mingda Chen, Sebastian Goo dman, Kevin Gimpel, Piyush Sharma, and\nRadu Soricut. Albert: A lite bert for self-supervised learn ing of language representations. In Proceedings\nof International Conference on Learning Representations , 2020.\n[Lee et al., 2023] Harrison Lee, Samrat Phatale, Hassan Mans oor, Kellie Ren Lu, Thomas Mesnard, Johan\nFerret, Colton Bishop, Ethan Hall, Victor Carbune, and Abhi nav Rastogi. Rlaif: Scaling reinforcement\nlearning from human feedback with ai feedback. arXiv preprint arXiv:2309.00267 , 2023.\n[Lester et al., 2021] Brian Lester, Rami Al-Rfou, and Noah Co nstant. The power of scale for parameter-\nef\ufb01cient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in N atural Lan-\nguage Processing , pages 3045\u20133059, 2021.\n[Lewis et al., 2020] Mike Lewis, Yinhan Liu, Naman Goyal, Mar jan Ghazvininejad, Abdelrahman Mo-\nhamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. B art: Denoising sequence-to-sequence\npre-training for natural language generation, translatio n, and comprehension. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational Lingui stics, pages 7871\u20137880, 2020.\n[Li et al., 2023] Bei Li, Rui Wang, Junliang Guo, Kaitao Song, Xu Tan, Hany Hassan, Arul Menezes, Tong\nXiao, Jiang Bian, and JingBo Zhu. Deliberate then generate: Enhanced prompting framework for text\ngeneration. arXiv preprint arXiv:2305.19835 , 2023a.\n[Li, 2011] Hang Li. Learning to Rank for Information Retrieval and Natural Lang uage Processing . Online\naccess: Morgan & Claypool Synthesis Collection Five. Morga n & Claypool Publishers, 2011. ISBN\n9781608457076.\n[Li et al., 2022] Huayang Li, Yixuan Su, Deng Cai, Yan Wang, an d Lemao Liu. A survey on retrieval-\naugmented text generation. arXiv preprint arXiv:2202.01110 , 2022.\n[Li",
    "e1dd881812a73465": "3] Bei Li, Rui Wang, Junliang Guo, Kaitao Song, Xu Tan, Hany Hassan, Arul Menezes, Tong\nXiao, Jiang Bian, and JingBo Zhu. Deliberate then generate: Enhanced prompting framework for text\ngeneration. arXiv preprint arXiv:2305.19835 , 2023a.\n[Li, 2011] Hang Li. Learning to Rank for Information Retrieval and Natural Lang uage Processing . Online\naccess: Morgan & Claypool Synthesis Collection Five. Morga n & Claypool Publishers, 2011. ISBN\n9781608457076.\n[Li et al., 2022] Huayang Li, Yixuan Su, Deng Cai, Yan Wang, an d Lemao Liu. A survey on retrieval-\naugmented text generation. arXiv preprint arXiv:2202.01110 , 2022.\n[Li et al., 2024] Shanda Li, Chong You, Guru Guruganesh, Josh ua Ainslie, Santiago Ontanon, Manzil\nZaheer, Sumit Sanghai, Yiming Yang, Sanjiv Kumar, and Srina dh Bhojanapalli. Functional interpolation\nfor relative positions improves long context transformers . In The Twelfth International Conference on\nLearning Representations , 2024.\n[Li et al., 2023] Shenggui Li, Fuzhao Xue, Chaitanya Baranwa l, Yongbin Li, and Yang You. Sequence\nparallelism: Long sequence training from system perspecti ve. In Proceedings of the 61st Annual Meeting\nof the Association for Computational Linguistics (Volume 1 : Long Papers) , pages 2391\u20132404, 2023b.\n[Li and Liang, 2021] Xiang Lisa Li and Percy Liang. Pre\ufb01x-tun ing: Optimizing continuous prompts for\ngeneration. In Proceedings of the 59th Annual Meeting of the Association fo r Computational Linguistics\nand the 11th International Joint Conference on Natural Lang uage Processing (Volume 1: Long Papers) ,\npages 4582\u20134597, 2021.\n[Li, 2023] Yinheng Li. A practical survey on zero-shot promp t design for in-context learning. In Proceed-\nings of the 14th International Conference on Recent Advance s in Natural Language Processing , pages\n641\u2013647, 2023.\n[Li et al., 2023] Yucheng Li, Bo Dong, Frank Guerin, and Cheng hua Lin. Compressing context to enhance\ninference ef\ufb01ciency of large language models. In Proceedings of the 2023 Conference on Empirical\nMethods in Natural Language Processing , pages 6342\u20136353, 2023c.\n[Lialin et al., 2023] Vladislav Lialin, Vijeta Deshpande, a nd Anna Rumshisky. Scaling down to scale up:\nA guide to parameter-ef\ufb01cient \ufb01ne-tuning. arXiv preprint arXiv:2303.15647 , 2023.\n[Lightman et al., 2024] Hunter Lightman, Vineet Kosaraju, Y uri Burda, Harrison Edwards, Bowen Baker,\nTeddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Kar l Cobbe. Let\u2019s verify step by step. In The\nTwelfth International Conference on Learning Representat ions, 2024.\n[Liu et al., 2024] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wan g, Bochao Wu, Chengda Lu, Chenggang\n\n--- Page 220 ---\n\n4.5 Summary 213\nZhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepsee k-v3 technical report. arXiv preprint\narXiv:2412.19437 , 2024a.\n[Liu et al., 2022] Jiachang Liu, Dinghan Shen, Yizhe Zhang, W illiam B Dolan, Lawrence Carin, and\nWeizhu Chen. What makes good in-context examples for gpt-3? InProceedings of Deep Learning Inside\nOut (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning\nArchitectures , pages 100\u2013114, 2022.\n[Liu et al., 2023] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zheng bao Jiang, Hiroaki Hayashi, and Graham\nNeubig. Pre-train, prompt, and predict",
    "573bd4e0a39a965a": "5 Summary 213\nZhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepsee k-v3 technical report. arXiv preprint\narXiv:2412.19437 , 2024a.\n[Liu et al., 2022] Jiachang Liu, Dinghan Shen, Yizhe Zhang, W illiam B Dolan, Lawrence Carin, and\nWeizhu Chen. What makes good in-context examples for gpt-3? InProceedings of Deep Learning Inside\nOut (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning\nArchitectures , pages 100\u2013114, 2022.\n[Liu et al., 2023] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zheng bao Jiang, Hiroaki Hayashi, and Graham\nNeubig. Pre-train, prompt, and predict: A systematic surve y of prompting methods in natural language\nprocessing. ACM Computing Surveys , 55(9):1\u201335, 2023a.\n[Liu et al., 2024] Tianqi Liu, Yao Zhao, Rishabh Joshi, Misha Khalman, Mohammad Saleh, Peter J Liu, and\nJialu Liu. Statistical rejection sampling improves prefer ence optimization. In The Twelfth International\nConference on Learning Representations , 2024b.\n[Liu, 2009] Tie-Yan Liu. Learning to rank for information re trieval. Foundations and Trends\u00ae in Informa-\ntion Retrieval , 3(3):225\u2013331, 2009.\n[Liu et al., 2023] Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming D ing, Yujie Qian, Zhilin Yang, and Jie\nTang. Gpt understands, too. AI Open , 2023b.\n[Liu et al., 2023] Xiaoxia Liu, Jingyi Wang, Jun Sun, Xiaohan Yuan, Guoliang Dong, Peng Di, Wenhai\nWang, and Dongxia Wang. Prompting frameworks for large lang uage models: A survey. arXiv preprint\narXiv:2311.12785 , 2023c.\n[Liu et al., 2024] Xinyu Liu, Runsong Zhao, Pengcheng Huang, Chunyang Xiao, Bei Li, Jingang Wang,\nTong Xiao, and Jingbo Zhu. Forgetting curve: A reliable meth od for evaluating memorization capability\nfor long-context models. In Proceedings of the 2024 Conference on Empirical Methods in N atural\nLanguage Processing , pages 4667\u20134682, 2024c.\n[Liu et al., 2019] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,\nMike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Robert a: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692 , 2019.\n[Longpre et al., 2023] Shayne Longpre, Le Hou, Tu Vu, Albert W ebson, Hyung Won Chung, Yi Tay,\nDenny Zhou, Quoc V . Le, Barret Zoph, Jason Wei, and Adam Rober ts. The \ufb02an collection: Designing\ndata and methods for effective instruction tuning. In International Conference on Machine Learning ,\npages 22631\u201322648. PMLR, 2023.\n[Ma et al., 2023] Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxi an He, Liangke Gui, Graham Neubig,\nJonathan May, and Luke Zettlemoyer. Mega: Moving average eq uipped gated attention. In The Eleventh\nInternational Conference on Learning Representations , 2023.\n[Ma et al., 2024] Xuezhe Ma, Xiaomeng Yang, Wenhan Xiong, Bei di Chen, Lili Yu, Hao Zhang, Jonathan\nMay, Luke Zettlemoyer, Omer Levy, and Chunting Zhou. Megalo don: Ef\ufb01cient llm pretraining and\ninference with unlimited context length. arXiv preprint arXiv:2404.08801 , 2024.\n[Madaan et al., 2024] Aman Madaan, Niket Tandon, Prakhar Gup ta, Skyler Hallinan, Luyu Gao,\nSarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye , Yiming Yang,",
    "0ecb503d2865bd8a": "ig,\nJonathan May, and Luke Zettlemoyer. Mega: Moving average eq uipped gated attention. In The Eleventh\nInternational Conference on Learning Representations , 2023.\n[Ma et al., 2024] Xuezhe Ma, Xiaomeng Yang, Wenhan Xiong, Bei di Chen, Lili Yu, Hao Zhang, Jonathan\nMay, Luke Zettlemoyer, Omer Levy, and Chunting Zhou. Megalo don: Ef\ufb01cient llm pretraining and\ninference with unlimited context length. arXiv preprint arXiv:2404.08801 , 2024.\n[Madaan et al., 2024] Aman Madaan, Niket Tandon, Prakhar Gup ta, Skyler Hallinan, Luyu Gao,\nSarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye , Yiming Yang, Shashank Gupta, Bod-\nhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck , Amir Yazdanbakhsh, and Peter Clark.\nSelf-re\ufb01ne: Iterative re\ufb01nement with self-feedback. Advances in Neural Information Processing Sys-\ntems, 36, 2024.\n[Manning, 2022] Christopher D Manning. Human language unde rstanding & reasoning. Daedalus , 151\n(2):127\u2013138, 2022.\n[Marcus, 1993] Gary F Marcus. Negative evidence in language acquisition. Cognition , 46(1):53\u201385, 1993.\n[Martins et al., 2022] Pedro Henrique Martins, Zita Marinho , and Andr\u00e9 FT Martins. \u221e-former: In\ufb01nite\nmemory transformer-former: In\ufb01nite memory transformer. I nProceedings of the 60th Annual Meeting\nof the Association for Computational Linguistics (Volume 1 : Long Papers) , pages 5468\u20135485, 2022.\n\n--- Page 221 ---\n\n214 Alignment\n[Mavi et al., 2024] Vaibhav Mavi, Anubhav Jangra, and Adam Ja towt. Multi-hop question answering.\nFoundations and Trends\u00ae in Information Retrieval , 17(5):457\u2013586, 2024.\n[Michel et al., 2019] Paul Michel, Omer Levy, and Graham Neub ig. Are sixteen heads really better than\none? Advances in neural information processing systems , 32, 2019.\n[Micikevicius et al., 2018] Paulius Micikevicius, Sharan N arang, Jonah Alben, Gregory Diamos, Erich\nElsen, David Garcia, Boris Ginsburg, Michael Houston, Olek sii Kuchaiev, Ganesh Venkatesh, and Hao\nWu. Mixed precision training. In Proceedings of International Conference on Learning Repre sentations ,\n2018.\n[Miettinen, 1999] Kaisa Miettinen. Nonlinear multiobjective optimization , volume 12. Springer Science\n& Business Media, 1999.\n[Mikolov et al., 2013] Tomas Mikolov, Kai Chen, Greg Corrado , and Jeffrey Dean. Ef\ufb01cient estimation\nof word representations in vector space. In Proceedings of the International Conference on Learning\nRepresentations (ICLR 2013) , 2013a.\n[Mikolov et al., 2013] Tomas Mikolov, Ilya Sutskever, Kai Ch en, Greg Corrado, and Jeffrey Dean. Dis-\ntributed representations of words and phrases and their com positionality. In Proceedings of the 26th In-\nternational Conference on Neural Information Processing S ystems - Volume 2 , pages 3111\u20133119, 2013b.\n[Min et al., 2019] Sewon Min, Victor Zhong, Luke Zettlemoyer , and Hannaneh Hajishirzi. Multi-hop read-\ning comprehension through question decomposition and resc oring. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguistics , pages 6097\u20136109, 2019.\n[Minaee et al., 2024] Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard\nSocher, Xavier Amatriain, and Jianfeng Gao. Large language models: A survey. arXiv preprint\narXiv:2402.06196 , 2024.\n[Mishra et al., 2022] Swaroop Mishra, Daniel Khashabi, Chit ta",
    "4cac8de7b47bed9b": " ystems - Volume 2 , pages 3111\u20133119, 2013b.\n[Min et al., 2019] Sewon Min, Victor Zhong, Luke Zettlemoyer , and Hannaneh Hajishirzi. Multi-hop read-\ning comprehension through question decomposition and resc oring. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguistics , pages 6097\u20136109, 2019.\n[Minaee et al., 2024] Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard\nSocher, Xavier Amatriain, and Jianfeng Gao. Large language models: A survey. arXiv preprint\narXiv:2402.06196 , 2024.\n[Mishra et al., 2022] Swaroop Mishra, Daniel Khashabi, Chit ta Baral, and Hannaneh Hajishirzi. Cross-\ntask generalization via natural language crowdsourcing in structions. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Linguistics ( Volume 1: Long Papers) , pages 3470\u20133487,\n2022.\n[Mnih et al., 2016] V olodymyr Mnih, Adri\u00e0 Puigdom\u00e8nech Badi a, Mehdi Mirza, Alex Graves, Tim Harley,\nTimothy P Lillicrap, David Silver, and Koray Kavukcuoglu. A synchronous methods for deep reinforce-\nment learning. In Proceedings of the 33rd International Conference on Intern ational Conference on\nMachine Learning , pages 1928\u20131937, 2016.\n[Mohtashami and Jaggi, 2024] Amirkeivan Mohtashami and Mar tin Jaggi. Random-access in\ufb01nite context\nlength for transformers. Advances in Neural Information Processing Systems , 36, 2024.\n[Mu et al., 2024] Jesse Mu, Xiang Li, and Noah Goodman. Learni ng to compress prompts with gist tokens.\nAdvances in Neural Information Processing Systems , 36, 2024.\n[Munkhdalai et al., 2024] Tsendsuren Munkhdalai, Manaal Fa ruqui, and Siddharth Gopal. Leave no context\nbehind: Ef\ufb01cient in\ufb01nite context transformers with in\ufb01ni- attention. arXiv preprint arXiv:2404.07143 ,\n2024.\n[Nakano et al., 2021] Reiichiro Nakano, Jacob Hilton, Suchi r Balaji, Jeff Wu, Long Ouyang, Christina\nKim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, Wi lliam Saunders, Xu Jiang, Karl Cobbe, Tyna\nEloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman.\nWebgpt: Browser-assisted question-answering with human f eedback. arXiv preprint arXiv:2112.09332 ,\n2021.\n[Narayanan et al., 2021] Deepak Narayanan, Mohammad Shoeyb i, Jared Casper, Patrick LeGresley,\nMostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Pre thvi Kashinkunti, Julie Bernauer, Bryan\nCatanzaro, Amar Phanishayee, and Matei Zaharia. Ef\ufb01cient l arge-scale language model training on\ngpu clusters using megatron-lm. In Proceedings of the International Conference for High Perfo rmance\nComputing, Networking, Storage and Analysis , pages 1\u201315, 2021.\n\n--- Page 222 ---\n\n4.5 Summary 215\n[Ng et al., 1999] Andrew Y Ng, Daishi Harada, and Stuart J Russ ell. Policy invariance under reward\ntransformations: Theory and application to reward shaping . InProceedings of the Sixteenth International\nConference on Machine Learning , pages 278\u2013287, 1999.\n[OpenAI, 2024] OpenAI. Learning to reason with llms, Septem ber 2024. URL https://openai.com/\nindex/learning-to-reason-with-llms/ .\n[Ouyang et al., 2022] Long Ouyang, Jeffrey Wu, Xu Jiang, Diog o Almeida, Carroll L. Wainwright, Pamela\nMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Al ex Ray, John Schulman, Jacob Hilton,\nFraser Kelton, Luke",
    "eee723ad9edda4c5": "1.\n\n--- Page 222 ---\n\n4.5 Summary 215\n[Ng et al., 1999] Andrew Y Ng, Daishi Harada, and Stuart J Russ ell. Policy invariance under reward\ntransformations: Theory and application to reward shaping . InProceedings of the Sixteenth International\nConference on Machine Learning , pages 278\u2013287, 1999.\n[OpenAI, 2024] OpenAI. Learning to reason with llms, Septem ber 2024. URL https://openai.com/\nindex/learning-to-reason-with-llms/ .\n[Ouyang et al., 2022] Long Ouyang, Jeffrey Wu, Xu Jiang, Diog o Almeida, Carroll L. Wainwright, Pamela\nMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Al ex Ray, John Schulman, Jacob Hilton,\nFraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, P eter Welinder, Paul F. Christiano, Jan\nLeike, and Ryan Lowe. Training language models to follow ins tructions with human feedback. Advances\nin Neural Information Processing Systems , 35:27730\u201327744, 2022.\n[Pal et al., 2023] Koyena Pal, Jiuding Sun, Andrew Yuan, Byro n C Wallace, and David Bau. Future lens:\nAnticipating subsequent tokens from a single hidden state. InProceedings of the 27th Conference on\nComputational Natural Language Learning (CoNLL) , pages 548\u2013560, 2023.\n[Pan et al., 2022] Alexander Pan, Kush Bhatia, and Jacob Stei nhardt. The effects of reward misspeci\ufb01ca-\ntion: Mapping and mitigating misaligned models. In International Conference on Learning Representa-\ntions , 2022.\n[Pan et al., 2024] Liangming Pan, Michael Saxon, Wenda Xu, De epak Nathani, Xinyi Wang, and\nWilliam Yang Wang. Automatically correcting large languag e models: Surveying the landscape of\ndiverse automated correction strategies. Transactions of the Association for Computational Linguis tics,\n12:484\u2013506, 2024.\n[Parisi et al., 2022] Aaron Parisi, Yao Zhao, and Noah Fiedel . Talm: Tool augmented language models.\narXiv preprint arXiv:2205.12255 , 2022.\n[Parisi et al., 2019] German I Parisi, Ronald Kemker, Jose L P art, Christopher Kanan, and Stefan Wermter.\nContinual lifelong learning with neural networks: A review .Neural networks , 113:54\u201371, 2019.\n[Parmar et al., 2018] Niki Parmar, Ashish Vaswani, Jakob Usz koreit, Lukasz Kaiser, Noam Shazeer,\nAlexander Ku, and Dustin Tran. Image transformer. In International conference on machine learn-\ning, pages 4055\u20134064. PMLR, 2018.\n[Penedo et al., 2023] Guilherme Penedo, Quentin Malartic, D aniel Hesslow, Ruxandra Cojocaru, Alessan-\ndro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam A lmazrouei, and Julien Launay. The re\ufb01ned-\nweb dataset for falcon llm: outperforming curated corpora w ith web data, and web data only. arXiv\npreprint arXiv:2306.01116 , 2023.\n[Peng et al., 2024] Bowen Peng, Jeffrey Quesnelle, Honglu Fa n, and Enrico Shippole. YaRN: Ef\ufb01cient con-\ntext window extension of large language models. In The Twelfth International Conference on Learning\nRepresentations , 2024.\n[Pennington et al., 2014] Jeffrey Pennington, Richard Soch er, and Christopher D. Manning. Glove: Global\nvectors for word representation. In Proceedings of Empirical Methods in Natural Language Proce ssing\n(EMNLP) , pages 1532\u20131543, 2014.\n[Peters et al., 2018] Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark,\nKenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. In Proceedings of the\n2018 Conference of the North American Chapter of the Associa tion for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (",
    "17f0a7828a7e8a51": " Jeffrey Quesnelle, Honglu Fa n, and Enrico Shippole. YaRN: Ef\ufb01cient con-\ntext window extension of large language models. In The Twelfth International Conference on Learning\nRepresentations , 2024.\n[Pennington et al., 2014] Jeffrey Pennington, Richard Soch er, and Christopher D. Manning. Glove: Global\nvectors for word representation. In Proceedings of Empirical Methods in Natural Language Proce ssing\n(EMNLP) , pages 1532\u20131543, 2014.\n[Peters et al., 2018] Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark,\nKenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. In Proceedings of the\n2018 Conference of the North American Chapter of the Associa tion for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long Papers) , 2018.\n[Plackett, 1975] Robin L Plackett. The analysis of permutat ions. Journal of the Royal Statistical Society\nSeries C: Applied Statistics , 24(2):193\u2013202, 1975.\n[Prasad et al., 2023] Archiki Prasad, Peter Hase, Xiang Zhou , and Mohit Bansal. Grips: Gradient-free, edit-\nbased instruction search for prompting large language mode ls. In Proceedings of the 17th Conference of\nthe European Chapter of the Association for Computational L inguistics , pages 3845\u20133864, 2023.\n\n--- Page 223 ---\n\n216 Alignment\n[Press et al., 2022] O\ufb01r Press, Noah Smith, and Mike Lewis. Tr ain short, test long: Attention with lin-\near biases enables input length extrapolation. In Proceedings of International Conference on Learning\nRepresentations , 2022.\n[Press et al., 2023] O\ufb01r Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis.\nMeasuring and narrowing the compositionality gap in langua ge models. In Findings of the Association\nfor Computational Linguistics: EMNLP 2023 , pages 5687\u20135711, 2023.\n[Pryzant et al., 2023] Reid Pryzant, Dan Iter, Jerry Li, Yin T at Lee, Chenguang Zhu, and Michael Zeng.\nAutomatic prompt optimization with \"gradient descent\" and beam search. In The 2023 Conference on\nEmpirical Methods in Natural Language Processing , 2023.\n[Qiu et al., 2020] Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai, and Xuanjing Huang.\nPre-trained models for natural language processing: A surv ey.Science China Technological Sciences ,\n63(10):1872\u20131897, 2020.\n[Radford et al., 2018] Alec Radford, Karthik Narasimhan, Ti m Salimans, and Ilya Sutskever. Improving\nlanguage understanding by generative pre-training. OpenAI , 2018.\n[Radford et al., 2019] Alec Radford, Jeffrey Wu, Rewon Child , David Luan, Dario Amodei, and Ilya\nSutskever. Language models are unsupervised multitask lea rners. OpenAI blog , 1(8), 2019.\n[Radford et al., 2021] Alec Radford, Jong Wook Kim, Chris Hal lacy, Aditya Ramesh, Gabriel Goh, Sand-\nhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin , Jack Clark, Gretchen Krueger, and Ilya\nSutskever. Learning transferable visual models from natur al language supervision. In International\nconference on machine learning , pages 8748\u20138763. PMLR, 2021.\n[Rae et al., 2019] Jack W Rae, Anna Potapenko, Siddhant M Jaya kumar, Chloe Hillier, and Timothy P\nLillicrap. Compressive transformers for long-range seque nce modelling. In International Conference on\nLearning Representations , 2019.\n[Rafailov et al., 2024] Rafael Rafailov, Archit Sharma, Eri c Mitchell, Christopher D Manning, Stefano\nErmon, and Chelsea Finn. Direct preference optimization: Y our language model is secretly a reward\nmodel. Advances in Neural Information Processing Systems , 36, 2024.\n[Raffel et al., 2020] Colin Raffel,",
    "1b138fec891979f6": " Clark, Gretchen Krueger, and Ilya\nSutskever. Learning transferable visual models from natur al language supervision. In International\nconference on machine learning , pages 8748\u20138763. PMLR, 2021.\n[Rae et al., 2019] Jack W Rae, Anna Potapenko, Siddhant M Jaya kumar, Chloe Hillier, and Timothy P\nLillicrap. Compressive transformers for long-range seque nce modelling. In International Conference on\nLearning Representations , 2019.\n[Rafailov et al., 2024] Rafael Rafailov, Archit Sharma, Eri c Mitchell, Christopher D Manning, Stefano\nErmon, and Chelsea Finn. Direct preference optimization: Y our language model is secretly a reward\nmodel. Advances in Neural Information Processing Systems , 36, 2024.\n[Raffel et al., 2020] Colin Raffel, Noam Shazeer, Adam Rober ts, Katherine Lee, Sharan Narang, Michael\nMatena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the li mits of transfer learning with a uni\ufb01ed\ntext-to-text transformer. Journal of Machine Learning Research , 21(140):1\u201367, 2020.\n[Ramachandran et al., 2017] Prajit Ramachandran, Barret Zo ph, and Quoc V Le. Searching for activation\nfunctions. arXiv preprint arXiv:1710.05941 , 2017.\n[Rolnick et al., 2019] David Rolnick, Arun Ahuja, Jonathan S chwarz, Timothy Lillicrap, and Gregory\nWayne. Experience replay for continual learning. Advances in Neural Information Processing Systems ,\n32, 2019.\n[Rosenfeld et al., 2020] Jonathan S Rosenfeld, Amir Rosenfe ld, Yonatan Belinkov, and Nir Shavit. A con-\nstructive prediction of the generalization error across sc ales. In Proceedings of International Conference\non Learning Representations , 2020.\n[Ruan et al., 2024] Junhao Ruan, Long Meng, Weiqiao Shan, Ton g Xiao, and Jingbo Zhu. A survey of llm\nsurveys. https://github.com/NiuTrans/ABigSurveyOfLLMs , 2024.\n[Rubin et al., 2022] Ohad Rubin, Jonathan Herzig, and Jonath an Berant. Learning to retrieve prompts\nfor in-context learning. In Proceedings of the 2022 Conference of the North American Cha pter of the\nAssociation for Computational Linguistics: Human Languag e Technologies , pages 2655\u20132671, 2022.\n[Russell, 2019] Stuart Russell. Human Compatible: Arti\ufb01cial Intelligence and the Problem o f Controls .\nViking, 2019.\n[Sanh et al., 2020] Victor Sanh, Thomas Wolf, and Alexander R ush. Movement pruning: Adaptive sparsity\nby \ufb01ne-tuning. Advances in Neural Information Processing Systems , 33:20378\u201320389, 2020.\n\n--- Page 224 ---\n\n4.5 Summary 217\n[Sanh et al., 2022] Victor Sanh, Albert Webson, Colin Raffel , Stephen Bach, Lintang Sutawika, Zaid\nAlyafeai, Antoine Chaf\ufb01n, Arnaud Stiegler, Arun Raja, Mana n Dey, M Saiful Bari, Canwen Xu, Urmish\nThakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Ki m, Gunjan Chhablani, Nihal Nayak,\nDebajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen,\nZheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht\nSharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Bider-\nman, Leo Gao, Thomas Wolf, and Alexander M Rush. Multitask pr ompted training enables zero-shot\ntask generalization. In Proceedings of International Conference on Learning Repre sentations , 2022.\n[Schick et al., 2023] Timo Schick, Jane A. Yu, Zhengbao Jiang , Fabio Petroni, Patrick Lewis, Gautier Izac-\nard, Qingfei You, Christoforos Nalmp",
    "9edbb7c0ea4ca1a1": " Nihal Nayak,\nDebajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen,\nZheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht\nSharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Bider-\nman, Leo Gao, Thomas Wolf, and Alexander M Rush. Multitask pr ompted training enables zero-shot\ntask generalization. In Proceedings of International Conference on Learning Repre sentations , 2022.\n[Schick et al., 2023] Timo Schick, Jane A. Yu, Zhengbao Jiang , Fabio Petroni, Patrick Lewis, Gautier Izac-\nard, Qingfei You, Christoforos Nalmpantis, Edouard Grave, and Sebastian Riedel. PEER: A collaborative\nlanguage model. In Proceedings of The Eleventh International Conference on Le arning Representations ,\n2023.\n[Schick et al., 2024] Timo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta Raileanu, Maria Lomeli, Eric\nHambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scia lom. Toolformer: Language models can\nteach themselves to use tools. Advances in Neural Information Processing Systems , 36, 2024.\n[Schmidhuber, 2015] J\u00fcrgen Schmidhuber. Deep learning in n eural networks: An overview. Neural net-\nworks , 61:85\u2013117, 2015.\n[Schulman et al., 2015] John Schulman, Sergey Levine, Phili pp Moritz, Michael Jordan, and Pieter Abbeel.\nTrust region policy optimization. In Proceedings of the 32nd International Conference on Intern ational\nConference on Machine Learning-Volume 37 , pages 1889\u20131897, 2015.\n[Schulman et al., 2017] John Schulman, Filip Wolski, Praful la Dhariwal, Alec Radford, and Oleg Klimov.\nProximal policy optimization algorithms. arXiv preprint arXiv:1707.06347 , 2017.\n[Sennrich et al., 2016] Rico Sennrich, Barry Haddow, and Ale xandra Birch. Improving neural machine\ntranslation models with monolingual data. In Proceedings of the 54th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers) , pages 86\u201396, 2016.\n[Seo et al., 2017] Minjoon Seo, Aniruddha Kembhavi, Ali Farh adi, and Hannaneh Hajishirzi. Bidirectional\nattention \ufb02ow for machine comprehension. In Proceedings of International Conference on Learning\nRepresentations , 2017.\n[Shannon, 1951] Claude E Shannon. Prediction and entropy of printed english. Bell system technical\njournal , 30(1):50\u201364, 1951.\n[Shaw et al., 2018] Peter Shaw, Jakob Uszkoreit, and Ashish V aswani. Self-attention with relative position\nrepresentations. In Proceedings of the 2018 Conference of the North American Cha pter of the Associ-\nation for Computational Linguistics: Human Language Techn ologies, Volume 2 (Short Papers) , pages\n464\u2013468, 2018.\n[Shazeer, 2019] Noam Shazeer. Fast transformer decoding: O ne write-head is all you need. arXiv preprint\narXiv:1911.02150 , 2019.\n[Shazeer, 2020] Noam Shazeer. Glu variants improve transfo rmer. arXiv preprint arXiv:2002.05202 , 2020.\n[Shen et al., 2020] Sheng Shen, Zhen Dong, Jiayu Ye, Linjian M a, Zhewei Yao, Amir Gholami, Michael W\nMahoney, and Kurt Keutzer. Q-bert: Hessian based ultra low p recision quantization of bert. In Proceed-\nings of the AAAI Conference on Arti\ufb01cial Intelligence , volume 34, pages 8815\u20138821, 2020.\n[Shoeybi et al., 2019] Mohammad Shoeybi, Mostofa Patwary, R aul P",
    "8fecccd1b4b85672": ": O ne write-head is all you need. arXiv preprint\narXiv:1911.02150 , 2019.\n[Shazeer, 2020] Noam Shazeer. Glu variants improve transfo rmer. arXiv preprint arXiv:2002.05202 , 2020.\n[Shen et al., 2020] Sheng Shen, Zhen Dong, Jiayu Ye, Linjian M a, Zhewei Yao, Amir Gholami, Michael W\nMahoney, and Kurt Keutzer. Q-bert: Hessian based ultra low p recision quantization of bert. In Proceed-\nings of the AAAI Conference on Arti\ufb01cial Intelligence , volume 34, pages 8815\u20138821, 2020.\n[Shoeybi et al., 2019] Mohammad Shoeybi, Mostofa Patwary, R aul Puri, Patrick LeGresley, Jared Casper,\nand Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model\nparallelism. arXiv preprint arXiv:1909.08053 , 2019.\n[Skalse et al., 2022] Joar Skalse, Nikolaus Howe, Dmitrii Kr asheninnikov, and David Krueger. De\ufb01ning\nand characterizing reward gaming. Advances in Neural Information Processing Systems , 35:9460\u20139471,\n2022.\n[Snell et al., 2022] Charlie Snell, Dan Klein, and Ruiqi Zhon g. Learning by distilling context. arXiv\n\n--- Page 225 ---\n\n218 Alignment\npreprint arXiv:2209.15189 , 2022.\n[Socher et al., 2013] Richard Socher, Alex Perelygin, Jean W u, Jason Chuang, Christopher D Manning,\nAndrew Y Ng, and Christopher Potts. Recursive deep models fo r semantic compositionality over a\nsentiment treebank. In Proceedings of the 2013 conference on empirical methods in n atural language\nprocessing , pages 1631\u20131642, 2013.\n[Song et al., 2019] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. Mass: Masked sequence\nto sequence pre-training for language generation. In International Conference on Machine Learning ,\npages 5926\u20135936. PMLR, 2019.\n[Stiennon et al., 2020] Nisan Stiennon, Long Ouyang, Jeffre y Wu, Daniel Ziegler, Ryan Lowe, Chelsea\nV oss, Alec Radford, Dario Amodei, and Paul F Christiano. Lea rning to summarize with human feedback.\nAdvances in Neural Information Processing Systems , 33:3008\u20133021, 2020.\n[Su et al., 2024] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfen g Pan, Wen Bo, and Yunfeng Liu. Ro-\nformer: Enhanced transformer with rotary position embeddi ng.Neurocomputing , 568:127063, 2024.\n[Sun et al., 2020] Zhiqing Sun, Hongkun Yu, Xiaodan Song, Ren jie Liu, Yiming Yang, and Denny Zhou.\nMobilebert: a compact task-agnostic bert for resource-lim ited devices. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics , pages 2158\u20132170, 2020.\n[Sutskever et al., 2014] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with\nneural networks. Advances in neural information processing systems , 27, 2014.\n[Sutton and Barto, 2018] Richard S. Sutton and Andrew G. Bart o.Reinforcement Learning: An Introduc-\ntion (2nd ed.) . The MIT Press, 2018.\n[Szepesv\u00e1ri, 2010] Csaba Szepesv\u00e1ri. Algorithms for reinf orcement learning. Synthesis Lectures on Arti-\n\ufb01cial Intelligence and Machine Learning , 4(1):1\u2013103, 2010.\n[Talmor and Berant, 2018] Alon Talmor and Jonathan Berant. T he web as a knowledge-base for answering\ncomplex questions. arXiv preprint arXiv:1803.06643 , 2018.\n[Taori et al., 2023] Rohan Taori, Ishaan Gul",
    "6fa623cc7dda5f39": ". Sequence to sequence learning with\nneural networks. Advances in neural information processing systems , 27, 2014.\n[Sutton and Barto, 2018] Richard S. Sutton and Andrew G. Bart o.Reinforcement Learning: An Introduc-\ntion (2nd ed.) . The MIT Press, 2018.\n[Szepesv\u00e1ri, 2010] Csaba Szepesv\u00e1ri. Algorithms for reinf orcement learning. Synthesis Lectures on Arti-\n\ufb01cial Intelligence and Machine Learning , 4(1):1\u2013103, 2010.\n[Talmor and Berant, 2018] Alon Talmor and Jonathan Berant. T he web as a knowledge-base for answering\ncomplex questions. arXiv preprint arXiv:1803.06643 , 2018.\n[Taori et al., 2023] Rohan Taori, Ishaan Gulrajani, Tianyi Z hang, Yann Dubois, Xuechen Li, Carlos\nGuestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanfor d alpaca: An instruction-following llama\nmodel. https://github.com/tatsu-lab/stanford_alpaca , 2023.\n[Tay et al., 2020] Yi Tay, Mostafa Dehghani, Dara Bahri, and D onald Metzler. Ef\ufb01cient transformers: A\nsurvey. CoRR , abs/2009.06732, 2020.\n[Team et al., 2024] Gemma Team, Morgane Riviere, Shreya Path ak, Pier Giuseppe Sessa, Cassidy Hardin,\nSurya Bhupatiraju, L\u00e9onard Hussenot, Thomas Mesnard, Boba k Shahriari, Alexandre Ram\u00e9, et al.\nGemma 2: Improving open language models at a practical size. arXiv preprint arXiv:2408.00118 , 2024.\n[Teknium, 2023] Teknium. Openhermes 2.5: An open dataset of synthetic data for generalist llm assistants,\n2023. URL https://huggingface.co/datasets/teknium/OpenHermes- 2.5.\n[Touvron et al., 2023] Hugo Touvron, Thibaut Lavril, Gautie r Izacard, Xavier Martinet, Marie-Anne\nLachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Ro-\ndriguez, Armand Joulin, Edouard Grave, and Guillaume Lampl e. Llama: Open and ef\ufb01cient foundation\nlanguage models. arXiv preprint arXiv:2302.13971 , 2023a.\n[Touvron et al., 2023] Hugo Touvron, Louis Martin, Kevin Sto ne, Peter Albert, Amjad Almahairi,\nYasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel,\nLukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem C ucurull, David Esiobu, Jude Fernan-\ndes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanu j Goswami, Naman Goyal, Anthony\nHartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kar das, Viktor Kerkez, Madian Khabsa, Is-\nabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Ann e Lachaux, Thibaut Lavril, Jenya Lee,\nDiana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, T odor Mihaylov, Pushkar Mishra, Igor\nMolybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Ra shi Rungta, Kalyan Saladi, Alan Schel-\nten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xi aoqing Ellen Tan, Binh Tang, Ross Taylor,\n\n--- Page 226 ---\n\n4.5 Summary 219\nAdina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robe rt Stojnic, Sergey Edunov, and Thomas\nScialom. Llama ",
    "3ccbc77058d2f7d2": "oura, Marie-Ann e Lachaux, Thibaut Lavril, Jenya Lee,\nDiana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, T odor Mihaylov, Pushkar Mishra, Igor\nMolybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Ra shi Rungta, Kalyan Saladi, Alan Schel-\nten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xi aoqing Ellen Tan, Binh Tang, Ross Taylor,\n\n--- Page 226 ---\n\n4.5 Summary 219\nAdina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robe rt Stojnic, Sergey Edunov, and Thomas\nScialom. Llama 2: Open foundation and \ufb01ne-tuned chat models .arXiv preprint arXiv:2307.09288 ,\n2023b.\n[Uesato et al., 2022] Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa\nWang, Antonia Creswell, Geoffrey Irving, and Irina Higgins . Solving math word problems with process-\nand outcome-based feedback. arXiv preprint arXiv:2211.14275 , 2022.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki Pa rmar, Jakob Uszkoreit, Llion Jones, Aidan N\nGomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is al l you need. In Proceedings of Advances in\nNeural Information Processing Systems , volume 30, 2017.\n[V on Oswald et al., 2023] Johannes V on Oswald, Eyvind Niklas son, Ettore Randazzo, Jo\u00e3o Sacramento,\nAlexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyr ov. Transformers learn in-context by\ngradient descent. In Proceedings of International Conference on Machine Learni ng, pages 35151\u2013\n35174. PMLR, 2023.\n[Wang et al., 2024] Chenglong Wang, Hang Zhou, Yimin Hu, Yifu Huo, Bei Li, Tongran Liu, Tong Xiao,\nand Jingbo Zhu. Esrl: Ef\ufb01cient sampling-based reinforceme nt learning for sequence generation. In\nProceedings of the AAAI Conference on Arti\ufb01cial Intelligen ce, pages 19107\u201319115, 2024.\n[Wang et al., 2023] Liyuan Wang, Xingxing Zhang, Hang Su, and Jun Zhu. A comprehensive survey of\ncontinual learning: Theory, method and application. arXiv preprint arXiv:2302.00487 , 2023a.\n[Wang et al., 2019] Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F Wong, and\nLidia S Chao. Learning deep transformer models for machine t ranslation. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational Lingui stics, pages 1810\u20131822, 2019.\n[Wang et al., 2022] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou.\nRationale-augmented ensembles in language models. arXiv preprint arXiv:2207.00747 , 2022a.\n[Wang et al., 2023] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang,\nAakanksha Chowdhery, and Denny Zhou. Self-consistency imp roves chain of thought reasoning in lan-\nguage models. In Proceedings of The Eleventh International Conference on Le arning Representations ,\n2023b.\n[Wang et al., 2022] Yizhong Wang, Swaroop Mishra, Pegah Alip oormolabashi, Yeganeh Kordi, Amirreza\nMirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekar an, Anjana Arunkumar, David Stap, Es-\nhaan Pathak, Giannis Karamanolakis, Haizhi Gary Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson,\nKirby Kuznia, Krima Doshi",
    "8dccbd177015dd73": " Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang,\nAakanksha Chowdhery, and Denny Zhou. Self-consistency imp roves chain of thought reasoning in lan-\nguage models. In Proceedings of The Eleventh International Conference on Le arning Representations ,\n2023b.\n[Wang et al., 2022] Yizhong Wang, Swaroop Mishra, Pegah Alip oormolabashi, Yeganeh Kordi, Amirreza\nMirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekar an, Anjana Arunkumar, David Stap, Es-\nhaan Pathak, Giannis Karamanolakis, Haizhi Gary Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson,\nKirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel , Mehrad Moradshahi, Mihir Parmar, Mi-\nrali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit V erma, Ravsehaj Singh Puri, Rushang Karia,\nSavan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Suj an Reddy A, Sumanta Patro, Tanay Dixit,\nand Xudong Shen. Super-naturalinstructions: Generalizat ion via declarative instructions on 1600+ nlp\ntasks. In Proceedings of the 2022 Conference on Empirical Methods in N atural Language Processing ,\npages 5085\u20135109, 2022b.\n[Wang et al., 2023] Yizhong Wang, Hamish Ivison, Pradeep Das igi, Jack Hessel, Tushar Khot, Khy-\nathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A . Smith, Iz Beltagy, and Hannaneh\nHajishirzi. How far can camels go? exploring the state of ins truction tuning on open resources. Ad-\nvances in Neural Information Processing Systems , 36:74764\u201374786, 2023c.\n[Wang et al., 2023] Yizhong Wang, Yeganeh Kordi, Swaroop Mis hra, Alisa Liu, Noah A Smith, Daniel\nKhashabi, and Hannaneh Hajishirzi. Self-instruct: Aligni ng language models with self-generated in-\nstructions. In Proceedings of the 61st Annual Meeting of the Association fo r Computational Linguistics\n(Volume 1: Long Papers) , pages 13484\u201313508, 2023d.\n[Wang et al., 2023] Zhenyi Wang, Enneng Yang, Li Shen, and Hen g Huang. A comprehensive survey of\nforgetting in deep learning beyond continual learning. arXiv preprint arXiv:2307.09218 , 2023e.\n\n--- Page 227 ---\n\n220 Alignment\n[Warstadt et al., 2019] Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network accept-\nability judgments. Transactions of the Association for Computational Linguis tics, 7:625\u2013641, 2019.\n[Wei et al., 2022] Jason Wei, Maarten Bosma, Vincent Zhao, Ke lvin Guu, Adams Wei Yu, Brian Lester, Nan\nDu, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In Proceedings\nof International Conference on Learning Representations , 2022a.\n[Wei et al., 2022] Jason Wei, Yi Tay, Rishi Bommasani, Colin R affel, Barret Zoph, Sebastian Borgeaud,\nDani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, E d H. Chi, Tatsunori Hashimoto, Oriol\nVinyals, Percy Liang, Jeff Dean, and William Fedus. Emergen t abilities of large language models. arXiv\npreprint arXiv:2206.07682 , 2022b.\n[Wei et al., 2022] Jason Wei, Xuezhi Wang, Dale Schuurmans, M aarten Bosma, Brian Ichter, Fei Xia,\nEd H. Chi, Quoc V . Le, and Denny Zhou. Chain-of-thought promp ting elicits reasoning in large language\nmodels. Advances in Neural Information Processing Systems , 35:24824\u201324837, 2022c.\n[Welle",
    "3eb0ec13b980aca6": "] Jason Wei, Yi Tay, Rishi Bommasani, Colin R affel, Barret Zoph, Sebastian Borgeaud,\nDani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, E d H. Chi, Tatsunori Hashimoto, Oriol\nVinyals, Percy Liang, Jeff Dean, and William Fedus. Emergen t abilities of large language models. arXiv\npreprint arXiv:2206.07682 , 2022b.\n[Wei et al., 2022] Jason Wei, Xuezhi Wang, Dale Schuurmans, M aarten Bosma, Brian Ichter, Fei Xia,\nEd H. Chi, Quoc V . Le, and Denny Zhou. Chain-of-thought promp ting elicits reasoning in large language\nmodels. Advances in Neural Information Processing Systems , 35:24824\u201324837, 2022c.\n[Welleck et al., 2023] Sean Welleck, Ximing Lu, Peter West, F aeze Brahman, Tianxiao Shen, Daniel\nKhashabi, and Yejin Choi. Generating sequences by learning to self-correct. In Proceedings of The\nEleventh International Conference on Learning Representa tions , 2023.\n[Weng, 2021] Lilian Weng. How to train really large models on many gpus? lilianweng.github.io , Sep\n2021. URL https://lilianweng.github.io/posts/2021-09-25-train -large/ .\n[Wiener, 1960] Norbert Wiener. Some moral and technical con sequences of automation: As machines\nlearn they may develop unforeseen strategies at rates that b af\ufb02e their programmers. Science , 131(3410):\n1355\u20131358, 1960.\n[Williams et al., 2018] Adina Williams, Nikita Nangia, and S amuel Bowman. A broad-coverage challenge\ncorpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computational Ling uistics: Human Language Technologies,\nVolume 1 (Long Papers) , pages 1112\u20131122, 2018.\n[Williams, 1992] Ronald J Williams. Simple statistical gra dient-following algorithms for connectionist\nreinforcement learning. Machine learning , 8:229\u2013256, 1992.\n[Wingate et al., 2022] David Wingate, Mohammad Shoeybi, and Taylor Sorensen. Prompt compression\nand contrastive conditioning for controllability and toxi city reduction in language models. In Findings\nof the Association for Computational Linguistics: EMNLP 20 22, pages 5621\u20135634, 2022.\n[Wu et al., 2024] Wilson Wu, John X Morris, and Lionel Levine. Do language models plan for future\ntokens? arXiv preprint arXiv:2404.00859 , 2024.\n[Wu et al., 2021] Yuhuai Wu, Markus Norman Rabe, DeLesley Hut chins, and Christian Szegedy. Memo-\nrizing transformers. In Proceedings of International Conference on Learning Repre sentations , 2021.\n[Wu et al., 2023] Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu,\nNoah A. Smith, Mari Ostendorf, and Hannaneh Hajishirzi. Fin e-grained human feedback gives better\nrewards for language model training. In Thirty-seventh Conference on Neural Information Processi ng\nSystems , 2023.\n[Xia et al., 2024] Mengzhou Xia, Sadhika Malladi, Suchin Gur urangan, Sanjeev Arora, and Danqi Chen.\nLess: Selecting in\ufb02uential data for targeted instruction t uning. arXiv preprint arXiv:2402.04333 , 2024.\n[Xiao et al., 2024] Guangxuan Xiao, Yuandong Tian, Beidi Che n, Song Han, and Mike Lewis. Ef\ufb01cient\nstreaming language models with attention sinks. In Proceedings of The Twelfth International Conference\non Learning Representations , 2024.\n[Xiao and Zhu, 2023] Tong Xiao and Jingbo Zhu. Introduction t o transformers: an nlp perspective. arXiv\npreprint arXiv:2311.17633 , 2023.\n[Xiao et al., 2013] Tong Xiao, Jingbo Zhu, and Tongran Liu. Ba",
    "9ea187dbb860edea": " al., 2024] Mengzhou Xia, Sadhika Malladi, Suchin Gur urangan, Sanjeev Arora, and Danqi Chen.\nLess: Selecting in\ufb02uential data for targeted instruction t uning. arXiv preprint arXiv:2402.04333 , 2024.\n[Xiao et al., 2024] Guangxuan Xiao, Yuandong Tian, Beidi Che n, Song Han, and Mike Lewis. Ef\ufb01cient\nstreaming language models with attention sinks. In Proceedings of The Twelfth International Conference\non Learning Representations , 2024.\n[Xiao and Zhu, 2023] Tong Xiao and Jingbo Zhu. Introduction t o transformers: an nlp perspective. arXiv\npreprint arXiv:2311.17633 , 2023.\n[Xiao et al., 2013] Tong Xiao, Jingbo Zhu, and Tongran Liu. Ba gging and boosting statistical machine\ntranslation systems. Arti\ufb01cial Intelligence , 195:496\u2013527, 2013.\n\n--- Page 228 ---\n\n4.5 Summary 221\n[Xiao et al., 2019] Tong Xiao, Yinqiao Li, Jingbo Zhu, Zhengt ao Yu, and Tongran Liu. Sharing attention\nweights for fast transformer. In Proceedings of the Twenty-Eighth International Joint Conf erence on\nArti\ufb01cial Intelligence (IJCAI-19) , pages 5292\u20135298, 2019.\n[Xie et al., 2022] Sang Michael Xie, Aditi Raghunathan, Perc y Liang, and Tengyu Ma. An explanation\nof in-context learning as implicit bayesian inference. In Proceedings of International Conference on\nLearning Representations , 2022.\n[Xin et al., 2020] Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and Jimmy Lin. Deebert: Dynamic early\nexiting for accelerating bert inference. In Proceedings of the 58th Annual Meeting of the Association fo r\nComputational Linguistics , pages 2246\u20132251, 2020.\n[Xu et al., 2024] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao,\nQingwei Lin, and Daxin Jiang. Wizardlm: Empowering large pr e-trained language models to follow\ncomplex instructions. In The Twelfth International Conference on Learning Represen tations , 2024.\n[Yang et al., 2024] An Yang, Baosong Yang, Beichen Zhang, Bin yuan Hui, Bo Zheng, Bowen Yu,\nChengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwe n2. 5 technical report. arXiv preprint\narXiv:2412.15115 , 2024.\n[Yang et al., 2019] Zhilin Yang, Zihang Dai, Yiming Yang, Jai me Carbonell, Russ R Salakhutdinov, and\nQuoc V Le. Xlnet: Generalized autoregressive pretraining f or language understanding. Advances in\nneural information processing systems , 32, 2019.\n[Yao et al., 2024] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Sh afran, Tom Grif\ufb01ths, Yuan Cao, and Karthik\nNarasimhan. Tree of thoughts: Deliberate problem solving w ith large language models. Advances in\nNeural Information Processing Systems , 36, 2024.\n[Yarowsky, 1995] David Yarowsky. Unsupervised word sense d isambiguation rivaling supervised methods.\nInProceedings of the 33rd annual meeting of the association fo r computational linguistics , pages 189\u2013\n196, 1995.\n[Yu et al., 2023] Zihan Yu, Liang He, Zhen Wu, Xinyu Dai, and Ji ajun Chen. Towards better chain-of-\nthought prompting strategies: A survey. arXiv preprint arXiv:2310.04959 , 2023.\n[Zaheer et al., 2020] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, C. Alberti,\nS. Onta\u00f1\u00f3n, Philip Pham, Anirudh Ravula, Qifan Wang, L. Yang , and A. Ahmed. Big bird: Transformers\nfor longer sequences. Advances in neural information processing systems , 33:17283\u201317297, 2020",
    "9e1b78beb88defc9": "iguation rivaling supervised methods.\nInProceedings of the 33rd annual meeting of the association fo r computational linguistics , pages 189\u2013\n196, 1995.\n[Yu et al., 2023] Zihan Yu, Liang He, Zhen Wu, Xinyu Dai, and Ji ajun Chen. Towards better chain-of-\nthought prompting strategies: A survey. arXiv preprint arXiv:2310.04959 , 2023.\n[Zaheer et al., 2020] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, C. Alberti,\nS. Onta\u00f1\u00f3n, Philip Pham, Anirudh Ravula, Qifan Wang, L. Yang , and A. Ahmed. Big bird: Transformers\nfor longer sequences. Advances in neural information processing systems , 33:17283\u201317297, 2020.\n[Zellers et al., 2018] Rowan Zellers, Yonatan Bisk, Roy Schw artz, and Yejin Choi. Swag: A large-scale\nadversarial dataset for grounded commonsense inference. I nProceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing , pages 93\u2013104, 2018.\n[Zhang and Sennrich, 2019] Biao Zhang and Rico Sennrich. Roo t mean square layer normalization. Ad-\nvances in Neural Information Processing Systems , 32, 2019.\n[Zhang et al., 2023] Zhuosheng Zhang, Yao Yao, Aston Zhang, X iangru Tang, Xinbei Ma, Zhiwei He,\nYiming Wang, Mark Gerstein, Rui Wang, Gongshen Liu, and Hai Z hao. Igniting language intelli-\ngence: The hitchhiker\u2019s guide from chain-of-thought reaso ning to language agents. arXiv preprint\narXiv:2311.11797 , 2023a.\n[Zhang et al., 2023] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought\nprompting in large language models. In The Eleventh International Conference on Learning Represe n-\ntations , 2023b.\n[Zhao et al., 2024] Hao Zhao, Maksym Andriushchenko, France sco Croce, and Nicolas Flammarion. Long\nis more for alignment: A simple but tough-to-beat baseline f or instruction \ufb01ne-tuning. arXiv preprint\narXiv:2402.04833 , 2024.\n[Zhao et al., 2023] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,\nYingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifa n Du, Chen Yang, Yushuo Chen, Z. Chen,\nJinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jianyun Nie, and Ji rong Wen.\n\n--- Page 229 ---\n\n222 Alignment\nA survey of large language models. arXiv preprint arXiv:2303.18223 , 2023.\n[Zhou et al., 2023] Chunting Zhou, Pengfei Liu, Puxin Xu, Sri ni Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma,\nAvia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike L ewis, Luke Zettlemoyer, and Omer\nLevy. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206 , 2023a.\n[Zhou et al., 2023] Denny Zhou, Nathanael Sch\u00e4rli, Le Hou, Ja son Wei, Nathan Scales, Xuezhi Wang, Dale\nSchuurmans, Claire Cui, Olivier Bousquet, Quoc V . Le, and Ed H. Chi. Least-to-most prompting enables\ncomplex reasoning in large language models. In Proceedings of The Eleventh International Conference\non Learning Representations , 2023b.\n[Zhou et al., 2020] Wangchunshu Zhou, Canwen Xu, Tao Ge, Juli an McAuley, Ke Xu, and Furu Wei. Bert\nloses patience: Fast and robust inference with early exit. Advances in Neural Information Processing\nSystems , 33:18330\u201318341, 2020.\n[Zhou et al., 2023] Yongchao Zhou, Andrei Ioan Mures",
    "78d6f3bbc8533901": "Xiv:2305.11206 , 2023a.\n[Zhou et al., 2023] Denny Zhou, Nathanael Sch\u00e4rli, Le Hou, Ja son Wei, Nathan Scales, Xuezhi Wang, Dale\nSchuurmans, Claire Cui, Olivier Bousquet, Quoc V . Le, and Ed H. Chi. Least-to-most prompting enables\ncomplex reasoning in large language models. In Proceedings of The Eleventh International Conference\non Learning Representations , 2023b.\n[Zhou et al., 2020] Wangchunshu Zhou, Canwen Xu, Tao Ge, Juli an McAuley, Ke Xu, and Furu Wei. Bert\nloses patience: Fast and robust inference with early exit. Advances in Neural Information Processing\nSystems , 33:18330\u201318341, 2020.\n[Zhou et al., 2023] Yongchao Zhou, Andrei Ioan Muresanu, Ziw en Han, Keiran Paster, Silviu Pitis, Harris\nChan, and Jimmy Ba. Large language models are human-level pr ompt engineers. In The Eleventh\nInternational Conference on Learning Representations , 2023c.\n[Zoph and Le, 2016] Barret Zoph and Quoc Le. Neural architect ure search with reinforcement learning. In\nProceedings of International Conference on Learning Repre sentations , 2016.\n[Zoph et al., 2020] Barret Zoph, Golnaz Ghiasi, Tsung-Yi Lin , Yin Cui, Hanxiao Liu, Ekin Dogus Cubuk,\nand Quoc Le. Rethinking pre-training and self-training. Advances in neural information processing\nsystems , 33:3833\u20133845, 2020.\n\n--- Page 230 ---\n\nIndex\nk-NN, 74\nk-NN LM, 76\nk-NN language modeling, 76\nk-nearest neighbors, 74\nA2C, 175\naction-value function, 171\nadvantage, 175\nadvantage actor-critic, 175\nAgent, 47\nALiBi, 85\nalignment, 46\nattention with linear biases, 85\nautomated machine learning, 137\nautomatic prompt design, 137\nAutoML, 137\nautonomous agents, 134\nBART, 19\nBERT, 1\nBest-of-Nsampling, 197\nBoN sampling, 197\nBradley-Terry model, 178\ncalculation annotation, 113\ncatastrophic forgetting, 34\ncausal language modeling, 9\nchain of thought, 113\nchain-of-thought prompting, 53\ncompletion, 6\ncompositional generalization, 122\nCoT, 113\nCOT prompting, 53\ncross-lingual language models, 28\ncumulative reward, 172\ndeliberate-then-generate, 126\ndemonstrations, 6\ndirect preference optimization, 190\nDocument Rotation, 20\nDPO, 190\nDTG, 126\nemergent abilities, 63\nexternal memories, 74\nExtrapolation, 81few-shot COT prompting, 54\ngated linear unit, 58\ngaussian error linear unit, 58\nGeLU, 58\nGLU, 58\nGPT, 1\nGQA, 80\nGrouped query attention, 80\nhard prompts, 140\nhuman preference alignment, 152\nICL, 53\nICT, 6\nimportance sampling, 180\nin-context learning, 6,53,95\ninput inversion, 163\ninstruction alignment, 152\ninstruction \ufb01ne-tuning, 43,154\ninterference, 30\ninternal memories, 74\nInterpolation, 82\nirreducible error, 63\nkey-value cache, 68\nKV cache, 68\nlabel mapping, 105\nLearning from Human Feedback, 47\nleast-to-most prompting, 119\nlong-context LLMs, 66\nmasked language modeling, 1,9\nmBERT, 28\nmemory-based methods, 74\nMQA, 79\nmulti-lingual BERT, 28\nmulti-query attention, 79\nNAS, 137\nneural architecture search, 137\nnext sentence prediction, 13\nNSP, 13\nof\ufb02ine reinforcement learning, 193\none-shot COT prompting, 54\nOutcome-based Approaches, 195\n223\n\n--- Page 231 ---\n\n224 Alignment\noveroptimization problem, 189\nPerformance Estimation, 137\n",
    "2d83549eda2ec324": " 43,154\ninterference, 30\ninternal memories, 74\nInterpolation, 82\nirreducible error, 63\nkey-value cache, 68\nKV cache, 68\nlabel mapping, 105\nLearning from Human Feedback, 47\nleast-to-most prompting, 119\nlong-context LLMs, 66\nmasked language modeling, 1,9\nmBERT, 28\nmemory-based methods, 74\nMQA, 79\nmulti-lingual BERT, 28\nmulti-query attention, 79\nNAS, 137\nneural architecture search, 137\nnext sentence prediction, 13\nNSP, 13\nof\ufb02ine reinforcement learning, 193\none-shot COT prompting, 54\nOutcome-based Approaches, 195\n223\n\n--- Page 231 ---\n\n224 Alignment\noveroptimization problem, 189\nPerformance Estimation, 137\nperformance function, 172\nperformance gap recovered, 167\npermuted language modeling, 11\nPGR, 167\nPlackett-Luce model, 184\nPPO, 50,181\npre\ufb01x \ufb01ne-tuning, 144\npre\ufb01x language modeling, 16\nproblem decomposition, 116\nProcess-based Approaches, 195\nprompt embeddings, 148\nprompt engineering, 95\nprompt optimization, 137\nPrompt Search Space, 137\nprompting engineering, 51\nproximal policy optimization, 50,181\nQ-value function, 171\nRAG, 76\nratio function, 180\nrecti\ufb01ed linear unit, 58\nreinforcement learning from human feedback,\n47,153\nrejection sampling, 198\nrelation extraction, 108\nReLU, 58\nretrieval-augmented generation, 76\nreturn, 172\nreward gaming, 189\nreward hacking, 189\nReward Model, 47\nRLHF, 47,153\nRoBERTa, 26\nsample ef\ufb01cient, 164\nscaling laws, 63\nself-consistency, 129\nself-instruct, 160\nself-supervised learning, 3\nself-training, 3\nSentence Reordering, 19\nSequence Encoding Models, 3\nSequence Generation Models, 3\nSFT, 47,152single-round prediction, 155\nsoft prompts, 140\nSpan Masking, 19\nstate-value function, 171\nStrong Ceiling Performance, 167\nSub-problem Generation, 118\nSub-problem Solving, 118\nsuper\ufb01cial alignment hypothesis, 164\nSupervised Fine-tuning, 47\nsupervised \ufb01ne-tuning, 152\nsupervised learning, 2\nsurrogate objective, 180\nT5,15\nTD,176\ntemporal difference, 176\ntext completion, 109\ntext transformation, 109\nToken Deletion, 19\nToken Masking, 19\nTransformers, 1\ntranslation language modeling, 29\ntrust regions, 181\nunsupervised learning, 2\nWeak Performance, 167\nweak-to-strong generalization, 166\nWeak-to-strong Performance, 167\nXLMs, 28\nzero-shot COT, 54\nzero-shot learning, 45"
  }
}