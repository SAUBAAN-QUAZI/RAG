{
  "doc_id": "e7db7b5e614181ea",
  "content": "Transformer From ScratchTransformer From Scratch\nWith PyT orch🔥 With PyT orch🔥\n2024 | © LUIS FERNANDO TORRES\nTable of Contents\nIntroduction\nTransformer Architecture\nInput Embeddings\nPositional Encoding\n\nLayer Normalization\nFeed-Forward Network\nMulti-Head Attention\nResidual Connection\nEncoder\nDecoder\nBuilding the T ransformer\nTokenizer\nLoading Dataset\nValidation Loop\nTraining Loop\nConclusion\nIntroduction Introduction\nIn 2017, the Google Research team published a paper called \"Attention Is\nAll You Need\" , which presented the Transformer architecture and was a\nparadigm shift in Machine Learning, especially in Deep Learning and the\nfield of natural language processing.\nThe Transformer , with its parallel processing capabilities, allowed for\nmore ef ficient and scalable models, making it easier to train them on large\ndatasets. It also demonstrated superior performance in several NLP\ntasks, such as sentiment analysis and text generation tasks.\nThe archicture presented in this paper served as the foundation for\nsubsequent models like GPT  and BER T. Besides NLP , the Transformer\narchitecture is used in other fields, like audio processing and computer\nvision. You can see the usage of Transformers in audio classification in\nthe notebook Audio Data: Music Genre Classification.\n\nEven though you can easily employ dif ferent types of Transformers with\nthe 🤗Transformers  library , it is crucial to understand how things truly\nwork by building them from scratch.\nIn this notebook, we will explore the Transformer architecture and all its\ncomponents. I will use PyT orch to build all the necessary structures and\nblocks, and I will use the Coding a T ransformer from scratch on\nPyTorch, with full explanation, training and inference  video posted by\nUmar Jamil  on YouTube as reference.\nLet's start by importing all the necessary libraries.\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumP\ny version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected ver\nsion 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\nIn [1]:# Importing libraries\n# PyTorch\nimport torch\nimport torch.nn  as nn\nfrom torch.utils.data  import Dataset, DataLoader , random_split\nfrom torch.utils.tensorboard  import SummaryWriter\n# Math\nimport math\n# HuggingFace libraries \nfrom datasets  import load_dataset\nfrom tokenizers  import Tokenizer\nfrom tokenizers.models  import WordLevel\nfrom tokenizers.trainers  import WordLevelTrainer\nfrom tokenizers.pre_tokenizers  import Whitespace\n# Pathlib \nfrom pathlib import Path\n# typing\nfrom typing import Any\n# Library for progress bars in loops\nfrom tqdm import tqdm\n# Importing library of warnings\nimport warnings\n\nTransformer Architecture Transformer Architecture\nBefore coding, let's take a look at the Transformer architecture.\nSource: Attention Is All You Need\n\nThe Transformer architecture has two main blocks: the encoder  and the\ndecoder . Let's take a look at them further .\nEncoder:  It has a Multi-Head Attention  mechanism and a fully connected\nFeed-Forward  network. There are also residual connections around the\ntwo sub-layers, plus layer normalization for the output of each sub-layer .\nAll sub-layers in the model and the embedding layers produce outputs of\ndimension .\nDecoder:  The decoder follows a similar structure, but it inserts a third\nsub-layer that performs multi-head attention over the output of the\nencoder block. There is also a modification of the self-attention sub-layer\nin the decoder block to avoid positions from attending to subsequent\npositions. This masking ensures that the predictions for position  depend\nsolely on the known outputs at positions less than .\nBoth the encoder and decode blocks are repeated  times. In the\noriginal paper , they defined , and we will define a similar value in\nthis notebook.\nInput Embeddings Input Embeddings\nWhen we observe the Transformer architecture image above, we can see\nthat the Embeddings represent the first step of both blocks.\nThe InputEmbedding  class below is responsible for converting the input\ntext into numerical vectors of d_model  dimensions. To prevent that our\ninput embeddings become extremely small, we normalize them by\nmultiplying them by the .\nIn the image below , we can see how the embeddings are created. First,\nwe have a sentence that gets split into tokens—we will explore what\ntokens are later on—. Then, the token IDs—identification numbers—are\ntransformed into the embeddings, which are high-dimensional vectors.dmodel=512\ni\ni\nN\nN=6\n√dmodel\n\n101 2023 2003 1037 7953 1012 102\"This is a input text.\"\n[CLS] This is a input . [SEP]\n 0.0390,\n-0.0123,\n-0.0208,\n...-0.0558, \n0.0151, \n0.0031,\n...-0.0440,\n-0.0236,\n-0.0283,\n... 0.01 19,\n-0.0037,\n-0.0402,\n...0069, \n0.0057,\n-0.0016,\n...0.0199,\n-0.0095,\n-0.0099,\n...-0.0788, \n0.0202,\n-0.0352,\n...EmbeddingsTokenization\nSource: vaclavkosar.com\nPositional Encoding Positional Encoding\nIn the original paper , the authors add the positional encodings to the input\nembeddings at the bottom of both the encoder and decoder blocks so the\nmodel can have some information about the relative or absolute position\nof the tokens in the sequence. The positional encodings have the same\ndimension  as the embeddings, so that the two vectors can be\nsummed and we can combine the semantic content from the word\nembeddings and positional information from the positional encodings.\nIn the PositionalEncoding  class below , we will create a matrix of\npositional encodings pe with dimensions (seq_len, d_model) . We\nwill start by filling it with s.We will then apply the sine function to even\nIn [2]:# Creating Input Embeddings\nclass InputEmbeddings (nn.Module):\n    \n    def __init__ (self, d_model: int, vocab_size : int):\n        super().__init__ ()\n        self.d_model = d_model # Dimension of vectors (512)\n        self.vocab_size  = vocab_size  # Size of the vocabulary\n        self.embedding  = nn.Embedding (vocab_size , d_model) # PyTorch layer that con\n        \n    def forward(self, x):\n        return self.embedding (x) * math.sqrt(self.d_model) # Normalizing the varian\ndmodel\n0\n\nindices of the positional encoding matrix while the cosine function is\napplied to the odd ones.\n \n \nWe apply the sine and cosine functions because it allows the model to\ndetermine the position of a word based on the position of other words in\nthe sequence, since for any fixed of fset ,  can be represented\nas a linear function of . This happens due to the properties of sine\nand cosine functions, where a shift in the input results in a predictable\nchange in the output.Even Indices (2i):PE(pos, 2i)=sin(pos\n100002i/\nOdd Indices (2i+1):PE(pos, 2i+1)=cos(\nkPEpos+k\nPEpos\nIn [3]:\n# Creating the Positional Encoding\nclass PositionalEncoding (nn.Module):\n    \n    def __init__ (self, d_model: int, seq_len: int, dropout: float) -> None:\n        super().__init__ ()\n        self.d_model = d_model # Dimensionality of the model\n        self.seq_len = seq_len # Maximum sequence length\n        self.dropout = nn.Dropout(dropout) # Dropout layer to prevent overfitting\n        \n        # Creating a positional encoding matrix of shape (seq_len, d_model) filled \n        pe = torch.zeros(seq_len, d_model) \n        \n        # Creating a tensor representing positions (0 to seq_len - 1)\n        position  = torch.arange(0, seq_len, dtype = torch.float).unsqueeze (1) # Tra\n        \n        # Creating the division term for the positional encoding formula\n        div_term  = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000\n        \n        # Apply sine to even indices in pe\n        pe[:, 0::2] = torch.sin(position  * div_term )\n        # Apply cosine to odd indices in pe\n        pe[:, 1::2] = torch.cos(position  * div_term )\n        \n        # Adding an extra dimension at the beginning of pe matrix for batch handlin\n        pe = pe.unsqueeze (0)\n        \n        # Registering 'pe' as buffer. Buffer is a tensor not considered as a model \n        self.register_buffer ('pe', pe) \n        \n    def forward(self,x):\n        # Addind positional encoding to the input tensor X\n\nLayer Normalization Layer Normalization\nWhen we look at the encoder and decoder blocks, we see several\nnormalization layers called Add & Norm .\nThe LayerNormalization  class below performs layer normalization on\nthe input data. During its forward pass, we compute the mean and\nstandard deviation of the input data. W e then normalize the input data by\nsubtracting the mean and dividing by the standard deviation plus a small\nnumber called epsilon to avoid any divisions by zero. This process results\nin a normalized output with a mean 0 and a standard deviation 1.\nWe will then scale the normalized output by a learnable parameter\nalpha  and add a learnable parameter called bias . The training\nprocess is responsible for adjusting these parameters. The final result is a\nlayer-normalized tensor , which ensures that the scale of the inputs to\nlayers in the network is consistent.\nFeed-Forward Network Feed-Forward Network\nIn the fully connected feed-forward network, we apply two linear\ntransformations with a ReLU activation in between. W e can\nmathematically represent this operation as:        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_ (False)\n        return self.dropout(x) # Dropout for regularization\nIn [4]:\n# Creating Layer Normalization\nclass LayerNormalization (nn.Module):\n    \n    def __init__ (self, eps: float = 10**-6) -> None: # We define epsilon as 0.00000\n        super().__init__ ()\n        self.eps = eps\n        \n        # We define alpha as a trainable parameter and initialize it with ones\n        self.alpha = nn.Parameter (torch.ones(1)) # One-dimensional tensor that will\n        \n        # We define bias as a trainable parameter and initialize it with zeros\n        self.bias = nn.Parameter (torch.zeros(1)) # One-dimensional tenso that will \n        \n    def forward(self, x):\n        mean = x.mean(dim = -1, keepdim = True) # Computing the mean of the input d\n        std = x.std(dim = -1, keepdim = True) # Computing the standard deviation of\n        \n        # Returning the normalized input\n        return self.alpha * (x-mean) / (std + self.eps) + self.bias\n\n and  are the weights, while  and  are the biases of the two\nlinear transformations.\nIn the FeedForwardBlock  below , we will define the two linear\ntransformations— self.linear_1  and self.linear_2 —and the\ninner-layer d_ff . The input data will first pass through the\nself.linear_1  transformation, which increases its dimensionality from\nd_model  to d_ff . The output of this operation passes through the\nReLU activation function, which introduces non-linearity so the network\ncan learn more complex patterns, and the self.dropout  layer is\napplied to mitigate overfitting. The final operation is the self.linear_2\ntransformation to the dropout-modified tensor , which transforms it back to\nthe original d_model  dimension.\nMulti-Head Attention Multi-Head Attention\nThe Multi-Head Attention is the most crucial component of the\nTransformer . It is responsible for helping the model to understand\ncomplex relationships and patterns in the data.\nThe image below displays how the Multi-Head Attention works. It doesn't\ninclude batch  dimension because it only illustrates the process for one\nsingle sentence.FFN(x)=max(0,xW1+b1)W2+b2 (3)\nW1W2 b1b2\nIn [5]:\n# Creating Feed Forward Layers\nclass FeedForwardBlock (nn.Module):\n    \n    def __init__ (self, d_model: int, d_ff: int, dropout: float) -> None:\n        super().__init__ ()\n        # First linear transformation\n        self.linear_1  = nn.Linear(d_model, d_ff) # W1 & b1\n        self.dropout = nn.Dropout(dropout) # Dropout to prevent overfitting\n        # Second linear transformation\n        self.linear_2  = nn.Linear(d_ff, d_model) # W2 & b2\n        \n    def forward(self, x):\n        # (Batch, seq_len, d_model) --> (batch, seq_len, d_ff) -->(batch, seq_len, \n        return self.linear_2 (self.dropout(torch.relu(self.linear_1 (x))))\n\nSource: YouTube: Coding a Transformer from scratch on PyTorch, with full\nexplanation, training and inference  by Umar Jamil .\nThe Multi-Head Attention block receives the input data split into queries,\nkeys, and values organized into matrices , , and . Each matrix\ncontains dif ferent facets of the input, and they have the same dimensions\nas the input.\nWe then linearly transform each matrix by their respective weight\nmatrices , , and . These transformations will result in new\nmatrices , , and , which will be split into smaller matrices\ncorresponding to dif ferent heads , allowing the model to attend to\ninformation from dif ferent representation subspaces in parallel. This split\ncreates multiple sets of queries, keys, and values for each head.\nFinally , we concatenate every head into an  matrix, which is then\ntransformed by another weight matrix  to produce the multi-head\nattention output, a matrix  that retains the input dimensionality .QKV\nWQWKWV\nQ′K′V′\nh\nH\nWo\nMH−A\nIn [6]:\n# Creating the Multi-Head Attention block\nclass MultiHeadAttentionBlock (nn.Module):\n    \n    def __init__ (self, d_model: int, h: int, dropout: float) -> None: # h = number \n        super().__init__ ()\n        self.d_model = d_model\n        self.h = h\n        \n        # We ensure that the dimensions of the model is divisible by the number of \n        assert d_model % h == 0, 'd_model is not divisible by h'\n\nResidual Connection Residual Connection\nWhen we look at the architecture of the Transformer , we see that each\nsub-layer , including the self-attention  and Feed Forward  blocks, adds its\noutput to its input before passing it to the Add & Norm  layer . This\napproach integrates the output with the original input in the Add & Norm\nlayer . This process is known as the skip connection, which allows the        \n        # d_k is the dimension of each attention head's key, query, and value vecto\n        self.d_k = d_model // h # d_k formula, like in the original \"Attention Is A\n        \n        # Defining the weight matrices\n        self.w_q = nn.Linear(d_model, d_model) # W_q\n        self.w_k = nn.Linear(d_model, d_model) # W_k\n        self.w_v = nn.Linear(d_model, d_model) # W_v\n        self.w_o = nn.Linear(d_model, d_model) # W_o\n        \n        self.dropout = nn.Dropout(dropout) # Dropout layer to avoid overfitting\n        \n    \n    @staticmethod\n    def attention (query, key, value, mask, dropout: nn.Dropout):# mask => When we w\n        \n        d_k = query.shape[-1] # The last dimension of query, key, and value\n        \n        # We calculate the Attention(Q,K,V) as in the formula in the image above \n        attention_scores  = (query @ key.transpose (-2,-1)) / math.sqrt(d_k) # @ = Ma\n        \n        # Before applying the softmax, we apply the mask to hide some interactions \n        if mask is not None: # If a mask IS defined...\n            attention_scores .masked_fill_ (mask == 0, -1e9) # Replace each value whe\n        attention_scores  = attention_scores .softmax(dim = -1) # Applying softmax\n        if dropout is not None: # If a dropout IS defined...\n            attention_scores  = dropout(attention_scores ) # We apply dropout to prev\n            \n        return (attention_scores  @ value), attention_scores  # Multiply the output m\n        \n    def forward(self, q, k, v, mask): \n        \n        query = self.w_q(q) # Q' matrix\n        key = self.w_k(k) # K' matrix\n        value = self.w_v(v) # V' matrix\n        \n        \n        # Splitting results into smaller matrices for the different heads\n        # Splitting embeddings (third dimension) into h parts\n        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transp\n        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose (1,2)\n        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transp\n        \n        # Obtaining the output and the attention scores\n        x, self.attention_scores  = MultiHeadAttentionBlock .attention (query, key, va\n        \n        # Obtaining the H matrix\n        x = x.transpose (1, 2).contiguous ().view(x.shape[0], -1, self.h * self.d_k)\n        \n        return self.w_o(x) # Multiply the H matrix by the weight matrix W_o, result\n\nTransformer to train deep networks more ef fectively by providing a\nshortcut for the gradient to flow through during backpropagation.\nThe ResidualConnection  class below is responsible for this process.\nEncoder Encoder\nWe will now build the encoder . We create the EncoderBlock  class,\nconsisting of the Multi-Head Attention and Feed Forward layers, plus the\nresidual connections.\nIn [7]:# Building Residual Connection\nclass ResidualConnection (nn.Module):\n    def __init__ (self, dropout: float) -> None:\n        super().__init__ ()\n        self.dropout = nn.Dropout(dropout) # We use a dropout layer to prevent over\n        self.norm = LayerNormalization () # We use a normalization layer \n    \n    def forward(self, x, sublayer ):\n        # We normalize the input and add it to the original input 'x'. This creates\n        return x + self.dropout(sublayer (self.norm(x))) \n\nEncoder block. Source: researchgate.net .\nIn the original paper , the Encoder Block repeats six times. W e create the\nEncoder  class as an assembly of multiple EncoderBlock s. We also\nadd layer normalization as a final step after processing the input through\nall its blocks.\nIn [8]:\n# Building Encoder Block\nclass EncoderBlock (nn.Module):\n    \n    # This block takes in the MultiHeadAttentionBlock and FeedForwardBlock, as well\n    def __init__ (self, self_attention_block : MultiHeadAttentionBlock , feed_forward_\n        super().__init__ ()\n        # Storing the self-attention block and feed-forward block\n        self.self_attention_block  = self_attention_block\n\nDecoder Decoder\nSimilarly , the Decoder also consists of several DecoderBlocks that repeat\nsix times in the original paper . The main dif ference is that it has an\nadditional sub-layer that performs multi-head attention with a cross-\nattention  component that uses the output of the Encoder as its keys and\nvalues while using the Decoder's input as queries.        self.feed_forward_block  = feed_forward_block\n        self.residual_connections  = nn.ModuleList ([ResidualConnection (dropout) for \n        \n    def forward(self, x, src_mask ):\n        # Applying the first residual connection with the self-attention block\n        x = self.residual_connections [0](x, lambda x: self.self_attention_block (x, \n        \n        # Applying the second residual connection with the feed-forward block \n        x = self.residual_connections [1](x, self.feed_forward_block )\n        return x # Output tensor after applying self-attention and feed-forward lay\nIn [9]:\n# Building Encoder \n# An Encoder can have several Encoder Blocks\nclass Encoder(nn.Module):\n    \n    # The Encoder takes in instances of 'EncoderBlock'\n    def __init__ (self, layers: nn.ModuleList ) -> None:\n        super().__init__ ()\n        self.layers = layers # Storing the EncoderBlocks\n        self.norm = LayerNormalization () # Layer for the normalization of the outpu\n        \n    def forward(self, x, mask):\n        # Iterating over each EncoderBlock stored in self.layers\n        for layer in self.layers:\n            x = layer(x, mask) # Applying each EncoderBlock to the input tensor 'x'\n        return self.norm(x) # Normalizing output\n\nDecoder block. Source: edlitera.com .\nFor the Output Embedding, we can use the same InputEmbeddings\nclass we use for the Encoder . You can also notice that the self-attention\nsub-layer is masked , which restricts the model from accessing future\nelements in the sequence.\nWe will start by building the DecoderBlock  class, and then we will build\nthe Decoder  class, which will assemble multiple DecoderBlock s.\nIn [10]:\n# Building Decoder Block\nclass DecoderBlock (nn.Module):\n    \n    # The DecoderBlock takes in two MultiHeadAttentionBlock. One is self-attention,\n    # It also takes in the feed-forward block and the dropout rate\n    def __init__ (self,  self_attention_block : MultiHeadAttentionBlock , cross_attent\n        super().__init__ ()\n        self.self_attention_block  = self_attention_block\n        self.cross_attention_block  = cross_attention_block\n        self.feed_forward_block  = feed_forward_block\n        self.residual_connections  = nn.ModuleList ([ResidualConnection (dropout) for \n        \n    def forward(self, x, encoder_output , src_mask , tgt_mask ):\n        \n        # Self-Attention block with query, key, and value plus the target language \n        x = self.residual_connections [0](x, lambda x: self.self_attention_block (x, \n        \n        # The Cross-Attention block using two 'encoder_ouput's for key and value pl\n        x = self.residual_connections [1](x, lambda x: self.cross_attention_block (x,\n        \n        # Feed-forward block with residual connections\n        x = self.residual_connections [2](x, self.feed_forward_block )\n        return x\nIn [11]:\n# Building Decoder\n# A Decoder can have several Decoder Blocks\nclass Decoder(nn.Module):\n    \n    # The Decoder takes in instances of 'DecoderBlock'\n    def __init__ (self, layers: nn.ModuleList ) -> None:\n        super().__init__ ()\n        \n        # Storing the 'DecoderBlock's\n        self.layers = layers\n        self.norm = LayerNormalization () # Layer to normalize the output\n        \n    def forward(self, x, encoder_output , src_mask , tgt_mask ):\n        \n        # Iterating over each DecoderBlock stored in self.layers\n        for layer in self.layers:\n            # Applies each DecoderBlock to the input 'x' plus the encoder output an\n\nYou can see in the Decoder image that after running a stack of\nDecoderBlock s, we have a Linear Layer and a Softmax function to the\noutput of probabilities. The ProjectionLayer  class below is\nresponsible for converting the output of the model into a probability\ndistribution over the vocabulary , where we select each output token from\na vocabulary of possible tokens.\nBuilding the Transformer Building the Transformer\nWe finally have every component of the Transformer architecture ready .\nWe may now construct the Transformer by putting it all together .\nIn the Transformer  class below , we will bring together all the\ncomponents of the model's architecture.            x = layer(x, encoder_output , src_mask , tgt_mask )\n        return self.norm(x) # Returns normalized output\nIn [12]:\n# Buiding Linear Layer\nclass ProjectionLayer (nn.Module):\n    def __init__ (self, d_model: int, vocab_size : int) -> None: # Model dimension an\n        super().__init__ ()\n        self.proj = nn.Linear(d_model, vocab_size ) # Linear layer for projecting th\n    def forward(self, x):\n        return torch.log_softmax (self.proj(x), dim = -1) # Applying the log Softmax\nIn [13]:\n# Creating the Transformer Architecture\nclass Transformer (nn.Module):\n    \n    # This takes in the encoder and decoder, as well the embeddings for the source \n    # It also takes in the Positional Encoding for the source and target language, \n    def __init__ (self, encoder: Encoder, decoder: Decoder, src_embed : InputEmbeddin\n        super().__init__ ()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.src_embed  = src_embed\n        self.tgt_embed  = tgt_embed\n        self.src_pos = src_pos\n        self.tgt_pos = tgt_pos\n        self.projection_layer  = projection_layer\n        \n    # Encoder     \n    def encode(self, src, src_mask ):\n        src = self.src_embed (src) # Applying source embeddings to the input source \n        src = self.src_pos(src) # Applying source positional encoding to the source\n        return self.encoder(src, src_mask ) # Returning the source embeddings plus a\n    \n    # Decoder\n    def decode(self, encoder_output , src_mask , tgt, tgt_mask ):\n        tgt = self.tgt_embed (tgt) # Applying target embeddings to the input target \n        tgt = self.tgt_pos(tgt) # Applying target positional encoding to the target\n        \n        # Returning the target embeddings, the output of the encoder, and both sour\n\nThe architecture is finally ready . We now define a function called\nbuild_transformer , in which we define the parameters and everything\nwe need to have a fully operational Transformer model for the task of\nmachine translation .\nWe will set the same parameters as in the original paper , Attention Is All\nYou Need , where  = 512,  = 6,  = 8, dropout rate  = 0.1,\nand  = 2048.        # The target mask ensures that the model won't 'see' future elements of the\n        return self.decoder(tgt, encoder_output , src_mask , tgt_mask )\n    \n    # Applying Projection Layer with the Softmax function to the Decoder output\n    def project(self, x):\n        return self.projection_layer (x)\ndmodelNh Pdrop\ndff\nIn [14]:\n# Building & Initializing Transformer\n# Definin function and its parameter, including model dimension, number of encoder \ndef build_transformer (src_vocab_size : int, tgt_vocab_size : int, src_seq_len : int, t\n    \n    # Creating Embedding layers\n    src_embed  = InputEmbeddings (d_model, src_vocab_size ) # Source language (Source \n    tgt_embed  = InputEmbeddings (d_model, tgt_vocab_size ) # Target language (Target \n    \n    # Creating Positional Encoding layers\n    src_pos = PositionalEncoding (d_model, src_seq_len , dropout) # Positional encodi\n    tgt_pos = PositionalEncoding (d_model, tgt_seq_len , dropout) # Positional encodi\n    \n    # Creating EncoderBlocks\n    encoder_blocks  = [] # Initial list of empty EncoderBlocks\n    for _ in range(N): # Iterating 'N' times to create 'N' EncoderBlocks (N = 6)\n        encoder_self_attention_block  = MultiHeadAttentionBlock (d_model, h, dropout)\n        feed_forward_block  = FeedForwardBlock (d_model, d_ff, dropout) # FeedForward\n        \n        # Combine layers into an EncoderBlock\n        encoder_block  = EncoderBlock (encoder_self_attention_block , feed_forward_blo\n        encoder_blocks .append(encoder_block ) # Appending EncoderBlock to the list o\n        \n    # Creating DecoderBlocks\n    decoder_blocks  = [] # Initial list of empty DecoderBlocks\n    for _ in range(N): # Iterating 'N' times to create 'N' DecoderBlocks (N = 6)\n        decoder_self_attention_block  = MultiHeadAttentionBlock (d_model, h, dropout)\n        decoder_cross_attention_block  = MultiHeadAttentionBlock (d_model, h, dropout\n        feed_forward_block  = FeedForwardBlock (d_model, d_ff, dropout) # FeedForward\n        \n        # Combining layers into a DecoderBlock\n        decoder_block  = DecoderBlock (decoder_self_attention_block , decoder_cross_at\n        decoder_blocks .append(decoder_block ) # Appending DecoderBlock to the list o\n        \n    # Creating the Encoder and Decoder by using the EncoderBlocks and DecoderBlocks\n    encoder = Encoder(nn.ModuleList (encoder_blocks ))\n    decoder = Decoder(nn.ModuleList (decoder_blocks ))\n    \n    # Creating projection layer\n    projection_layer  = ProjectionLayer (d_model, tgt_vocab_size ) # Map the output of\n    \n    # Creating the transformer by combining everything above\n\nThe model is now ready to be trained!\nTokenizer Tokenizer\nTokenization is a crucial preprocessing step for our Transformer model. In\nthis step, we convert raw text into a number format that the model can\nprocess.\nThere are several Tokenization strategies. W e will use the word-level\ntokenization  to transform each word in a sentence into a token.\nDifferent tokenization strategies. Source: shaankhosla.substack.com .\nAfter tokenizing a sentence, we map each token to an unique integer ID\nbased on the created vocabulary present in the training corpus during the    transformer  = Transformer (encoder, decoder, src_embed , tgt_embed , src_pos, tgt_\n    \n    # Initialize the parameters\n    for p in transformer .parameters ():\n        if p.dim() > 1:\n            nn.init.xavier_uniform_ (p)\n            \n    return transformer  # Assembled and initialized Transformer. Ready to be trained\n\ntraining of the tokenizer . Each integer number represents a specific word\nin the vocabulary .\nBesides the words in the training corpus, Transformers use special tokens\nfor specific purposes. These are some that we will define right away:\n• [UNK]:  This token is used to identify an unknown word in the sequence.\n• [PAD]:  Padding token to ensure that all sequences in a batch have the\nsame length, so we pad shorter sentences with this token. W e use\nattention masks to \"tell\" the model to ignore the padded tokens during\ntraining since they don't have any real meaning to the task.\n• [SOS]:  This is a token used to signal the Start of Sentence .\n• [EOS]:  This is a token used to signal the End of Sentence .\nIn the build_tokenizer  function below , we ensure a tokenizer is ready\nto train the model. It checks if there is an existing tokenizer , and if that is\nnot the case, it trains a new tokenizer .\nIn [15]:\n# Defining Tokenizer\ndef build_tokenizer (config, ds, lang):\n    \n    # Crating a file path for the tokenizer \n    tokenizer_path  = Path(config['tokenizer_file' ].format(lang))\n    \n    # Checking if Tokenizer already exists\n    if not Path.exists(tokenizer_path ): \n        \n        # If it doesn't exist, we create a new one\n        tokenizer  = Tokenizer (WordLevel (unk_token  = '[UNK]')) # Initializing a new \n        tokenizer .pre_tokenizer  = Whitespace () # We will split the text into tokens\n        \n        # Creating a trainer for the new tokenizer\n        trainer = WordLevelTrainer (special_tokens  = [\"[UNK]\", \"[PAD]\", \n                                                     \"[SOS]\", \"[EOS]\"], min_frequen\n        \n        # Training new tokenizer on sentences from the dataset and language specifi\n        tokenizer .train_from_iterator (get_all_sentences (ds, lang), trainer = traine\n        tokenizer .save(str(tokenizer_path )) # Saving trained tokenizer to the file \n    else:\n        tokenizer  = Tokenizer .from_file (str(tokenizer_path )) # If the tokenizer alr\n    return tokenizer  # Returns the loaded tokenizer or the trained tokenizer\n\nLoading Dataset Loading Dataset\nFor this task, we will use the OpusBooks dataset , available on 🤗\nHugging Face. This dataset consists of two features, id and\ntranslation . The translation  feature contains pairs of sentences in\ndifferent languages, such as Spanish and Portuguese, English and\nFrench, and so forth.\nI first tried translating sentences from English to Portuguese—my native\ntongue — but there are only 1.4k examples for this pair , so the results\nwere not satisfying in the current configurations for this model. I then tried\nto use the English-French pair due to its higher number of examples—\n127k—but it would take too long to train with the current configurations. I\nthen opted to train the model on the English-Italian pair , the same one\nused in the Coding a Transformer from scratch on PyT orch, with full\nexplanation, training and inference video, as that was a good balance\nbetween performance and time of training.\nWe start by defining the get_all_sentences  function to iterate over the\ndataset and extract the sentences according to the language pair defined\n—we will do that later .\nThe get_ds  function is defined to load and prepare the dataset for\ntraining and validation. In this function, we build or load the tokenizer , split\nthe dataset, and create DataLoaders, so the model can successfully\niterate over the dataset in batches. The result of these functions is\ntokenizers for the source and target languages plus the DataLoader\nobjects.\nIn [16]:# Iterating through dataset to extract the original sentence and its translation \ndef get_all_sentences (ds, lang):\n    for pair in ds:\n        yield pair['translation' ][lang]\nIn [17]:\ndef get_ds(config):\n    \n    # Loading the train portion of the OpusBooks dataset.\n    # The Language pairs will be defined in the 'config' dictionary we will build l\n    ds_raw = load_dataset ('opus_books' , f'{config[\"lang_src\" ]}-{config[\"lang_tgt\" ]}\n    \n    # Building or loading tokenizer for both the source and target languages \n    tokenizer_src  = build_tokenizer (config, ds_raw, config['lang_src' ])\n    tokenizer_tgt  = build_tokenizer (config, ds_raw, config['lang_tgt' ])\n\nWe define the casual_mask  function to create a mask for the attention\nmechanism of the decoder . This mask prevents the model from having\ninformation about future elements in the sequence.\nWe start by making a square grid filled with ones. W e determine the grid\nsize with the size  parameter . Then, we change all the numbers above\nthe main diagonal line to zeros. Every number on one side becomes a\nzero, while the rest remain ones. The function then flips all these values,\nturning ones into zeros and zeros into ones. This process is crucial for\nmodels that predict future tokens in a sequence.\nThe BilingualDataset  class processes the texts of the target and\nsource languages in the dataset by tokenizing them and adding all the\nnecessary special tokens. This class also certifies that the sentences are\nwithin a maximum sequence length for both languages and pads all\nnecessary sentences.    \n    # Splitting the dataset for training and validation \n    train_ds_size  = int(0.9 * len(ds_raw)) # 90% for training\n    val_ds_size  = len(ds_raw) - train_ds_size  # 10% for validation\n    train_ds_raw , val_ds_raw  = random_split (ds_raw, [train_ds_size , val_ds_size ]) #\n                                    \n    # Processing data with the BilingualDataset class, which we will define below\n    train_ds  = BilingualDataset (train_ds_raw , tokenizer_src , tokenizer_tgt , config[\n    val_ds = BilingualDataset (val_ds_raw , tokenizer_src , tokenizer_tgt , config['lan\n                                    \n    # Iterating over the entire dataset and printing the maximum length found in th\n    max_len_src  = 0\n    max_len_tgt  = 0\n    for pair in ds_raw:\n        src_ids = tokenizer_src .encode(pair['translation' ][config['lang_src' ]]).ids\n        tgt_ids = tokenizer_src .encode(pair['translation' ][config['lang_tgt' ]]).ids\n        max_len_src  = max(max_len_src , len(src_ids))\n        max_len_tgt  = max(max_len_tgt , len(tgt_ids))\n        \n    print(f'Max length of source sentence: {max_len_src }')\n    print(f'Max length of target sentence: {max_len_tgt }')\n    \n    # Creating dataloaders for the training and validadion sets\n    # Dataloaders are used to iterate over the dataset in batches during training a\n    train_dataloader  = DataLoader (train_ds , batch_size  = config['batch_size' ], shuf\n    val_dataloader  = DataLoader (val_ds, batch_size  = 1, shuffle = True)\n    \n    return train_dataloader , val_dataloader , tokenizer_src , tokenizer_tgt  # Returni\nIn [18]:\ndef casual_mask (size):\n        # Creating a square matrix of dimensions 'size x size' filled with ones\n        mask = torch.triu(torch.ones(1, size, size), diagonal  = 1).type(torch.int)\n        return mask == 0\nIn [19]:\nclass BilingualDataset (Dataset):\n    \n\n    # This takes in the dataset contaning sentence pairs, the tokenizers for target\n    # 'seq_len' defines the sequence length for both languages\n    def __init__ (self, ds, tokenizer_src , tokenizer_tgt , src_lang , tgt_lang , seq_le\n        super().__init__ ()\n        \n        self.seq_len = seq_len\n        self.ds = ds\n        self.tokenizer_src  = tokenizer_src\n        self.tokenizer_tgt  = tokenizer_tgt\n        self.src_lang  = src_lang\n        self.tgt_lang  = tgt_lang\n        \n        # Defining special tokens by using the target language tokenizer\n        self.sos_token  = torch.tensor([tokenizer_tgt .token_to_id (\"[SOS]\")], dtype=t\n        self.eos_token  = torch.tensor([tokenizer_tgt .token_to_id (\"[EOS]\")], dtype=t\n        self.pad_token  = torch.tensor([tokenizer_tgt .token_to_id (\"[PAD]\")], dtype=t\n        \n    # Total number of instances in the dataset (some pairs are larger than others)\n    def __len__(self):\n        return len(self.ds)\n    \n    # Using the index to retrive source and target texts\n    def __getitem__ (self, index: Any) -> Any:\n        src_target_pair  = self.ds[index]\n        src_text  = src_target_pair ['translation' ][self.src_lang ]\n        tgt_text  = src_target_pair ['translation' ][self.tgt_lang ]\n        \n        # Tokenizing source and target texts \n        enc_input_tokens  = self.tokenizer_src .encode(src_text ).ids\n        dec_input_tokens  = self.tokenizer_tgt .encode(tgt_text ).ids\n        \n        # Computing how many padding tokens need to be added to the tokenized texts\n        # Source tokens\n        enc_num_padding_tokens  = self.seq_len - len(enc_input_tokens ) - 2 # Subtrac\n        # Target tokens\n        dec_num_padding_tokens  = self.seq_len - len(dec_input_tokens ) - 1 # Subtrac\n        \n        # If the texts exceed the 'seq_len' allowed, it will raise an error. This m\n        # given the current sequence length limit (this will be defined in the conf\n        if enc_num_padding_tokens  < 0 or dec_num_padding_tokens  < 0:\n            raise ValueError ('Sentence is too long' )\n         \n        # Building the encoder input tensor by combining several elements\n        encoder_input  = torch.cat(\n            [\n            self.sos_token , # inserting the '[SOS]' token\n            torch.tensor(enc_input_tokens , dtype = torch.int64), # Inserting the to\n            self.eos_token , # Inserting the '[EOS]' token\n            torch.tensor([self.pad_token ] * enc_num_padding_tokens , dtype = torch.i\n            ]\n        )\n        \n        # Building the decoder input tensor by combining several elements\n        decoder_input  = torch.cat(\n            [\n                self.sos_token , # inserting the '[SOS]' token \n                torch.tensor(dec_input_tokens , dtype = torch.int64), # Inserting th\n                torch.tensor([self.pad_token ] * dec_num_padding_tokens , dtype = tor\n            ]\n        \n        )\n        \n        # Creating a label tensor, the expected output for training the model\n\nValidation Loop Validation Loop\nWe will now create two functions for the validation loop. The validation\nloop is crucial to evaluate model performance in translating sentences\nfrom data it has not seen during training.\nWe will define two functions. The first function, greedy_decode , gives\nus the model's output by obtaining the most probable next token. The\nsecond function, run_validation , is responsible for running the\nvalidation process in which we decode the model's output and compare it\nwith the reference text for the target sentence.        label = torch.cat(\n            [\n                torch.tensor(dec_input_tokens , dtype = torch.int64), # Inserting th\n                self.eos_token , # Inserting the '[EOS]' token \n                torch.tensor([self.pad_token ] * dec_num_padding_tokens , dtype = tor\n                \n            ]\n        )\n        \n        # Ensuring that the length of each tensor above is equal to the defined 'se\n        assert encoder_input .size(0) == self.seq_len\n        assert decoder_input .size(0) == self.seq_len\n        assert label.size(0) == self.seq_len\n        \n        return {\n            'encoder_input' : encoder_input ,\n            'decoder_input' : decoder_input , \n            'encoder_mask' : (encoder_input  != self.pad_token ).unsqueeze (0).unsqueez\n            'decoder_mask' : (decoder_input  != self.pad_token ).unsqueeze (0).unsqueez\n            'label': label,\n            'src_text' : src_text ,\n            'tgt_text' : tgt_text\n        }    \nIn [20]:\n# Define function to obtain the most probable next token\ndef greedy_decode (model, source, source_mask , tokenizer_src , tokenizer_tgt , max_len\n    # Retrieving the indices from the start and end of sequences of the target toke\n    sos_idx = tokenizer_tgt .token_to_id ('[SOS]')\n    eos_idx = tokenizer_tgt .token_to_id ('[EOS]')\n    \n    # Computing the output of the encoder for the source sequence\n    encoder_output  = model.encode(source, source_mask )\n    # Initializing the decoder input with the Start of Sentence token\n    decoder_input  = torch.empty(1,1).fill_(sos_idx).type_as(source).to(device)\n    \n\n    # Looping until the 'max_len', maximum length, is reached\n    while True:\n        if decoder_input .size(1) == max_len:\n            break\n            \n        # Building a mask for the decoder input\n        decoder_mask  = casual_mask (decoder_input .size(1)).type_as(source_mask ).to(d\n        \n        # Calculating the output of the decoder\n        out = model.decode(encoder_output , source_mask , decoder_input , decoder_mask\n        \n        # Applying the projection layer to get the probabilities for the next token\n        prob = model.project(out[:, -1])\n        \n        # Selecting token with the highest probability\n        _, next_word  = torch.max(prob, dim=1)\n        decoder_input  = torch.cat([decoder_input , torch.empty(1,1). type_as(source)\n        \n        # If the next token is an End of Sentence token, we finish the loop\n        if next_word  == eos_idx:\n            break\n            \n    return decoder_input .squeeze(0) # Sequence of tokens generated by the decoder\nIn [21]:\n# Defining function to evaluate the model on the validation dataset\n# num_examples = 2, two examples per run\ndef run_validation (model, validation_ds , tokenizer_src , tokenizer_tgt , max_len, dev\n    model.eval() # Setting model to evaluation mode\n    count = 0 # Initializing counter to keep track of how many examples have been p\n    \n    console_width  = 80 # Fixed witdh for printed messages\n    \n    # Creating evaluation loop\n    with torch.no_grad(): # Ensuring that no gradients are computed during this pro\n        for batch in validation_ds :\n            count += 1\n            encoder_input  = batch['encoder_input' ].to(device)\n            encoder_mask  = batch['encoder_mask' ].to(device)\n            \n            # Ensuring that the batch_size of the validation set is 1\n            assert encoder_input .size(0) ==  1, 'Batch size must be 1 for validatio\n            \n            # Applying the 'greedy_decode' function to get the model's output for t\n            model_out  = greedy_decode (model, encoder_input , encoder_mask , tokenizer\n            \n            # Retrieving source and target texts from the batch\n            source_text  = batch['src_text' ][0]\n            target_text  = batch['tgt_text' ][0] # True translation \n            model_out_text  = tokenizer_tgt .decode(model_out .detach().cpu().numpy())\n            \n            # Printing results\n            print_msg ('-'*console_width )\n            print_msg (f'SOURCE: {source_text }')\n            print_msg (f'TARGET: {target_text }')\n            print_msg (f'PREDICTED: {model_out_text }')\n            \n            # After two examples, we break the loop\n            if count == num_examples :\n                break\n\nTraining Loop Training Loop\nWe are ready to train our Transformer model on the OpusBook dataset\nfor the English to Italian translation task.\nWe first start by defining the get_model  function to load the model by\ncalling the build_transformer  function we have previously defined.\nThis function uses the config  dictionary to set a few parameters.\nI have mentioned the config  dictionary several times throughout this\nnotebook. Now , it is time to create it.\nIn the following cell, we will define two functions to configure our model\nand the training process.\nIn the get_config  function, we define crucial parameters for the training\nprocess. batch_size  for the number of training examples used in one\niteration, num_epochs  as the number of times the entire dataset is\npassed forward and backward through the Transformer , lr as the\nlearning rate for the optimizer , etc. W e will also finally define the pairs\nfrom the OpusBook dataset, 'lang_src': 'en'  for selecting English as\nthe source language and 'lang_tgt': 'it'  for selecting Italian as the\ntarget language.\nThe get_weights_file_path  function constructs the file path for\nsaving or loading model weights for any specific epoch.\nIn [22]:# We pass as parameters the config dictionary, the length of the vocabylary of the \ndef get_model (config, vocab_src_len , vocab_tgt_len ):\n    \n    # Loading model using the 'build_transformer' function.\n    # We will use the lengths of the source language and target language vocabulari\n    model = build_transformer (vocab_src_len , vocab_tgt_len , config['seq_len' ], conf\n    return model\nIn [23]:\n# Define settings for building and training the transformer model\ndef get_config ():\n    return{\n\nWe finally define our last function, train_model , which takes the\nconfig  arguments as input.\nIn this function, we will set everything up for the training. W e will load the\nmodel and its necessary components onto the GPU for faster training, set\nthe Adam  optimizer , and configure the CrossEntropyLoss  function to\ncompute the dif ferences between the translations output by the model\nand the reference translations from the dataset.\nEvery loop necessary for iterating over the training batches, performing\nbackpropagation, and computing the gradients is in this function. W e will\nalso use it to run the validation function and save the current state of the\nmodel.        'batch_size' : 8,\n        'num_epochs' : 20,\n        'lr': 10**-4,\n        'seq_len' : 350,\n        'd_model' : 512, # Dimensions of the embeddings in the Transformer. 512 like\n        'lang_src' : 'en',\n        'lang_tgt' : 'it',\n        'model_folder' : 'weights' ,\n        'model_basename' : 'tmodel_' ,\n        'preload' : None,\n        'tokenizer_file' : 'tokenizer_ {0}.json',\n        'experiment_name' : 'runs/tmodel'\n    }\n    \n# Function to construct the path for saving and retrieving model weights\ndef get_weights_file_path (config, epoch: str):\n    model_folder  = config['model_folder' ] # Extracting model folder from the config\n    model_basename  = config['model_basename' ] # Extracting the base name for model \n    model_filename  = f\"{model_basename }{epoch}.pt\" # Building filename\n    return str(Path('.')/ model_folder / model_filename ) # Combining current directo\nIn [24]:\ndef train_model (config):\n    # Setting up device to run on GPU to train faster\n    device = torch.device('cuda' if torch.cuda.is_available () else 'cpu')\n    print(f\"Using device {device}\")\n    \n    # Creating model directory to store weights\n    Path(config['model_folder' ]).mkdir(parents=True, exist_ok =True)\n    \n    # Retrieving dataloaders and tokenizers for source and target languages using t\n    train_dataloader , val_dataloader , tokenizer_src , tokenizer_tgt  = get_ds(config)\n    \n    # Initializing model on the GPU using the 'get_model' function\n    model = get_model (config,tokenizer_src .get_vocab_size (), tokenizer_tgt .get_voca\n    \n    # Tensorboard\n    writer = SummaryWriter (config['experiment_name' ])\n    \n    # Setting up the Adam optimizer with the specified learning rate from the '\n    # config' dictionary plus an epsilon value\n\n    optimizer  = torch.optim.Adam(model.parameters (), lr=config['lr'], eps = 1e-9)\n    \n    # Initializing epoch and global step variables\n    initial_epoch  = 0\n    global_step  = 0\n    \n    # Checking if there is a pre-trained model to load\n    # If true, loads it\n    if config['preload' ]:\n        model_filename  = get_weights_file_path (config, config['preload' ])\n        print(f'Preloading model {model_filename }')\n        state = torch.load(model_filename ) # Loading model\n        \n        # Sets epoch to the saved in the state plus one, to resume from where it st\n        initial_epoch  = state['epoch'] + 1\n        # Loading the optimizer state from the saved model\n        optimizer .load_state_dict (state['optimizer_state_dict' ])\n        # Loading the global step state from the saved model\n        global_step  = state['global_step' ]\n        \n    # Initializing CrossEntropyLoss function for training\n    # We ignore padding tokens when computing loss, as they are not relevant for th\n    # We also apply label_smoothing to prevent overfitting\n    loss_fn = nn.CrossEntropyLoss (ignore_index  = tokenizer_src .token_to_id ('[PAD]')\n    \n    # Initializing training loop \n    \n    # Iterating over each epoch from the 'initial_epoch' variable up to\n    # the number of epochs informed in the config\n    for epoch in range(initial_epoch , config['num_epochs' ]):\n        \n        # Initializing an iterator over the training dataloader\n        # We also use tqdm to display a progress bar\n        batch_iterator  = tqdm(train_dataloader , desc = f'Processing epoch {epoch:02\n        \n        # For each batch...\n        for batch in batch_iterator :\n            model.train() # Train the model\n            \n            # Loading input data and masks onto the GPU\n            encoder_input  = batch['encoder_input' ].to(device)\n            decoder_input  = batch['decoder_input' ].to(device)\n            encoder_mask  = batch['encoder_mask' ].to(device)\n            decoder_mask  = batch['decoder_mask' ].to(device)\n            \n            # Running tensors through the Transformer\n            encoder_output  = model.encode(encoder_input , encoder_mask )\n            decoder_output  = model.decode(encoder_output , encoder_mask , decoder_inp\n            proj_output  = model.project(decoder_output )\n            \n            # Loading the target labels onto the GPU\n            label = batch['label'].to(device)\n            \n            # Computing loss between model's output and true labels\n            loss = loss_fn(proj_output .view(-1, tokenizer_tgt .get_vocab_size ()), la\n            \n            # Updating progress bar\n            batch_iterator .set_postfix ({f\"loss\": f\"{loss.item():6.3f}\"})\n            \n            writer.add_scalar ('train loss' , loss.item(), global_step )\n            writer.flush()\n            \n            # Performing backpropagation\n            loss.backward ()\n\nWe can now train the model!\nUsing device cuda\nDownloading builder script:   0%|          | 0.00/2.40k [00:00<?, ?B/s]\nDownloading metadata:   0%|          | 0.00/7.98k [00:00<?, ?B/s]\nDownloading and preparing dataset opus_books/en-it (download: 3.14 MiB, generated:  \n8.58 MiB, post-processed: Unknown size, total: 11.72 MiB) to /root/.cache/huggingf\nace/datasets/opus_books/en-it/1.0.0/e8f950a4f32dc39b7f9088908216cd2d7e21ac35f893d0\n4d39eb594746af2daf...\nDownloading data:   0%|          | 0.00/3.30M [00:00<?, ?B/s]\nGenerating train split:   0%|          | 0/32332 [00:00<?, ? examples/s]\nDataset opus_books downloaded and prepared to /root/.cache/huggingface/datasets/op\nus_books/en-it/1.0.0/e8f950a4f32dc39b7f9088908216cd2d7e21ac35f893d04d39eb594746af2\ndaf. Subsequent calls will reuse this data.\nMax length of source sentence: 309\nMax length of target sentence: 274\nProcessing epoch 00: 100%|██████████| 3638/3638 [15:31<00:00,  3.91it/s, loss=5.92\n6]\n--------------------------------------------------------------------------------\nSOURCE: 'All right,' said the Englishman. 'And where are you going, my lord?' he a\nsked unexpectedly, addressing him as 'my lord,' which he hardly ever did.\nTARGET: — Tutto è in ordine — disse l’inglese. — E voi dove andate, milord? — chie\nse inaspettatamente, adoperando questa denominazione di my-Lord che non usava quas\ni mai.\nPREDICTED: — Sì , — disse il signor Rochester , — disse il signor Rochester , — e  \nche vi , — e che vi il signor Rochester .\n--------------------------------------------------------------------------------\nSOURCE: “Yes, yes,” says he, “you teachee me good, you teachee them good.” “No, n\no, Friday,” says I, “you shall go without me; leave me here to live by myself, as  \nI did before.”\nTARGET: Voi aver insegnato me il bene; insegnare il bene loro! — No, no, Venerdì;  \nandrete senza di me; lasciatemi vivere qui solo, come ho fatto in passato.»\nPREDICTED: E che è un ’ altra cosa , Jane , e vi è un ’ altra , che vi è un ' altr\na cosa , che non vi è un ’ altra cosa , che non vi , che vi , che vi , e che vi ,  \nche vi .            \n            # Updating parameters based on the gradients\n            optimizer .step()\n            \n            # Clearing the gradients to prepare for the next batch\n            optimizer .zero_grad ()\n            \n            global_step  += 1 # Updating global step count\n            \n        # We run the 'run_validation' function at the end of each epoch\n        # to evaluate model performance\n        run_validation (model, val_dataloader , tokenizer_src , tokenizer_tgt , config[\n         \n        # Saving model\n        model_filename  = get_weights_file_path (config, f'{epoch:02d}')\n        # Writting current model state to the 'model_filename'\n        torch.save({\n            'epoch': epoch, # Current epoch\n            'model_state_dict' : model.state_dict (),# Current model state\n            'optimizer_state_dict' : optimizer .state_dict (), # Current optimizer sta\n            'global_step' : global_step  # Current global step \n        }, model_filename )\nIn [25]:\nif __name__  == '__main__' :\n    warnings .filterwarnings ('ignore' ) # Filtering warnings\n    config = get_config () # Retrieving config settings\n    train_model (config) # Training model with the config arguments\n\nProcessing epoch 01: 100%|██████████| 3638/3638 [15:32<00:00,  3.90it/s, loss=3.43\n7]\n--------------------------------------------------------------------------------\nSOURCE: In the first place, it occurred to me to consider what business an English  \nship could have in that part of the world, since it was not the way to or from any  \npart of the world where the English had any traffic; and I knew there had been no  \nstorms to drive them in there in distress; and that if they were really English it  \nwas most probable that they were here upon no good design; and that I had better c\nontinue as I was than fall into the hands of thieves and murderers.\nTARGET: Prima di tutto andava ruminando in mia testa, qual razza di faccende potes\nse condurre una nave inglese in questa parte del mondo, ove, nè andando nè tornand\no, gl’Inglesi non avevano alcuna sorta di traffico. Sapeva d’altra parte non esser\ne occorse burrasche o altri disastri di mare che li avessero potuto costringere a  \ncercar quivi un riparo; dalle quali cose argomentava che se erano Inglesi, probabi\nlmente non erano qui con buon disegno, e che valea meglio per me il continuare nel\nla vita di prima, che cadere in mano di ladri o d’assassini.\nPREDICTED: In quel momento , per me , perchè non mi , perchè non poteva essere più  \ndi cui non poteva essere in tal modo di cui non poteva essere di non , e di non ,  \ne di non , e di non , e di non , e di non , e di non , e di non , e di non , e di  \nnon .\n--------------------------------------------------------------------------------\nSOURCE: And here I am living; my children growing, my husband returns to the famil\ny and feels his error, grows purer and better, and I live...\nTARGET: Ed ecco, io vivo. I bambini crescono, mio marito ritorna in famiglia e sen\nte il torto suo e diventa sempre migliore, e io vivo....\nPREDICTED: E io sono , e a me , e con la sua vita , e la sua vita , e .\nProcessing epoch 02: 100%|██████████| 3638/3638 [15:33<00:00,  3.90it/s, loss=5.41\n0]\n--------------------------------------------------------------------------------\nSOURCE: 'I wonder what they'll do next!\nTARGET: Chi sa che faranno dopo!\nPREDICTED: — Io , come sono in modo di fare !\n--------------------------------------------------------------------------------\nSOURCE: He thought himself her idol, ugly as he was: he believed, as he said, that  \nshe preferred his \"_taille d'athlete_\" to the elegance of the Apollo Belvidere.\nTARGET: Così egli, benché brutto, si credeva adorato e credeva che la giovane pref\nerisse la sua figura di atleta all'eleganza dell'Apollo di Belvedere.\nPREDICTED: Egli pensava che la sua domanda era stata , e disse che era stata così  \ncome se fosse stata la sua posizione , — disse il suo carattere . — La la della su\na vita .\nProcessing epoch 03: 100%|██████████| 3638/3638 [15:33<00:00,  3.90it/s, loss=3.84\n8]\n--------------------------------------------------------------------------------\nSOURCE: \"What's what?\" asked Harris and I.\nTARGET: — Che cosa c’è? — domandammo Harris e io.\nPREDICTED: — Che cosa ? — domandai Harris .\n--------------------------------------------------------------------------------\nSOURCE: If she did, she need not coin her smiles so lavishly, flash her glances so  \nunremittingly, manufacture airs so elaborate, graces so multitudinous. It seems to  \nme that she might, by merely sitting quietly at his side, saying little and lookin\ng less, get nigher his heart.\nTARGET: Mi pare che le basterebbe di sedersi tranquillamente accanto a lui, di par\nlar poco e di guardarlo anche meno, e giungerebbe al suo cuore.\nPREDICTED: Se non avesse bisogno di lei , non si , e , , , , , , , , , , , , , , ,  \n, , , , , , e , , , , , , la sua gioia .\nProcessing epoch 04: 100%|██████████| 3638/3638 [15:30<00:00,  3.91it/s, loss=3.80\n7]\n\n--------------------------------------------------------------------------------\nSOURCE: CHAPTER XXVIII\nTARGET: VIII.\nPREDICTED: XXVIII\n--------------------------------------------------------------------------------\nSOURCE: Cooler and fresher at the moment the gale seemed to visit my brow: I could  \nhave deemed that in some wild, lone scene, I and Jane were meeting.\nTARGET: \"In quel momento una brezza più fresca mi sfiorò la fronte. \"Avrei potuto  \ncredere che Jane ed io ci fossimo incontrati in qualche luogo deserto.\nPREDICTED: \" e la notte , perché la mia presenza mi , e che avevo veduto la mia ca\nsa , Jane .\nProcessing epoch 05: 100%|██████████| 3638/3638 [15:28<00:00,  3.92it/s, loss=3.92\n1]\n--------------------------------------------------------------------------------\nSOURCE: 'Call it what you like,' said the Cat. 'Do you play croquet with the Queen  \nto-day?'\nTARGET: — Di' come ti pare, — rispose il Gatto. — Vai oggi dalla Regina a giocare  \na croquet?\nPREDICTED: — Forse voi , — disse il Gatto , — ti prego di di con la Regina ?\n--------------------------------------------------------------------------------\nSOURCE: The Dormouse shook its head impatiently, and said, without opening its eye\ns, 'Of course, of course; just what I was going to remark myself.'\nTARGET: Il Ghiro scosse la testa con atto d'impazienza, e senza aprire gli occhi d\nisse: — Già! Già! stavo per dirlo io.\nPREDICTED: Il Ghiro prese il capo e il capo , senza capire , si voltò verso di lui  \n, ma egli mi fece dire : “ Non ho detto che cosa ho detto ”.\nProcessing epoch 06: 100%|██████████| 3638/3638 [15:30<00:00,  3.91it/s, loss=3.39\n5]\n--------------------------------------------------------------------------------\nSOURCE: When she got up, the previous day appeared in her memory as in a fog.\nTARGET: Quando si fu alzata, le venne in mente, come in una nebbia, la giornata pr\necedente.\nPREDICTED: Quando si svegliò , la giornata si calmò in una nebbia , la nebbia come  \nuna nebbia .\n--------------------------------------------------------------------------------\nSOURCE: 'Is it long since you went to see them?'\nTARGET: — È da molto che manchi da loro?\nPREDICTED: — È arrivato da tempo , da voi ?\nProcessing epoch 07: 100%|██████████| 3638/3638 [15:30<00:00,  3.91it/s, loss=3.92\n7]\n--------------------------------------------------------------------------------\nSOURCE: \"To be active: as active as I can.\nTARGET: — Voglio essere operosa per quanto è possibile.\nPREDICTED: — È strano , come posso .\n--------------------------------------------------------------------------------\nSOURCE: It is a veritable picture of an old country inn, with green, square courty\nard in front, where, on seats beneath the trees, the old men group of an evening t\no drink their ale and gossip over village politics; with low, quaint rooms and lat\nticed windows, and awkward stairs and winding passages.\nTARGET: È un vero quadro d’un vecchio albergo di campagna, con un verde cortile qu\nadrato, dove, sui sedili sotto gli alberi, i vecchi si riuniscono la sera a bere l\na birra e a discutere della politica paesana; con stanze, bizzarre camere e finest\nre ingraticciate, e delle scale malcomode e dei corridoi tortuosi.\nPREDICTED: È un quadro di campagna , con un ufficiale che si era messo in piedi ,  \ncon la sua costruzione , dove si , il verde della città e il di , il dei , dei , i  \ndi e i di , dei , dei e dei , e i e dei .\nProcessing epoch 08: 100%|██████████| 3638/3638 [15:30<00:00,  3.91it/s, loss=3.42\n0]\n\n--------------------------------------------------------------------------------\nSOURCE: How shall I do it?' he asked himself, trying to find expression for what h\ne had been thinking and the feelings he had lived through in that short night.\nTARGET: Come farò tutto questo?» si chiese, cercando di esprimere a se stesso ciò  \nche aveva pensato e sentito in quella breve nottata.\nPREDICTED: Come farò ? — chiese , cercando di capire , cercando di capire quello c\nhe era stato accaduto e che egli aveva sempre provato in quella notte .\n--------------------------------------------------------------------------------\nSOURCE: Having invited Helen and me to approach the table, and placed before each  \nof us a cup of tea with one delicious but thin morsel of toast, she got up, unlock\ned a drawer, and taking from it a parcel wrapped in paper, disclosed presently to  \nour eyes a good-sized seed-cake. \"I meant to give each of you some of this to take  \nwith you,\" said she, \"but as there is so little toast, you must have it now,\" and  \nshe proceeded to cut slices with a generous hand.\nTARGET: Ella invitò Elena e me ad avvicinarci alla tavola, collocò dinanzi a noi l\ne tazze e i crostini, poi tolse da un cassetto un maestoso pan pepato, ravvolto co\nn cura, e la sua mano generosa ce ne tagliò delle fette grosse.\nPREDICTED: Dopo aver Elena e mi la tavola , e poi ci si avvicinò al tè , ma con un  \nbimbo , che conteneva un bimbo , e poi , poi un cassetto , che , un pezzetto di ca\nrta , disse , dicendo che non era altro che qualcuno di , — come un , — e quando c\ni a destra , — e a destra , — ecco che ci sia qualche cosa che ci sia un po ' di p\nane , con un , e , — disse , — ma che ci a destra .\nProcessing epoch 09: 100%|██████████| 3638/3638 [15:34<00:00,  3.89it/s, loss=2.90\n3]\n--------------------------------------------------------------------------------\nSOURCE: I shall not stay long at Morton, now that my father is dead, and that I am  \nmy own master.\nTARGET: Non rimarrò lungamente a Morton ora che mio padre è morto e che son padron\ne delle mie azioni.\nPREDICTED: Non tornerò più a Morton , che è morta , e il mio padrone è morto e son\no il mio padrone .\n--------------------------------------------------------------------------------\nSOURCE: And in examining their actions and lives one cannot see that they owed any\nthing to fortune beyond opportunity, which brought them the material to mould into  \nthe form which seemed best to them. Without that opportunity their powers of mind  \nwould have been extinguished, and without those powers the opportunity would have  \ncome in vain.\nTARGET: Et esaminando le azioni e vita loro, non si vede che quelli avessino altro  \ndalla fortuna che la occasione; la quale dette loro materia a potere introdurvi dr\nento quella forma parse loro; e sanza quella occasione la virtù dello animo loro s\ni sarebbe spenta, e sanza quella virtù la occasione sarebbe venuta invano.\nPREDICTED: E , in quel modo le cose e la vita non sanno che la fortuna di questa o\nccasione si per acquistare tutto il bene che li assai bene , e che il loro fine er\na stato spento , e che il loro fine non sarebbe stato necessario avere .\nProcessing epoch 10: 100%|██████████| 3638/3638 [15:34<00:00,  3.89it/s, loss=3.73\n4]\n--------------------------------------------------------------------------------\nSOURCE: \"Our uncle John is dead,\" said he.\nTARGET: Egli entrò dicendo: — Lo zio John è morto.\nPREDICTED: — Mio zio John è morto , — disse .\n--------------------------------------------------------------------------------\nSOURCE: All that does not matter.\nTARGET: Questo non significa nulla.\nPREDICTED: Tutto questo non lo conosce .\nProcessing epoch 11: 100%|██████████| 3638/3638 [15:33<00:00,  3.90it/s, loss=3.05\n5]\n\n--------------------------------------------------------------------------------\nSOURCE: Mrs. P. used to come up and say she was very sorry - for herself, she like\nd to hear him - but the lady upstairs was in a very delicate state, and the doctor  \nwas afraid it might injure the child.\nTARGET: La signora Poppets soleva presentarsi a dire che le dispiaceva moltissimo  \n— quanto a lei andava matta per la musica — ma la signora di sopra era in istato i\nnteressante, e il dottore temeva che quel suono potesse nuocere al bambino.\nPREDICTED: La signora Poppets ci si coricò e cominciò a parlare con lei , perché g\nli piaceva sentire una donna magra e di , ma che il dottore era in un abisso , e i\nl bambino non poteva sopportare , che il bambino gli fosse apparso con l ’ aiuto d\nel bambino .\n--------------------------------------------------------------------------------\nSOURCE: \"A true Janian reply! Good angels be my guard!\nTARGET: — È una risposta degna di Jane!\nPREDICTED: — Un vero ! — rispose Bianca , sorridendo .\nProcessing epoch 12: 100%|██████████| 3638/3638 [15:34<00:00,  3.89it/s, loss=2.74\n0]\n--------------------------------------------------------------------------------\nSOURCE: If you had seen her as I have who have spent the whole winter with her, yo\nu would pity her.\nTARGET: Se tu la vedessi come l’ho vista io (ho passato tutto l’inverno con lei),  \nne avresti pena.\nPREDICTED: Se aveste visto come è andata a finire tutta la giornata , con lei , la  \navrebbe fatto pena .\n--------------------------------------------------------------------------------\nSOURCE: Anna had come out from behind the screen to meet him, and Levin saw in the  \ndim light of the study the woman of the portrait, in a dark dress of different sha\ndes of blue, not in the same attitude, not with the same expression, but on the sa\nme height of beauty as that on which the artist had caught her in the portrait.\nTARGET: Anna gli era uscita incontro di là dalla grata e Levin vide, nella penombr\na dello studio, quella stessa donna del ritratto, in abito scuro d’un turchino can\ngiante, non nella posa, non con l’espressione, ma della stessa bellezza con cui er\na stata colta dall’artista nel ritratto.\nPREDICTED: Anna era uscito dal tramezzo con l ’ aiuto della donna e vide Levin nel\nlo studio del ritratto di lei che in un vestito di merletto in un vestito di merle\ntto e senza le sue maniere , ma che in quel suo ritratto non era nel ritratto affa\ntto chiaro che l ’ artista sul suo quadro .\nProcessing epoch 13: 100%|██████████| 3638/3638 [15:35<00:00,  3.89it/s, loss=2.71\n3]\n--------------------------------------------------------------------------------\nSOURCE: Yesterday he betrayed himself – he wants the divorce and a marriage in ord\ner to burn his boats.\nTARGET: Ieri se l’è lasciato sfuggire: vuole il divorzio e il matrimonio per bruci\nare le sue navi.\nPREDICTED: La sua bontà ha voluto anche lui , per ottenere un divorzio e fare dell\ne sue barche .\n--------------------------------------------------------------------------------\nSOURCE: I know nothing, I understand nothing.'\nTARGET: Io non so nulla e non capisco nulla.\nPREDICTED: Io non capisco nulla .\nProcessing epoch 14: 100%|██████████| 3638/3638 [15:35<00:00,  3.89it/s, loss=2.57\n7]\n\n--------------------------------------------------------------------------------\nSOURCE: \"Well, Jane Eyre, and are you a good child?\"\nTARGET: — Ebbene, Jane Eyre, siete una buona bambina?\nPREDICTED: — Ebbene , Jane Eyre e siete una bambina ?\n--------------------------------------------------------------------------------\nSOURCE: 'Come, Anna Arkadyevna,' began Korsunsky, drawing her bare arm under his,  \n'I have such a good idea for a cotillion – Un bijou.'\nTARGET: — Su via, Anna Arkad’evna — prese a dire Korsunskij, mettendo il braccio n\nudo di lei sotto la manica del suo frac. — Che idea mi è venuta per il cotillon! U\nn bijou!\nPREDICTED: — Andiamo , Anna Arkad ’ evna — disse Korsunskij , alzandosi sotto il b\nraccio . — Ho un gran pensiero per una parte di , un , per la .\nProcessing epoch 15: 100%|██████████| 3638/3638 [15:34<00:00,  3.89it/s, loss=2.61\n8]\n--------------------------------------------------------------------------------\nSOURCE: 'Gentlemen! To-morrow at dawn!' Levin mumbled drowsily, and fell asleep.\nTARGET: — Signori, a domani, appena si fa giorno! — e s’addormentò.\nPREDICTED: — , l ’ alba — disse Levin , in fretta .\n--------------------------------------------------------------------------------\nSOURCE: But after that hour another passed, a second, a third, and all the five ho\nurs that he had set himself as the longest term of possible endurance, and still t\nhe situation was unchanged; and he went on enduring, for there was nothing else to  \ndo but to endure – thinking every moment that he had reached the utmost limit of e\nndurance and that in a moment his heart would burst with pity.\nTARGET: Ed era passata soltanto un’ora. Ma dopo quest’ora, ne passò ancora un’altr\na, poi ne passarono due, tre, tutte e cinque le ore, e le cose erano sempre allo s\ntesso punto; e lui sopportava ancora, perché non c’era più niente da fare se non p\nazientare, pensando, ogni momento, d’essere giunto al limite della sopportazione e  \nche il cuore, subito, da un momento all’altro, si sarebbe spezzato dalla pena.\nPREDICTED: Ma dopo un ’ ora passò un ’ altra , e tutto il terzo piano , e tutti i  \ncampi i campi i capi , che aveva potuto ancora peggio ; e la situazione tutta nell\na situazione non c ’ era nessun altro che , non desiderando che nulla di meglio ,  \ne si provava in un momento che tutti gli stati il cuore e si sarebbe il cuore di f\norza e si sarebbe il cuore .\nProcessing epoch 16: 100%|██████████| 3638/3638 [15:35<00:00,  3.89it/s, loss=2.18\n9]\n\n--------------------------------------------------------------------------------\nSOURCE: Meeting his look, her face suddenly assumed a coldly severe expression, as  \nif to say: 'It is not forgotten.\nTARGET: Nell’incontrare lo sguardo di lui, il viso di Anna, d’un tratto, prese u\nn’espressione dura, come a dirgli: “Non è dimenticato.\nPREDICTED: L ’ espressione del viso di lei , un tratto di fredda , come se volesse  \ndire , non è dimenticato .\n--------------------------------------------------------------------------------\nSOURCE: I asked him all the particulars of their voyage, and found they were a Spa\nnish ship, bound from the Rio de la Plata to the Havanna, being directed to leave  \ntheir loading there, which was chiefly hides and silver, and to bring back what Eu\nropean goods they could meet with there; that they had five Portuguese seamen on b\noard, whom they took out of another wreck; that five of their own men were drowned  \nwhen first the ship was lost, and that these escaped through infinite dangers and  \nhazards, and arrived, almost starved, on the cannibal coast, where they expected t\no have been devoured every moment.\nTARGET: Interrogato da me su i particolari del suo viaggio, mi raccontò come avess\ne fatto parte de’ naviganti d’un vascello spagnuolo che veniva dal Rio la Plata pe\nr condursi all’Avana a lasciare ivi il loro carico, consistente principalmente in  \npellami o argento, e riportarne quelle merci pregiate in Europa in cui si sarebber\no abbattuti; come avessero preso a bordo cinque marinai portoghesi salvatisi da un  \naltro naufragio; come cinque de’ loro fossero rimasi annegati quando il loro vasce\nllo perì; come campati in mezzo ad infiniti pericoli e traversie fossero arrivati  \nquasi morti di fame ad una costa di cannibali, ove si aspettavano a ciascun istant\ne di essere divorati.\nPREDICTED: tutte le predette cose non erano buone , e compresi che fosse alquanto  \n, una nave che venne ad esse dal vascello le la paura de ’ marinai , che si per po\ntere trovare il vascello , e che si per non essere a furia di migliaia d ’ uomini  \nin mezzo : quando quando quando avessero abbandonato il vascello si , fu veduto co\nme ombre loro , si , e quando tutti quegli sciagurati a levante sulla costa della  \nspiaggia , e donde sarebbero stati trasportati a quando sarebbero stati e venuti ,  \ne altre che sarebbero rimasti inevitabilmente .\nProcessing epoch 17: 100%|██████████| 3638/3638 [15:34<00:00,  3.89it/s, loss=2.66\n2]\n--------------------------------------------------------------------------------\nSOURCE: \"Where the devil is Rochester?\" cried Colonel Dent.\nTARGET: — Dove diavolo è Rochester? — esclamò il colonnello Dent.\nPREDICTED: — Dov ' è il diavolo ? — esclamò il colonnello Dent e il colonnello Den\nt .\n--------------------------------------------------------------------------------\nSOURCE: \"Yes, she is alive; and more sensible and collected than she was.\nTARGET: — Sì, vive, ma da ieri non è più in sé.\nPREDICTED: — Sì , lei è più viva e più nervosa di lei .\nProcessing epoch 18: 100%|██████████| 3638/3638 [15:34<00:00,  3.89it/s, loss=2.15\n2]\n\n--------------------------------------------------------------------------------\nSOURCE: I asked the captain if he was willing to venture with these hands on board  \nthe ship; but as for me and my man Friday, I did not think it was proper for us to  \nstir, having seven men left behind; and it was employment enough for us to keep th\nem asunder, and supply them with victuals.\nTARGET: Chiesi al capitano s’egli credea d’avventurarsi con questa gente all’arrem\nbaggio del vascello; perchè quanto a me e al servo mio Venerdì, non pensai ne conv\nenisse il moverci dall’isola, ove ne rimanevano sette uomini da guardare. Era assa\ni briga per noi il tenerli disgiunti e provvedere al giornaliero lor vitto; quanto  \nai cinque della caverna, trovai opportuno il lasciarli legati.\nPREDICTED: Feci ciò che fu , se il capitano non avesse voluto queste mie mani ; ma  \nper altro non era facile a Venerdì , nè sapeva solamente colpire soltanto a Venerd\nì di che far la mia preda su le otto uomini , che era divenuto , se non mi riusciv\na a mangiare . , era più allegra o d ’ uomini armati .\n--------------------------------------------------------------------------------\nSOURCE: I must, then, repeat continually that we are for ever sundered:--and yet,  \nwhile I breathe and think, I must love him.\"\nTARGET: Debbo dunque convincermi che saremo separati per sempre, ma che debbo amar\nlo per tutta la vita.\nPREDICTED: Devo partire perché siamo sempre più gravi , ma sono ritto di fronte a  \nlui e bisogna .\nProcessing epoch 19: 100%|██████████| 3638/3638 [15:33<00:00,  3.90it/s, loss=2.09\n4]\n--------------------------------------------------------------------------------\nSOURCE: 'Duties to go to a concert...'\nTARGET: — Gli obblighi di andare al concerto...\nPREDICTED: — Oggi si è stabilito al concerto .\n--------------------------------------------------------------------------------\nSOURCE: 'What!\nTARGET: — Che cosa?\nPREDICTED: — Che cosa ?\nAs you can see below , we trained for 20 epochs, and the model has been\nslowly improving. The last epoch had the best performance, at 2.094.\nTraining for more epochs, as well as fine-tuning some parameters, could\nlead to more promising results.\nConclusion Conclusion\nIn this notebook, we have explored the original Transformer architecture\nin depth, as presented in the Attention Is All You Need  research paper .\nWe used PyT orch to implement it step-by-step on a language translation\ntask using the OpusBook dataset for English-to-Italian translation.\nThe Transformer is a revolutionary step towards the most advanced\nmodels we have today , such as OpenAI's GPT -4 model. And that is why it\n\nis so relevant to comprehend how this architecture works and what it can\nachieve.\nThe resources behind this notebook are the paper \"Attention Is All You\nNeed\"  and the YouTube video Coding a T ransformer from scratch on\nPyTorch, with full explanation, training and inference  posted by Umar\nJamil . I highly suggest you check both materials for a deeper\nunderstanding of the Transformer .\nIf you liked the content of this notebook, feel free to leave an upvote and\nshare it with friends and colleagues. I am also eager to read your\ncomments, suggestions, and opinions.\nThank you very much!\nLuis Fernando T orres, 2024\nLet's connect!🔗\nLinkedIn  • Medium  • Hugging F ace\nLike my cont ent? Feel fr ee to Buy Me a Co ffee ☕\nhttps://luuisot orres.github.io/",
  "metadata": {
    "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpdyhqfe64.pdf",
    "filename": "tmpdyhqfe64.pdf",
    "file_type": "pdf",
    "file_size": 4569490,
    "file_size_mb": 4.36,
    "file_hash": "ea05f2b4440262395ecfd641bbcc98b5f3ccfd285f4a2e922585f4d088dc00b3",
    "creation_date": "2025-03-15T00:50:43.702765",
    "modification_date": "2025-03-15T00:50:43.962804",
    "processing_date": "2025-03-15T00:50:50.079411",
    "page_count": 38,
    "page_size_sample": [
      "594.96x841.92",
      "594.96x841.92",
      "594.96x841.92",
      "594.96x841.92",
      "594.96x841.92"
    ],
    "title": "Transformer Architecture LLM.pdf",
    "author": "Unknown",
    "description": "",
    "original_filename": "Transformer Architecture LLM.pdf",
    "content_type": "application/pdf",
    "batch_upload": true,
    "batch_index": 2,
    "timestamp": "2025-03-15T00:50:50.079411"
  }
}