{
  "doc_id": "4df0d9d04e6d4d5a",
  "content": "Part 1Transformers\n2\n\n\nTransformers\n3•Tokenizaton•Input Embeddings•PositionEncodings•Residuals •Query •Key•Value•Add & Norm•Encoder•Decoder•Attention•Self Attention•Multi Head Attention•Masked Attention•Encoder Decoder Attention•Output Probabilities  / Logits•Softmax•Encoder-Decoder models•Decoder only models\n\nTransformers\n4•Tokenizaton•Input Embeddings•PositionEncodings•Residuals •Query •Key•Value•Add & Norm•Encoder•Decoder•Attention•Self Attention•Multi Head Attention•Masked Attention•Encoder Decoder Attention•Output Probabilities  / Logits•Softmax•Encoder-Decoder models•Decoder only models\n\n\n5Machine Translation\nInputsI ate an apple TargetsIch have einenapfelgegessen\n\n6InputsProcessing InputsInputsI ate an apple \n\n7\nIateanapple<eos>\nGenerate Input EmebeddingsI ate an apple TokenizerInputs\n\n8\nIateanapple<eos>Embedding Layer\nGenerate Input EmebeddingsI ate an apple TokenizerdmodelInputs\n\n9\nIateanapple<eos>WHERE IS THE CONTEXT ?Encoder\n\n10Encoder\nIateanapple<eos>BLACK BOXOF SORTS\n\n\n11Iateanapple<eos>LEARN TO ADD CONTEXT BLACK BOXOF SORTS\nEncoder\n\n12Iateanapple<eos>CONTEXTUALLY RICH EMBEDDINGSLEARN TO ADD CONTEXT BLACK BOXOF SORTS\nEncoder\n\n13Iateanapple<eos>CONTEXTUALLY RICH EMBEDDINGSLEARN TO ADD CONTEXT BLACK BOXOF SORTS⍺[ ij ]?  \nEncoder\n\n14Iateanapple<eos>CONTEXTUALLY RICH EMBEDDINGSLEARN TO ADD CONTEXT BLACK BOXOF SORTS⍺[ ij ]?  ∑  ∏ ?\nEncoder\n\n15\nFrom lecture 18:⍺[ ij ]?  Attention\n\n16\n•Query•Key•ValueFrom lecture 18:⍺[ ij ]?  Attention\n\n17Query, Key & ValueDatabase{Key, Value store}\n\n\n18\nDatabase{Key, Value store}{Query: “Order details of order_104”}OR{Query: “Order details of order_106”}Query, Key & Value\n\n19{Key, Value store}{Query: “Order details of order_104”}OR{Query: “Order details of order_106”}\nQuery, Key & Value\n\n20{Key, Value store}\n{Query: “Order details of order_104”}OR{Query: “Order details of order_106”}Query, Key & Value\n\n21{Key, Value store}\n{Query: “Order details of order_104”}OR{Query: “Order details of order_106”}Query, Key & Value\n\n22{Key, Value store}\nDone at the same time !!{Query: “Order details of order_104”}OR{Query: “Order details of order_106”}Query, Key & Value\n\n23\nKey1.Interacts directly with Queries2.Distinguishes one object from another3.Identify which object is the most relevant and by how much Value1.Actual details of the object2.More finegrainedQuery1.Search for info {Query: “Order details of order_104”}OR{Query: “Order details of order_106”}Query, Key & Value\n\n24Attention\nQueryKeyValueKey Value Store\n\n25QueryKeyValueKey Value StoreAttention\n\n26QueryKeyValueKey Value StoreDone at the same time !!Attention\n\n27QueryKeyValueKey Value StoreQQKTsoftmax( !\"!√$)softmax( !\"!√$)VParallelizable !!!Attention\n\n28QueryKeyValueKey Value StoreQQKTsoftmax( !\"!√$)softmax( !\"!√$)VParallelizable !!!Attention FilterAttention\n\n29I1I2I3I4I5Iateanapple<eos>Attention\n\n30I1I2I3I4I5Iateanapple<eos>WQWKWVQ1K1V1WQWKWVQ2K2V2WQWKWVQ3K3V3WQWKWVQ4K4V4WQWKWVQ5K5V5Dimensions across QKV have been dropped for brevityAttention\n\n31I1I2I3I4I5Iateanapple<eos>WQWKWVQ1K1V1WQWKWVQ2K2V2WQWKWVQ3K3V3WQWKWVQ4K4V4WQWKWVQ5K5V5⊗e1,1Dimensions across QKV have been dropped for brevityAttention\n\nsoftmax\n32I1I2I3I4I5Iateanapple<eos>WQWKWVQ1K1V1WQWKWVQ2K2V2WQWKWVQ3K3V3WQWKWVQ4K4V4WQWKWVQ5K5V5⊗e1,1α1,1Dimensions across QKV have been dropped for brevityAttention\n\nsoftmax\n33I1I2I3I4I5Iateanapple<eos>WQWKWVQ1K1V1WQWKWVQ2K2V2WQWKWVQ3K3V3WQWKWVQ4K4V4WQWKWVQ5K5V5⊗e1,1α1,1⊗Dimensions across QKV have been dropped for brevityAttention\n\nsoftmax\n34I1I2I3I4I5Iateanapple<eos>WQWKWVQ1K1V1WQWKWVQ2K2V2WQWKWVQ3K3V3WQWKWVQ4K4V4WQWKWVQ5K5V5⊗e1,1α1,1⊗e1,2α1,2⊗⊗Dimensions across QKV have been dropped for brevityAttention\n\nsoftmax\n35I1I2I3I4I5Iateanapple<eos>WQWKWVQ1K1V1WQWKWVQ2K2V2WQWKWVQ3K3V3WQWKWVQ4K4V4WQWKWVQ5K5V5⊗e1,1α1,1⊗e1,2α1,2⊗⊗⊗e1,3α1,3⊗Dimensions across QKV have been dropped for brevityAttention\n\nsoftmax\n36I1I2I3I4I5Iateanapple<eos>WQWKWVQ1K1V1WQWKWVQ2K2V2WQWKWVQ3K3V3WQWKWVQ4K4V4WQWKWVQ5K5V5⊗e1,1α1,1⊗e1,2α1,2⊗⊗⊗e1,3α1,3⊗⊗α1,4⊗e1,4Dimensions across QKV have been dropped for brevityAttention\n\nsoftmax\n37I1I2I3I4I5Iateanapple<eos>WQWKWVQ1K1V1WQWKWVQ2K2V2WQWKWVQ3K3V3WQWKWVQ4K4V4WQWKWVQ5K5V5⊗e1,1α1,1⊗e1,2α1,2⊗⊗⊗e1,3α1,3⊗⊗α1,4⊗e1,4⊗e1,5α1,5⊗Dimensions across QKV have been dropped for brevityAttention\n\n38I1I2I3I4I5WQWKWVQ1K1V1Iateanapple<eos>⊗e1,1softmaxα1,1⊗\nWQWKWVQ2K2V2⊗e1,2α1,2⊗\nWQWKWVQ3K3V3⊗e1,3α1,3⊗\nWQWKWVQ4K4V4⊗e1,4α1,4⊗\nWQWKWVQ5K5V5⊗e1,5α1,5⊗∑Z1Dimensions across QKV have been dropped for brevityContextually rich embeddingAttention\n\n39I1I2I3I4I5Iateanapple<eos>WQWKWVQ1K1V1WQWKWVQ2K2V2WQWKWVQ3K3V3WQWKWVQ4K4V4WQWKWVQ5K5V5Dimensions across QKV have been dropped for brevityAttention\n\n40I1I2I3I4I5WQWKWVQ1K1V1Iateanapple<eos>⊗e1,1softmaxα1,1⊗\nWQWKWVQ2K2V2⊗e1,2α1,2⊗\nWQWKWVQ3K3V3⊗e1,3α1,3⊗\nWQWKWVQ4K4V4⊗e1,4α1,4⊗\nWQWKWVQ5K5V5⊗e1,5α1,5⊗∑Z1Dimensions across QKV have been dropped for brevityContextually rich embeddingAttention\n\n41I1I2I3I4I5WQWKWVQ1K1V1Iateanapple<eos>⊗e1,1softmaxα1,1⊗\nWQWKWVQ2K2V2⊗e1,2α1,2⊗\nWQWKWVQ3K3V3⊗e1,3α1,3⊗\nWQWKWVQ4K4V4⊗e1,4α1,4⊗\nWQWKWVQ5K5V5⊗e1,5α1,5⊗∑Z1Dimensions across QKV have been dropped for brevityParallelizedContextually rich embeddingAttention\n\n42Poll 1 @1296 \nWhich of the following are true about attention? (Select all that apply)a.To calculate attention weights for input I2, you would use key k2, and all queriesb.To calculate attention weights for input I2, you would use query q2, and all keysc.We scale the QKT product to bring attention weights in the range of [0,1]d.We scale the QKT product to allow for numerical stability \n\n43\nWhich of the following are true about attention? (Select all that apply)a.To calculate attention weights for input I2, you would use key k2, and all queriesb.To calculate attention weights for input I2, you would use query q2, and all keysc.We scale the QKT product to bring attention weights in the range of [0,1]d.We scale the QKT product to allow for numerical stability Poll 1 @1296 \n\n44Positional Encoding\nIateanapple<eos>\n\n45\nPositional EncodingIateanapple<eos>appleateanI<eos>Positional Encoding\n\n46\nPositional EncodingRequirements for Positional Encodings •Some representation of time ? (like seq2seq ?)•Should be unique for each position –not cyclicPositional Encoding\n\n47\nPositional EncodingRequirements for Positional Encodings •Some representation of time ? (like seq2seq ?)•Should be unique for each position –not cyclicPossible Candidates :\"!\"#= \"!+∆%\"!\"#= &$!∆%\"!\"#= \"!.!∆%Positional Encoding\n\n48\nPositional EncodingRequirements for Positional Encodings •Some representation of time ? (like seq2seq ?)•Should be unique for each position –not cyclicPossible Candidates :\"!\"#= \"!+∆%\"!\"#= &$!∆%\"!\"#= \"!.!∆%\nPositional Encoding\n\n49\nPositional EncodingRequirements for Positional Encodings •Some representation of time ? (like seq2seq ?)•Should be unique for each position –not cyclic•BoundedPossible Candidates :\"!\"#= \"!+∆%\"!\"#= &$!∆%\"!\"#= \"!.!∆%\nPositional Encoding\n\n50\nPositional EncodingRequirements for Positional Encodings •Some representation of time ? (like seq2seq ?)•Should be unique for each position –not cyclic•BoundedPossible Candidates :P(t + t’) = Mt’ xP(t)Positional Encoding\n\n51\nPositional EncodingRequirements for Positional Encodings •Some representation of time ? (like seq2seq ?)•Should be unique for each position –not cyclic•BoundedPossible Candidates :P(t + t’) = Mt’ x P(t)M ?1.Should beaunitarymatrix2.Magnitudesofeigenvalueshouldbe1-> normpreservingPositional Encoding\n\n52\nPositional EncodingRequirements for Positional Encodings •Some representation of time ? (like seq2seq ?)•Should be unique for each position –not cyclic•BoundedPossible Candidates :P(t + t’) = Mt’ x P(t)M 1.The matrix can be learnt2.Producesunique rotated embeddings each timePositional Encoding\n\n53\nRotary Positional Embedding\nREF: Rotary Positional Embeddings\n!\n\n\n54\nPositional EncodingRequirements for Positional Encodings •Some representation of time ? (like seq2seq ?)•Should be unique for each position –not cyclic•BoundedActual Candidates :sine(g(t))cosine(g(t))\nPositional Encoding\n\n55\nPositional EncodingRequirements for g(t) •Must have same dimensions as input embeddings•Must produce overall unique encodings pos -> idxof the token in input sentencei-> ithdimension out of d\nPositional Encoding\n\n56\nPositional Encoding\nRequirements for g(t) •Must have same dimensions as input embeddings•Must produce overall unique encodings pos -> idxof the token in input sentencei-> ithdimension out of d\nPositional Encoding\n\n57\nIateanapple<eos>Embedding Layer\nInputI ate an apple TokenizerInput EmbeddingsPosition EncodingsFinal Input Embeddings\nTokensPositional Encoding\n\n58Iateanapple<eos>CONTEXTUALLY RICH EMBEDDINGSLEARN TO ADD CONTEXT BLACK BOXOF SORTS⍺[ ij ]∑\nEncoder\n\n59Self Attention\nFrom lecture 18:\n\n60Theanimaldidn’tcrossthestreetbecauseitwastoowideSelf Attention\n\n61Theanimaldidn’tcrossthestreetbecauseitwastoowide?\ncoreference resolution ?Self Attention\n\n62\nSelf Attention\n\n63\nSelf Attention\n\n64\nSelf Attention\n\n65\nQuery InputsKey InputsValue Inputs==Self Attention\nSELF\n\n66WQ\nWKWv\nInput EmbeddingsSelf Attention\n\n67\nWQWKWV\nInput Embeddings\nQ ProjectionsK ProjectionsV Projections\nSelf Attention\nInput EmbeddingsInput Embeddings\n\n68\nQProjectionKProjection\nT\n√\"!\"#$%softmax\nSelf Attention\n\n69\nT\n√\"!\"#$%softmax\n!( T2 xdmodel)\nQProjectionKProjectionSelf Attention\n\n70\nVProjection\n√\"!\"#$%softmax\nT!( T2 xdmodel)\nQProjectionKProjectionSelf Attention\n\n71\nAttention: Z\nSelf Attention\n\n72Theanimaldidn’tcrossthestreetbecauseitwastoowidecoreference resolution \nSelf Attention\n\n73Theanimaldidn’tcrossthestreetbecauseitwastoowide?\ncoreference resolution \nSelf Attention\n\n74WQ\nWKWv\nSelf Attention\nInput Embeddings\n\n75Multi-Head Attention\nWQ1, WQ2, … WQH, \nH..21H..21WK1, WK2, … WKH, H..21WV1, WV2, … WVH, \nInput Embeddings\n\n76\nWQiWKiWVi\nInputsInputsInputs\nH..21H..21H..21WKiWVi\nQiH..21WKiH..21KiWViH..21Vi\nMulti-Head Attention\n\n77\nQiKiViT\n√\"!\"#$%softmax\nfor all i∈[1, h]Multi-Head Attention\n\n78\nMulti Head Attention : Z\n…Z1Z2Zh\nCONCATMulti-Head Attention\n\n\n79Theanimaldidn’tcrossthestreetbecauseitwastoowidecoreference resolution \nPart of Speech ?Semantic relationships ?Sentence boundaries ?Comparisons ?Context ? \nMulti-Head Attention\n\n80\nNorm(Z)\nInputNormalization(Z)•Mean 0, Std dev 1•Stabilizes training•Regularization effectAdd & Norm\nAdd -> Residuals•Avoidvanishing gradients•Train deeper networks\n\n81\nNorm(Z)\nInputFeed Forward \nFeed Forward•Non Linearity•Complex Relationships•Learn from each other\nResidualsFeed Forward\n\n82\nNorm(Z)\nInputFeed Forward \nAdd & Norm \nResidualsAdd & Norm\n\n83Encoders\nEncoder    ENCODER\n\n84\nENCODERENCODERENCODER...Encoder    \nEncoders\nOutput from EncoderiInput to Encoderi+1\n\n85Transformers\nüTokenizatonüInput Embeddings üPositionEncodingsüResiduals üQuery üKey üValueüAdd & Norm üEncoder •DecoderüAttention üSelf Attention üMulti Head Attention •Masked Attention•Encoder Decoder Attention•Output Probabilities  / Logits•Softmax•Encoder-Decoder models•Decoder only models\n\n86Machine Translation\nInputsI ate an apple TargetsIch have einenapfelgegessen\n\n87Targets\nTargetsIch have einenapfelgegessen\n\n88IchhaveeinenapfelgegessenEmbedding Layer + Positional Encoding\nGenerate Target EmebeddingsIch have einenapfelgegessenTokenizer<eos><sos>\nTargets\n\n89Masked Multi HeadAttention\nIchhaveeinenapfelgegessen<eos><sos>\n\n90\nDecoding step by step (using Teacher Forcing)Ichhave<sos>Ichhaveeinen<sos>Ichhaveeinenapfel<sos>Ichhaveeinenapfelgegessen<sos><sos>Ich<sos>\nIchhaveeinenapfelgegessen<eos><sos>1234567InferenceMasked Multi HeadAttention\n\n91\nDecoding step by step (using Teacher Forcing)Ichhave<sos>Ichhaveeinen<sos>Ichhaveeinenapfel<sos>Ichhaveeinenapfelgegessen<sos><sos>Ich<sos>\nIchhaveeinenapfelgegessen<eos><sos>1234567Parallelized ?Masked Multi HeadAttentionInference\n\n92\n<sos>Decoding step by step (using Teacher Forcing)Ichhaveeinenapfelgegessen<eos>TrainingMasked Multi HeadAttention\n\n93\n<sos>Decoding step by step (using Teacher Forcing)Ichhaveeinenapfelgegessen<eos>Outputs at time T should only pay attention to outputs until time T-1TrainingMasked Multi HeadAttention\n\n94\nIchhave<sos>Ichhaveeinen<sos>Ichhaveeinenapfel<sos>Ichhaveeinenapfelgegessen<sos><sos>Ich<sos>\nIchhaveeinenapfelgegessen<eos><sos>1234567Decoding step by step (using Teacher Forcing)Ichhaveeinenapfelgegessen<eos>haveeinenapfelgegessen<eos>einenapfelgegessen<eos>apfelgegessen<eos>gegessen<eos><eos>Masked Multi HeadAttention\n\n95\nIchhave<sos>Ichhaveeinen<sos>Ichhaveeinenapfel<sos>Ichhaveeinenapfelgegessen<sos><sos>Ich<sos>\nIchhaveeinenapfelgegessen<eos><sos>1234567Decoding step by step (using Teacher Forcing)Ichhaveeinenapfelgegessen<eos>haveeinenapfelgegessen<eos>einenapfelgegessen<eos>apfelgegessen<eos>gegessen<eos><eos>Mask the available attention values ?Masked Multi HeadAttention\n\n96\nIchhave<sos>Ichhaveeinen<sos>Ichhaveeinenapfel<sos>Ichhaveeinenapfelgegessen<sos><sos>Ich<sos>\nIchhaveeinenapfelgegessen<eos><sos>1234567Decoding step by step (using Teacher Forcing)-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nMasked Multi HeadAttention\n\n97\nIchhave<sos>Ichhaveeinen<sos>Ichhaveeinenapfel<sos>Ichhaveeinenapfelgegessen<sos><sos>Ich<sos>\nIchhaveeinenapfelgegessen<eos><sos>1234567Decoding step by step (using Teacher Forcing)\nSoftmax-> 0.               -> 0-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nMasked Multi Head Attention\n\n98Masked Multi Head Attention\nMasked Multi Head Attention : Z’QKT\nAttention Mask: M\n=Masked AttentionMasked Multi HeadAttention\n\n99Masked Multi Head Attention\nMasked Multi Head Attention : Z’\nMasked AttentionValuesMasked Multi HeadAttention\n\n100Norm(Z’)InputEncoder Decoder Attention ?\nEncoder Decoder Attention\nAdd & Norm \n\n101Encoder Decoder Attention ?\nEncoder Decoder Attention\n\n102\nEncoder SelfAttention 1.Queries from Encoder Inputs2.Keys from Encoder Inputs3.Values from Encoder InputsDecoder Masked SelfAttention 1.Queries from Decoder Inputs2.Keys from Decoder Inputs3.Values from Decoder InputsEncoder Decoder Attention\n\n103Attention{Key, Value store}{Query: “Order details of order_104”}{Query: “Order details of order_106”}\n\n\n104\nEncoderKeys from Encoder OutputsValues from Encoder OutputsDecoderQueries from Decoder InputsNOTE: Every decoder block receives the same FINAL encoder outputEncoder Decoder Attention\n\n105\nQdKe\nsoftmax( (#)$%*&'#$()\nsoftmax( (#)$%*&'#$().        Ve\nZ’’\nEncoder Decoder Attention\n\n106Norm(Z’’)\nFeed Forward \n•Non Linearity•Complex Relationships•Learn from each other\nResiduals\nAdd n Norm Decoder Self AttnEncoder Decoder Attention\n\n107DecoderDECODER\n\n\n108DECODERDECODERDECODER...\nDecoder\nDecoder output\n\n\n109Linear\nFinal Decoder Output\n…\nLinear \nsoftmaxLinear weights are often tied withinput embedding matrix\n\n110Softmax\n…TdV Output Probabilities\n\nWhich of the following are true about transformers?a.Transformers can always be run in parallelb.Transformer decoders can only be parallelized during trainingc.Positional encodings help parallelize the transformer encoderd.Queries, keys, and values are obtained by splitting the input into 3 equal segmentse.Multiheaded attention helps transformers find different kinds of relations between the tokensf.During decoding, decoder outputs function as queries and keys while the values come from the encoderPoll 2 (@1297)\n\nWhich of the following are true about transformers?a.Transformers can always be run in parallelb.Transformer decoders can only be parallelized during trainingc.Positional encodings help parallelize the transformer encoderd.Queries, keys, and values are obtained by splitting the input into 3 equal segmentse.Multiheaded attention helps transformers find different kinds of relations between the tokensf.During decoding, decoder outputs function as queries and keys while the values come from the encoderPoll 2 (@1126)\n\n113Transformers\nInputsI ate an apple TargetsIch have einenapfelgegessenMachine Translation\n\n114Transformers\nüTokenizatonüInput Embeddings üPositionEncodingsüResiduals üQuery üKey üValueüAdd & Norm üEncoder üDecoderüAttention üSelf Attention üMulti Head Attention üMasked AttentionüEncoder Decoder AttentionüOutput Probabilities  / LogitsüSoftmax•Encoder-Decoder models•Decoder only models\n\nPart 2LLM\n109\n\n\n110\nTransformers, mid-2017\n\n111\nTransformers, mid-2017\nRepresentation\n\n112\nTransformers, mid-2017\nRepresentationGeneration\n\n113\nTransformers, mid-2017\nRepresentationGenerationInput – input tokensOutput – hidden statesInput – output tokens and hidden states*Output – output tokens\n\n114\nTransformers, mid-2017\nRepresentationGenerationInput – input tokensOutput – hidden statesModel can see all timestepsInput – output tokens and hidden states*Output – output tokensModel can only see previous timesteps\n\n115\nTransformers, mid-2017\nRepresentationGenerationInput – input tokensOutput – hidden statesModel can see all timestepsDoes not usually output tokens, so no inherent auto-regressivityInput – output tokens and hidden states*Output – output tokensModel can only see previous timestepsModel is auto-regressive with previous timesteps’ outputs\n\n116\nTransformers, mid-2017\nRepresentationGenerationInput – input tokensOutput – hidden statesModel can see all timestepsDoes not usually output tokens, so no inherent auto-regressivityCan also be adapted to generate tokens by appending a module that maps hidden state dimensionality to vocab sizeInput – output tokens and hidden states*Output – output tokensModel can only see previous timestepsModel is auto-regressive with previous timesteps’ outputsCan also be adapted to generate hidden states by looking before token outputs\n\n117\n2018 –The Inception of the LLM Era\nRepresentationGenerationBERTOct 2018GPTJun 2018\n\nBERT - Bidirectional Encoder Representations\n118•One of the biggest challenges in LM-building used to be the lack of task-specific training data.•What if we learn an effective representation that can be applied to a variety of downstream tasks?•Word2vec (2013)•GloVe (2014)\n\nBERT - Bidirectional Encoder Representations\n119BERT Pre-Training Corpus:•English Wikipedia - 2,500 million words•Book Corpus - 800 million words\n\nBERT - Bidirectional Encoder Representations\n120BERT Pre-Training Corpus:•English Wikipedia - 2,500 million words•Book Corpus - 800 million wordsBERT Pre-Training Tasks:•MLM (Masked Language Modeling)•NSP (Next Sentence Prediction)\n\nBERT - Bidirectional Encoder Representations\n121BERT Pre-Training Corpus:•English Wikipedia - 2,500 million words•Book Corpus - 800 million wordsBERT Pre-Training Tasks:•MLM (Masked Language Modeling)•NSP (Next Sentence Prediction)BERT Pre-Training Results:•BERT-Base – 110M Params•BERT-Large – 340M Params\n\nBERT - Bidirectional Encoder Representations\n122MLM (Masked Language Modeling)\nHoware<MASK>doingtoday<SEP><CLS>BERTHoware<MASK>doingtoday<SEP><CLS>youthey…60%20%…Prediction head\n\nBERT - Bidirectional Encoder Representations\n123MLM (Masked Language Modeling)\n……<SEP>……<SEP><CLS>BERT……<SEP>……<SEP><CLS>is_nextnot_next95%5%Prediction head\n\nBERT - Bidirectional Encoder Representations\n124BERT Fine-Tuning:•Simply add a task-specific module after the last encoder layer to map it to the desired dimension.•Classification Tasks:•Add a feed-forward layer on top of the encoder output for the [CLS] token•Question Answering Tasks:•Train two extra vectors to mark the beginning and end of answer from paragraph•…\n\nBERT - Bidirectional Encoder Representations\n125BERT Evaluation:•General Language Understanding Evaluation (GLUE)•Sentence pair tasks•Single sentence classification•Standford Question Answering Dataset (SQuAD)\n\nBERT - Bidirectional Encoder Representations\n126BERT Evaluation:\n\n\nBERT - Bidirectional Encoder Representations\n127What is our takeaway from BERT?•Pre-training tasks can be invented flexibly…•Effective representations can be derived from a flexible regime of pre-training tasks.\n\nBERT - Bidirectional Encoder Representations\n128What is our takeaway from BERT?•Pre-training tasks can be invented flexibly…•Effective representations can be derived from a flexible regime of pre-training tasks.•Different NLP tasks seem to be highly transferable with each other...•As long as we have effective representations, that seems to form a general model which can serve as the backbone for many specialized models.\n\nBERT - Bidirectional Encoder Representations\n129What is our takeaway from BERT?•Pre-training tasks can be invented flexibly…•Effective representations can be derived from a flexible regime of pre-training tasks.•Different NLP tasks seem to be highly transferable with each other...•As long as we have effective representations, that seems to form a general model which can serve as the backbone for many specialized models.•And scaling works!!!•340M is considered large in 2018\n\n130\n2018 –The Inception of the LLM Era\nRepresentationGenerationBERTOct 2018GPTJun 2018\n\nGPT –Generative Pretrained Transformer\n131•Similarly motivated as BERT, though differently designed•Can we leverage large amounts of unlabeled data to pretrain an LM that understands general patterns?\n\nGPT –Generative Pretrained Transformer\n132GPT Pre-Training Corpus:•Similarly, BooksCorpus and English WikipediaGPT Pre-Training Tasks:•Predict the next token, given the previous tokens•More learning signals than MLMGPT Pre-Training Results:•GPT – 117M Params•Similarly competitive on GLUE and SQuAD\n\n133GPT –Generative Pretrained TransformerGPT Fine-Tuning:•Prompt-format task-specific text as a continuous stream for the model to fitSummarize this article:The summary is:SummarizationAnswer the question based on the context.Question:QAContext:\nAnswer:\n\nGPT –Generative Pretrained Transformer\n134What is our takeaway from GPT?•The Effectiveness of Self-Supervised Learning•Specifically, the model seems to be able to learn from generating the language itself, rather than from any specific task we might cook up.\n\nGPT –Generative Pretrained Transformer\n135What is our takeaway from GPT?•The Effectiveness of Self-Supervised Learning•Specifically, the model seems to be able to learn from generating the language itself, rather than from any specific task we might cook up.•Language Model as a Knowledge Base•Specifically, a generatively pretrained model seems to have a decent zero-shot performance on a range of NLP tasks.\n\nGPT –Generative Pretrained Transformer\n136What is our takeaway from GPT?•The Effectiveness of Self-Supervised Learning•Specifically, the model seems to be able to learn from generating the language itself, rather than from any specific task we might cook up.•Language Model as a Knowledge Base•Specifically, a generatively pretrained model seems to have a decent zero-shot performance on a range of NLP tasks.•And scaling works!!!\n\nPollPiazza @1291The original GPT’s parameter count is closest to…A.117B.117KC.117MD.117B\n\nPollPiazza@1291The original GPT’s parameter count is closest to…A.117B.117KC.117MD.117B\n\n139\nRepresentationGenerationBERTOct 2018GPTJun 2018The LLM Era –Paradigm Shift in Machine Learning\n\n140\nThe LLM Era –Paradigm Shift in Machine Learning\nRepresentationGenerationBERT – 2018DistilBERT – 2019RoBERTa – 2019ALBERT – 2019ELECTRA – 2020DeBERTa – 2020…GPT – 2018GPT-2 – 2019GPT-3 – 2020GPT-Neo – 2021GPT-3.5 (ChatGPT) – 2022LLaMA – 2023 GPT-4 – 2023…T5 – 2019BART – 2019mT5 – 2021…\n\n141From both BERT and GPT, we learn that…•Transformers seem to provide a new class of generalist models that are capable of capturing knowledge which is more fundamental than task-specific abilities.The LLM Era –Paradigm Shift in Machine LearningBefore LLMsSince LLMs•Feature Engineering•How do we design or select the best features for a task?\n\n142From both BERT and GPT, we learn that…•Transformers seem to provide a new class of generalist models that are capable of capturing knowledge which is more fundamental than task-specific abilities.The LLM Era –Paradigm Shift in Machine LearningBefore LLMsSince LLMs•Feature Engineering•How do we design or select the best features for a task?•Model Selection•Which model is best for which type of task?\n\n143From both BERT and GPT, we learn that…•Transformers seem to provide a new class of generalist models that are capable of capturing knowledge which is more fundamental than task-specific abilities.The LLM Era –Paradigm Shift in Machine LearningBefore LLMsSince LLMs•Feature Engineering•How do we design or select the best features for a task?•Model Selection•Which model is best for which type of task?•Transfer Learning•Given scarce labeled data, how do we transfer knowledge from other domains?\n\n144From both BERT and GPT, we learn that…•Transformers seem to provide a new class of generalist models that are capable of capturing knowledge which is more fundamental than task-specific abilities.The LLM Era –Paradigm Shift in Machine LearningBefore LLMsSince LLMs•Feature Engineering•How do we design or select the best features for a task?•Model Selection•Which model is best for which type of task?•Transfer Learning•Given scarce labeled data, how do we transfer knowledge from other domains?•Overfitting vs Generalization•How do we balance complexity and capacity to prevent overfitting while maintaining good performance?\n\n145From both BERT and GPT, we learn that…•Transformers seem to provide a new class of generalist models that are capable of capturing knowledge which is more fundamental than task-specific abilities.The LLM Era –Paradigm Shift in Machine LearningBefore LLMsSince LLMs•Feature Engineering•How do we design or select the best features for a task?•Model Selection•Which model is best for which type of task?•Transfer Learning•Given scarce labeled data, how do we transfer knowledge from other domains?•Overfitting vs Generalization•How do we balance complexity and capacity to prevent overfitting while maintaining good performance?•Pre-training and Fine-tuning•How do we leverage large scales of unlabeled data out there previously under-leveraged?\n\n146From both BERT and GPT, we learn that…•Transformers seem to provide a new class of generalist models that are capable of capturing knowledge which is more fundamental than task-specific abilities.The LLM Era –Paradigm Shift in Machine LearningBefore LLMsSince LLMs•Feature Engineering•How do we design or select the best features for a task?•Model Selection•Which model is best for which type of task?•Transfer Learning•Given scarce labeled data, how do we transfer knowledge from other domains?•Overfitting vs Generalization•How do we balance complexity and capacity to prevent overfitting while maintaining good performance?•Pre-training and Fine-tuning•How do we leverage large scales of unlabeled data out there previously under-leveraged?•Zero-shot and Few-shot learning•How can we make models perform on tasks they are not trained on?\n\n147From both BERT and GPT, we learn that…•Transformers seem to provide a new class of generalist models that are capable of capturing knowledge which is more fundamental than task-specific abilities.The LLM Era –Paradigm Shift in Machine LearningBefore LLMsSince LLMs•Feature Engineering•How do we design or select the best features for a task?•Model Selection•Which model is best for which type of task?•Transfer Learning•Given scarce labeled data, how do we transfer knowledge from other domains?•Overfitting vs Generalization•How do we balance complexity and capacity to prevent overfitting while maintaining good performance?•Pre-training and Fine-tuning•How do we leverage large scales of unlabeled data out there previously under-leveraged?•Zero-shot and Few-shot learning•How can we make models perform on tasks they are not trained on?•Prompting•How do we make models understand their task simply by describing it in natural language?\n\n148From both BERT and GPT, we learn that…•Transformers seem to provide a new class of generalist models that are capable of capturing knowledge which is more fundamental than task-specific abilities.The LLM Era –Paradigm Shift in Machine LearningBefore LLMsSince LLMs•Feature Engineering•How do we design or select the best features for a task?•Model Selection•Which model is best for which type of task?•Transfer Learning•Given scarce labeled data, how do we transfer knowledge from other domains?•Overfitting vs Generalization•How do we balance complexity and capacity to prevent overfitting while maintaining good performance?•Pre-training and Fine-tuning•How do we leverage large scales of unlabeled data out there previously under-leveraged?•Zero-shot and Few-shot learning•How can we make models perform on tasks they are not trained on?•Prompting•How do we make models understand their task simply by describing it in natural language?•Interpretability and Explainability•How can we understand the inner workings of our own models?\n\n149•What has caused this paradigm shift?The LLM Era –Paradigm Shift in Machine Learning\n\n150•What has caused this paradigm shift?•Problem in recurrent networks•Information is effectively lost during encoding of long sequences•Sequential nature disables parallel training and favors late timestep inputsThe LLM Era –Paradigm Shift in Machine Learning\n\n151•What has caused this paradigm shift?•Problem in recurrent networks•Information is effectively lost during encoding of long sequences•Sequential nature disables parallel training and favors late timestep inputs•Solution: Attention mechanism•Handling long-range dependencies•Parallel training•Dynamic attention weights based on inputsThe LLM Era –Paradigm Shift in Machine Learning\n\n152•Attention and Transformer – is this the end?The LLM Era –Paradigm Shift in Machine Learning\n\n153•Attention and Transformer – is this the end?•Problem in current Transformer-based LLMs??The LLM Era –Paradigm Shift in Machine Learning\n\nPollPiazza @1292What might be a flaw of our current Transformer-based LLMs?Freeform response\n\n155•Attention and Transformer – is this the end?•Problem in current Transformer-based LLMs??•True understanding the material vs. memorization and pattern-matching•Cannot reliably follow rules – factual hallucination e.g. inability in arithmeticThe LLM Era –Paradigm Shift in Machine Learning\n\n156•Attention and Transformer – is this the end?•Problem in current Transformer-based LLMs??•True understanding the material vs. memorization and pattern-matching•Cannot reliably follow rules – factual hallucination e.g. inability in arithmetic•Solution: ???The LLM Era –Paradigm Shift in Machine Learning",
  "metadata": {
    "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpj0tvw7t8.pdf",
    "filename": "tmpj0tvw7t8.pdf",
    "file_type": "pdf",
    "file_size": 4561380,
    "file_size_mb": 4.35,
    "file_hash": "5071b9e9ec6d6037e31cea02b8a66a7f829a51da8afb25a57d543cd2a1c3063e",
    "creation_date": "2025-03-14T22:00:21.264992",
    "modification_date": "2025-03-14T22:00:21.276294",
    "processing_date": "2025-03-14T22:00:25.090962",
    "CreationDate": "D:20250130182757",
    "Creator": "PDFium",
    "Producer": "PDFium",
    "page_count": 161,
    "page_size_sample": [
      "960.0x540.0",
      "960.0x540.0",
      "960.0x540.0",
      "960.0x540.0",
      "960.0x540.0"
    ],
    "title": "Transformers Architecture.pdf",
    "author": "Unknown",
    "description": "",
    "original_filename": "Transformers Architecture.pdf",
    "content_type": "application/pdf",
    "batch_upload": true,
    "batch_index": 2,
    "timestamp": "2025-03-14T22:00:25.090962"
  }
}