[
  {
    "chunk_id": "ef61b505a78b1a55",
    "content": "arXiv:2501.09223v1  [cs.CL]  16 Jan 2025Foundations of\nLarge Language Models\nTong Xiao and Jingbo Zhu\nJanuary 17, 2025\nNLP Lab, Northeastern University & NiuTrans Research\n\nCopyright © 2021-2025 Tong Xiao and Jingbo Zhu\nNATURAL LANGUAGE PROCESSING LAB, NORTHEASTERN UNIVERSITY\n&\nNIUTRANS RESEARCH\nLicensed under the Creative Commons Attribution-NonComme rcial 4.0 Unported License (the\n“License”). You may not use this ﬁle except in compliance wit h the License. You may ob-\ntain a copy of the License at http://creativecommons.org/licenses/by-nc/4.0 . Unless\nrequired by applicable law or agreed to in writing, software distributed under the License is dis-\ntributed on an “ AS IS ”BASIS ,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND , either\nexpress or implied. See the License for the speciﬁc language governing permissions and limita-\ntions under the License.\nJanuary 17, 2025\n\nPreface\nLarge language models originated from natural language pro cessing, but they have undoubtedly\nbecome one of the most revolutionary technological advance ments in the ﬁeld of artiﬁcial intelli-\ngence in recent years. An important insight brought by large language models is that knowledge\nof the world and languages can be acquired through large-sca le language modeling tasks, and\nin this way, we can create a universal model that handles dive rse problems. This discovery has\nprofoundly impacted the research methodologies in natural language processing and many related\ndisciplines. We have shifted from training specialized sys tems from scratch using a large amount\nof labeled data to a new paradigm of using large-scale pre-tr aining to obtain foundation models,\nwhich are then ﬁne-tuned, aligned, and prompted.\nThis book aims to outline the basic concepts of large languag e models and introduce the\nrelated techniques. As the title suggests, the book focuses more on the foundational aspects of\nlarge language models rather than providing comprehensive coverage of all cutting-edge methods.\nThe book consists of four chapters:\n• Chapter 1 introduces the basics of pre-training. This is th e foundation of large language\nmodels, and common pre-training methods and model architec tures will be discussed here.\n• Chapter 2 introduces generative models, which are the larg e language models we commonly\nrefer to today. After presenting the basic process of buildi ng these models, we will also\nexplore how to scale up model training and handle long texts.\n• Chapter 3 introduces prompting methods for large language models. We will discuss var-\nious prompting strategies, along with more advanced method s such as chain-of-thought\nreasoning and automatic prompt design.\n• Chapter 4 introduces alignment methods for large language models. This chapter focuses\non instruction ﬁne-tuning and alignment based on human feed back.\nIf readers have some background in machine learning and natu ral language processing, along\nwith a certain understanding of neural networks like Transf ormers, reading this book will be quite\neasy. However, even without this prior knowledge, it is stil l perfectly ﬁne, as we have made the\ncontent of each chapter as self-contained as possible, ensu ring that readers will not be burdened\nwith too much reading difﬁculty.\nIn writing this book, we have gradually realized that it is mo re like a compilation of \"notes\" we\nhave taken while learning about large language models. Thro ugh this note-taking writing style, we\nhope to offer readers a ﬂexible learning path. Whether they w ish to dive deep into a speciﬁc area\nor gain a comprehensive understanding of large language mod els, they will ﬁnd the knowledge\nand insights they need within these \"notes\".\nWe would like to thank the students in our laboratory and all o ur friends who have shared\nwith us their views on large language models and helped with c orrections of errors in writing. In\nparticular, we wish to thank Weiqiao Shan, Yongyu Mu, Chengl ong Wang, Kaiyan Chang, Yuchun\nFan, Hang Zhou, Xinyu Liu, Huiwen Bao, Tong Zheng, Junhao Rua n, and Qing Yang.\nii\n\nNotation\navariable\narow vector or matrix\nf(a)function ofa\nmaxf(a)maximum value of f(a)\narg maxaf(a)value ofathat maximizes f(a)\nxinput token sequence to a model\nxjinput token at position j\nyoutput token sequence produced by a model\nyioutput token at position",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 0,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "cc7fa626125b8df3",
    "content": " a comprehensive understanding of large language mod els, they will ﬁnd the knowledge\nand insights they need within these \"notes\".\nWe would like to thank the students in our laboratory and all o ur friends who have shared\nwith us their views on large language models and helped with c orrections of errors in writing. In\nparticular, we wish to thank Weiqiao Shan, Yongyu Mu, Chengl ong Wang, Kaiyan Chang, Yuchun\nFan, Hang Zhou, Xinyu Liu, Huiwen Bao, Tong Zheng, Junhao Rua n, and Qing Yang.\nii\n\nNotation\navariable\narow vector or matrix\nf(a)function ofa\nmaxf(a)maximum value of f(a)\narg maxaf(a)value ofathat maximizes f(a)\nxinput token sequence to a model\nxjinput token at position j\nyoutput token sequence produced by a model\nyioutput token at position i\nθmodel parameters\nPr(a)probability of a\nPr(a|b)conditional probability of agivenb\nPr(·|b)probability distribution of a variable given b\nPrθ(a)probability of aas parameterized by θ\nhthidden state at time step tin sequential models\nH matrix of all hidden states over time in a sequence\nQ,K,V query, key, and value matrices in attention mechanisms\nSoftmax( A)Softmax function that normalizes the input vector or matrix A\nLloss function\nD dataset used for training or ﬁne-tuning a model\n∂L\n∂θgradient of the loss function Lwith respect to the parameters θ\nKL(p||q)KL divergence between distributions pandq\niii\n\nContents\n1 Pre-training 1\n1.1 Pre-training NLP Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\n1.1.1 Unsupervised, Supervised and Self-supervised Pre-t raining . . . . . . . . 2\n1.1.2 Adapting Pre-trained Models . . . . . . . . . . . . . . . . . . . . . . . . 3\n1.2 Self-supervised Pre-training Tasks . . . . . . . . . . . . . . . . . . . . . . . . . 7\n1.2.1 Decoder-only Pre-training . . . . . . . . . . . . . . . . . . . . . . . . . 7\n1.2.2 Encoder-only Pre-training . . . . . . . . . . . . . . . . . . . . . . . . . 8\n1.2.3 Encoder-Decoder Pre-training . . . . . . . . . . . . . . . . . . . . . . . 15\n1.2.4 Comparison of Pre-training Tasks . . . . . . . . . . . . . . . . . . . . . 20\n1.3 Example: BERT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n1.3.1 The Standard Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n1.3.2 More Training and Larger Models . . . . . . . . . . . . . . . . . . . . . 27\n1.3.3 More Efﬁcient Models . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n1.3.4 Multi-lingual Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n1.4 Applying BERT Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n1.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n2 Generative Models 36\n2.1 A Brief Introduction to LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n2.1.1 Decoder-only Transformers . .",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 1,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "6131345adef703c1",
    "content": " . . . . . . . . . . . . . . 27\n1.3.4 Multi-lingual Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n1.4 Applying BERT Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n1.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n2 Generative Models 36\n2.1 A Brief Introduction to LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n2.1.1 Decoder-only Transformers . . . . . . . . . . . . . . . . . . . . . . . . 38\n2.1.2 Training LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n2.1.3 Fine-tuning LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n2.1.4 Aligning LLMs with the World . . . . . . . . . . . . . . . . . . . . . . 46\n2.1.5 Prompting LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n2.2 Training at Scale . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\n2.2.1 Data Preparation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\n2.2.2 Model Modiﬁcations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n2.2.3 Distributed Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\n2.2.4 Scaling Laws . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\n2.3 Long Sequence Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\n2.3.1 Optimization from HPC Perspectives . . . . . . . . . . . . . . . . . . . 67\n2.3.2 Efﬁcient Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . 68\n2.3.3 Cache and Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\n2.3.4 Sharing across Heads and Layers . . . . . . . . . . . . . . . . . . . . . 80\niv\n\nv\n2.3.5 Position Extrapolation and Interpolation . . . . . . . . . . . . . . . . . . 82\n2.3.6 Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\n2.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94\n3 Prompting 96\n3.1 General Prompt Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97\n3.1.1 Basics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97\n3",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 2,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "a1ea3fe3c448d4b7",
    "content": " . . . . . . . . . . . . . . . . . 82\n2.3.6 Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\n2.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94\n3 Prompting 96\n3.1 General Prompt Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97\n3.1.1 Basics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97\n3.1.2 In-context Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99\n3.1.3 Prompt Engineering Strategies . . . . . . . . . . . . . . . . . . . . . . . 101\n3.1.4 More Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106\n3.2 Advanced Prompting Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . 115\n3.2.1 Chain of Thought . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115\n3.2.2 Problem Decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . 117\n3.2.3 Self-reﬁnement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124\n3.2.4 Ensembling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130\n3.2.5 RAG and Tool Use . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134\n3.3 Learning to Prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138\n3.3.1 Prompt Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139\n3.3.2 Soft Prompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142\n3.3.3 Prompt Length Reduction . . . . . . . . . . . . . . . . . . . . . . . . . 152\n3.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153\n4 Alignment 155\n4.1 An Overview of LLM Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . 155\n4.2 Instruction Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157\n4.2.1 Supervised Fine-tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . 157\n4.2.2 Fine-tuning Data Acquisition . . . . . . . . . . . . . . . . . . . . . . . . 161\n4.2.3 Fine-tuning with Less Data . . . . . . . . . . . . . . . . . . . . . . . . . 166\n4.2.4 Instruction Generalization . . . . . . . . . . . . . . . . . . . . . . . . . 167\n4.2",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 3,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "1b428004ea344411",
    "content": " . . 155\n4.2 Instruction Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157\n4.2.1 Supervised Fine-tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . 157\n4.2.2 Fine-tuning Data Acquisition . . . . . . . . . . . . . . . . . . . . . . . . 161\n4.2.3 Fine-tuning with Less Data . . . . . . . . . . . . . . . . . . . . . . . . . 166\n4.2.4 Instruction Generalization . . . . . . . . . . . . . . . . . . . . . . . . . 167\n4.2.5 Using Weak Models to Improve Strong Models . . . . . . . . . . . . . . 169\n4.3 Human Preference Alignment: RLHF . . . . . . . . . . . . . . . . . . . . . . . 172\n4.3.1 Basics of Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . 173\n4.3.2 Training Reward Models . . . . . . . . . . . . . . . . . . . . . . . . . . 179\n4.3.3 Training LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182\n4.4 Improved Human Preference Alignment . . . . . . . . . . . . . . . . . . . . . . 187\n4.4.1 Better Reward Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . 187\n\nvi Notation\n4.4.2 Direct Preference Optimization . . . . . . . . . . . . . . . . . . . . . . 193\n4.4.3 Automatic Preference Data Generation . . . . . . . . . . . . . . . . . . 196\n4.4.4 Step-by-step Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . 198\n4.4.5 Inference-time Alignment . . . . . . . . . . . . . . . . . . . . . . . . . 200\n4.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201\nBibliography 203\n\nCHAPTER 1\nPre-training\nThe development of neural sequence models, such as Transformers [Vaswani et al. ,2017 ], along\nwith the improvements in large-scale self-supervised lear ning, has opened the door to universal\nlanguage understanding and generation. This achievement i s largely motivated by pre-training:\nwe separate common components from many neural network-bas ed systems, and then train them\non huge amounts of unlabeled data using self-supervision. T hese pre-trained models serve as\nfoundation models that can be easily adapted to different ta sks via ﬁne-tuning or prompting. As a\nresult, the paradigm of NLP has been enormously changed. In m any cases, large-scale supervised\nlearning for speciﬁc tasks is no longer required, and instea d, we only need to adapt pre-trained\nfoundation models.\nWhile pre-training has gained popularity in recent NLP rese arch, this concept dates back\ndecades to the early days of deep learning. For example, earl y attempts to pre-train deep learning\nsystems include unsupervised learning for RNNs, deep feedf orward networks, autoencoders, and\nothers [ Schmidhuber ,2015 ]. In the modern era of deep learning, we experienced a resurg ence of\npre-training, caused in part by the large-scale unsupervis ed learning of various word embedding\nmodels [ Mikolov et al. ,2013b ;Pennington et al. ,2014 ]. During the same period, pre-training also\nattracted signiﬁcant interest in computer vision, where th e backbone models were trained on rel-\natively large labeled datasets such as ImageNet, and then ap plied to",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 4,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "fb27bcb54332af80",
    "content": " and instea d, we only need to adapt pre-trained\nfoundation models.\nWhile pre-training has gained popularity in recent NLP rese arch, this concept dates back\ndecades to the early days of deep learning. For example, earl y attempts to pre-train deep learning\nsystems include unsupervised learning for RNNs, deep feedf orward networks, autoencoders, and\nothers [ Schmidhuber ,2015 ]. In the modern era of deep learning, we experienced a resurg ence of\npre-training, caused in part by the large-scale unsupervis ed learning of various word embedding\nmodels [ Mikolov et al. ,2013b ;Pennington et al. ,2014 ]. During the same period, pre-training also\nattracted signiﬁcant interest in computer vision, where th e backbone models were trained on rel-\natively large labeled datasets such as ImageNet, and then ap plied to different downstream tasks\n[He et al. ,2019 ;Zoph et al. ,2020 ]. Large-scale research on pre-training in NLP began with th e\ndevelopment of language models using self-supervised lear ning. This family of models covers\nseveral well-known examples like BERT [Devlin et al. ,2019 ] and GPT [Brown et al. ,2020 ], all\nwith a similar idea that general language understanding and generation can be achieved by train-\ning the models to predict masked words in a huge amount of text . Despite the simple nature of\nthis approach, the resulting models show remarkable capabi lity in modeling linguistic structure,\nthough they are not explicitly trained to achieve this. The g enerality of the pre-training tasks\nleads to systems that exhibit strong performance in a large v ariety of NLP problems, even outper-\nforming previously well-developed supervised systems. Mo re recently, pre-trained large language\nmodels have achieved a greater success, showing the excitin g prospects for more general artiﬁcial\nintelligence [ Bubeck et al. ,2023 ].\nThis chapter discusses the concept of pre-training in the co ntext of NLP. It begins with a gen-\neral introduction to pre-training methods and their applic ations. BERT is then used as an example\nto illustrate how a sequence model is trained via a self-supe rvised task, called masked language\nmodeling . This is followed by a discussion of methods for adapting pre -trained sequence mod-\nels for various NLP tasks. Note that in this chapter, we will f ocus primarily on the pre-training\nparadigm in NLP, and therefore, we do not intend to cover deta ils about generative large language\nmodels. A detailed discussion of these models will be left to subsequent chapters.\n1.1 Pre-training NLP Models\nThe discussion of pre-training issues in NLP typically invo lves two types of problems: sequence\nmodeling (or sequence encoding) and sequence generation. W hile these problems have different\n1\n\n2 Pre-training\nforms, for simplicity, we describe them using a single model deﬁned as follows:\no=g(x0,x1,...,x m;θ)\n=gθ(x0,x1,...,x m) (1.1)\nwhere {x0,x1,...,x m}denotes a sequence of input tokens1,x0denotes a special symbol ( ⟨s⟩or\n[CLS] ) attached to the beginning of a sequence, g(·;θ)(also written as gθ(·)) denotes a neural\nnetwork with parameters θ, and odenotes the output of the neural network. Different problem s\ncan vary based on the form of the output o. For example, in token prediction problems (as in\nlanguage modeling), ois a distribution over a vocabulary; in sequence encoding pr oblems, ois a\nrepresentation of the input sequence, often expressed as a r eal-valued vector sequence.\nThere are two fundamental issues here.\n• Optimizing θon a pre-training task. Unlike standard learning problems i n NLP, pre-training\ndoes not assume speciﬁc downstream tasks to which the model w ill be applied. Instead, the\ngoal is to train a model that can generalize across various ta sks.\n• Applying the pre-trained model gˆθ(·)to downstream tasks. To adapt the model to these\ntasks, we need to adjust the parameters ˆθslightly using labeled data or prompt the model\nwith task descriptions.\nIn this section, we discuss the basic ideas in addressing the se issues.\n1.1.1 Unsupervised, Supervised and Self-supervised Pre-t raining\nIn deep learning, pre-training refers to the process of opti mizing a neural network before it",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 5,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "ba2dd2fc52611e01",
    "content": " sequence encoding pr oblems, ois a\nrepresentation of the input sequence, often expressed as a r eal-valued vector sequence.\nThere are two fundamental issues here.\n• Optimizing θon a pre-training task. Unlike standard learning problems i n NLP, pre-training\ndoes not assume speciﬁc downstream tasks to which the model w ill be applied. Instead, the\ngoal is to train a model that can generalize across various ta sks.\n• Applying the pre-trained model gˆθ(·)to downstream tasks. To adapt the model to these\ntasks, we need to adjust the parameters ˆθslightly using labeled data or prompt the model\nwith task descriptions.\nIn this section, we discuss the basic ideas in addressing the se issues.\n1.1.1 Unsupervised, Supervised and Self-supervised Pre-t raining\nIn deep learning, pre-training refers to the process of opti mizing a neural network before it is\nfurther trained/tuned and applied to the tasks of interest. This approach is based on an assumption\nthat a model pre-trained on one task can be adapted to perform another task. As a result, we do\nnot need to train a deep, complex neural network from scratch on tasks with limited labeled data.\nInstead, we can make use of tasks where supervision signals a re easier to obtain. This reduces the\nreliance on task-speciﬁc labeled data, enabling the develo pment of more general models that are\nnot conﬁned to particular problems.\nDuring the resurgence of neural networks through deep learn ing, many early attempts to\nachieve pre-training were focused on unsupervised learning . In these methods, the parame-\nters of a neural network are optimized using a criterion that is not directly related to speciﬁc tasks.\nFor example, we can minimize the reconstruction cross-entr opy of the input vector for each layer\n[Bengio et al. ,2006 ]. Unsupervised pre-training is commonly employed as a prel iminary step\nbefore supervised learning, offering several advantages, such as aiding in the discovery of better\nlocal minima and adding a regularization effect to the train ing process [ Erhan et al. ,2010 ]. These\nbeneﬁts make the subsequent supervised learning phase easi er and more stable.\nA second approach to pre-training is to pre-train a neural ne twork on supervised learning\ntasks. For example, consider a sequence model designed to en code input sequences into some\n1Here we assume that tokens are basic units of text that are sep arated through tokenization. Sometimes, we will use\nthe terms token andword interchangeably, though they have closely related but slig htly different meanings in NLP.\n\n1.1 Pre-training NLP Models 3\nrepresentations. In pre-training, this model is combined w ith a classiﬁcation layer to form a clas-\nsiﬁcation system. This system is then trained on a pre-train ing task, such as classifying sentences\nbased on sentiment (e.g., determining if a sentence conveys a positive or negative sentiment).\nThen, we adapt the sequence model to a downstream task. We bui ld a new classiﬁcation system\nbased on this pre-trained sequence model and a new classiﬁca tion layer (e.g., determining if a\nsequence is subjective or objective). Typically, we need to ﬁne-tune the parameters of the new\nmodel using task-speciﬁc labeled data, ensuring the model i s optimally adjusted to perform well\non this new type of data. The ﬁne-tuned model is then employed to classify new sequences for\nthis task. An advantage of supervised pre-training is that t he training process, either in the pre-\ntraining or ﬁne-tuning phase, is straightforward, as it fol lows the well-studied general paradigm\nof supervised learning in machine learning. However, as the complexity of the neural network\nincreases, the demand for more labeled data also grows. This , in turn, makes the pre-training task\nmore difﬁcult, especially when large-scale labeled data is not available.\nA third approach to pre-training is self-supervised learning . In this approach, a neural net-\nwork is trained using the supervision signals generated by i tself, rather than those provided by\nhumans. This is generally done by constructing its own train ing tasks directly from unlabeled\ndata, such as having the system create pseudo labels. While s elf-supervised learning has recently\nemerged as a very popular method in NLP, it is not a new concept . In machine learning, a related\nconcept is self-training where a model is iteratively improved by learning from the ps eudo labels\nassigned to a",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 6,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "32a86a1974b0b564",
    "content": "uning phase, is straightforward, as it fol lows the well-studied general paradigm\nof supervised learning in machine learning. However, as the complexity of the neural network\nincreases, the demand for more labeled data also grows. This , in turn, makes the pre-training task\nmore difﬁcult, especially when large-scale labeled data is not available.\nA third approach to pre-training is self-supervised learning . In this approach, a neural net-\nwork is trained using the supervision signals generated by i tself, rather than those provided by\nhumans. This is generally done by constructing its own train ing tasks directly from unlabeled\ndata, such as having the system create pseudo labels. While s elf-supervised learning has recently\nemerged as a very popular method in NLP, it is not a new concept . In machine learning, a related\nconcept is self-training where a model is iteratively improved by learning from the ps eudo labels\nassigned to a dataset. To do this, we need some seed data to bui ld an initial model. This model\nthen generates pseudo labels for unlabeled data, and these p seudo labels are subsequently used to\niteratively reﬁne and bootstrap the model itself. Such a met hod has been successfully used in sev-\neral NLP areas, such as word sense disambiguation [ Yarowsky ,1995 ] and document classiﬁcation\n[Blum and Mitchell ,1998 ]. Unlike the standard self-training method, self-supervi sed pre-training\nin NLP does not rely on an initial model for annotating the dat a. Instead, all the supervision sig-\nnals are created from the text, and the entire model is traine d from scratch. A well-known example\nof this is training sequence models by successively predict ing a masked word given its preceding\nor surrounding words in a text. This enables large-scale sel f-supervised learning for deep neural\nnetworks, leading to the success of pre-training in many und erstanding, writing, and reasoning\ntasks.\nFigure 1.1shows a comparison of the above three pre-training approach es. Self-supervised\npre-training is so successful that most current state-of-t he-art NLP models are based on this\nparadigm. Therefore, in this chapter and throughout this bo ok, we will focus on self-supervised\npre-training. We will show how sequence models are pre-trai ned via self-supervision and how the\npre-trained models are applied.\n1.1.2 Adapting Pre-trained Models\nAs mentioned above, two major types of models are widely used in NLP pre-training.\n•Sequence Encoding Models . Given a sequence of words or tokens, a sequence encoding\nmodel represents this sequence as either a real-valued vect or or a sequence of vectors, and\nobtains a representation of the sequence. This representat ion is typically used as input to\nanother model, such as a sentence classiﬁcation system.\n\n4 Pre-training\nUnsupervised SupervisedPre-training Training\nUnlabeled\nDataLabeled\nData\n(a) Unsupervised Pre-trainingSupervised SupervisedPre-training Tuning\nLabeled\nData\nTask 1Labeled\nData\nTask 2\n(b) Supervised Pre-trainingSelf-\nSupervisedSupervisedZero/Few\nShot Learning\nPre-training TuningPrompting\nUnlabeled\nDataLabeled\nData\n(c) Self-supervised Pre-training\nFig. 1.1: Illustration of unsupervised, supervised, and self-super vised pre-training. In unsupervised pre-training, the\npre-training is performed on large-scale unlabeled data. I t can be viewed as a preliminary step to have a good starting\npoint for the subsequent optimization process, though cons iderable effort is still required to further train the model\nwith labeled data after pre-training. In supervised pre-tr aining, the underlying assumption is that different (super vised)\nlearning tasks are related. So we can ﬁrst train the model on o ne task, and transfer the resulting model to another task\nwith some training or tuning effort. In self-supervised pre -training, a model is pre-trained on large-scale unlabeled data\nvia self-supervision. The model can be well trained in this w ay, and we can efﬁciently adapt it to new tasks through\nﬁne-tuning or prompting.\n•Sequence Generation Models . In NLP, sequence generation generally refers to the prob-\nlem of generating a sequence of tokens based on a given contex t. The term context has\ndifferent meanings across applications. For example, it re fers to the preceding tokens in\nlanguage modeling, and refers to the source-language seque nce in machine translation2.\nWe need",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 7,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "3199f5ba718e35da",
    "content": " further train the model\nwith labeled data after pre-training. In supervised pre-tr aining, the underlying assumption is that different (super vised)\nlearning tasks are related. So we can ﬁrst train the model on o ne task, and transfer the resulting model to another task\nwith some training or tuning effort. In self-supervised pre -training, a model is pre-trained on large-scale unlabeled data\nvia self-supervision. The model can be well trained in this w ay, and we can efﬁciently adapt it to new tasks through\nﬁne-tuning or prompting.\n•Sequence Generation Models . In NLP, sequence generation generally refers to the prob-\nlem of generating a sequence of tokens based on a given contex t. The term context has\ndifferent meanings across applications. For example, it re fers to the preceding tokens in\nlanguage modeling, and refers to the source-language seque nce in machine translation2.\nWe need different techniques for applying these models to do wnstream tasks after pre-training.\nHere we are interested in the following two methods.\n1.1.2.1 Fine-tuning of Pre-trained Models\nFor sequence encoding pre-training, a common method of adap ting pre-trained models is ﬁne-\ntuning. Let Encode θ(·)denote an encoder with parameters θ, for example, Encode θ(·)can be a\nstandard Transformer encoder. Provided we have pre-traine d this model in some way and obtained\nthe optimal parameters ˆθ, we can employ it to model any sequence and generate the corre sponding\nrepresentation, like this\nH= Encode ˆθ(x) (1.2)\nwhere xis the input sequence {x0,x1,...,x m}, and His the output representation which is a\nsequence of real-valued vectors {h0,h1,...,hm}. Because the encoder does not work as a stan-\ndalone NLP system, it is often integrated as a component into a bigger system. Consider, for\nexample, a text classiﬁcation problem in which we identify t he polarity (i.e., positive, negative,\n2More precisely, in auto-regressive decoding of machine tra nslation, each target-language token is generated based\non both its preceding tokens and source-language sequence.\n\n1.1 Pre-training NLP Models 5\nand neutral) of a given text. We can build a text classiﬁcatio n system by stacking a classiﬁer\non top of the encoder. Let Classifyω(·)be a neural network with parameters ω. Then, the text\nclassiﬁcation model can be expressed in the form\nPrω,ˆθ(·|x) = Classifyω(H)\n= Classifyω(Encode ˆθ(x)) (1.3)\nHere Prω,ˆθ(·|x)is a probability distribution over the label set {positive,negative,neutral }, and\nthe label with the highest probability in this distribution is selected as output. To keep the notation\nuncluttered, we will use Fω,ˆθ(·)to denote Classifyω(Encode ˆθ(·)).\nBecause the model parameters ωandˆθare not optimized for the classiﬁcation task, we cannot\ndirectly use this model. Instead, we must use a modiﬁed versi on of the model that is adapted to\nthe task. A typical way is to ﬁne-tune the model by giving expl icit labeling in downstream tasks.\nWe can train Fω,ˆθ(·)on a labeled dataset, treating it as a common supervised lear ning task. The\noutcome of the ﬁne-tuning is the parameters ˜ωand˜θthat are further optimized. Alternatively,\nwe can freeze the encoder parameters ˆθto maintain their pre-trained state, and focus solely on\noptimizingω. This allows the classiﬁer to be efﬁciently adapted to work i n tandem with the\npre-trained encoder.\nOnce we have obtained a ﬁne-tuned model, we can use it to class ify a new text. For example,\nsuppose we have a comment posted on a travel website:\nI love the food here. It’s amazing!\nWe ﬁrst tokenize this text into tokens3, and then feed the token sequence xnewinto the ﬁne-tuned\nmodelF˜ω,˜θ(·). The model generates a distribution over classes by\nF˜ω,˜θ(xnew) =[\nPr(positive |xnew) Pr(negative |xnew) Pr(neutral |xnew)]\n(1.4",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 8,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "d2158412c1965a80",
    "content": "and˜θthat are further optimized. Alternatively,\nwe can freeze the encoder parameters ˆθto maintain their pre-trained state, and focus solely on\noptimizingω. This allows the classiﬁer to be efﬁciently adapted to work i n tandem with the\npre-trained encoder.\nOnce we have obtained a ﬁne-tuned model, we can use it to class ify a new text. For example,\nsuppose we have a comment posted on a travel website:\nI love the food here. It’s amazing!\nWe ﬁrst tokenize this text into tokens3, and then feed the token sequence xnewinto the ﬁne-tuned\nmodelF˜ω,˜θ(·). The model generates a distribution over classes by\nF˜ω,˜θ(xnew) =[\nPr(positive |xnew) Pr(negative |xnew) Pr(neutral |xnew)]\n(1.4)\nAnd we select the label of the entry with the maximum value as o utput. In this example it is\npositive .\nIn general, the amount of labeled data used in ﬁne-tuning is s mall compared to that of the\npre-training data, and so ﬁne-tuning is less computational ly expensive. This makes the adaption\nof pre-trained models very efﬁcient in practice: given a pre -trained model and a downstream task,\nwe just need to collect some labeled data, and slightly adjus t the model parameters on this data. A\nmore detailed discussion of ﬁne-tuning can be found in Secti on1.4.\n1.1.2.2 Prompting of Pre-trained Models\nUnlike sequence encoding models, sequence generation mode ls are often employed independently\nto address language generation problems, such as question a nswering and machine translation,\nwithout the need for additional modules. It is therefore str aightforward to ﬁne-tune these models\n3The text can be tokenized in many different ways. One of the si mplest is to segment the text into English words\nand punctuations {I,love,the,food,here,.,It,’s,amazing,!}\n\n6 Pre-training\nas complete systems on downstream tasks. For example, we can ﬁne-tune a pre-trained encoder-\ndecoder multilingual model on some bilingual data to improv e its performance on a speciﬁc trans-\nlation task.\nAmong various sequence generation models, a notable exampl e is the large language models\ntrained on very large amounts of data. These language models are trained to simply predict the next\ntoken given its preceding tokens. Although token predictio n is such a simple task that it has long\nbeen restricted to “language modeling” only, it has been fou nd to enable the learning of the general\nknowledge of languages by repeating the task a large number o f times. The result is that the\npre-trained large language models exhibit remarkably good abilities in token prediction, making\nit possible to transform numerous NLP problems into simple t ext generation problems through\nprompting the large language models. For example, we can fra me the above text classiﬁcation\nproblem as a text generation task\nI love the food here. It’s amazing! I’m\nHere indicates the word or phrase we want to predict (call it the completion ). If the predicted\nword is happy , orglad, orsatisﬁed or a related positive word, we can classify the text as positive .\nThis example shows a simple prompting method in which we conc atenate the input text with I’m\nto form a prompt. Then, the completion helps decide which lab el is assigned to the original text.\nGiven the strong performance of language understanding and generation of large language\nmodels, a prompt can instruct the models to perform more comp lex tasks. Here is a prompt where\nwe prompt the LLM to perform polarity classiﬁcation with an i nstruction.\nAssume that the polarity of a text is a label chosen from {posi tive, negative,\nneutral}. Identify the polarity of the input.\nInput : I love the food here. It’s amazing!\nPolarity :\nThe ﬁrst two sentences are a description of the task. Input andPolarity are indicators of the input\nand output, respectively. We expect the model to complete th e text and at the same time give the\ncorrect polarity label. By using instruction-based prompt s, we can adapt large language models to\nsolve NLP problems without the need for additional training .\nThis example also demonstrates the zero-shot learning capa bility of large language models,\nwhich can perform tasks that were not observed during the tra ining phase. Another method for\nenabling new capabilities in a neural network is few-shot le arning. This is typically achieved\nthrough in",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 9,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "3f0d41f01c59038a",
    "content": " prompt where\nwe prompt the LLM to perform polarity classiﬁcation with an i nstruction.\nAssume that the polarity of a text is a label chosen from {posi tive, negative,\nneutral}. Identify the polarity of the input.\nInput : I love the food here. It’s amazing!\nPolarity :\nThe ﬁrst two sentences are a description of the task. Input andPolarity are indicators of the input\nand output, respectively. We expect the model to complete th e text and at the same time give the\ncorrect polarity label. By using instruction-based prompt s, we can adapt large language models to\nsolve NLP problems without the need for additional training .\nThis example also demonstrates the zero-shot learning capa bility of large language models,\nwhich can perform tasks that were not observed during the tra ining phase. Another method for\nenabling new capabilities in a neural network is few-shot le arning. This is typically achieved\nthrough in-context learning (ICT). More speciﬁcally, we add some samples that demonstrate ho w\nan input corresponds to an output. These samples, known as demonstrations , are used to teach\nlarge language models how to perform the task. Below is an exa mple involving demonstrations\n\n1.2 Self-supervised Pre-training Tasks 7\nAssume that the polarity of a text is a label chosen from {posi tive, negative,\nneutral}. Identify the polarity of the input.\nInput : The trafﬁc is terrible during rush hours, making it difﬁcul t to reach the\nairport on time.\nPolarity : Negative\nInput : The weather here is wonderful.\nPolarity : Positive\nInput : I love the food here. It’s amazing!\nPolarity :\nPrompting and in-context learning play important roles in t he recent rise of large language\nmodels. We will discuss these issues more deeply in Chapter 3 . However, it is worth noting\nthat while prompting is a powerful way to adapt large languag e models, some tuning efforts are\nstill needed to ensure the models can follow instructions ac curately. Additionally, the ﬁne-tuning\nprocess is crucial for aligning the values of these models wi th human values. More detailed\ndiscussions of ﬁne-tuning can be found in Chapter 4.\n1.2 Self-supervised Pre-training Tasks\nIn this section, we consider self-supervised pre-training approaches for different neural architec-\ntures, including decoder-only, encoder-only, and encoder -decoder architectures. We restrict our\ndiscussion to Transformers since they form the basis of most pre-trained models in NLP. How-\never, pre-training is a broad concept, and so we just give a br ief introduction to basic approaches\nin order to make this section concise.\n1.2.1 Decoder-only Pre-training\nThe decoder-only architecture has been widely used in devel oping language models [ Radford et al. ,\n2018 ]. For example, we can use a Transformer decoder as a language model by simply removing\ncross-attention sub-layers from it. Such a model predicts t he distribution of tokens at a position\ngiven its preceding tokens, and the output is the token with t he maximum probability. The stan-\ndard way to train this model, as in the language modeling prob lem, is to minimize a loss function\nover a collection of token sequences. Let Decoder θ(·)denote a decoder with parameters θ. At\neach position i, the decoder generates a distribution of the next tokens bas ed on its preceding\ntokens {x0,...,x i}, denoted by Prθ(·|x0,...,x i)(orpθ\ni+1for short). Suppose we have the gold-\nstandard distribution at the same position, denoted by pgold\ni+1. For language modeling, we can think\nofpgold\ni+1as a one-hot representation of the correct predicted word. W e then deﬁne a loss function\nL(pθ\ni+1,pgold\ni+1)to measure the difference between the model prediction and t he true prediction. In\nNLP, the log-scale cross-entropy loss is typically used.\nGiven a sequence of mtokens {x0,...,x m}, the loss on this sequence is the sum of the loss\n\n8 Pre-training\nover the positions {0,...,m −1}, given by\nLoss θ(x0,...,x m) =m−1∑\ni=0L(pθ\ni+1,pgold\ni+1)\n=m−1∑\ni=0LogCrossEntropy( pθ\ni+1,pgold\ni+1) (1.5)\nwhere LogCrossEntropy( ·)is the log-scale cross-entropy",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 10,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "a20f015684f8400f",
    "content": "\nofpgold\ni+1as a one-hot representation of the correct predicted word. W e then deﬁne a loss function\nL(pθ\ni+1,pgold\ni+1)to measure the difference between the model prediction and t he true prediction. In\nNLP, the log-scale cross-entropy loss is typically used.\nGiven a sequence of mtokens {x0,...,x m}, the loss on this sequence is the sum of the loss\n\n8 Pre-training\nover the positions {0,...,m −1}, given by\nLoss θ(x0,...,x m) =m−1∑\ni=0L(pθ\ni+1,pgold\ni+1)\n=m−1∑\ni=0LogCrossEntropy( pθ\ni+1,pgold\ni+1) (1.5)\nwhere LogCrossEntropy( ·)is the log-scale cross-entropy, and pgold\ni+1is the one-hot representation\nofxi+1.\nThis loss function can be extended to a set of sequences D. In this case, the objective of\npre-training is to ﬁnd the best parameters that minimize the loss on D\nˆθ= arg min\nθ∑\nx∈DLoss θ(x) (1.6)\nNote that this objective is mathematically equivalent to ma ximum likelihood estimation, and can\nbe re-expressed as\nˆθ= arg max\nθ∑\nx∈Dlog Pr θ(x)\n= arg max\nθ∑\nx∈Di−1∑\ni=0log Pr θ(xi+1|x0,...,x i) (1.7)\nWith these optimized parameters ˆθ, we can use the pre-trained language model Decoder ˆθ(·)\nto compute the probability Prˆθ(xi+1|x0,...,x i)at each position of a given sequence.\n1.2.2 Encoder-only Pre-training\nAs deﬁned in Section 1.1.2.1 , an encoder Encoder θ(·)is a function that reads a sequence of\ntokens x=x0...xmand produces a sequence of vectors H=h0...hm4. Training this model is\nnot straightforward, as we do not have gold-standard data fo r measuring how good the output of\nthe real-valued function is. A typical approach to encoder p re-training is to combine the encoder\nwith some output layers to receive supervision signals that are easier to obtain. Figure 1.2shows\na common architecture for pre-training Transformer encode rs, where we add a Softmax layer on\ntop of the Transformer encoder. Clearly, this architecture is the same as that of the decoder-based\nlanguage model, and the output is a sequence of probability d istributions\n\npW,θ\n1...\npW,θ\nm\n= Softmax W(Encoder θ(x)) (1.9)\n4If we view hias a row vector, Hcan be written as\nH=\nh0\n...\nhm\n (1.8)\n\n1.2 Self-supervised Pre-training Tasks 9\nx0x1x2x3x4\n(masked)e0 e1 e2 e3 e4EncoderSoftmaxmodel reconstructs the masked tokenE.g., evaluate how well theSelf-supervision\n(a) Pre-trainingx0x1x2x3x4e0 e1 e2 e3 e4Pre-trained EncoderPrediction NetworkOutput for Downstream Tasks\n(b) Applying the Pre-trained Encoder\nFig. 1.2: Pre-training a Transformer encoder (left) and then applyin g the pre-trained encoder (right). In the pre-training\nphase, the encoder, together with a Softmax layer, is traine d via self-supervision. In the application phase, the Softm ax\nlayer is removed, and the pre-trained encoder is combined wi th a prediction network to address speciﬁc problems. In\ngeneral, for better adaptation to these tasks, the system is ﬁne-tuned using labeled data.\nHere pW,θ\ni is the output distribution Pr(·|x)at positioni. We use Softmax W(·)to denote that\nthe Softmax layer is parameterized by W, that is, Softmax W(H) = Softmax( H·W). For\nnotation simplicity, we will sometimes drop the superscrip tsWandθafﬁxed to each probability\ndistribution.\n",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 11,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "090549e959bd8380",
    "content": " Encoder\nFig. 1.2: Pre-training a Transformer encoder (left) and then applyin g the pre-trained encoder (right). In the pre-training\nphase, the encoder, together with a Softmax layer, is traine d via self-supervision. In the application phase, the Softm ax\nlayer is removed, and the pre-trained encoder is combined wi th a prediction network to address speciﬁc problems. In\ngeneral, for better adaptation to these tasks, the system is ﬁne-tuned using labeled data.\nHere pW,θ\ni is the output distribution Pr(·|x)at positioni. We use Softmax W(·)to denote that\nthe Softmax layer is parameterized by W, that is, Softmax W(H) = Softmax( H·W). For\nnotation simplicity, we will sometimes drop the superscrip tsWandθafﬁxed to each probability\ndistribution.\nThe difference between this model and standard language mod els is that the output pihas\ndifferent meanings in encoder pre-training and language mo deling. In language modeling, piis\nthe probability distribution of predicting the next word. T his follows an auto-regressive decoding\nprocess: a language model only observes the words up to posit ioniand predicts the next. By\ncontrast, in encoder pre-training, the entire sequence can be observed at once, and so it makes no\nsense to predict any of the tokens in this sequence.\n1.2.2.1 Masked Language Modeling\nOne of the most popular methods of encoder pre-training is masked language modeling , which\nforms the basis of the well-known BERT model [ Devlin et al. ,2019 ]. The idea of masked lan-\nguage modeling is to create prediction challenges by maskin g out some of the tokens in the input\nsequence and training a model to predict the masked tokens. I n this sense, the conventional lan-\nguage modeling problem, which is sometimes called causal language modeling , is a special case\nof masked language modeling: at each position, we mask the to kens in the right-context, and pre-\ndict the token at this position using its left context. Howev er, in causal language modeling we\nonly make use of the left-context in word prediction, while t he prediction may depend on tokens\nin the right-context. By contrast, in masked language model ing, all the unmasked tokens are used\nfor word prediction, leading to a bidirectional model that m akes predictions based on both left and\nright-contexts.\n\n10 Pre-training\nMore formally, for an input sequence x=x0...xm, suppose that we mask the tokens at po-\nsitions A(x) = {i1,...,i u}. Hence we obtain a masked token sequence ¯xwhere the token at\neach position in A(x)is replaced with a special symbol [MASK] . For example, for the following\nsequence\nThe early bird catches the worm\nwe may have a masked token sequence like this\nThe[MASK] bird catches the [MASK]\nwhere we mask the tokens early andworm (i.e.,i1= 2andi2= 6).\nNow we have two sequences xand¯x. The model is then optimized so that we can correctly\npredict xbased on ¯x. This can be thought of as an autoencoding-like process, and the train-\ning objective is to maximize the reconstruction probabilit yPr(x|¯x). Note that there is a simple\nposition-wise alignment between xand¯x. Because an unmasked token in ¯xis the same as the to-\nken in xat the same position, there is no need to consider the predict ion for this unmasked token.\nThis leads to a simpliﬁed training objective which only maxi mizes the probabilities for masked\ntokens. We can express this objective in a maximum likelihoo d estimation fashion\n(ˆW,ˆθ) = arg max\nW,θ∑\nx∈D∑\ni∈A(x)log PrW,θ\ni(xi|¯x) (1.10)\nor alternatively express it using the cross-entropy loss\n(ˆW,ˆθ) = arg min\nW,θ∑\nx∈D∑\ni∈A(x)LogCrossEntropy( pW,θ\ni,pgold\ni) (1.11)\nwhere PrW,θ\nk(xk|¯x)is the probability of the true token xkat positionkgiven the corrupted input\nx, and pW,θ\nkis the probability distribution at position kgiven the corrupted input x. To illustrate,\nconsider the above example where two tokens of the sequence “ the early bird catches the",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 12,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "3e58c62a4c9cf281",
    "content": ". We can express this objective in a maximum likelihoo d estimation fashion\n(ˆW,ˆθ) = arg max\nW,θ∑\nx∈D∑\ni∈A(x)log PrW,θ\ni(xi|¯x) (1.10)\nor alternatively express it using the cross-entropy loss\n(ˆW,ˆθ) = arg min\nW,θ∑\nx∈D∑\ni∈A(x)LogCrossEntropy( pW,θ\ni,pgold\ni) (1.11)\nwhere PrW,θ\nk(xk|¯x)is the probability of the true token xkat positionkgiven the corrupted input\nx, and pW,θ\nkis the probability distribution at position kgiven the corrupted input x. To illustrate,\nconsider the above example where two tokens of the sequence “ the early bird catches the worm ”\nare masked. For this example, the objective is to maximize th e sum of log-scale probabilities\nLoss = log Pr( x2=early|¯x= [CLS] The[MASK]\n¯x2bird catches the [MASK]\n¯x6) +\nlog Pr(x6=worm |¯x= [CLS] The[MASK]\n¯x2bird catches the [MASK]\n¯x6)(1.12)\nOnce we obtain the optimized parametersˆWandˆθ, we can dropˆW. Then, we can further\nﬁne-tune the pre-trained encoder Encoder ˆθ(·)or directly apply it to downstream tasks.\n1.2.2.2 Permuted Language Modeling\nWhile masked language modeling is simple and widely applied , it introduces new issues. One\ndrawback is the use of a special token, [MASK] , which is employed only during training but not\n\n1.2 Self-supervised Pre-training Tasks 11\nat test time. This leads to a discrepancy between training an d inference. Moreover, the auto-\nencoding process overlooks the dependencies between maske d tokens. For example, in the above\nexample, the prediction of x2(i.e., the ﬁrst masked token) is made independently of x6(i.e., the\nsecond masked token), though x6should be considered in the context of x2.\nThese issues can be addressed using the permuted language modeling approach to pre-\ntraining [ Yang et al. ,2019 ]. Similar to causal language modeling, permuted language m odeling\ninvolves making sequential predictions of tokens. However , unlike causal modeling where predic-\ntions follow the natural sequence of the text (like left-to- right or right-to-left), permuted language\nmodeling allows for predictions in any order. The approach i s straightforward: we determine an\norder for token predictions and then train the model in a stan dard language modeling manner, as\ndescribed in Section 1.2.1 . Note that in this approach, the actual order of tokens in the text remains\nunchanged, and only the order in which we predict these token s differs from standard language\nmodeling. For example, consider a sequence of 5 tokens x0x1x2x3x4. Let eirepresent the em-\nbedding ofxi(i.e., combination of the token embedding and positional em bedding). In standard\nlanguage modeling, we would generate this sequence in the or der ofx0→x1→x2→x3→x4.\nThe probability of the sequence can be modeled via a generati on process.\nPr(x) = Pr(x0)·Pr(x1|x0)·Pr(x2|x0,x1)·Pr(x3|x0,x1,x2)·\nPr(x4|x0,x1,x2,x3)\n= Pr(x0)·Pr(x1|e0)·Pr(x2|e0,e1)·Pr(x3|e0,e1,e2)·\nPr(x4|e0,e1,e2,e3) (1.13)\nNow, let us consider a different order for token prediction: x0→x4→x2→x1→x3. The\nsequence generation process can then be expressed as follow s:\nPr(x) = Pr(x0)·Pr(x4|e0)·Pr(x2|e0,e4)·Pr(x1|e0,e4,e2)·\nPr(x3|e0,e4,e2,e1) (",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 13,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "eb8e194a96339fc6",
    "content": " process.\nPr(x) = Pr(x0)·Pr(x1|x0)·Pr(x2|x0,x1)·Pr(x3|x0,x1,x2)·\nPr(x4|x0,x1,x2,x3)\n= Pr(x0)·Pr(x1|e0)·Pr(x2|e0,e1)·Pr(x3|e0,e1,e2)·\nPr(x4|e0,e1,e2,e3) (1.13)\nNow, let us consider a different order for token prediction: x0→x4→x2→x1→x3. The\nsequence generation process can then be expressed as follow s:\nPr(x) = Pr(x0)·Pr(x4|e0)·Pr(x2|e0,e4)·Pr(x1|e0,e4,e2)·\nPr(x3|e0,e4,e2,e1) (1.14)\nThis new prediction order allows for the generation of some t okens to be conditioned on a\nbroader context, rather than being limited to just the prece ding tokens as in standard language\nmodels. For example, in generating x3, the model considers both its left-context (i.e., e0,e1,e2)\nand right-context (i.e., e4). The embeddings e0,e1,e2,e4incorporate the positional information\nofx0,x1,x2,x4, preserving the original order of the tokens. As a result, th is approach is somewhat\nakin to masked language modeling: we mask out x3and use its surrounding tokens x0,x1,x2,x4\nto predict this token.\nThe implementation of permuted language models is relative ly easy for Transformers. Be-\ncause the self-attention model is insensitive to the order o f inputs, we do not need to explicitly\nreorder the sequence to have a factorization like Eq. ( 1.14). Instead, permutation can be done\nby setting appropriate masks for self-attention. For examp le, consider the case of computing\nPr(x1|e0,e4,e2). We can place x0,x1,x2,x3,x4in order and block the attention from x3tox1\nin self-attention, as illustrated below\n\n12 Pre-training\nx0x1x2x3x4\nMasks for Self-attention:\nBlue box = valid attention\nGray box = blocked attention\nFor a more illustrative example, we compare the self-attent ion masking results of causal language\nmodeling, masked language modeling and permuted language m odeling in Figure 1.3.\n1.2.2.3 Pre-training Encoders as Classiﬁers\nAnother commonly-used idea to train an encoder is to conside r classiﬁcation tasks. In self-\nsupervised learning, this is typically done by creating new classiﬁcation challenges from the unla-\nbeled text. There are many different ways to design the class iﬁcation tasks. Here we present two\npopular tasks.\nA simple method, called next sentence prediction (NSP ), is presented in BERT’s original\npaper [ Devlin et al. ,2019 ]. The assumption of NSP is that a good text encoder should cap ture\nthe relationship between two sentences. To model such a rela tionship, in NSP we can use the\noutput of encoding two consecutive sentences Sent AandSent Bto determine whether Sent Bis\nthe next sentence following Sent A. For example, suppose Sent A=’It is raining . ’ andSent B=\n’I need an umbrella . ’ . The input sequence of the encoder could be\n[CLS] It is raining . [SEP] I need an umbrella . [SEP]\nwhere [CLS] is the start symbol (i.e., x0) which is commonly used in encoder pre-training, and\n[SEP] is a separator that separates the two sentences. The process ing of this sequence follows a\nstandard procedure of Transformer encoding: we ﬁrst repres ent each token xias its corresponding\nembedding ei, and then feed the embedding sequence {e0,...,em}into the encoder to obtain the\noutput sequence {h0,...,hm}. Since h0is generally considered as the representation of the entire\nsequence, we add a Softmax layer on top of it to construct a bin ary classiﬁcation system. This\nprocess is illustrated as follows\ntoken: [CLS] It is raining . [SEP] I need an umbrella . [SEP]\nembedding: e0e1e2 e3e4e5e6e7e8 e9 e10e11\n↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 14,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "4f156212141e52b7",
    "content": "CLS] is the start symbol (i.e., x0) which is commonly used in encoder pre-training, and\n[SEP] is a separator that separates the two sentences. The process ing of this sequence follows a\nstandard procedure of Transformer encoding: we ﬁrst repres ent each token xias its corresponding\nembedding ei, and then feed the embedding sequence {e0,...,em}into the encoder to obtain the\noutput sequence {h0,...,hm}. Since h0is generally considered as the representation of the entire\nsequence, we add a Softmax layer on top of it to construct a bin ary classiﬁcation system. This\nprocess is illustrated as follows\ntoken: [CLS] It is raining . [SEP] I need an umbrella . [SEP]\nembedding: e0e1e2 e3e4e5e6e7e8 e9 e10e11\n↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓\nEncoder\n↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓\nencoding: h0h1h2 h3h4h5h6h7h8 h9 h10h11\n↓\nSoftmax\n↓\nIs Next or Not?\n\n1.2 Self-supervised Pre-training Tasks 13\nx0\nx0x1\nx1x2\nx2x3\nx3x4\nx4Pr(x0) = 1\nPr(x1|e0)\nPr(x2|e0,e1)\nPr(x3|e0,e1,e2)\nPr(x4|e0,e1,e2,e3)\n(a) Causal Language Modeling (order: x0→x1→x2→x3→x4)\nx0\nx0x1\nx1x2\nx2x3\nx3x4\nx4masked masked\nmaskedmasked1\nPr(x1|e0,emask,e2,emask,e4)\n1\nPr(x3|e0,emask,e2,emask,e4)\n1\n(b) Masked Language Modeling (order: x0,[MASK],x2,[MASK],x4→x1,x3)\nx0\nx0x1\nx1x2\nx2x3\nx3x4\nx4Pr(x0) = 1\nPr(x1|e0,e4,e2)\nPr(x2|e0,e4)\nPr(x3|e0,e4,e2,e1)\nPr(x4|e0)\n(c) Permuted Language Modeling (order: x0→x4→x2→x1→x3)\nFig. 1.3: Comparison of self-attention masking results of causal lan guage modeling, masked language modeling and\npermuted language modeling. The gray cell denotes the token at positionjdoes not attend to the token at position i.\nThe blue cell (i,j)denotes that the token at position jattends to the token at position i.emask represents the embedding\nof the symbol [MASK] , which is a combination of the token embedding and the positi onal embedding.\nIn order to generate training samples, we need two sentences each time, one for Sent Aand\nthe other for Sent B. A simple way to do this is to utilize the natural sequence of t wo consecu-\ntive sentences in the text. For example, we obtain a positive sample by using actual consecutive\nsentences, and a negative sample by using randomly sampled s entences. Consequently, training\nthis model is the same as training a classiﬁer. Typically, NS P is used as an additional training loss\n\n14 Pre-training\nfunction for pre-training based on masked language modelin g.\nA second example of training Transformer encoders as classi ﬁers is to apply classiﬁcation-\nbased supervision signals to each output of an encoder. For e xample, Clark et al. [2019 ] in their\nELECTRA model, propose training a Transformer encoder to id entify whether each input token\nis identical to the original input or has been altered in some manner. The ﬁrst step of this method\nis to generate a new sequence from a given sequence of tokens, where some of the tokens are\naltered. To do this, a small masked language model (call it th e generator) is applied: we randomly\nmask some of the tokens, and train this model to predict the ma sked tokens. For each training\nsample, this masked language model outputs a token at each ma sked position, which might be\ndifferent from the original token. At the same time, we train another Transformer encoder (call it\nthe discriminator) to determine whether each predicted tok en is the same as the original",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 15,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "f96e7e20358bc860",
    "content": " is to apply classiﬁcation-\nbased supervision signals to each output of an encoder. For e xample, Clark et al. [2019 ] in their\nELECTRA model, propose training a Transformer encoder to id entify whether each input token\nis identical to the original input or has been altered in some manner. The ﬁrst step of this method\nis to generate a new sequence from a given sequence of tokens, where some of the tokens are\naltered. To do this, a small masked language model (call it th e generator) is applied: we randomly\nmask some of the tokens, and train this model to predict the ma sked tokens. For each training\nsample, this masked language model outputs a token at each ma sked position, which might be\ndifferent from the original token. At the same time, we train another Transformer encoder (call it\nthe discriminator) to determine whether each predicted tok en is the same as the original token or\naltered. More speciﬁcally, we use the generator to generate a sequence where some of the tokens\nare replaced. Below is an illustration.\noriginal: [CLS] The boy spent hours working on toys .\n↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓\nmasked: [CLS] The boy spent [MASK] working on [MASK] .\n↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓\nGenerator (small masked language model)\n↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓\nreplaced: [CLS] The boy spent decades working on toys .\nThen, we use the discriminator to label each of these tokens a sorginal orreplaced , as follows\nreplaced: [CLS] The boy spent decades working on toys .\n↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓\nDiscriminator (the model we want)\n↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓\nlabel: original original original original replaced original original original original\nFor training, the generator is optimized as a masked languag e model with maximum likelihood\nestimation, and the discriminator is optimized as a classiﬁ er using a classiﬁcation-based loss. In\nELECTRA, the maximum likelihood-based loss and the classiﬁ cation-based loss are combined for\njointly training both the generator and discriminator. An a lternative approach is to use generative\nadversarial networks (GANs), that is, the generator is trai ned to fool the discriminator, and the dis-\ncriminator is trained to distinguish the output of the gener ator from the true distribution. However,\nGAN-style training complicates the training task and is mor e difﬁcult to scale up. Nevertheless,\nonce training is complete, the generator is discarded, and t he encoding part of the discriminator is\napplied as the pre-trained model for downstream tasks.\n\n1.2 Self-supervised Pre-training Tasks 15\n1.2.3 Encoder-Decoder Pre-training\nIn NLP, encoder-decoder architectures are often used to mod el sequence-to-sequence problems,\nsuch as machine translation and question answering. In addi tion to these typical sequence-to-\nsequence problems in NLP, encoder-decoder models can be ext ended to deal with many other\nproblems. A simple idea is to consider text as both the input a nd output of a problem, and so\nwe can directly apply encoder-decoder models. For example, given a text, we can ask a model to\noutput a text describing the sentiment of the input text, suc h aspositive ,negative , and neutral .\nSuch an idea allows us to develop a single text-to-text syste m to address any NLP problem.\nWe can formulate different problems into the same text-to-t ext format. We ﬁrst train an encoder-\ndecoder model to gain general-purpose knowledge of languag e via self-supervision. This model\nis then ﬁne-tuned for speciﬁc downstream tasks using target ed text-to-text data.\n1.2.3.1 Masked Encoder-Decoder Pre-training\nInRaffel et al. [2020 ]’sT5model, many different tasks are framed as the same text-to-t ext task.\nEach sample in T5 follows the format\nSource Text →Target Text\nHere →separates the source text, which consists of a task descript ion or instruction and the input\ngiven to the system, from the target text, which is the respon se to the input task. As an example,\nconsider a task of translating from Chinese to English. A tra ining sample can be expressed as\n[CLS] Translate from Chinese to English: 你好！→ ⟨s⟩Hello!\nwhere [CLS] and⟨s⟩are the start symbols on the source and target sides, respect ively5.\nLikewise, we can express other tasks in",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 16,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "73580214e20aa8e6",
    "content": "-tuned for speciﬁc downstream tasks using target ed text-to-text data.\n1.2.3.1 Masked Encoder-Decoder Pre-training\nInRaffel et al. [2020 ]’sT5model, many different tasks are framed as the same text-to-t ext task.\nEach sample in T5 follows the format\nSource Text →Target Text\nHere →separates the source text, which consists of a task descript ion or instruction and the input\ngiven to the system, from the target text, which is the respon se to the input task. As an example,\nconsider a task of translating from Chinese to English. A tra ining sample can be expressed as\n[CLS] Translate from Chinese to English: 你好！→ ⟨s⟩Hello!\nwhere [CLS] and⟨s⟩are the start symbols on the source and target sides, respect ively5.\nLikewise, we can express other tasks in the same way. For exam ple\n[CLS] Answer: when was Albert Einstein born?\n→ ⟨s⟩He was born on March 14, 1879.\n[CLS] Simplify: the professor, who has has published numerous pap ers in his ﬁeld,\nwill be giving a lecture on the topic next week.\n→ ⟨s⟩The experienced professor will give a lecture next week.\n[CLS] Score the translation from English to Chinese. English: whe n in Rome, do as\nthe Romans do. Chinese: 人在罗马就像罗马人一样做事。\n→ ⟨s⟩0.81\nwhere instructions are highlighted in gray. An interesting case is that in the last example we\n5We could use the same start symbol for different sequences. H ere we use different symbols to distinguish the\nsequences on the encoder and decoder-sides.\n\n16 Pre-training\nreframe the scoring problem as the text generation problem. Our goal is to generate a text repre-\nsenting the number 0.81, rather than outputting it as a numerical value.\nThe approach described above provides a new framework of uni versal language understanding\nand generation. Both the task instructions and the problem i nputs are provided to the system\nin text form. The system then follows the instructions to com plete the task. This method puts\ndifferent problems together, with the beneﬁt of training a s ingle model that can perform many\ntasks simultaneously.\nIn general, ﬁne-tuning is necessary for adapting the pre-tr ained model to a speciﬁc downstream\ntask. In this process, one can use different ways to instruct the model for the task, such as using a\nshort name of the task as the preﬁx to the actual input sequenc e or providing a detailed description\nof the task. Since the task instructions are expressed in tex t form and involved as part of the input,\nthe general knowledge of instruction can be gained through l earning the language understanding\nmodels in the pre-training phase. This may help enable zero- shot learning. For example, pre-\ntrained models can generalize to address new problems where the task instructions have never\nbeen encountered.\nThere have been several powerful methods of self-supervise d learning for either Transformer\nencoders or decoders. Applying these methods to pre-train e ncoder-decoder models is relatively\nstraightforward. One common choice is to train encoder-dec oder models as language models. For\nexample, the encoder receives a sequence preﬁx, while the de coder generates the remaining se-\nquence. However, this differs from standard causal languag e modeling, where the entire sequence\nis autoregressively generated from the ﬁrst token. In our ca se, the encoder processes the preﬁx at\nonce, and then the decoder predicts subsequent tokens in the manner of causal language modeling.\nPut more precisely, this is a preﬁx language modeling problem: a language model predicts the\nsubsequent sequence given a preﬁx, which serves as the conte xt for prediction.\nConsider the following example\n[CLS] The puppies are frolicking  \nPreﬁx→ ⟨s⟩outside the house .\nSubsequent Sequence\nWe can directly train an encoder-decoder model using exampl es like this. Then, the encoder learns\nto understand the preﬁx, and the decoder learns to continue w riting based on this understanding.\nFor large-scale pre-training, it is easy to create a large nu mber of training examples from unlabeled\ntext.\nIt is worth noting that for pre-trained encoder-decoder mod els to be effective in multi-lingual\nand cross-lingual tasks",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 17,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "2eb69ab047d1cc6f",
    "content": " of causal language modeling.\nPut more precisely, this is a preﬁx language modeling problem: a language model predicts the\nsubsequent sequence given a preﬁx, which serves as the conte xt for prediction.\nConsider the following example\n[CLS] The puppies are frolicking  \nPreﬁx→ ⟨s⟩outside the house .\nSubsequent Sequence\nWe can directly train an encoder-decoder model using exampl es like this. Then, the encoder learns\nto understand the preﬁx, and the decoder learns to continue w riting based on this understanding.\nFor large-scale pre-training, it is easy to create a large nu mber of training examples from unlabeled\ntext.\nIt is worth noting that for pre-trained encoder-decoder mod els to be effective in multi-lingual\nand cross-lingual tasks, such as machine translation, they should be trained with multi-lingual\ndata. This typically requires that the vocabulary includes tokens from all the languages. By\ndoing so, the models can learn shared representations acros s different languages, thereby enabling\ncapabilities in both language understanding and generatio n in a multi-lingual and cross-lingual\ncontext.\nA second approach to pre-training encoder-decoder models i s masked language modeling. In\nthis approach, as discussed in Section 1.2.2 , tokens in a sequence are randomly replaced with a\nmask symbol, and the model is then trained to predict these ma sked tokens based on the entire\nmasked sequence.\nAs an illustration, consider the task of masking and reconst ructing the sentence\n\n1.2 Self-supervised Pre-training Tasks 17\nThe puppies are frolicking outside the house .\nBy masking two tokens (say, frolicking andthe), we have the BERT-style input and output of the\nmodel, as follows\n[CLS] The puppies are [MASK] outside [MASK] house .\n→ ⟨s⟩ frolicking the\nHere denotes the masked position at which we do not make token pred ictions. By varying the\npercentage of the tokens in the text, this approach can be gen eralized towards either BERT-style\ntraining or language modeling-style training [ Song et al. ,2019 ]. For example, if we mask out all\nthe tokens, then the model is trained to generate the entire s equence\n[CLS] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MAS K]\n→ ⟨s⟩The puppies are frolicking outside the house .\nIn this case, we train the decoder as a language model.\nNote that, in the context of the encoder-decoder architectu re, we can use the encoder to read\nthe masked sequence, and use the decoder to predict the origi nal sequence. With this objective,\nwe essentially have a denoising autoencoder: the encoder tr ansforms a corrupted input into some\nhidden representation, and the decoder reconstructs the un corrupted input from this hidden repre-\nsentation. Here is an example of input and output for denoisi ng training.\n[CLS] The puppies are [MASK] outside [MASK] house .\n→ ⟨s⟩The puppies are frolicking outside the house .\nBy learning to map from this corrupted sequence to its uncorr upted counterpart, the model gains\nthe ability to understand on the encoder side and to generate on the decoder side. See Figure 1.4\nfor an illustration of how an encoder-decoder model is train ed with BERT-style and denoising\nautoencoding objectives.\nAs we randomly select tokens for masking, we can certainly ma sk consecutive tokens [ Joshi et al. ,\n2020 ]. Here is an example.\n[CLS] The puppies are [MASK] outside [MASK] [MASK] .\n→ ⟨s⟩The puppies are frolicking outside the house .\nAnother way to consider consecutive masked tokens is to repr esent them as spans. Here we\nfollow Raffel et al. [2020 ]’s work, and use [X],[Y]and[Z]to denote sentinel tokens that cover\none or more consecutive masked tokens. Using this notation, we can re-express the above training\nexample as\n[CLS] The puppies are [X] outside [Y] .\n→ ⟨s⟩[X] frolicking [Y] the house [Z]\n\n18 Pre-training\n[CLS] Thepuppies are [M] in[M] house .Encoder Decoder\nfrolicking[M] [M] [M] ⟨s⟩ [M] the [M][M] [M] [M] frolicking [M] the [M] [M",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 18,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "fb4aeef72478a66d",
    "content": "CLS] The puppies are [MASK] outside [MASK] [MASK] .\n→ ⟨s⟩The puppies are frolicking outside the house .\nAnother way to consider consecutive masked tokens is to repr esent them as spans. Here we\nfollow Raffel et al. [2020 ]’s work, and use [X],[Y]and[Z]to denote sentinel tokens that cover\none or more consecutive masked tokens. Using this notation, we can re-express the above training\nexample as\n[CLS] The puppies are [X] outside [Y] .\n→ ⟨s⟩[X] frolicking [Y] the house [Z]\n\n18 Pre-training\n[CLS] Thepuppies are [M] in[M] house .Encoder Decoder\nfrolicking[M] [M] [M] ⟨s⟩ [M] the [M][M] [M] [M] frolicking [M] the [M] [M]Loss\n(a) Training an encoder-decoder model with BERT-style mask ed language modeling\n[CLS] Thepuppies are [M] in[M] house .Encoder Decoder\nfrolickingare puppies The ⟨s⟩ in the houseThe puppies arefrolicking in the house .\n(b) Training an encoder-decoder model with denoising autoe ncodingLoss over the sequence\nFig. 1.4: Training an encoder-decoder model using BERT-style and den oising autoencoding methods. In both methods,\nthe input to the encoder is a corrupted token sequence where s ome tokens are masked and replaced with [MASK] (or\n[M]for short). The decoder predicts these masked tokens, but in different ways. In BERT-style training, the decoder\nonly needs to compute the loss for the masked tokens, while th e remaining tokens in the sequence can be simply treated\nas[MASK] tokens. In denoising autoencoding, the decoder predicts th e sequence of all tokens in an autoregressive\nmanner. As a result, the loss is obtained by accumulating the losses of all these tokens, as in standard language\nmodeling.\nThe idea is that we represent the corrupted sequence as a sequ ence containing placeholder\nslots. The training task is to ﬁll these slots with the correc t tokens using the surrounding context.\nAn advantage of this approach is that the sequences used in tr aining would be shorter, making the\ntraining more efﬁcient. Note that masked language modeling provides a very general framework\nfor training encoder-decoder models. Various settings can be adjusted to have different training\nversions, such as altering the percentage of tokens masked a nd the maximum length of the masked\nspans.\n1.2.3.2 Denoising Training\nIf we view the problem of training encoder-decoder models as a problem of training denoising\nautoencoders, there will typically be many different metho ds for introducing input corruption and\nreconstructing the input. For instance, beyond randomly ma sking tokens, we can also alter some\nof them or rearrange their order.\nSuppose we have an encoder-decoder model that can map an inpu t sequence xto an output\n\n1.2 Self-supervised Pre-training Tasks 19\nsequence y\ny= Decode ω(Encode θ(x))\n= Model θ,ω(x) (1.15)\nwhereθandωare the parameters of the encoder and the decoder, respectiv ely. In denoising\nautoencoding problems, we add some noise to xto obtain a noisy, corrupted input xnoise. By\nfeeding xnoise into the encoder, we wish the decoder to output the original i nput. The training\nobjective can be deﬁned as\n(ˆθ,ˆω) = arg max\nθ,ωLoss(Model θ,ω(xnoise),x) (1.16)\nHere the loss function Loss(Model θ,ω(xnoise),x)evaluates how well the model Model θ,ω(xnoise)\nreconstructs the original input x. We can choose the cross-entropy loss as usual.\nAs the model architecture and the training approach have bee n developed, the remaining issue\nis the corruption of the input. Lewis et al. [2020 ], in their BART model, propose corrupting the\ninput sequence in several different ways.\n•Token Masking . This is the same masking method that we used in masked langua ge mod-\neling. The tokens in the input sequence are randomly selecte d and masked.\n•Token Deletion . This method is similar to token masking. However, rather th an replacing\nthe selected tokens with a special symbol [MASK] , these tokens are removed from the\nsequence. See the following example for a comparison of the t oken",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 19,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "438d30e66acb4155",
    "content": "�,ω(xnoise),x) (1.16)\nHere the loss function Loss(Model θ,ω(xnoise),x)evaluates how well the model Model θ,ω(xnoise)\nreconstructs the original input x. We can choose the cross-entropy loss as usual.\nAs the model architecture and the training approach have bee n developed, the remaining issue\nis the corruption of the input. Lewis et al. [2020 ], in their BART model, propose corrupting the\ninput sequence in several different ways.\n•Token Masking . This is the same masking method that we used in masked langua ge mod-\neling. The tokens in the input sequence are randomly selecte d and masked.\n•Token Deletion . This method is similar to token masking. However, rather th an replacing\nthe selected tokens with a special symbol [MASK] , these tokens are removed from the\nsequence. See the following example for a comparison of the t oken masking and token\ndeletion methods.\nOriginal ( x): The puppies are frolicking outside the house .\nToken Masking ( xnoise): The puppies are [MASK] outside [MASK] house .\nToken Deletion ( xnoise): The puppies are frolicking outside thehouse .\nwhere the underlined tokens in the original sequence are mas ked or deleted.\n•Span Masking . Non-overlapping spans are randomly sampled over the seque nce. Each\nspan is masked by [MASK] . We also consider spans of length 0, and, in such cases, [MASK]\nis simply inserted at a position in the sequence. For example , we can use span masking to\ncorrupt the above sequence as\nOriginal ( x): The 0 puppies are frolicking outside the house .\nSpan Masking ( xnoise): The [MASK] puppies are [MASK] house .\nHere the span frolicking outside the is replaced with a single [MASK] .0indicates a length-\n0 span, and so we insert an [MASK] between Theandpuppies . Span masking introduces\nnew prediction challenges in which the model needs to know ho w many tokens are gener-\nated from a span. This problem is very similar to fertility mo deling in machine translation\n[Brown et al. ,1993 ].\n\n20 Pre-training\nIf we consider a sequence consisting of multiple sentences, additional methods of corruption\ncan be applied. In the BART model, there are two such methods.\n•Sentence Reordering . This method randomly permutes the sentences so that the mod el can\nlearn to reorder sentences in a document. Consider, for exam ple, two consecutive sentences\nHard work leads to success . Success brings happiness .\nWe can reorder the two sentences to have a corrupted input seq uence\nSuccess brings happiness . Hard work leads to success .\n•Document Rotation . The goal of this task is to identify the start token of the seq uence.\nFirst, a token is randomly selected from the sequence. Then, the sequence is rotated so that\nthe selected token is the ﬁrst token. For example, suppose we select the token leads from\nthe above sequence. The rotated sequence is\nleads to success . Success brings happiness . Hard work Hard workselected\nwhere the subsequence Hard work before leads is appended to the end of the sequence.\nFor pre-training, we can apply multiple corruption methods to learn robust models, for ex-\nample, we randomly choose one of them for each training sampl e. In practice, the outcome of\nencoder-decoder pre-training depends heavily on the input corruption methods used, and so we\ntypically need to choose appropriate training objectives t hrough careful experimentation.\n1.2.4 Comparison of Pre-training Tasks\nSo far, we have discussed a number of pre-training tasks. Sin ce the same training objective can\napply to different architectures (e.g., using masked langu age modeling for both encoder-only and\nencoder-decoder pre-training), categorizing pre-traini ng tasks based solely on model architecture\ndoes not seem ideal. Instead, we summarize these tasks based on the training objectives.\n•Language Modeling . Typically, this approach refers to an auto-regressive gen eration pro-\ncedure of sequences. At one time, it predicts the next token b ased on its previous context.\n•Masked Language Modeling . Masked Language Modeling belongs to a general mask-\npredict framework. It randomly masks tokens in a sequence an d predicts these tokens using\nthe entire masked sequence.\n\n1.3 Example: BERT 21\n•Permuted Language Modeling . Permuted language modeling follows a similar idea to\nmasked language modeling, but considers the order of (maske d) token prediction. It reorders\nthe input sequence and predicts the tokens sequentially. Ea ch prediction is based on some\ncontext tokens that are randomly selected.\n•Discriminative Training . In discriminative training,",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 20,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "c1985545e5453f47",
    "content": " and\nencoder-decoder pre-training), categorizing pre-traini ng tasks based solely on model architecture\ndoes not seem ideal. Instead, we summarize these tasks based on the training objectives.\n•Language Modeling . Typically, this approach refers to an auto-regressive gen eration pro-\ncedure of sequences. At one time, it predicts the next token b ased on its previous context.\n•Masked Language Modeling . Masked Language Modeling belongs to a general mask-\npredict framework. It randomly masks tokens in a sequence an d predicts these tokens using\nthe entire masked sequence.\n\n1.3 Example: BERT 21\n•Permuted Language Modeling . Permuted language modeling follows a similar idea to\nmasked language modeling, but considers the order of (maske d) token prediction. It reorders\nthe input sequence and predicts the tokens sequentially. Ea ch prediction is based on some\ncontext tokens that are randomly selected.\n•Discriminative Training . In discriminative training, supervision signals are crea ted from\nclassiﬁcation tasks. Models for pre-training are integrat ed into classiﬁers and trained to-\ngether with the remaining parts of the classiﬁers to enhance their classiﬁcation performance.\n•Denoising Autoencoding . This approach is applied to the pre-training of encoder-de coder\nmodels. The input is a corrupted sequence and the encoder-de coder models are trained to\nreconstruct the original sequence.\nTable 1.1illustrates these methods and their variants using example s. The use of these ex-\namples does not distinguish between models, but we mark the m odel architectures where the\npre-training tasks can be applied. In each example, the inpu t consists of a token sequence, and the\noutput is either a token sequence or some probabilities. For generation tasks, such as language\nmodeling, superscripts are used to indicate the generation order on the target side. If the super-\nscripts are omitted, it indicates that the output sequence c an be generated either autoregressively\nor simultaneously. On the source side, we assume that the seq uence undergoes a standard Trans-\nformer encoding process, meaning that each token can see the entire sequence in self-attention.\nThe only exception is in permuted language modeling, where a n autoregressive generation pro-\ncess is implemented by setting attention masks on the encode r side. To simplify the discussion,\nwe remove the token ⟨s⟩from the target-side of each example.\nWhile these pre-training tasks are different, it is possibl e to compare them in the same frame-\nwork and experimental setup [ Dong et al. ,2019 ;Raffel et al. ,2020 ;Lewis et al. ,2020 ]. Note that\nwe cannot list all the pre-training tasks here as there are ma ny of them. For more discussions on\npre-training tasks, the interested reader may refer to some surveys on this topic [ Qiu et al. ,2020 ;\nHan et al. ,2021 ].\n1.3 Example: BERT\nIn this section, we introduce BERT models, which are among th e most popular and widely used\npre-trained sequence encoding models in NLP.\n1.3.1 The Standard Model\nThe standard BERT model, which is proposed in Devlin et al. [2019 ]’s work, is a Transformer\nencoder trained using both masked language modeling and nex t sentence prediction tasks. The\nloss used in training this model is a sum of the loss of the two t asks.\nLoss BERT = Loss MLM + Loss NSP (1.17)\nAs is regular in training deep neural networks, we optimize t he model parameters by minimizing\nthis loss. To do this, a number of training samples are collec ted. During training, a batch of\n\n22 Pre-training\nMethod Enc Dec E-D Input Output\nCausal LM • • The1kitten2is3chasing4the5ball6.7\nPreﬁx LM • •[C]The kitten is chasing1the2ball3.4\nMasked LM • •[C]The kitten [M]chasing the [M]. is ball\nMASS-style • •[C]The kitten [M] [M] [M] ball . is chasing the\nBERT-style • •[C]The kitten [M]playing the[M]. kitten is chasing ball\nPermuted LM • [C]The kitten is chasing the ball . The5kitten7is6chasing1the4ball2.3\nNext Sentence • [C]The kitten is chasing the ball . Pr(IsNext |representation-of-[C])\nPrediction Birds eat worms .\nSentence • Encode a sentence as haand Score( ha,hb)\nComparison another sentence as hb\nToken Classiﬁcation • [C",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 21,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "eb5e9c0a4ce6ba65",
    "content": "\nCausal LM • • The1kitten2is3chasing4the5ball6.7\nPreﬁx LM • •[C]The kitten is chasing1the2ball3.4\nMasked LM • •[C]The kitten [M]chasing the [M]. is ball\nMASS-style • •[C]The kitten [M] [M] [M] ball . is chasing the\nBERT-style • •[C]The kitten [M]playing the[M]. kitten is chasing ball\nPermuted LM • [C]The kitten is chasing the ball . The5kitten7is6chasing1the4ball2.3\nNext Sentence • [C]The kitten is chasing the ball . Pr(IsNext |representation-of-[C])\nPrediction Birds eat worms .\nSentence • Encode a sentence as haand Score( ha,hb)\nComparison another sentence as hb\nToken Classiﬁcation • [C]The kitten is chasing the ball . Pr(·|The) Pr( ·|kitten) ...Pr(·|.)\nToken Reordering •[C]. kitten the chasing The is ball The1kitten2is3chasing4the5ball6.7\nToken Deletion •[C]The kitten is chasing the ball . The1kitten2is3chasing4the5ball6.7\nSpan Masking •[C]The kitten [M]is[M]. The1kitten2is3chasing4the5ball6.7\nSentinel Masking •[C]The kitten [X]the[Y] [X]1is2chasing3[Y]4ball5.6\nSentence •[C]The ball rolls away swiftly . The The1kitten2is3chasing4the5ball6.7\nReordering kitten is chasing the ball . The8ball9rolls10away11swiftly12.13\nDocument •[C]chasing the ball . The ball rolls The1kitten2is3chasing4the5ball6.7\nRotation away swiftly . The kitten is The8ball9rolls10away11swiftly12.13\nTable 1.1: Comparison of pre-training tasks, including language modeling ,masked language modeling ,permuted\nlanguage modeling ,discriminative training , and denoising autoencoding .[C] = [CLS] ,[M] = [MASK] ,[X],[Y] =\nsentinel tokens . Enc, Dec and E-D indicate whether the approach can be applie d to encoder-only, decoder-only,\nencoder-decoder models, respectively. For generation tas ks, superscripts are used to represent the order of the token s.\ntraining samples is randomly selected from this collection at a time, and Loss BERT is accumulated\nover these training samples. Then, the model parameters are updated via gradient descent or its\nvariants. This process is repeated many times until some sto pping criterion is satisﬁed, such as\nwhen the training loss converges.\n1.3.1.1 Loss Functions\nIn general, BERT models are used to represent a single senten ce or a pair of sentences, and thus\ncan handle various downstream language understanding prob lems. In this section we assume that\nthe input representation is a sequence containing two sente ncesSent AandSent B, expressed as\n[CLS] Sent A[SEP] Sent B[SEP]\nHere we follow the notation in BERT’s paper and use [SEP] to denote the separator.\nGiven this sequence, we can obtain LossMLM andLossNSP separately. For masked language\nmodeling, we predict a subset of the tokens in the sequence. T ypically, a certain percentage of the\n\n1.3 Example: BERT 23\ntokens are randomly selected, for example, in the standard B ERT model, 15% of the tokens in\neach sequence are selected. Then the sequence is modiﬁed in t hree ways\n•Token Masking .80% of the selected tokens are masked and replaced with the symbo l\n[MASK] . For example\nOriginal: [CLS] It is raining . [SEP] I need an umbrella . [SEP]\nMasked: [CLS] It is [MASK] . [SEP] I need [MASK] umbrella . [SE P]\nwhere the selected tokens are underlined. Predicting maske d tokens makes the model learn\nto represent tokens from their surrounding context.\n•Random Replacement .10% of the selected tokens are changed to a random token. For\nexample,\nOriginal: [CLS] It is raining . [SEP] I need an umbrella . [SEP]\nRandom Token: [CLS] It is raining . [SEP] I need an hat. [SEP]\nThis",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 22,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "7eb8742abaffc00e",
    "content": " in the standard B ERT model, 15% of the tokens in\neach sequence are selected. Then the sequence is modiﬁed in t hree ways\n•Token Masking .80% of the selected tokens are masked and replaced with the symbo l\n[MASK] . For example\nOriginal: [CLS] It is raining . [SEP] I need an umbrella . [SEP]\nMasked: [CLS] It is [MASK] . [SEP] I need [MASK] umbrella . [SE P]\nwhere the selected tokens are underlined. Predicting maske d tokens makes the model learn\nto represent tokens from their surrounding context.\n•Random Replacement .10% of the selected tokens are changed to a random token. For\nexample,\nOriginal: [CLS] It is raining . [SEP] I need an umbrella . [SEP]\nRandom Token: [CLS] It is raining . [SEP] I need an hat. [SEP]\nThis helps the model learn to recover a token from a noisy inpu t.\n•Unchanged .10% of the selected tokens are kept unchanged. For example,\nOriginal: [CLS] It is raining . [SEP] I need an umbrella . [SEP]\nUnchanged Token: [CLS] It is raining . [SEP] Ineed an umbrella . [SEP]\nThis is not a difﬁcult prediction task, but can guide the mode l to use easier evidence for\nprediction.\nLetA(x)be the set of selected positions of a given token sequence x, and ¯xbe the modiﬁed\nsequence of x. The loss function of masked language modeling can be deﬁned as\nLoss MLM =−∑\ni∈A(x)log Pr i(xi|¯x) (1.18)\nwhere Pri(xi|¯x)is the probability of predicting xiat the position igiven ¯x. Figure 1.5shows a\nrunning example of computing Loss MLM .\nFor next sentence prediction, we follow the method describe d in Section 1.2.2.3 . Each training\nsample is classiﬁed into a label set {IsNext,NotNext }, for example,\nSequence: [CLS] It is raining . [SEP] I need an umbrella . [SEP ]\nLabel: IsNext\nSequence: [CLS] The cat sleeps on the windowsill . [SEP] Appl es grow on trees . [SEP]\nLabel: NotNext\n\n24 Pre-training\n[CLS] It is raining . [SEP] I need an umbrella . [SEP] Input:\nSelect tokens with a probability of 15%\n[CLS] It is raining .[SEP] Ineed an umbrella .[SEP] Token Selection:\nMask selected tokens with a probability of 80%\n[CLS] It is [MASK] .[SEP] Ineed [MASK] umbrella .[SEP] Token Masking:\nAlter selected tokens with a probability of 10%\n[CLS] It is [MASK] .[SEP] Ineed [MASK] hat.[SEP] Token:\nReplacement\nKeep selected tokens unchanged with a probability of 10%\n[CLS] It is [MASK] .[SEP] Ineed [MASK] hat . [SEP] Unchanged:\nTrain the Transformer encoder with the modiﬁed sequence\n[CLS] It is[MASK] .[SEP] I need [MASK] hat .[SEP]e0 e1 e2 e3 e4 e5 e6 e7 e8 e9 e10 e11h0 h1 h2 h3 h4 h5 h6 h7 h8 h9 h10 h11training I an umbrella\nTransformer Encoder\nFig. 1.5: A running example of BERT-style masked language modeling. F irst,15% tokens are randomly selected.\nThese selected tokens are then processed in one of three ways : replaced with a [MASK] token ( 80% of the time),\nreplaced with a random token ( 10% of the time), or kept unchanged ( 10% of the time). The model is trained to predict\nthese selected tokens based on the modiﬁed sequence. eirepresents the embedding of the token at the position i. Gray\nboxes represent the Softmax layers.\nThe output vector of the encoder for the ﬁrst token [CLS] is viewed as the sequence representation,\ndenoted by hcls(orh0). A classiﬁer is built on top of hcls. Then, we can compute the probability of\na labelcgiven hcls, i.e., Pr(c|hcls). There are many loss functions one can choose for classiﬁcat ion\nproblems. For example, in maximum likelihood training, we c an deﬁne Loss NSPas\nLoss NSP =−log Pr(c",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 23,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "24c36330668b9748",
    "content": " three ways : replaced with a [MASK] token ( 80% of the time),\nreplaced with a random token ( 10% of the time), or kept unchanged ( 10% of the time). The model is trained to predict\nthese selected tokens based on the modiﬁed sequence. eirepresents the embedding of the token at the position i. Gray\nboxes represent the Softmax layers.\nThe output vector of the encoder for the ﬁrst token [CLS] is viewed as the sequence representation,\ndenoted by hcls(orh0). A classiﬁer is built on top of hcls. Then, we can compute the probability of\na labelcgiven hcls, i.e., Pr(c|hcls). There are many loss functions one can choose for classiﬁcat ion\nproblems. For example, in maximum likelihood training, we c an deﬁne Loss NSPas\nLoss NSP =−log Pr(cgold|hcls) (1.19)\n\n1.3 Example: BERT 25\nwherecgoldis the correct label for this sample.\n1.3.1.2 Model Setup\nAs shown in Figure 1.6, BERT models are based on the standard Transformer encoder a rchitecture.\nThe input is a sequence of embeddings, each being the sum of th e token embedding, the positional\nembedding, and the segment embedding.\ne=x+epos+eseg (1.20)\nBoth the token embedding ( x) and positional embedding ( epos) are regular, as in Transformer\nmodels. The segment embedding ( eseg) is a new type of embedding that indicates whether a token\nbelongs to Sent AorSent B. This can be illustrated by the following example.\nToken [CLS] It is raining . [SEP] I need an umbrella . [SEP]\nxx0 x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 x11\neposPE(0) PE(1) PE(2) PE(3) PE(4) PE(5) PE(6) PE(7) PE(8) PE(9) P E(10) PE(11)\neseg eA eA eA eA eA eA eB eB eB eB eB eB\nThe main part of BERT models is a multi-layer Transformer net work. A Transformer layer\nconsists of a self-attention sub-layer and an FFN sub-layer . Both of them follow the post-norm\narchitecture: output = LNorm( F(input) + input) , whereF(·)is the core function of the sub-\nlayer (either a self-attention model or an FFN), and LNorm( ·)is the layer normalization unit.\nTypically, a number of Transformer layers are stacked to for m a deep network. At each position of\nthe sequence, the output representation is a real-valued ve ctor which is produced by the last layer\nof the network.\nThere are several aspects one may consider in developing BER T models.\n•Vocabulary Size (|V|). In Transformers, each input token is represented as an ent ry in a\nvocabularyV. Large vocabularies can cover more surface form variants of words, but may\nlead to increased storage requirements.\n•Embedding Size (de). Every token is represented as a de-dimensional real-valued vector.\nAs presented above, this vector is the sum of the token embedd ing, positional embedding,\nand segment embedding, all of which are also de-dimensional real-valued vectors.\n•Hidden Size (d). The input and output of a sub-layer are of ddimensions. Besides, most\nof the hidden states of a sub-layer are d-dimensional vectors. In general, dcan be roughly\nviewed as the width of the network.\n•Number of Heads (nhead). In self-attention sub-layers, one needs to specify the nu mber\nof heads used in multi-head self-attention. The larger this number is, the more sub-spaces\nattention is performed. In practical systems, we often set nhead≥4.\n•FFN Hidden Size (dﬀn). The size of the hidden layer of the FFNs used in Transformer s is\ntypically larger than d. For example, a typical setting is dﬀn= 4d. For larger Transformers,\nsuch as recent large models, dﬀnmay be set to a very large value.\n\n26 Pre-training\nSelf-attention\nPosition Token Segment\nx0x1...xmLayer NormalizationFFNLayer NormalizationOutput Layer...\nh0h1...hm\nlayers\nInput\nxicorresponds to an entry of VEmbedding\ne=x+epos+eseg∈",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 24,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "6a5dc012a782e8fa",
    "content": "•Number of Heads (nhead). In self-attention sub-layers, one needs to specify the nu mber\nof heads used in multi-head self-attention. The larger this number is, the more sub-spaces\nattention is performed. In practical systems, we often set nhead≥4.\n•FFN Hidden Size (dﬀn). The size of the hidden layer of the FFNs used in Transformer s is\ntypically larger than d. For example, a typical setting is dﬀn= 4d. For larger Transformers,\nsuch as recent large models, dﬀnmay be set to a very large value.\n\n26 Pre-training\nSelf-attention\nPosition Token Segment\nx0x1...xmLayer NormalizationFFNLayer NormalizationOutput Layer...\nh0h1...hm\nlayers\nInput\nxicorresponds to an entry of VEmbedding\ne=x+epos+eseg∈RdeSelf-attention Sub-layer\nhidden size: d\nnumber of heads: nheadFFN Sub-layer\nhidden size: d\nFFN hidden size: dﬀnEncoder Output\nhi∈Rdis the contextual\nrepresentation of xi\nFig. 1.6: The model Architecture of BERT (Transformer encoder). The i nput tokens are ﬁrst represented as embed-\ndings, each of which is the sum of the corresponding token emb edding, positional embedding and segment embedding.\nThen, the embedding sequence is processed by a stack of Trans former layers. Each layer in this stack includes a self-\nattention sub-layer and a FFN sub-layer. The output of the BE RT model is a sequence of vectors produced by the ﬁnal\nTransformer layer.\n•Model Depth (L). Using deep networks is an effective way to improve the expr essive power\nof Transformers. For BERT models, Lis typically set to 12or24. However, networks with\neven greater depth are also feasible and can be applied for fu rther enhancements.\nDifferent settings of these hyper-parameters lead to diffe rent model sizes. There are two\nwidely-used BERT models.\n• BERT base:d= 768 ,L= 12 ,nhead= 12 , total number of parameters = 110 M.\n• BERT large:d= 1,024,L= 14 ,nhead= 16 , total number of parameters = 340 M.\nTraining BERT models follows the standard training process of Transformers. Training larger\nmodels such as BERT large requires more training effort and time. This is a common prob lem\nfor pre-training, especially when a model is trained on a ver y large amount of data. In practice,\n\n1.3 Example: BERT 27\nthere are often considerations of training efﬁciency. For e xample, a practice is to ﬁrst train a\nBERT model on relatively short sequences for a large number o f training steps, and then continue\ntraining it on full-length sequences for the remaining trai ning steps.\n1.3.2 More Training and Larger Models\nBERT is a milestone model in NLP, sparking many subsequent ef forts to improve it. One direction\nis to scale up the model itself, including increasing traini ng data and developing larger models.\nRoBERTa , an extension of the standard BERT model, is an example of suc h efforts [ Liu et al. ,\n2019 ]. It introduces two major improvements. First, simply usin g more training data and more\ncompute can improve BERT models without need of changing the model architectures. Second,\nremoving the NSP loss does not decrease the performance on do wnstream tasks if the training is\nscaled up. These ﬁndings suggest exploring a general direct ion of pre-training: we can continue\nto improve pre-training by scaling it up on simple pre-train ing tasks.\nA second approach to improving BERT models is to increase the number of model parame-\nters. For example, in He et al. [2021 ]’s work, a 1.5 billion-parameter BERT-like model is built b y\nincreasing both the model depth and hidden size. However, sc aling up BERT and various other\npre-trained models introduces new challenges in training, for example, training very large models\noften becomes unstable and difﬁcult to converge. This makes the problem more complicated, and\nrequires careful consideration of various aspects, includ ing model architecture, parallel computa-\ntion, parameter initialization, and so on. In another examp le,Shoeybi et al. [2019 ] successfully\ntrained a 3.9 billion-parameter BERT-like model, where hun dreds of GPUs were used to manage\nthe increased computational demands.\n1.3.3 More Efﬁcient",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 25,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "fde544f35d7c963c",
    "content": " pre-train ing tasks.\nA second approach to improving BERT models is to increase the number of model parame-\nters. For example, in He et al. [2021 ]’s work, a 1.5 billion-parameter BERT-like model is built b y\nincreasing both the model depth and hidden size. However, sc aling up BERT and various other\npre-trained models introduces new challenges in training, for example, training very large models\noften becomes unstable and difﬁcult to converge. This makes the problem more complicated, and\nrequires careful consideration of various aspects, includ ing model architecture, parallel computa-\ntion, parameter initialization, and so on. In another examp le,Shoeybi et al. [2019 ] successfully\ntrained a 3.9 billion-parameter BERT-like model, where hun dreds of GPUs were used to manage\nthe increased computational demands.\n1.3.3 More Efﬁcient Models\nCompared to its predecessors, BERT is a relatively large mod el for the time it was proposed.\nThis increase in model size results in larger memory require ments and a consequent slowdown in\nsystem performance. Developing smaller and faster BERT mod els is part of the broader challenge\nof building efﬁcient Transformers, which has been extensiv ely discussed in Tay et al. [2020 ]’s\nwork and Xiao and Zhu [2023 ]’s work. However, a deeper discussion of this general topic is\nbeyond the scope of our current discussion. Here we instead c onsider a few efﬁcient variants of\nBERT.\nSeveral threads of research are of interest to NLP researche rs in developing efﬁcient BERT\nmodels. First, work on knowledge distillation, such as trai ning student models with the output\nof well-trained teacher models, shows that smaller BERT mod els can be obtained by transferring\nknowledge from larger BERT models. Given that BERT models ar e multi-layer networks with\nseveral different types of layers, knowledge distillation can be applied at different levels of repre-\nsentation. For example, beyond distilling knowledge from t he output layers, it is also possible to\nincorporate training loss that measures the difference in o utput of hidden layers between teacher\nmodels and student models [ Sun et al. ,2020 ;Jiao et al. ,2020 ]. Indeed, knowledge distillation has\nbeen one of the most widely-used techniques for learning sma ll pre-trained models.\nSecond, conventional model compression methods can be dire ctly applied to compress BERT\nmodels. One common approach is to use general-purpose pruni ng methods to prune the Trans-\nformer encoding networks [ Gale et al. ,2019 ]. This generally involves removing entire layers\n\n28 Pre-training\n[Fan et al. ,2019 ] or a certain percentage of parameters in the networks [ Sanh et al. ,2020 ;Chen et al. ,\n2020 ]. Pruning is also applicable to multi-head attention model s. For example, Michel et al.\n[2019 ] show that removing some of the heads does not signiﬁcantly d ecrease the performance\nof BERT models, but speeds up the inference of these models. A nother approach to compress-\ning BERT models is quantization [ Shen et al. ,2020 ]. By representing model parameters as low-\nprecision numbers, the models can be greatly compressed. Wh ile this method is not speciﬁc to\nBERT models, it proves effective for large Transformer-bas ed architectures.\nThird, considering that BERT models are relatively deep and large networks, another thread\nof research uses dynamic networks to adapt these models for e fﬁcient inference. An idea in this\nparadigm is to dynamically choose the layers for processing a token, for example, in depth-\nadaptive models we exit at some optimal depth and thus skip th e rest of the layers in the layer\nstack [ Xin et al. ,2020 ;Zhou et al. ,2020 ]. Similarly, we can develop length-adaptive models in\nwhich the length of the input sequence is dynamically adjust ed. For example, we can skip some of\nthe tokens in the input sequence so that the model can reduce c omputational load on less important\ntokens, enhancing overall efﬁciency.\nFourth, it is also possible to share parameters across layer s to reduce the size of BERT models.\nA simple way to do this is to share the parameters of a whole Tra nsformer layer across the layer\nstack [ Dehghani et al. ,2018 ;Lan et al. ,2020 ]. In addition to the reduced number of parameters,\nthis enables reuse of the same layer in a multi-layer Transfo rmer network",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 26,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "9c5dc2626ef9dc83",
    "content": ", for example, in depth-\nadaptive models we exit at some optimal depth and thus skip th e rest of the layers in the layer\nstack [ Xin et al. ,2020 ;Zhou et al. ,2020 ]. Similarly, we can develop length-adaptive models in\nwhich the length of the input sequence is dynamically adjust ed. For example, we can skip some of\nthe tokens in the input sequence so that the model can reduce c omputational load on less important\ntokens, enhancing overall efﬁciency.\nFourth, it is also possible to share parameters across layer s to reduce the size of BERT models.\nA simple way to do this is to share the parameters of a whole Tra nsformer layer across the layer\nstack [ Dehghani et al. ,2018 ;Lan et al. ,2020 ]. In addition to the reduced number of parameters,\nthis enables reuse of the same layer in a multi-layer Transfo rmer network, leading to savings of\nmemory footprint at test time.\n1.3.4 Multi-lingual Models\nThe initial BERT model was primarily focused on English. Soo n after this model was proposed,\nit was extended to many languages. One simple way to do this is to develop a separate model\nfor each language. Another approach, which has become more p opular in recent work on large\nlanguage models, is to train multi-lingual models directly on data from all the languages. In\nresponse, multi-lingual BERT (mBERT ) models were developed by training them on text from\n104 languages6. The primary difference from monolingual BERT models is tha t mBERT models\nuse larger vocabularies to cover tokens from multiple langu ages. As a result, the representations\nof tokens from different languages are mapped into the same s pace, allowing for the sharing of\nknowledge across languages via this universal representat ion model.\nOne important application of multi-lingual pre-trained mo dels is cross-lingual learning. In the\ncross-lingual setting, we learn a model on tasks in one langu age, and apply it to the same tasks\nin another language. In cross-lingual text classiﬁcation, for example, we ﬁne-tune a multi-lingual\npre-trained model on English annotated documents. Then, we use the ﬁne-tuned model to classify\nChinese documents.\nAn improvement to multi-lingual pre-trained models like mB ERT is to introduce bilingual data\ninto pre-training. Rather than training solely on monoling ual data from multiple languages, bilin-\ngual training explicitly models the relationship between t okens in two languages. The resulting\nmodel will have innate cross-lingual transfer abilities, a nd thus can be easily adapted to different\nlanguages. Lample and Conneau [2019 ] propose an approach to pre-training cross-lingual lan-\nguage models (XLMs ). In their work, a cross-lingual language model can be train ed in either the\ncausal language modeling or masked language modeling manne r. For masked language modeling\n6https://github.com/google-research/bert/\n\n1.3 Example: BERT 29\npre-training, the model is treated as an encoder. The traini ng objective is the same as BERT: we\nmaximize the probabilities of some randomly selected token s which are either masked, replaced\nwith random tokens, or kept unchanged in the input. If we cons ider bilingual data in pre-training,\nwe sample a pair of aligned sentences each time. Then, the two sentences are packed together to\nform a single sequence used for training. For example, consi der an English-Chinese sentence pair\n鲸鱼是哺乳动物。↔Whales are mammals .\nWe can pack them to obtain a sequence, like this\n[CLS]鲸鱼是哺乳动物。[SEP] Whales are mammals . [SEP]\nWe then select a certain percentage of the tokens and replace them with [MASK] .\n[CLS] [MASK]是[MASK]动物。[SEP] Whales [MASK] [MASK] .[SEP]\nThe goal of pre-training is to maximize the product of the pro babilities of the masked tokens given\nthe above sequence. By performing training in this way, the m odel can learn to represent both the\nEnglish and Chinese sequences, as well as to capture the corr espondences between tokens in the\ntwo languages. For example, predicting the Chinese token 鲸鱼 may require the information\nfrom the English token Whales . Aligning the representations of the two languages essenti ally\ntransforms the model into a “translation” model. So this tra ining objective",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 27,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "761657725359d703",
    "content": " to obtain a sequence, like this\n[CLS]鲸鱼是哺乳动物。[SEP] Whales are mammals . [SEP]\nWe then select a certain percentage of the tokens and replace them with [MASK] .\n[CLS] [MASK]是[MASK]动物。[SEP] Whales [MASK] [MASK] .[SEP]\nThe goal of pre-training is to maximize the product of the pro babilities of the masked tokens given\nthe above sequence. By performing training in this way, the m odel can learn to represent both the\nEnglish and Chinese sequences, as well as to capture the corr espondences between tokens in the\ntwo languages. For example, predicting the Chinese token 鲸鱼 may require the information\nfrom the English token Whales . Aligning the representations of the two languages essenti ally\ntransforms the model into a “translation” model. So this tra ining objective is also called transla-\ntion language modeling . Figure 1.7shows an illustration of this approach.\nA beneﬁt of multi-lingual pre-trained models is their inher ent capability of handling code-\nswitching. In NLP and linguistics, code-switching refers t o switching among languages in a text.\nFor example, the following is a mixed language text containi ng both Chinese and English:\n周末我们打算去做hiking，你想一起来吗？\n(We plan to go hiking this weekend, would you like to join us?)\nFor multi-lingual pre-trained models, we do not need to iden tify whether a token is Chinese or\nEnglish. Instead, every token is just an entry of the shared v ocabulary. This can be imagined as\ncreating a “new” language that encompasses all the language s we want to process.\nThe result of multi-lingual pre-training is inﬂuenced by se veral factors. Given that the model\narchitecture is ﬁxed, one needs to specify the size of the sha red vocabulary, the number (or per-\ncentage) of samples in each language, the size of the model, a nd so on. Conneau et al. [2020 ]\npoint out several interesting issues regarding large-scal e multi-lingual pre-training for XLM-like\nmodels. First, as the number of supported languages increas es, a larger model is needed to handle\nthese languages. Second, a larger shared vocabulary is help ful for modeling the increased diver-\nsity in languages. Third, low-resource languages more easi ly beneﬁt from cross-lingual transfer\nfrom high-resource languages, particularly when similar h igh-resource languages are involved in\npre-training. However, interference may occur if the model is trained for an extended period,\n\n30 Pre-training\n[CLS] [MASK]是[MASK]动物。 [SEP] Whales [MASK] [MASK] .[SEP]\n(zh) (zh) (zh) (zh) (zh) (zh) (zh) (en) (en) (en) (en) (en)e0 e1 e2 e3 e4 e5 e6 e7 e8 e9 e10 e11h0 h1 h2 h3 h4 h5 h6 h7 h8 h9 h10 h11鲸鱼哺乳 are mammals\nTransformer Encoder\nFig. 1.7: An illustration of translation language modeling. For ease of understanding, we present a simple example\nwhere all the selected tokens are masked. The model is traine d to predict these masked tokens. As the sequence\ncontains tokens in two languages, predicting a token in one l anguage allows access to tokens in the other language,\nthereby enabling cross-lingual modeling. In Lample and Conneau [2019 ]’s work, an input embedding (i.e., ei) is the\nsum of the token embedding, positional embedding, and langu age embedding. This requires that each token is assigned\nwith a language label. Thus we can distinguish tokens in diff erent languages. In multi-lingual pre-training, particul arly\nin work using shared vocabularies, specifying the language to which a token belongs is not necessary. The use of\nlanguage embeddings in turn makes it difﬁcult to handle code -switching. Therefore, we assume here that all token\nrepresentations are language-independent.\nmeaning the overall performance of the pre-trained model st arts decreasing at a certain point dur-\ning pre-training. Thus, in practical systems, one may need t o stop the pre-training early to prevent\ninterference.\n1.4 Applying BERT Models\nOnce a BERT model is pre-trained, it can then be used to solve",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 28,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "248c2da894730067",
    "content": " In Lample and Conneau [2019 ]’s work, an input embedding (i.e., ei) is the\nsum of the token embedding, positional embedding, and langu age embedding. This requires that each token is assigned\nwith a language label. Thus we can distinguish tokens in diff erent languages. In multi-lingual pre-training, particul arly\nin work using shared vocabularies, specifying the language to which a token belongs is not necessary. The use of\nlanguage embeddings in turn makes it difﬁcult to handle code -switching. Therefore, we assume here that all token\nrepresentations are language-independent.\nmeaning the overall performance of the pre-trained model st arts decreasing at a certain point dur-\ning pre-training. Thus, in practical systems, one may need t o stop the pre-training early to prevent\ninterference.\n1.4 Applying BERT Models\nOnce a BERT model is pre-trained, it can then be used to solve N LP problems. But BERT models\nare not immediately ready for performing speciﬁc downstrea m tasks. In general, additional ﬁne-\ntuning work is required to make them adapt. As a ﬁrst step, we n eed a predictor to align the\noutput of the model with the problem of interest. Let BERT ˆθ(·)be a BERT model with pre-\ntrained parameters ˆθ, and Predict ω(·)be a prediction network with parameters ω. By integrating\nthe prediction network with the output of the BERT model, we d evelop a model to tackle the\ndownstream tasks. This model can be expressed as\ny= Predict ω(BERT ˆθ(x)) (1.21)\nwhere xis the input and yis the output that ﬁts the problem. For example, in classiﬁca tion\nproblems, the model outputs a probability distribution ove r labels.\nThen, we collect a set of labeled samples D, and ﬁne-tune the model by\n(˜ω,˜θ) = arg min\nω,ˆθ+∑\n(x,ygold)∈DLoss( yω,ˆθ+,ygold) (1.22)\n\n1.4 Applying BERT Models 31\nwhere (x,ygold)represents a tuple of an input and its corresponding output. The notation of this\nequation seems a bit complicated, but the training/tuning p rocess is standard. We optimize the\nmodel by minimizing the loss over the tuning samples. The out come is the optimized parameters\n˜ωand˜θ. The optimization starts with the pre-trained parameters ˆθ. Here we use ˆθ+to indicate that\nthe parameters are initialized with ˆθ, and use yω,ˆθ+to denote the model output computed using\nthe parameters ωandˆθ+.\nWith the ﬁne-tuned parameters ˜ωand˜θ, we can apply the model Predict ˜ω(BERT ˜θ(·))to new\ndata of the same tasks for which the model was ﬁne-tuned. The f orm of the downstream tasks\ndetermines the input and output formats of the model, as well as the architecture of the prediction\nnetwork. In the following we list some tasks to which BERT mod els are generally suited.\n•Classiﬁcation (Single Text). One of the most widely-used applications of B ERT models is\ntext classiﬁcation. In this task, a BERT model receives a seq uence of tokens and encodes\nit as a sequence of vectors. The ﬁrst output vector hcls(orh0) is typically used as the\nrepresentation of the entire text. The prediction network t akes hclsas input to produce a\ndistribution of labels. Let [CLS]x1x2...xmbe an input text. See below for an illustration of\nBERT-based text classiﬁcation.\n[CLS]x1x2...xm[SEP]ecls e1 e2... emem+1hcls h1 h2... hmhm+1Class\nBERT\nHere the gray box denotes the prediction network. Many NLP pr oblems can be categorized\nas text classiﬁcation tasks, and there have been several tex t classiﬁcation benchmarks for\nevaluating pre-trained models. For example, we can classif y texts by their grammatical cor-\nrectness (grammaticality) or emotional tone (sentiment) [ Socher et al. ,2013 ;Warstadt et al. ,\n2019 ]. Note that the prediction network could be any classiﬁcati on",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 29,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "33eb0b44e605d348",
    "content": ". The prediction network t akes hclsas input to produce a\ndistribution of labels. Let [CLS]x1x2...xmbe an input text. See below for an illustration of\nBERT-based text classiﬁcation.\n[CLS]x1x2...xm[SEP]ecls e1 e2... emem+1hcls h1 h2... hmhm+1Class\nBERT\nHere the gray box denotes the prediction network. Many NLP pr oblems can be categorized\nas text classiﬁcation tasks, and there have been several tex t classiﬁcation benchmarks for\nevaluating pre-trained models. For example, we can classif y texts by their grammatical cor-\nrectness (grammaticality) or emotional tone (sentiment) [ Socher et al. ,2013 ;Warstadt et al. ,\n2019 ]. Note that the prediction network could be any classiﬁcati on model, such as a deep\nneural network or a more traditional classiﬁcation model. T he entire model can then be\ntrained or ﬁne-tuned in the manner of a standard classiﬁcati on model. For example, the pre-\ndiction network can be simply a Softmax layer and the model pa rameters can be optimized\nby maximizing the probabilities of the correct labels.\n•Classiﬁcation (Pair of Texts). Classiﬁcation can also be performed on a pai r of texts. Sup-\npose we have two texts, x1...xmandy1...yn. We can concatenate these texts to form a single\nsequence with a length len. Then, we predict a label for this combined text sequence bas ed\non the hclsvector, as follows\n\n32 Pre-training\n[CLS]x1x2...xm[SEP]y1y2...yn[SEP]\nText 1 Text 2ecls e1 e2... emem+1em+2em+3... elen−1elenhcls h1 h2... hmhm+1hm+2hm+3... hlen−1hlenClass\nBERT\nwherelen=n+m+ 2. Text pair classiﬁcation covers several problems, includi ng se-\nmantic equivalence judgement (determine whether two texts are semantically equivalent)\n[Dolan and Brockett ,2005 ], text entailment judgement (determine whether a hypothes is\ncan be logically inferred or entailed from a premise) [ Bentivogli and Giampiccolo ,2011 ;\nWilliams et al. ,2018 ], grounded commonsense inference (determine whether an ev ent is\nlikely to happen given its context) [ Zellers et al. ,2018 ], and question-answering inference\n(determine whether an answer corresponds to a given questio n).\n•Regression . Instead of generating a label distribution, we can have the prediction network\noutput a real-valued score. For example, by adding a Sigmoid layer to the prediction net-\nwork, the system can be employed to compute the similarity be tween two given sentences.\nThe architecture is the same as that of BERT-based classiﬁca tion systems, with only the\nchange of the output layer.\n[CLS]x1x2...xm[SEP]y1y2...yn[SEP]\nText 1 Text 2ecls e1 e2... emem+1em+2em+3... elen−1elenhcls h1 h2... hmhm+1hm+2hm+3... hlen−1hlenNumber (similarity, evaluation score, etc.)\nBERT\nFor training or ﬁne-tuning, we can minimize the regression l oss of the model output as\nusual.\n•Sequence Labeling . Sequence labeling is a machine learning approach applicab le to a wide\nrange of NLP problems. This approach assigns a label to each t oken in an input sequence,\nand some linguistic annotations can then be derived from thi s sequence of labels. An ex-\nample of sequence labeling in NLP is part-of-speech (POS) ta gging. We label each word\nin a sentence with its corresponding POS tag. Another exampl e is named entity recognition\n(NER) in which we label each word with an NER tag, and named ent ities are identiﬁed\nusing these tags. See below for an illustration of the model a rchitecture for NER.\n\n1.4 Applying BERT Models 33\n[CLS]x1x2...xm[SEP]ecls e1 e2... emem+1hcls h1 h2... hmhm+1{B,I,O}{B,I,O} { B,I,O}Tag Tag Tag\n",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 30,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "caa8a47644360e2b",
    "content": " machine learning approach applicab le to a wide\nrange of NLP problems. This approach assigns a label to each t oken in an input sequence,\nand some linguistic annotations can then be derived from thi s sequence of labels. An ex-\nample of sequence labeling in NLP is part-of-speech (POS) ta gging. We label each word\nin a sentence with its corresponding POS tag. Another exampl e is named entity recognition\n(NER) in which we label each word with an NER tag, and named ent ities are identiﬁed\nusing these tags. See below for an illustration of the model a rchitecture for NER.\n\n1.4 Applying BERT Models 33\n[CLS]x1x2...xm[SEP]ecls e1 e2... emem+1hcls h1 h2... hmhm+1{B,I,O}{B,I,O} { B,I,O}Tag Tag Tag\nBERT\nHere {B,I,O}is the tag set of NER. For example, B-ORG means the beginning of an\norganization, I-ORG means the word is inside an organization, and Omeans the word does\nnot belong to any named entity. This NER model can output a dis tribution over the tag set\nat each position, denoted as pi. The training or ﬁne-tuning of the model can be performed\nover these distributions {p1,...,pm}. For example, suppose pi(tagi)is the probability of\nthe correct tag at position i. The training loss can be deﬁned to be the negative likelihoo d\nLoss = −1\nmm∑\ni=1logpi(tagi) (1.23)\nFinding the best label sequence given a trained NER model is a well-studied issue in NLP.\nThis is often achieved via dynamic programming, which, in th e context of path ﬁnding over\na lattice, has linear complexity [ Huang ,2009 ].\n•Span Prediction . Some NLP tasks require predicting a span in a text. A common e xample\nis reading comprehension. In this task, we are given a query x1...xmand a context text\ny1...yn. The goal is to identify a continuous span in y1...ynthat best answers the query.\nThis problem can be framed as a sequence labeling-like task i n which we predict a label for\neachyjto indicate the beginning or ending of the span. Following Seo et al. [2017 ], we add\ntwo networks on top of the BERT output for yj: one for generating the probability of yj\nbeing the beginning of the span (denoted by pbeg\nj), and one for generating the probability\nofyjbeing the ending of the span (denoted by pend\nj). The resulting model architecture is\nshown as follows\n[CLS]x1x2...xm[SEP]y1y2...yn[SEP]\nQuery Context Textecls e1 e2... emem+1em+2em+3... elen−1elenhcls h1 h2... hmhm+1hm+2hm+3... hlen−1hlen(pbeg\n1)Beg(pend\n1)End\n(pbeg\n2)Beg(pend\n2)End\n(pbeg\nn)Beg(pend\nn)End\nBERT\n\n34 Pre-training\nWe pack the query and context text together to obtain the inpu t sequence. The prediction\nnetworks are only applied to outputs for the context text, ge nerating the probabilities pbeg\nj\nandpend\njat each position. The loss can be computed by summing the log l ikelihoods of the\ntwo models across the entire context text.\nLoss = −1\nnn∑\nj=1(logpbeg\nj+ logpend\nj)(1.24)\nAt test time, we search for the best span by\n(ˆj1,ˆj2) = arg max\n1≤j1≤j2≤n(logpbeg\nj1+ logpend\nj2)(1.25)\n•Encoding for Encoder-decoder Models . While our focus in this section has been primarily\non language understanding problems, it is worth noting that BERT models can be applied\nto a broader range of NLP tasks. In fact, BERT models can be use d in all the scenarios\nwhere we need to encode a piece of text. One application that w e have not mentioned is\ntext generation which includes a range of tasks such as machi ne translation, summarization,\nquestion answering, and dialogue generation. These tasks c an be formulated as sequence-\nto-sequence problems:",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 31,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "203d81f66dd2d20a",
    "content": " = −1\nnn∑\nj=1(logpbeg\nj+ logpend\nj)(1.24)\nAt test time, we search for the best span by\n(ˆj1,ˆj2) = arg max\n1≤j1≤j2≤n(logpbeg\nj1+ logpend\nj2)(1.25)\n•Encoding for Encoder-decoder Models . While our focus in this section has been primarily\non language understanding problems, it is worth noting that BERT models can be applied\nto a broader range of NLP tasks. In fact, BERT models can be use d in all the scenarios\nwhere we need to encode a piece of text. One application that w e have not mentioned is\ntext generation which includes a range of tasks such as machi ne translation, summarization,\nquestion answering, and dialogue generation. These tasks c an be formulated as sequence-\nto-sequence problems: we use an encoder to represent the sou rce text, and a decoder to\ngenerate the corresponding target text. A straightforward method to apply BERT models\nis to consider them as encoders. Before ﬁne-tuning, we can in itialize the parameters of the\nencoder with those from a pre-trained BERT model. Then, the e ncoder-decoder model can\nbe ﬁne-tuned on pairs of texts as usual. The following shows t he architecture of a neural\nmachine translation system where a BERT model is applied on t he source side.\n[CLS]x1...xm[SEP]\nSource Textex\ncls ex\n1... ex\nmex\nm+1BERT (Encoder)Adapter\n⟨s⟩y1y2...yn−1ey\n0 ey\n1 ey\n2... ey\nn−1Decodery1y2y3...ynTarget Text\nHerex1...xmdenotes the source sequence, y1...yndenotes the target sequence, ex\n1...ex\nm\ndenotes the embedding sequence of x1...xm, and ey\n1...ey\nndenotes the embedding sequence\nofy1...yn. The adapter, which is optional, maps the output of the BERT m odel to the form\nthat is better suited to the decoder.\nFine-tuning BERT models is a complicated engineering probl em, inﬂuenced by many factors,\nsuch as the amount of ﬁne-tuning data, the model size, and the optimizer used in ﬁne-tuning.\nIn general, we wish to ﬁne-tune these models sufﬁciently so t hat they can perform well in the\ndownstream tasks. However, ﬁne-tuning BERT models for spec iﬁc tasks may lead to overﬁtting,\n\n1.5 Summary 35\nwhich in turn reduces their ability to generalize to other ta sks. For example, suppose we have a\nBERT model that performs well on a particular task. If we then ﬁne-tune it for new tasks, this\nmay decrease its performance on the original task. This prob lem is related to the catastrophic\nforgetting problem in continual training, where a neural network forge ts previously learned in-\nformation when updated on new samples. In practical applica tions, a common way to alleviate\ncatastrophic forgetting is to add some old data into ﬁne-tun ing and train the model with more\ndiverse data. Also, one may use methods specialized to catas trophic forgetting, such as experi-\nence replay [ Rolnick et al. ,2019 ] and elastic weight consolidation [ Kirkpatrick et al. ,2017 ]. The\ninterested reader can refer to some surveys for more detaile d discussions of this issue in continual\nlearning [ Parisi et al. ,2019 ;Wang et al. ,2023a ;e].\n1.5 Summary\nIn this chapter we have discussed the general idea of pre-tra ining in NLP. In particular, we have dis-\ncussed self-supervised pre-training and its application t o encode-only, decoder-only, and encoder-\ndecoder architectures. Moreover, we have presented and com pared a variety of pre-training tasks\nfor these architectures. As an example, BERT is used to illus trate how sequence models are pre-\ntrained via masked language modeling and applied to differe nt downstream tasks.\nRecent years have shown remarkable progress in NLP, led by th e large-scale use of self-\nsupervised pre-training. And sweeping advances are being m ade across many tasks, not only\nin NLP but also in computer vision and other areas of AI. One id ea behind these advances is that a\nsigniﬁcant amount of knowledge about the world",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 32,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "c09ee5ee9ad65e68",
    "content": "i et al. ,2019 ;Wang et al. ,2023a ;e].\n1.5 Summary\nIn this chapter we have discussed the general idea of pre-tra ining in NLP. In particular, we have dis-\ncussed self-supervised pre-training and its application t o encode-only, decoder-only, and encoder-\ndecoder architectures. Moreover, we have presented and com pared a variety of pre-training tasks\nfor these architectures. As an example, BERT is used to illus trate how sequence models are pre-\ntrained via masked language modeling and applied to differe nt downstream tasks.\nRecent years have shown remarkable progress in NLP, led by th e large-scale use of self-\nsupervised pre-training. And sweeping advances are being m ade across many tasks, not only\nin NLP but also in computer vision and other areas of AI. One id ea behind these advances is that a\nsigniﬁcant amount of knowledge about the world can be learne d by simply training these AI sys-\ntems on huge amounts of unlabeled data. For example, a langua ge model can learn some general\nknowledge of a language by repeatedly predicting masked wor ds in large-scale text. As a result,\nthis pre-trained language model can serve as a foundation mo del, which can be easily adapted to\naddress speciﬁc downstream NLP tasks. This paradigm shift i n NLP has enabled the development\nof incredibly powerful systems for language understanding , generation, and reasoning [ Manning ,\n2022 ]. However, it is important to recognize that we are still in t he early stages of creating truly in-\ntelligent systems, and there is a long way to go. Nevertheles s, large-scale pre-training has opened\na door to intelligent systems that researchers have long asp ired to develop, though several key re-\nsearch areas remain open for exploration, such as learning i ntelligence efﬁciently using reasonably\nsmall-sized data and acquiring complex reasoning and plann ing abilities.\nNote that this chapter is mostly introductory and cannot cov er all aspects of pre-training. For\nexample, there are many methods to ﬁne-tune a pre-trained mo del, offering different ways to better\nadapt the model to diverse situations. Moreover, large lang uage models, which are considered one\nof the most signiﬁcant achievements in AI in recent years, ar e skipped in this section. We leave\nthe discussion of these topics to the following chapters.\n\nCHAPTER 2\nGenerative Models\nOne of the most signiﬁcant advances in NLP in recent years mig ht be the development of large\nlanguage models (LLMs). This has helped create systems that can understand and generate nat-\nural languages like humans. These systems have even been fou nd to be able to reason, which\nis considered a very challenging AI problem. With these achi evements, NLP made big strides\nand entered a new era of research in which difﬁcult problems a re being solved, such as building\nconversational systems that can communicate with humans sm oothly.\nThe concept of language modeling or probabilistic language modeling dates back to early ex-\nperiments conducted by Shannon [1951 ]. In his work, a language model was designed to estimate\nthe predictability of English — how well can the next letter of a text be predicted when the pre -\ncedingNletters are known . Although Shannon’s experiments were preliminary, the fun damental\ngoals and methods of language modeling have remained largel y unchanged over the decades since\nthen. For quite a long period, particularly before 2010, the dominant approach to language mod-\neling was the n-gram approach [ Jurafsky and Martin ,2008 ]. Inn-gram language modeling, we\nestimate the probability of a word given its preceding n−1words, and thus the probability of a\nsequence can be approximated by the product of a series of n-gram probabilities. These proba-\nbilities are typically estimated by collecting smoothed re lative counts of n-grams in text. While\nsuch an approach is straightforward and simple, it has been e xtensively used in NLP. For example,\nthe success of modern statistical speech recognition and ma chine translation systems has largely\ndepended on the utilization of n-gram language models [ Jelinek ,1998 ;Koehn ,2010 ].\nApplying neural networks to language modeling has long been attractive, but a real break-\nthrough appeared as deep learning techniques advanced. A wi dely cited study is Bengio et al.\n[2003 ]’s work where n-gram probabilities are modeled via a feed-forward network and learned\nby training the network in an end-to-end fashion. A by-produ ct of this neural language model\nis the distributed representations of words, known as word e",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 33,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "30998f5df724d370",
    "content": "\nsequence can be approximated by the product of a series of n-gram probabilities. These proba-\nbilities are typically estimated by collecting smoothed re lative counts of n-grams in text. While\nsuch an approach is straightforward and simple, it has been e xtensively used in NLP. For example,\nthe success of modern statistical speech recognition and ma chine translation systems has largely\ndepended on the utilization of n-gram language models [ Jelinek ,1998 ;Koehn ,2010 ].\nApplying neural networks to language modeling has long been attractive, but a real break-\nthrough appeared as deep learning techniques advanced. A wi dely cited study is Bengio et al.\n[2003 ]’s work where n-gram probabilities are modeled via a feed-forward network and learned\nby training the network in an end-to-end fashion. A by-produ ct of this neural language model\nis the distributed representations of words, known as word e mbeddings. Rather than represent-\ning words as discrete variables, word embeddings map words i nto low-dimensional real-valued\nvectors, making it possible to compute the meanings of words and wordn-grams in a continu-\nous representation space. As a result, language models are n o longer burdened with the curse of\ndimensionality, but can represent exponentially many n-grams via a compact and dense neural\nmodel.\nThe idea of learning word representations through neural la nguage models inspired subsequent\nresearch in representation learning in NLP. However, this a pproach did not attract signiﬁcant in-\nterest in developing NLP systems in the ﬁrst few years after i ts proposal. Starting in about 2012,\nthough, advances were made in learning word embeddings from large-scale text via simple word\nprediction tasks. Several methods, such as Word2Vec, were p roposed to effectively learn such\nembeddings, which were then successfully applied in a varie ty of NLP systems [ Mikolov et al. ,\n2013a ;b]. As a result of these advances, researchers began to think o f learning representations of\nsequences using more powerful language models, such as LSTM -based models [ Sutskever et al. ,\n2014 ;Peters et al. ,2018 ]. And further progress and interest in sequence representa tion exploded\nafter Transformer was proposed. Alongside the rise of Trans former, the concept of language mod-\neling was generalized to encompass models that learn to pred ict words in various ways. Many\n36\n\n2.1 A Brief Introduction to LLMs 37\npowerful Transformer-based models were pre-trained using these word prediction tasks, and suc-\ncessfully applied to a variety of downstream tasks [ Devlin et al. ,2019 ].\nIndeed, training language models on large-scale data has le d NLP research to exciting times.\nWhile language modeling has long been seen as a foundational technique with no direct link to\nthe goals of artiﬁcial intelligence that researchers had ho ped for, it helps us see the emergence of\nintelligent systems that can learn a certain degree of gener al knowledge from repeatedly predicting\nwords in text. Recent research demonstrates that a single, w ell-trained LLM can handle a large\nnumber of tasks and generalize to perform new tasks with a sma ll adaptation effort [ Bubeck et al. ,\n2023 ]. This suggests a step towards more advanced forms of artiﬁc ial intelligence, and inspires\nfurther exploration into developing more powerful languag e models as foundation models.\nIn this chapter, we consider the basic concepts of generativ e LLMs. For simplicity, we use the\nterms large language models orLLMs to refer to generative models like GPT, though this term\ncan broadly cover other types of models like BERT. We begin by giving a general introduction\nto LLMs, including the key steps of building such models. We t hen discuss two scaling issues of\nLLMs: how LLMs are trained at scale, and how LLMs can be improv ed to handle very long texts.\nFinally, we give a summary of these discussions.\n2.1 A Brief Introduction to LLMs\nIn this section we give an introduction to the basic ideas of L LMs as required for the rest of this\nchapter and the following chapters. We will use terms word andtoken interchangeably. Both\nof them refer to the basic units used in language modeling, th ough their original meanings are\ndifferent.\nBefore presenting details, let us ﬁrst consider how languag e models work. The goal of lan-\nguage modeling is to predict the probability of a sequence of tokens occurring. Let {x0,x1,...,x m}\nbe a sequence of tokens, where x0is the start symbol ⟨s⟩(",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 34,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "db1286e50aa69c62",
    "content": "\nto LLMs, including the key steps of building such models. We t hen discuss two scaling issues of\nLLMs: how LLMs are trained at scale, and how LLMs can be improv ed to handle very long texts.\nFinally, we give a summary of these discussions.\n2.1 A Brief Introduction to LLMs\nIn this section we give an introduction to the basic ideas of L LMs as required for the rest of this\nchapter and the following chapters. We will use terms word andtoken interchangeably. Both\nof them refer to the basic units used in language modeling, th ough their original meanings are\ndifferent.\nBefore presenting details, let us ﬁrst consider how languag e models work. The goal of lan-\nguage modeling is to predict the probability of a sequence of tokens occurring. Let {x0,x1,...,x m}\nbe a sequence of tokens, where x0is the start symbol ⟨s⟩(or⟨SOS⟩)1. The probability of this se-\nquence can be deﬁned using the chain rule\nPr(x0,...,x m) = Pr(x0)·Pr(x1|x0)·Pr(x2|x0,x1)· · ·Pr(xm|x0,...,x m−1)\n=m∏\ni=0Pr(xi|x0,...,x i−1) (2.1)\nor alternatively in a logarithmic form\nlog Pr(x0,...,x m) =m∑\ni=0log Pr(xi|x0,...,x i−1) (2.2)\nHere Pr(xi|x0,...,x i−1)is the probability of the token xigiven all its previous tokens {x0,...,x i−1}\n2. In the era of deep learning, a typical approach to language m odeling is to estimate this\n1The start symbol can also be [CLS] following BERT models.\n2We assume that when i= 0 ,Pr(xi|x0,...,x i−1) = Pr(x0) = 1 . Hence Pr(x0,...,x m) =\nPr(x0) Pr(x1,...,x m|x0) = Pr(x1,...,x m|x0).\n\n38 Generative Models\nContext Predict Decision Rule Sequence Probability\n⟨s⟩ab arg maxx2∈VPr(x2|⟨s⟩a) Pr(⟨s⟩)·Pr(a|⟨s⟩)·Pr(b|⟨s⟩a)\n⟨s⟩a bc arg maxx3∈VPr(x3|⟨s⟩ab) Pr(⟨s⟩)·Pr(a|⟨s⟩)·Pr(b|⟨s⟩a)·\nPr(c|⟨s⟩ab)\n⟨s⟩a b cd arg maxx4∈VPr(x4|⟨s⟩abc)Pr(⟨s⟩)·Pr(a|⟨s⟩)·Pr(b|⟨s⟩a)·\nPr(c|⟨s⟩ab)·Pr(d|⟨s⟩abc)\nTable 2.1: Illustration of generating the three tokens b c d given the preﬁx ⟨s⟩avia a language model. In each step,\nthe model picks a token xifromVso that Pr(xi|x0,...,x i−1)is maximized. This token is then appended to the end\nof the context sequence. In the next step, we repeat the same p rocess, but based on the new context.\nprobability using a deep neural network. Neural networks tr ained to accomplish this task re-\nceive a sequence of tokens x0,...,x i−1and produce a distribution over the vocabulary V(de-\nnoted by Pr(·|x0,...,x i−1)). The probability Pr(xi|x0,...,x i−1)is the value of the i-th entry of\nPr(·|x0,...,x i−1).\nWhen applying a trained language model, a common task is to ﬁn d the most likely token given\nits previous context tokens. This token prediction task can be described as\nˆxi= arg max\nxi∈VPr(xi|x0,...,x i−1) (2.3)\nWe can perform word prediction multiple times to generate a c ontinuous text: each time we\npredict the best token ˆxi, and then add this predicted token to the context for predict ing the next\ntoken ˆxi+1. This results in a left-to-right generation process implem enting Eqs. ( 2.1) and ( 2.2). To\nillustrate, consider",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 35,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "3a96a4deb9e85787",
    "content": " Pr(·|x0,...,x i−1)). The probability Pr(xi|x0,...,x i−1)is the value of the i-th entry of\nPr(·|x0,...,x i−1).\nWhen applying a trained language model, a common task is to ﬁn d the most likely token given\nits previous context tokens. This token prediction task can be described as\nˆxi= arg max\nxi∈VPr(xi|x0,...,x i−1) (2.3)\nWe can perform word prediction multiple times to generate a c ontinuous text: each time we\npredict the best token ˆxi, and then add this predicted token to the context for predict ing the next\ntoken ˆxi+1. This results in a left-to-right generation process implem enting Eqs. ( 2.1) and ( 2.2). To\nillustrate, consider the generation of the following three words given the preﬁx ‘ ⟨s⟩a’, as shown\nin Table 2.1. Now we discuss how LLMs are constructed, trained, and appli ed.\n2.1.1 Decoder-only Transformers\nAs is standard practice, the input of a language model is a seq uence of tokens (denoted by\n{x0,...,x m−1}). For each step, an output token is generated, shifting the s equence one po-\nsition forward for the next prediction. To do this, the langu age model outputs a distribution\nPr(·|x0,...,x i−1)at each position i, and the token xiis selected according to this distribution.\nThis model is trained by maximizing the log likelihood∑m\ni=1log Pr(xi|x0,...,x i−1)3.\nHere, we focus on the decoder-only Transformer architectur e, as it is one of the most popular\nmodel architectures used in LLMs. The input sequence of toke ns is represented by a sequence\nofde-dimensional vectors {e0,...,em−1}.eiis the sum of the token embedding of xiand the\npositional embedding of i. The major body of the model is a stack of Transformer blocks ( or\nlayers). Each Transformer block has two stacked sub-layers , one for self-attention modeling and\none for FFN modeling. These sub-layers can be deﬁned using th e post-norm architecture\noutput = LNorm( F(input) + input) (2.4)\n3Note that∑m\ni=1log Pr(xi|x0,...,x i−1) =∑m\ni=0log Pr(xi|x0,...,x i−1)since log Pr(x0) = 0 .\n\n2.1 A Brief Introduction to LLMs 39\nor the pre-norm architecture\noutput = LNorm( F(input)) + input (2.5)\nwhere input andoutput denote the input and output, both being an m×dmatrix. The i-th rows\nofinput andoutput can be seen as contextual representations of the i-th token in the sequence.\nF(·)is the core function of a sub-layer. For FFN sub-layers, F(·)is a multi-layer FFN. For\nself-attention sub-layers, F(·)is a multi-head self-attention function. In general, self- attention is\nexpressed in a form of QKV attention\nAttqkv(Q,K,V) = Softmax(QKT\n√\nd+Mask )V (2.6)\nwhere Q,KandV∈Rm×dare the queries, keys, and values, respectively. It is impor tant to\nnote that only previous tokens are considered when predicti ng a token. So a masking variable\nMask ∈Rm×mis incorporated into self-attention to achieve this. The en try(i,k)ofMask has\na value of 0 if i≤k, and a value of −infotherwise.\nGiven a representation H∈Rm×d, the multi-head self-attention function can be deﬁned as\nF(H) = Merge(head 1,...,head τ)Whead(2.7)\nwhere Merge( ·)representees a concatenation of its inputs, and Whead∈Rd×drepresents a pa-\nrameter matrix. head jis the output of QKV attention on a sub-space of representati on\nhead j= Att qkv(Q[j],K[j],V[j]) (2.8)\nQ[j],K[j],andV[j]are the queries, keys, and values projected onto the j-th sub-space via linear\ntransformations\n",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 36,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "a7b35d1a3d78e3f3",
    "content": " a token. So a masking variable\nMask ∈Rm×mis incorporated into self-attention to achieve this. The en try(i,k)ofMask has\na value of 0 if i≤k, and a value of −infotherwise.\nGiven a representation H∈Rm×d, the multi-head self-attention function can be deﬁned as\nF(H) = Merge(head 1,...,head τ)Whead(2.7)\nwhere Merge( ·)representees a concatenation of its inputs, and Whead∈Rd×drepresents a pa-\nrameter matrix. head jis the output of QKV attention on a sub-space of representati on\nhead j= Att qkv(Q[j],K[j],V[j]) (2.8)\nQ[j],K[j],andV[j]are the queries, keys, and values projected onto the j-th sub-space via linear\ntransformations\nQ[j]=HWq\nj (2.9)\nK[j]=HWk\nj (2.10)\nV[j]=HWv\nj (2.11)\nwhere Wq\nj,Wk\nj, and Wv\nj∈Rd×d\nτare the parameter matrices of the transformations.\nSuppose we have LTransformer blocks. A Softmax layer is built on top of the out put of the\nlast block. The Softmax layer outputs a sequence of mdistributions over the vocabulary, like this\n\nPr(·|x0,...,x m−1)\n...\nPr(·|x0,x1)\nPr(·|x0)\n= Softmax( HLWo) (2.12)\nwhere HLis the output of the last Transformer block, and Wo∈Rd×|V|is the parameter matrix.\nFigure 2.1shows the Transformer architecture for language modeling. Applying this language\n\n40 Generative Models\nx0x1...xm−1e0 e1... em−1hL\n0 hL\n1... hL\nm−1...Pr(x1|x0)Pr(x2|x0x1)Pr(xm|x0x1...xm−1)x1x2...xm\nLanguage Model\nz0 z1... zm−1Post-norm or Pre-normPost-norm or Pre-norm\nSelf-attentionFFNLBlocks\nFig. 2.1: The Transformer-decoder architecture for language modeli ng. The central components are Lstacked Trans-\nformer blocks, each comprising a self-attention sub-layer and an FFN sub-layer. To prevent the model from accessing\nthe right-context, a masking variable is incorporated into self-attention. The output layer uses a Softmax function to\ngenerate a probability distribution for the next token, giv en the sequence of previous tokens. During inference, the\nmodel takes the previously predicted token to predict the ne xt one, repeating this process until the end of the sequence\nis reached. {z0,...,zm−1}denote the inputs of a Transformer block, and {hL\n0,...,hL\nm−1}denote the outputs of the\nlast Transformer block.\nmodel follows an autoregressive process. Each time the lang uage model takes a token xi−1as\ninput and predicts a token xithat maximizes the probability Pr(xi|x0,...,x i−1). It is important\nto note that, despite different implementation details, ma ny LLMs share the same architecture\ndescribed above. These models are called large because both their depth and width are signiﬁcant.\nTable 2.2shows the model sizes for a few LLMs, as well as their model set ups.\n2.1.2 Training LLMs\nNow suppose that we are given a training set DcomprisingKsequences. The log-likelihood of\neach sequence x=x0...xminDcan be calculated using a language model\nLθ(x) =m∑\ni=1log Pr θ(xi|x0,...,x i−1) (2.13)\nHere the subscript θafﬁxed to L(·)andPr(·)denotes the parameters of the language model. Then,\nthe objective of maximum likelihood training is deﬁned as\nˆθ= arg max\nθ∑\nx∈DLθ(x) (2.14)\nTraining Transformer-based language models with the above objective is commonly viewed\nas a standard optimization process for neural networks. Thi s can be achieved using gradient de-\nscent algorithms, which are",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 37,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "b036dc86e0ceff30",
    "content": "shows the model sizes for a few LLMs, as well as their model set ups.\n2.1.2 Training LLMs\nNow suppose that we are given a training set DcomprisingKsequences. The log-likelihood of\neach sequence x=x0...xminDcan be calculated using a language model\nLθ(x) =m∑\ni=1log Pr θ(xi|x0,...,x i−1) (2.13)\nHere the subscript θafﬁxed to L(·)andPr(·)denotes the parameters of the language model. Then,\nthe objective of maximum likelihood training is deﬁned as\nˆθ= arg max\nθ∑\nx∈DLθ(x) (2.14)\nTraining Transformer-based language models with the above objective is commonly viewed\nas a standard optimization process for neural networks. Thi s can be achieved using gradient de-\nscent algorithms, which are widely supported by off-the-sh elf deep learning toolkits. Somewhat\n\n2.1 A Brief Introduction to LLMs 41\nLLM # of Parameters DepthLWidthd# of Heads\n(Q/KV)\nGPT-1 [ Radford et al. ,2018 ] 0.117B 12 768 12/12\nGPT-2 [ Radford et al. ,2019 ] 1.5B 48 1,600 25/25\nGPT-3 [ Brown et al. ,2020 ] 175B 96 12,288 96/96\nLLaMA2 [ Touvron et al. ,2023b ]7B 32 4,096 32/32\n13B 40 5,120 40/40\n70B 80 8,192 64/64\nLLaMA3/3.1 [ Dubey et al. ,2024 ]8B 32 4,096 32/8\n70B 80 8,192 64/8\n405B 126 16,384 128/8\nGemma2 [ Team et al. ,2024 ]2B 26 2,304 8/4\n9B 42 3,584 16/8\n37B 46 4,608 32/16\nQwen2.5 [ Yang et al. ,2024 ]0.5B 24 896 14/2\n7B 28 3,584 28/4\n72B 80 8,192 64/8\nDeepSeek-V3 [ Liu et al. ,2024a ] 671B 61 7,168 128/128\nFalcon [ Penedo et al. ,2023 ]7B 32 4,544 71/71\n40B 60 8,192 128/128\n180B 80 14,848 232/232\nMistral [ Jiang et al. ,2023a ] 7B 32 4,096 32/32\nTable 2.2: Comparison of some LLMs in terms of model size, model depth, m odel width, and number of heads ( a/b\nmeansaheads for queries and bheads for both keys and values).\nsurprisingly, better results were continuously yielded as language models were evolved into more\ncomputationally intensive models and trained on larger dat asets [ Kaplan et al. ,2020 ]. These suc-\ncesses have led NLP researchers to continue increasing both the training data and model size in\norder to build more powerful language models.\nHowever, as language models become larger, we confront new t raining challenges, which\nsigniﬁcantly change the problem compared to training relat ively small models. One of these\nchallenges arises from the need for large-scale distribute d systems to manage the data, model\nparameters, training routines, and so on. Developing and ma intaining such systems requires a\nsigniﬁcant amount of work in both software and hardware engi neering, as well as expertise in deep\nlearning. A related issue is that when the training is scaled up, we need more computing resources\nto ensure the training process can be completed in an accepta ble time. For example, it generally\nrequires hundreds or thousands of GPUs to train an LLM with te ns of billions of parameters\nfrom scratch. This requirement drastically increases the c ost of training such models, especially\nconsidering that many training runs are needed as these mode ls are developed. Also, from the\nperspective of deep learning, the training process can beco me unstable if the neural networks are\nvery deep and/or the model size is very large. In response,",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 38,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "661d798d5a01144e",
    "content": " models. One of these\nchallenges arises from the need for large-scale distribute d systems to manage the data, model\nparameters, training routines, and so on. Developing and ma intaining such systems requires a\nsigniﬁcant amount of work in both software and hardware engi neering, as well as expertise in deep\nlearning. A related issue is that when the training is scaled up, we need more computing resources\nto ensure the training process can be completed in an accepta ble time. For example, it generally\nrequires hundreds or thousands of GPUs to train an LLM with te ns of billions of parameters\nfrom scratch. This requirement drastically increases the c ost of training such models, especially\nconsidering that many training runs are needed as these mode ls are developed. Also, from the\nperspective of deep learning, the training process can beco me unstable if the neural networks are\nvery deep and/or the model size is very large. In response, we typically need to modify the model\narchitecture to adapt LLMs to large-scale training. In Sect ion2.2we will present more discussions\non these issues.\n\n42 Generative Models\n2.1.3 Fine-tuning LLMs\nOnce we have pre-trained an LLM, we can then apply it to perfor m various NLP tasks. Tradi-\ntionally language models are used as components of other sys tems, for example, they are widely\napplied to score translations in statistical machine trans lation systems. By contrast, in generative\nAI, LLMs are considered complete systems and are employed to address NLP problems by mak-\ning use of their generation nature. A common approach is to de scribe the task we want to address\nin text and then prompt LLMs to generate text based on this des cription. This is a standard text\ngeneration task where we continue or complete the text start ing from a given context.\nMore formally, let x=x0...xmdenote a token sequence of context given by users, and\ny=y1...yndenote a token sequence following the context. Then, the inf erence of LLMs can be\ndeﬁned as a problem of ﬁnding the most likely sequence ybased on x:\nˆy= arg max\nylog Pr( y|x)\n= arg max\nyn∑\ni=1log Pr(yi|x0,...,x m,y1,...,y i−1) (2.15)\nHere∑n\ni=1log Pr(yi|x0,...,x m,y1,...,y i−1)essentially expresses the same thing as the right-\nhand side of Eq. ( 2.2). It models the log probability of predicting tokens from po sitionm+ 1,\nrather than position 0. Throughout this chapter and subsequent ones, we will emplo y separate\nvariables xandyto distinguish the input and output of an LLM, though they can be seen as sub-\nsequences from the same sequence. By adopting such notation , we see that the form of the above\nequation closely resembles those used in other text generat ion models in NLP, such as neural\nmachine translation models.\nTo illustrate how LLMs are applied, consider the problem of d etermining the grammaticality\nfor a given sentence. We can deﬁne a template like this\n{*sentence*}\nQuestion: Is this sentence grammatically correct?\nAnswer:\nHere represents the text we intend to generate. {*sentence*} is a placeholder variable that\nwill be replaced by the actual sentence provided by the users . For example, suppose we have a\nsentence “ John seems happy today. ”. We can replace the {*sentence*} in the template with this\nsentence to have an input to the language model\nJohn seems happy today.\nQuestion: Is this sentence grammatically correct?\nAnswer:\nTo perform the task, the language model is given the context x=“John seems happy today . \\n\nQuestion : Is this sentence grammatically correct? \\n Answer :”4. It then generates the following\n4\\n is a special character used for line breaks.\n\n2.1 A Brief Introduction to LLMs 43\ntext as the answer, based on the context. For example, the lan guage model may output “Yes” (i.e.,\ny= “Yes” ) if this text is the one with the maximum probability of predi ction given this context.\nLikewise, we can deﬁne more templates to address other tasks . For example, we can translate\nan English sentence into Chinese using the following templa te\n{*sentence*}\nQuestion: What is the Chinese translation of this English se ntence?\nAnswer:\nor using an instruction-like template\n{*sentence*",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 39,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "a0dd0ecb9efefc6c",
    "content": " happy today.\nQuestion: Is this sentence grammatically correct?\nAnswer:\nTo perform the task, the language model is given the context x=“John seems happy today . \\n\nQuestion : Is this sentence grammatically correct? \\n Answer :”4. It then generates the following\n4\\n is a special character used for line breaks.\n\n2.1 A Brief Introduction to LLMs 43\ntext as the answer, based on the context. For example, the lan guage model may output “Yes” (i.e.,\ny= “Yes” ) if this text is the one with the maximum probability of predi ction given this context.\nLikewise, we can deﬁne more templates to address other tasks . For example, we can translate\nan English sentence into Chinese using the following templa te\n{*sentence*}\nQuestion: What is the Chinese translation of this English se ntence?\nAnswer:\nor using an instruction-like template\n{*sentence*}\nTranslate this sentence from English into Chinese.\nor using a code-like template.\n[src-lang] = English [tgt-lang] = Chinese [input] = {*sente nce*}\n[output] =\nThe above templates provide a simple but effective method to “prompt” a single LLM to per-\nform various tasks without adapting the structure of the mod el. However, this approach requires\nthat the LLM can recognize and follow the instructions or que stions. One way to do this is to incor-\nporate training samples with instructions and their corres ponding responses into the pre-training\ndataset. While this method is straightforward, building an d training LLMs from scratch is com-\nputationally expensive. Moreover, making instruction-fo llowing data effective for pre-training\nrequires a signiﬁcant amount of such data, but collecting la rge-scale labeled data for all tasks of\ninterest is very difﬁcult.\nA second method, which has been a de facto standard in recent r esearch, is to adapt LLMs\nvia ﬁne-tuning. As such, the token prediction ability learn ed in the pre-training phase can be\ngeneralized to accomplish new tasks. The idea behind ﬁne-tu ning is that some general knowledge\nof language has been acquired in pre-training, but we need a m echanism to activate this knowledge\nfor applying it to new tasks. To achieve this, we can slightly ﬁne-tune the model parameters using\ninstruction-following data. This approach is called instruction ﬁne-tuning .\nAn instruction ﬁne-tuning sample, which is represented by a sequence of tokens, can be seen\nas a tuple consisting of an input and the desired output. Here , the input includes instructions,\nsystem information (or system preﬁx), and any other user-pr ovided information5. To illustrate,\nconsider the following examples (blue text = input and under lined text = output).\n5System information refers to a sequence of tokens added at th e beginning of an input in order to guide the behavior\nof an LLM, such as, you are a helpful assistant and should not output toxic conte nt.\n\n44 Generative Models\nRead the following sentence, and answer whether it is gramma tically correct.\nLLMs are powerful models but are expensive to build.\nYes\nLLMs are powerful models but are expensive to build.\nDoes this sentence make sense grammatically?\nAnswer Option\n- Yes\n- No\nYes\nYes or No? Yes = the following sentence is grammatically corr ect. No = it con-\ntains grammatical errors.\nLLMs are powerful models but are expensive to build.\nYes\nAll these samples describe the same binary classiﬁcation ta sk, but with different instructions.\nTo increase the diversity in the ﬁne-tuning data and make LLM s generalize, we can deﬁne more\ntasks using instructions.\nWrite a short story about three characters.\nCharacter 1: A boy whose name is Andy.\nCharacter 2: Andy’s dog.\nCharacter 3: Andy’s friend, Jane.\nIntheheart ofabustling city, Andy, hisloyal dog Rocket, and hisfriend\nJane discovered ahidden garden beneath theglow oftwilight. Surrounded by\nancient trees andwhispering winds, they shared dreams andsecrets, forging an\nunbreakable bond. Asthenight deepened, they made apact under thestars to\nalways ﬁndmagic inthemundane. Together, they realized thattrueadventure lay\ninthemoments spent with friends, andevery daywas atreasure waiting tobe\ndiscovered.\n\n2.1 A Brief Introduction to LLMs 45\nShow me a recipe for making ice cream.\nIngredients:\n2 cups heavy cream; 1 cup milk; 3/4 cup sugar; ",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 40,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "a9380858bbdcd3b5",
    "content": " instructions.\nWrite a short story about three characters.\nCharacter 1: A boy whose name is Andy.\nCharacter 2: Andy’s dog.\nCharacter 3: Andy’s friend, Jane.\nIntheheart ofabustling city, Andy, hisloyal dog Rocket, and hisfriend\nJane discovered ahidden garden beneath theglow oftwilight. Surrounded by\nancient trees andwhispering winds, they shared dreams andsecrets, forging an\nunbreakable bond. Asthenight deepened, they made apact under thestars to\nalways ﬁndmagic inthemundane. Together, they realized thattrueadventure lay\ninthemoments spent with friends, andevery daywas atreasure waiting tobe\ndiscovered.\n\n2.1 A Brief Introduction to LLMs 45\nShow me a recipe for making ice cream.\nIngredients:\n2 cups heavy cream; 1 cup milk; 3/4 cup sugar; 2 tsp vanilla ext ract;\nPinch of salt.\nSteps:\n1. Whisk all ingredients until sugar dissolves.\n2. Chill mixture for 2 hours in the fridge.\n3. Freeze in a shallow dish, stirring every 30 minutes, 3-4 ti mes.\n4. Freeze solid for 2 hours.\n5. Serve your homemade ice cream.\nIf you buy 5 apples and each apple costs $1.20, how much do you s pend in total?\n$6.00\nWrite a Python program to calculate the sum of squares of the f ollowing numbers.\n1 , 2 , 10 , -9 , 78\nnumbers = [1,2,10,-9 ,78]\nsum_of_squares = sum(x**2 for x in numbers)\nprint(sum_of_squares)\nTo acquire instruction-following abilities, a certain amo unt of ﬁne-tuning data is required. This\ndata may include diverse instructions and possible respons es. It has been found that scaling the\nnumber of ﬁne-tuning tasks is beneﬁcial for improving the pe rformance of LLMs [ Chung et al. ,\n2022 ]. Note that although more ﬁne-tuning data is favorable, the amount of this data is generally\norders of magnitude smaller than that of the pre-training da ta. For example, LLMs can be ﬁne-\ntuned with tens or hundreds of thousands of samples, or even f ewer if these samples are of high\nquality [ Zhou et al. ,2023a ;Chen et al. ,2023b ], whereas pre-training such models may require\nbillions or trillions of tokens, resulting in signiﬁcantly larger computational demands and longer\ntraining times [ Touvron et al. ,2023a ].\nIt is also worth noting that we should not expect the ﬁne-tuni ng data to cover all the down-\nstream tasks to which we intend to apply LLMs. A common unders tanding of how the pre-training\n+ ﬁne-tuning approach works is that LLMs have gained knowled ge for understanding instructions\nand generating responses in the pre-training phase. Howeve r, these abilities are not fully activated\nuntil we introduce some form of supervision. The general ins truction-following behavior emerges\nas we ﬁne-tune the models with a relatively small amount of la beled data. As a result, we can\nachieve some level of zero-shot learning : the ﬁne-tuned models can handle new tasks that they\nhave not been explicitly trained or ﬁne-tuned for [ Sanh et al. ,2022 ;Wei et al. ,2022a ]. This zero-\nshot learning ability distinguishes generative LLMs from e arlier pre-trained models like BERT,\nwhich are primarily ﬁne-tuned for speciﬁc tasks.\nOnce we have prepared a collection of instruction-describe d data, the ﬁne-tuning process is\nrelatively simple. This process can be viewed as a standard t raining process as pre-training, but on\na much smaller training dataset. Let Dtunebe the ﬁne-tuning dataset and ˆθbe the model parameters\n\n46 Generative Models\noptimized via pre-training. We can modify Eq. ( 2.14) to obtain the objective of ﬁne-tuning\n˜θ= arg max\nˆθ+∑\nsample ∈DtuneLˆθ+(sample) (2.16)\nHere ˜θdenotes the optimal parameters. The use of notation ˆθ+means that the ﬁne-tuning starts\nwith the pre-trained parameters ˆθ.\nFor each sample ∈ D tune, we divide it into an input segment xsample and an output segment\nysample",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 41,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "cb9ebc723b82f2f1",
    "content": "��c tasks.\nOnce we have prepared a collection of instruction-describe d data, the ﬁne-tuning process is\nrelatively simple. This process can be viewed as a standard t raining process as pre-training, but on\na much smaller training dataset. Let Dtunebe the ﬁne-tuning dataset and ˆθbe the model parameters\n\n46 Generative Models\noptimized via pre-training. We can modify Eq. ( 2.14) to obtain the objective of ﬁne-tuning\n˜θ= arg max\nˆθ+∑\nsample ∈DtuneLˆθ+(sample) (2.16)\nHere ˜θdenotes the optimal parameters. The use of notation ˆθ+means that the ﬁne-tuning starts\nwith the pre-trained parameters ˆθ.\nFor each sample ∈ D tune, we divide it into an input segment xsample and an output segment\nysample , that is,\nsample = [ ysample,xsample ] (2.17)\nWe then deﬁne the loss function to be\nLˆθ+(sample) = −log Pr ˆθ+(ysample |xsample ) (2.18)\nIn other words, we compute the loss over the sub-sequence ysample , rather than the entire sequence.\nIn a practical implementation of back-propagation for this equation, the sequence [ysample,xsample ]\nis constructed in the forward pass as usual. However, in the b ackward pass, error gradients are\npropagated back only through the parts of the network that co rrespond to ysample , leaving the rest\nof the network unchanged. As an example, consider a sequence\n⟨s⟩Square this number . 2 .  \nContext (Input)The result is 4 .\nPrediction (Output)\nThe loss is calculated and back propagated only for The resul t is4..\nInstruction ﬁne-tuning also requires substantial enginee ring work. In order to achieve satis-\nfactory results, one may experiment with different setting s of the learning rate, batch size, number\nof ﬁne-tuning steps, and so on. This typically requires many ﬁne-tuning runs and evaluations. The\ncost and experimental effort of ﬁne-tuning remain critical and should not be overlooked, though\nthey are much lower than those of the pre-training phase.\nWhile we focus on instruction ﬁne-tuning for an illustrativ e example here, ﬁne-tuning tech-\nniques play an important role in developing various LLMs and are more widely used. Examples\ninclude ﬁne-tuning LLMs as chatbots using dialog data, and a dapting these models to handle very\nlong sequences. The wide application of ﬁne-tuning has led r esearchers to improve these tech-\nniques, such as designing more efﬁcient ﬁne-tuning algorit hms. While the research on ﬁne-tuning\nis fruitful, in this section we just give a ﬂavour of the key st eps involved. We will see more detailed\ndiscussions on this topic in the following chapters.\n2.1.4 Aligning LLMs with the World\nInstruction ﬁne-tuning provides a simple way to adapt LLMs t o tasks that can be well deﬁned. This\nproblem can broadly be categorized as an alignment problem. Here, alignment is referred to as a\nprocess of guiding LLMs to behave in ways that align with huma n intentions. The guidance can\ncome from labeled data, human feedback, or any other form of h uman preferences. For example,\n\n2.1 A Brief Introduction to LLMs 47\nwe want LLMs not only to be accurate in following instruction s, but also to be unbiased, truthful,\nand harmless. So we need to supervise the models towards huma n values and expectations. A\ncommon example is that when we ask an LLM how to build a weapon, it may provide a list of key\nsteps to do so if it is not carefully aligned. However, a respo nsible model should recognize and\navoid responding to requests for harmful or illegal informa tion. Alignment in this case is crucial\nfor ensuring that LLMs act responsibly and in accordance wit h ethical guidelines.\nA related concept to alignment is AI safety. One ultimate goa l of AI is to build intelligent\nsystems that are safe and socially beneﬁcial. To achieve thi s goal we should keep these systems\nrobust, secure, and subjective, in any conditions of real-w orld use, even in conditions of misuse\nor adverse use",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 42,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "53515df33bede785",
    "content": "LMs 47\nwe want LLMs not only to be accurate in following instruction s, but also to be unbiased, truthful,\nand harmless. So we need to supervise the models towards huma n values and expectations. A\ncommon example is that when we ask an LLM how to build a weapon, it may provide a list of key\nsteps to do so if it is not carefully aligned. However, a respo nsible model should recognize and\navoid responding to requests for harmful or illegal informa tion. Alignment in this case is crucial\nfor ensuring that LLMs act responsibly and in accordance wit h ethical guidelines.\nA related concept to alignment is AI safety. One ultimate goa l of AI is to build intelligent\nsystems that are safe and socially beneﬁcial. To achieve thi s goal we should keep these systems\nrobust, secure, and subjective, in any conditions of real-w orld use, even in conditions of misuse\nor adverse use. For LLMs, the safety can be increased by align ing them with appropriate human\nguidance, such as human labeled data and interactions with u sers during application.\nAlignment is difﬁcult as human values and expectations are d iverse and shifting. Sometimes,\nit is hard to describe precisely what humans want, unless we s ee the response of LLMs to user\nrequests. This makes alignment no longer a problem of tuning LLMs on predeﬁned tasks, but a\nbigger problem of training them with the interactions with t he real world.\nAs a result of the concerns with controlling AI systems, ther e has been a surge in research\non the alignment issue for LLMs. Typically, two alignment st eps are adopted after LLMs are\npre-trained on large-scale unlabeled data.\n•Supervised Fine-tuning (SFT). This involves continuing the training of pre-trained LLM s\non new, task-oriented, labelled data. A commonly used SFT te chnique is instruction ﬁne-\ntuning. As described in the previous subsection, by learnin g from instruction-response\nannotated data, LLMs can align with the intended behaviors f or following instructions,\nthereby becoming capable of performing various instructio n-described tasks. Supervised\nﬁne-tuning can be seen as following the pre-training + ﬁne-t uning paradigm, and offers a\nrelatively straightforward method to adapt LLMs.\n•Learning from Human Feedback . After an LLM ﬁnishes pre-training and supervised ﬁne-\ntuning, it can be used to respond to user requests if appropri ately prompted. But this model\nmay generate content that is unfactual, biased, or harmful. To make the LLM more aligned\nwith the users, one simple approach is to directly learn from human feedback. For example,\ngiven some instructions and inputs provided by the users, ex perts are asked to evaluate how\nwell the model responds in accordance with their preference s and interests. This feedback\nis then used to further train the LLM for better alignment.\nA typical method for learning from human feedback is to consi der it as a reinforcement learn-\ning (RL) problem, known as reinforcement learning from human feedback (RLHF ) [Ouyang et al. ,\n2022 ]. The RLHF method was initially proposed to address general sequential decision-making\nproblems [ Christiano et al. ,2017 ], and was later successfully employed in the development of\nthe GPT series models [ Stiennon et al. ,2020 ]. As a reinforcement learning approach, the goal of\nRLHF is to learn a policy by maximizing some reward from the en vironment. Speciﬁcally, two\ncomponents are built in RLHF:\n•Agent . An agent, also called an LM agent, is the LLM that we want to tr ain. This agent\noperates by interacting with its environment: it receives a text from the environment and\n\n48 Generative Models\noutputs another text that is sent back to the environment. Th e policy of the agent is the\nfunction deﬁned by the LLM, that is, Pr(y|x).\n•Reward Model . A reward model is a proxy of the environment. Each time the ag ent\nproduces an output sequence, the reward model assigns this o utput sequence a numerical\nscore (i.e., the reward). This score tells the agent how good the output sequence is.\nIn RLHF, we need to perform two learning tasks: 1) reward mode l learning, which involves\ntraining a reward model using human feedback on the output of the agent, and 2) policy learning,\nwhich involves optimizing a policy guided by the reward mode l using reinforcement learning\nalgorithms. Here is a brief outline of the key steps involved in RLHF.\n• Build an",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 43,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "57f25b6090e50374",
    "content": ". This agent\noperates by interacting with its environment: it receives a text from the environment and\n\n48 Generative Models\noutputs another text that is sent back to the environment. Th e policy of the agent is the\nfunction deﬁned by the LLM, that is, Pr(y|x).\n•Reward Model . A reward model is a proxy of the environment. Each time the ag ent\nproduces an output sequence, the reward model assigns this o utput sequence a numerical\nscore (i.e., the reward). This score tells the agent how good the output sequence is.\nIn RLHF, we need to perform two learning tasks: 1) reward mode l learning, which involves\ntraining a reward model using human feedback on the output of the agent, and 2) policy learning,\nwhich involves optimizing a policy guided by the reward mode l using reinforcement learning\nalgorithms. Here is a brief outline of the key steps involved in RLHF.\n• Build an initial policy using pre-training and instructio n ﬁne-tuning.\n• Use the policy to generate multiple outputs for each input, and then collect human feedback\non these outputs (e.g., comparisons of the outputs).\n• Learn a reward model from the human feedback.\n• Fine-tune the policy with the supervision from the reward m odel.\nFigure 2.2shows an overview of RLHF. Given that this section serves onl y as a brief intro-\nduction to concepts of LLMs, a detailed discussion of RLHF te chniques will not be included. We\ninstead illustrate the basic ideas behind RLHF using a simpl e example.\nSuppose we have trained an LLM via pre-training and instruct ion ﬁne-tuning. This LLM is\ndeployed to respond to requests from users. For example, a us er may input\nHow can I live a more environmentally friendly life?\nWe use the LLM to generate 4 different outputs (denoted by {y1,...,y4}) by sampling the\noutput space\nOutput 1 ( y1): Consider switching to an electric vehicle or bicycle inst ead of\ntraditional cars to reduce carbon emissions and protect our planet.\nOutput 2 ( y2): Adopt a minimalist lifestyle. Own fewer possessions to re duce\nconsumption and the environmental impact of manufacturing and\ndisposal.\nOutput 3 ( y3): Go off-grid. Generate your own renewable energy and colle ct\nrainwater to become completely self-sufﬁcient and reduce r eliance\non non-renewable resources.\nOutput 4 ( y4): Support local farm products to reduce the carbon footprin t of\ntransporting food, while enjoying fresh, healthy food.\n\n2.1 A Brief Introduction to LLMs 49\nLLMPre-training Data\nI love the food here! ...How can I get there? ...SFT Data\nweather in London . ...Write a poem about the\nPre-training &\nSupervised ﬁne-tuning\n(a) Learning an Initial LLMLLMUser Input\nenvironmentally friendly?How can I live moreModel Output\n3. ............ 4. ............1. ............ 2. ............\nPredictingComparisons\ny1≻y4≻y2≻y3\nAnnotating Data with Human Preferences\n(b) Annotating Data with Human Preferences\nReward ModelComparison Data\n{(x,yk1≻yk2)}\nTraining\n(c) Training the Reward ModelLLM\n(Policy)Dataset D\nx∼ DInput-output Pairs\n{x,y}\nSampling yvia the Policy Pr(y|x)Reward ModelReward Scores\n{r(x,y)}\nEvaluate the Input-output Pairs\n(d) Training/Fine-tuning the PolicyRL Fine-tuning\nFig. 2.2: An overview of RLHF. There are 4 key steps involved: a) traini ng an initial LLM (i.e., policy) using pre-\ntraining and supervised ﬁne-tuning; b) collecting human pr eference data by ranking the outputs of the LLM; c) training\na reward model using the ranking results; d) RL ﬁne-tuning of the policy based on the reward model. Double line\narrows mean training or ﬁne-tuning.\nWe then ask annotators to evaluate these outputs. One straig htforward way is to assign a rating\nscore to each output. In this case, the reward model learning problem can be framed as a task of\ntraining a regression model. But giving numerical scores to LLM outputs is not an easy task for\nannotators. It is usually difﬁcult to design an annotation s tandard that all annotators can agree on\nand easily follow. An alternative method, which is more popu lar in the development of LLMs,",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 44,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "79af696fe8f0dc53",
    "content": ": a) traini ng an initial LLM (i.e., policy) using pre-\ntraining and supervised ﬁne-tuning; b) collecting human pr eference data by ranking the outputs of the LLM; c) training\na reward model using the ranking results; d) RL ﬁne-tuning of the policy based on the reward model. Double line\narrows mean training or ﬁne-tuning.\nWe then ask annotators to evaluate these outputs. One straig htforward way is to assign a rating\nscore to each output. In this case, the reward model learning problem can be framed as a task of\ntraining a regression model. But giving numerical scores to LLM outputs is not an easy task for\nannotators. It is usually difﬁcult to design an annotation s tandard that all annotators can agree on\nand easily follow. An alternative method, which is more popu lar in the development of LLMs, is\nto rank these outputs. For example, a possible ranking of the above outputs is\ny1≻y4≻y2≻y3\n\n50 Generative Models\nA reward model is then trained using this ranking result. In g eneral, a reward model in RLHF\nis a language model that shares the same architecture as the t arget LLM, but with a smaller model\nsize. Given the input xand output yk, we concatenate them to form a sequence seqk= [x,yk].\nThis sequence is processed from left to right using forced de coding. Since each position can\nonly access its left context in language modeling, the outpu t of the top-most Transformer layer at\nthe ﬁrst position cannot be used as the representation of the sequence. Instead, a special symbol\n(e.g., ⟨\\s⟩) is added to the end of the sequence, and the corresponding ou tput of the Transformer\nlayer stack is considered as the representation of the entir e sequence. An output layer, such as a\nlinear transformation layer, is built on top of this represe ntation to generate the reward, denoted\nbyR(seqk)orR(x,yk).\nWe train this reward model using ranking loss. For example, a pair-wise ranking loss function\ncan be written in the form\nLoss ω(Dr) = −E(x,yk1,yk2)∼D rlog(Sigmoid( Rω(x,yk1)−Rω(x,yk2))) (2.19)\nwhereωrepresents the parameters of the reward model, and Drrepresents a set of tuples of an\ninput and a pair of outputs. (x,yk1,yk2)∼ D ris a sampling operation which draws a sample\n(x,yk1,yk2)from Drwith some probability. As an example, suppose we ﬁrst draw a m odel\ninput xwith a uniform distribution and then draw a pair of model outp uts with a probability of\nyk1≻yk2given x(denoted by Pr(yk1≻yk2|x)). The corresponding loss function is given by\nLoss ω(Dr)\n=−∑\nPr(x)·Pr(yk1≻yk2|x)·log(Sigmoid( Rω(x,yk1)−Rω(x,yk2)))\n=−1\nK∑\nPr(yk1≻yk2|x)·log(Sigmoid( Rω(x,yk1)−Rω(x,yk2))) (2.20)\nwhereKrepresents the number of model inputs involved in sampling. While the form of these\nfunctions may seem complex, their idea is simple: we penaliz e the model if the predicted ranking\nof two outputs differs from the human-labeled ranking. By co ntrast, the model receives a bonus,\nif the predicted ranking matches the human-labeled ranking .\nWe can train the reward model by minimizing the above ranking loss\nˆω= arg min\nωLoss ω(Dr) (2.21)\nThe resulting model Rˆω(·)can be employed to evaluate any given pair of input and output . Note\nthat although the reward model is trained using a ranking-ba sed objective, it is used for scoring.\nThis allows it to provide continuous supervision signals, w hich is very beneﬁcial for training other\nmodels.\nWe now turn to the policy learning problem. A commonly adopte d objective is to maximize\nthe reward on a set of input-output pairs. Following an analo gous form of Eq. ( 2.16), we obtain a\nsimple training objective for RL ﬁne-tuning\n˜θ= arg max\nˆθ+E(x,yˆθ+)∼D",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 45,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "657759c49a27fb29",
    "content": ", the model receives a bonus,\nif the predicted ranking matches the human-labeled ranking .\nWe can train the reward model by minimizing the above ranking loss\nˆω= arg min\nωLoss ω(Dr) (2.21)\nThe resulting model Rˆω(·)can be employed to evaluate any given pair of input and output . Note\nthat although the reward model is trained using a ranking-ba sed objective, it is used for scoring.\nThis allows it to provide continuous supervision signals, w hich is very beneﬁcial for training other\nmodels.\nWe now turn to the policy learning problem. A commonly adopte d objective is to maximize\nthe reward on a set of input-output pairs. Following an analo gous form of Eq. ( 2.16), we obtain a\nsimple training objective for RL ﬁne-tuning\n˜θ= arg max\nˆθ+E(x,yˆθ+)∼D rlftRˆω(x,yˆθ+) (2.22)\nwhere the optimal parameters ˜θare obtained by ﬁne-tuning the pre-trained parameters ˆθ.Drlftis\n\n2.1 A Brief Introduction to LLMs 51\nthe RL ﬁne-tuning dataset. For each sample (x,yˆθ+),xis sampled from a prepared dataset of\ninput sequences, and yˆθ+is sampled from the distribution Prˆθ+(y|x)given by the policy.\nIn practice, more advanced reinforcement learning algorit hms, such as proximal policy opti-\nmization (PPO ), are often used for achieving more stable training, as well as better performance.\nWe leave the detailed discussion of reinforcement learning algorithms to the following parts of\nthis book where RLHF is extensively used for alignment.\nAn interesting question arises here: why not consider learn ing from human preferences as\na standard supervised learning problem? This question is cl osely related to our aforementioned\ndiscussion on the difﬁculty of data annotation. Often, desc ribing human values and goals is chal-\nlenging, and it is even more difﬁcult for humans to provide ou tputs that are well aligned. As an\nalternative, annotating the preferences of a given list of m odel outputs offers a simpler task. By\ndoing so, we can create a model that understands human prefer ences, which can then be used as\na reward model for training policies. From the perspective o f machine learning, RLHF is par-\nticularly useful for scenarios where the desired behavior o f an agent is difﬁcult to demonstrate\nbut can be easily recognized by humans. Another advantage of RLHF is its ability to explore the\nsample space. By employing sampling techniques, models tra ined with reinforcement learning\ncan venture beyond the annotated data set to explore additio nal samples. This exploratory ability\nallows RLHF to discover potentially beneﬁcial policies tha t are not immediately apparent from\nthe labeled data alone.\n2.1.5 Prompting LLMs\nWe have so far shown that LLMs can be used to perform various ta sks by giving them appropriate\nprompts. There are no restrictions on these prompts, which c an include any information we wish\nto ask or communicate with LLMs, such as natural language ins tructions and the context of con-\nversations. Since this approach requires no additional tra ining or tuning, adapting LLMs becomes\nhighly efﬁcient once they are developed. This somewhat inﬂu ences the paradigms in NLP: we\nno longer need to develop speciﬁc systems for individual tas ks but can instead use a single, well-\ntrained LLM to perform different tasks by prompting it. An ap pealing aspect of LLM prompting\narises as a result: users can easily have “customized” syste ms by designing their own prompts\nfor LLMs. Given the important role played by prompting in LLM s,prompting engineering has\nbecome a very active area of research in NLP.\nThe term prompt is used in many different ways in the literature. In this chap ter, this term\nrefers to the entire input to LLMs, and so we use the terms prompt andmodel input interchange-\nably. Before discussing prompting further, let us ﬁrst see a few examples where the prompts\nare more complex than those presented in the previous subsec tions. Note that this subsection is\nnot aimed at writing high-quality prompts but rather at high lighting some interesting issues in\nprompting LLMs.\nOne of the popular ways to use LLMs is to assign them a “role” pl ayed in generating responses.\nFor example, LLMs can act",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 46,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "cacdc51ac1469c9f",
    "content": "LM prompting\narises as a result: users can easily have “customized” syste ms by designing their own prompts\nfor LLMs. Given the important role played by prompting in LLM s,prompting engineering has\nbecome a very active area of research in NLP.\nThe term prompt is used in many different ways in the literature. In this chap ter, this term\nrefers to the entire input to LLMs, and so we use the terms prompt andmodel input interchange-\nably. Before discussing prompting further, let us ﬁrst see a few examples where the prompts\nare more complex than those presented in the previous subsec tions. Note that this subsection is\nnot aimed at writing high-quality prompts but rather at high lighting some interesting issues in\nprompting LLMs.\nOne of the popular ways to use LLMs is to assign them a “role” pl ayed in generating responses.\nFor example, LLMs can act as a psychologist when answering qu estions.\n\n52 Generative Models\nPlease explain what delayed gratiﬁcation is?\nNote, you are a researcher with a deep background in psycholo gy. Your own\nﬁeld of study is developmental psychology in children, with a focus on aspects\nof control and intelligence. You are very familiar with the c oncept of delayed\ngratiﬁcation and have applied it to your own research. You ha ve published papers\non delayed gratiﬁcation and conducted related experiments .\nDelayed gratiﬁcation istheprocess ofresisting animmediate reward in\nanticipation ofreceiving amore valuable reward inthefuture. Itisasigniﬁcant\nconcept indevelopmental psychology, especially inrelation tohow children learn\nself-control anddevelop cognitive abilities. Research shows thattheability to\ndelay gratiﬁcation isassociated with positive outcomes inacademic performance,\nsocial competence, andemotional well-being. Studies andexperiments inthis\narea highlight theimportance ofcultivating this skill early inlifetosupport\nlong-term success andresilience.\nAnother example is the use of LLMs in detecting and correctin g errors such as syntactic or\nsemantic mistakes in text. For an LLM which is trained on both code and natural language data,\nwe may use it for code debugging6.\nFix the bugs in this C language program.\n#include<stdio.h>\nint main() {\nprintg(\"Hello, World!\")\nreturn 0;\n}\n1. The function name ’printg’ is incorrect . It should be ’pri ntf’.\n2. There is a missing semicolon at the end of the ’printf’ func tion call.\nHere is the corrected version of the program:\n#include<stdio.h>\nint main() {\nprintf(\"Hello, World!\");\nreturn 0;\n}\nAlso, LLMs can be ﬁne-tuned to act as a conversation partner7.\n6In this example, the code is not tokenized for easier reading .\n7To ﬁne-tune an LLM for multi-turn dialogue, one needs to cons ider conversation history in the context for pre-\ndicting the response in the current round of conversation. T his makes the actual prompt used in response generation\nrelatively longer than that used in single-turn dialogue.\n\n2.1 A Brief Introduction to LLMs 53\nHi! I’m an assistant. How can I help you?\nWho won the FIFA World Cup 2022?\nArgentina won the FIFA World Cup 2022.\nWhere was it held?\nThe 2022 FIFA World Cup was held in Qatar.\nHow many times has Argentina won the World Cup?\nArgentina has won the FIFA World Cup three times.\n...\nThese examples and previous ones have shown that appropriat e responses can be generated\nvia prompts involving clear instructions and questions. Ho wever, when problem solving requires\nknowledge that is not explicitly speciﬁed, LLMs may make mis takes, even though the instructions\nare sufﬁciently clear and precise. A family of challenging t asks for LLMs involves arithmetic\nreasoning and commonsense reasoning. For example, we can as k an LLM to solve primary school\nmath problems presented in natural language.\nJack has 7 apples. He ate 2 of them for dinner, but then his mom g ave him 5 more\napples. The next day, Jack gave 3 apples to his friend John. Ho w many apples\ndoes Jack have left in the end?\nThe answer is 10.\nThe correct answer should be 7, so the model output is incorre ct.\nOne approach to addressing such issues is to incorporate lea rning into prompts, called in-\ncontext learning or (ICL). The idea of ICL is to demonstrate the ways to solve problems in\nprompts, and condition predictions on these demonstration",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 47,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "81dd2c86e40dd1ac",
    "content": " speciﬁed, LLMs may make mis takes, even though the instructions\nare sufﬁciently clear and precise. A family of challenging t asks for LLMs involves arithmetic\nreasoning and commonsense reasoning. For example, we can as k an LLM to solve primary school\nmath problems presented in natural language.\nJack has 7 apples. He ate 2 of them for dinner, but then his mom g ave him 5 more\napples. The next day, Jack gave 3 apples to his friend John. Ho w many apples\ndoes Jack have left in the end?\nThe answer is 10.\nThe correct answer should be 7, so the model output is incorre ct.\nOne approach to addressing such issues is to incorporate lea rning into prompts, called in-\ncontext learning or (ICL). The idea of ICL is to demonstrate the ways to solve problems in\nprompts, and condition predictions on these demonstration s. Here is an example where a similar\nproblem and the corresponding answer are presented in the pr ompt (green = demonstrations).\nTom has 12 marbles. He wins 7 more marbles in a game with his fri end but then\nloses 5 marbles the next day. His brother gives him another 3 m arbles as a gift.\nHow many marbles does Tom have now?\nThe answer is 17.\nJack has 7 apples. He ate 2 of them for dinner, but then his mom g ave him 5 more\napples. The next day, Jack gave 3 apples to his friend John. Ho w many apples\ndoes Jack have left in the end?\nThe answer is 12.\nBut the LLM still made mistakes this time. A reason for this mi ght be that solving math\nproblems does not only involve problem-answer mappings but also, to a larger extent, the under-\nlying logical inference in multiple steps. A method to impro ve the inference abilities of LLMs\nischain-of-thought prompting (COT prompting ) [Wei et al. ,2022c ]. In COT prompting, we\ndecompose complex reasoning problems into multiple proble m-solving intermediate steps. These\n\n54 Generative Models\nsteps are demonstrated in prompts so that LLMs can be prompte d to learn to mimic the underlying\nthought processes in solving the problems. COT prompting ha s been found to be very useful for\nachieving good results on challenging tasks, such as the GSM 8K mathematical reasoning bench-\nmark.\nConsider the above demonstration example Tom has 12 marbles. He wins 7 more marbles in\na game with his friend but then loses 5 marbles the next day. Hi s brother gives him another 3\nmarbles as a gift. How many marbles does Tom have now ? . The corresponding logical reasoning\nsteps can be structured:\n1. Tom starts with 12 marbles.\n2. He wins 7 more marbles in a game, so we add those to his initia l count: 12 marbles + 7\nmarbles = 19 marbles.\n3. The next day, Tom loses 5 marbles, so we subtract those from his current total: 19 marbles\n- 5 marbles = 14 marbles.\n4. His brother gives him 3 more marbles, so we add those to his c urrent total: 14 marbles + 3\nmarbles = 17 marbles.\n5. After all these events, Tom has 17 marbles.\nWe can add these reasoning steps into the prompt, and get the c orrect answer.\nTom has 12 marbles. He wins 7 more marbles in a game with his fri end but then\nloses 5 marbles the next day. His brother gives him another 3 m arbles as a gift.\nHow many marbles does Tom have now?\nTom initially has 12 marbles. After winning a game, he gains 7 more, increasing\nhis total to 19 marbles. He loses 5 marbles the next day, bring ing his total down\nto 14 marbles. His brother gifts him 3 more marbles, increasi ng his total to 17\nmarbles. Therefore, Tom now has 17 marbles. So the answer is 1 7.\nJack has 7 apples. He ate 2 of them for dinner, but then his mom g ave him 5 more\napples. The next day, Jack gave 3 apples to his friend John. Ho w many apples\ndoes Jack have left in the end?\nJack starts with 7apples. Heeats 2apples fordinner, sowesubtract 2from 7,\nleaving him with 5apples. Hismom gives him ",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 48,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "67ab9cd77a7f7aa7",
    "content": ".\nHow many marbles does Tom have now?\nTom initially has 12 marbles. After winning a game, he gains 7 more, increasing\nhis total to 19 marbles. He loses 5 marbles the next day, bring ing his total down\nto 14 marbles. His brother gifts him 3 more marbles, increasi ng his total to 17\nmarbles. Therefore, Tom now has 17 marbles. So the answer is 1 7.\nJack has 7 apples. He ate 2 of them for dinner, but then his mom g ave him 5 more\napples. The next day, Jack gave 3 apples to his friend John. Ho w many apples\ndoes Jack have left in the end?\nJack starts with 7apples. Heeats 2apples fordinner, sowesubtract 2from 7,\nleaving him with 5apples. Hismom gives him 5more apples, adding 5tohis\ncurrent total. Now hehas5+5=10apples. Thenext day, Jack gives 3apples to\nhisfriend John. Subtracting 3from 10,Jack isleftwith 7apples. Therefore, Jack\nhas7apples left. Sotheanswer is7.\nThis approach is also called one-shot COT prompting . Likewise, we can add more demon-\nstrations to have a few-shot COT prompting approach.\nFurthermore, a special kind of COT, zero-shot COT , introduces instructions that provoke\nLLMs to think, without presenting any intermediate reasoni ng steps in the prompts [ Kojima et al. ,\n\n2.1 A Brief Introduction to LLMs 55\n2022 ]. An interesting example is that by appending a simple sente nce “Let’s think step by step.”\nto the end of a prompt, LLMs can generate reasoning steps to re ach the ﬁnal answer.\nJack has 7 apples. He ate 2 of them for dinner, but then his mom g ave him 5 more\napples. The next day, Jack gave 3 apples to his friend John. Ho w many apples\ndoes Jack have left in the end?\nLet’s think step by step.\n1. Initial Quantity: Jack starts with 7 apples.\n2. After Dinner: He eats 2 apples, so 7−2 = 5 apples remain.\n3. His Mom Gives More: His mom gives him 5 more apples, so 5 + 5 = 10\napples are now with him.\n4. Giving to John: The next day, Jack gives 3 apples to his frie nd John,\nso10−3 = 7 apples are left.\nIn the end, Jack has 7 apples left.\nZero-shot, one-shot, and few-shot learning are common conc epts in the area of in-context\nlearning for LLMs and are not restricted to COT prompting. Br oadly speaking, any prompting\nthat involves only simple instructions without any demonst rations can be considered a form of\nzero-shot learning. This zero-shot learning ability emerg es as LLMs are pre-trained and/or ﬁne-\ntuned. Also, one-shot and few-shot learning methods are mor e often considered when LLMs do\nnot acquire the corresponding zero-shot learning ability. These methods are therefore important\nfor in-context learning when addressing new tasks. Example s include those for performing various\nNLP tasks by demonstrating task-formatted samples. See the following examples for sentiment\nsentence classiﬁcation and phrase translation via few-sho t learning.\nGiven the following text snippets, classify their sentimen t as Positive, Negative,\nor Neutral.\nExample 1: “I had an amazing day at the park!”\nSentiment: Positive\nExample 2: “The service at the restaurant was terrible.”\nSentiment: Negative\nExample 3: “I think it’s going to rain today.”\nSentiment: Neutral\nText: “This movie was a fantastic journey through imaginati on.”\nSentiment: Positive\n\n56 Generative Models\nTranslate the following Chinese phrases into English.\nExample 1: “你好”\nTranslation: “Hello”\nExample 2: “谢谢你”\nTranslation: “Thank you”\nPhrase to translate: “ 早上好”\nTranslation: “Good Morning”\nAbove, we have presented examples to illustrate the fundame ntal in-context learning capa-\nbilities of prompting LLMs. This section, however, does not include more advanced prompting\ntechniques in order to keep the content concise and compact. More discussions on prompting can\nbe found in Chapter 3.\n2.2 Training at Scale\nAs a ﬁrst step in developing LLMs, we need to train these model",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 49,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "3b437d8ae7d1a919",
    "content": "The service at the restaurant was terrible.”\nSentiment: Negative\nExample 3: “I think it’s going to rain today.”\nSentiment: Neutral\nText: “This movie was a fantastic journey through imaginati on.”\nSentiment: Positive\n\n56 Generative Models\nTranslate the following Chinese phrases into English.\nExample 1: “你好”\nTranslation: “Hello”\nExample 2: “谢谢你”\nTranslation: “Thank you”\nPhrase to translate: “ 早上好”\nTranslation: “Good Morning”\nAbove, we have presented examples to illustrate the fundame ntal in-context learning capa-\nbilities of prompting LLMs. This section, however, does not include more advanced prompting\ntechniques in order to keep the content concise and compact. More discussions on prompting can\nbe found in Chapter 3.\n2.2 Training at Scale\nAs a ﬁrst step in developing LLMs, we need to train these model s on large amounts of data.\nThe training task is itself standard: the objective is to max imize the likelihood, which can be\nachieved via gradient descent. However, as we scale up both t he model size and the amount\nof data, the problem becomes very challenging, for example, large models generally make the\ntraining unstable. In this section, we discuss several issu es of large-scale training for LLMs,\nincluding data preparation, model modiﬁcation, and distri buted training. We also discuss the\nscaling laws for LLMs, which help us understand their traini ng efﬁciency and effectiveness.\n2.2.1 Data Preparation\nThe importance of data cannot be overstated in NLP. As larger neural networks are developed,\nthe demand for data continues to increase. For example, deve loping LLMs may require trillions\nof tokens in pre-training (see Table 2.3), orders of magnitude larger than those used in training\nconventional NLP models. In general, we may want to gather as much training data as possible.\nHowever, larger training datasets do not mean better traini ng results, and the development of\nLLMs raises new issues in creating or collecting these datas ets.\nA ﬁrst issue is the quality of data. High-quality data has lon g been seen as crucial for training\ndata-driven NLP systems. Directly using raw text from vario us sources is in general undesirable.\nFor example, a signiﬁcant portion of the data used to train re cent LLMs comes from web scraping,\nwhich may contain errors and inappropriate content, such as toxic information and fabricated\nfacts. Also, the internet is ﬂooded with machine-generated content due to the widespread use of\nAI, presenting further challenges for processing and using web-scraped data. Researchers have\nfound that training LLMs on unﬁltered data is harmful [ Raffel et al. ,2020 ]. Improving data quality\ntypically involves incorporating ﬁltering and cleaning st eps in the data processing workﬂow. For\nexample, Penedo et al. [2023 ] show that by adopting a number of data processing technique s, 90%\nof their web-scraped data can be removed for LLM training. In addition to large-scale web-scraped\ndata, LLM training data often includes books, papers, user- generated data on social media, and\nso on. Most of the latest LLMs are trained on such combined dat asets, which are found to be\n\n2.2 Training at Scale 57\nLLM # of Tokens Data\nGPT3-175B [ Brown et al. ,2020 ] 0.5T Webpages, Books, Wikipedia\nFalcon-180B [ Almazrouei et al. ,2023 ] 3.5T Webpages, Books, Conversations,\nCode, Technical Articles\nLLaMA2-65B [ Touvron et al. ,2023a ]1.0T ∼1.4T Webpages, Code, Wikipedia,\nBooks, Papers, Q&As\nPaLM-450B [ Chowdhery et al. ,2022 ] 0.78T Webpages, Books, Conversations,\nCode, Wikipedia, News\nGemma-7B [ Gemma Team ,2024 ] 6T Webpages, Mathematics, Code\nTable 2.3: Amounts of training data used in some LLMs in terms of the numb er of tokens.\nimportant for the strong performance of the resulting model s.\nA second issue is the diversity of data. We want the training d ata to cover as many types of\ndata as possible, so that the trained models can adapt to diff erent downstream tasks easily. It has",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 50,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "37a26e6fd1d3149a",
    "content": " ] 3.5T Webpages, Books, Conversations,\nCode, Technical Articles\nLLaMA2-65B [ Touvron et al. ,2023a ]1.0T ∼1.4T Webpages, Code, Wikipedia,\nBooks, Papers, Q&As\nPaLM-450B [ Chowdhery et al. ,2022 ] 0.78T Webpages, Books, Conversations,\nCode, Wikipedia, News\nGemma-7B [ Gemma Team ,2024 ] 6T Webpages, Mathematics, Code\nTable 2.3: Amounts of training data used in some LLMs in terms of the numb er of tokens.\nimportant for the strong performance of the resulting model s.\nA second issue is the diversity of data. We want the training d ata to cover as many types of\ndata as possible, so that the trained models can adapt to diff erent downstream tasks easily. It has\nbeen widely recognized that the quality and diversity of tra ining data both play very important\nroles in LLMs. An interesting example is that incorporating programming code into training data\nhas been found to be beneﬁcial for LLMs. The beneﬁts are demon strated not only in enhancing the\nprogramming abilities of LLMs, but also in improving reason ing for complex problems, especially\nthose requiring COT prompting. The concept “diversity” can be extended to include language\ndiversity as well. For example, many LLMs are trained on mult i-lingual data, and therefore we\ncan handle multiple languages using a single model. While th is approach shows strong abilities\nin multi-lingual and cross-lingual tasks, its performance on speciﬁc languages largely depends on\nthe volume and quality of the data for those languages. It has been shown in some cases to provide\npoor results for low-resource languages.\nA third issue is the bias in training data. This is not a proble m that is speciﬁc to LLMs but\nexists in many NLP systems. A common example is gender bias, w here LLMs show a preference\nfor one gender over another. This can partly be attributed to class imbalance in the training data,\nfor example, the term nurses is more often associated with women. In order to debias the da ta,\nit is common practice to balance the categories of different language phenomena, such as gender,\nethnicity, and dialects. The bias in data is also related to t he diversity issue mentioned above.\nFor example, since many LLMs are trained and aligned with Eng lish-centric data, they are bi-\nased towards the cultural values and perspectives prevalen t among English-speaking populations.\nIncreasing language diversity in training data can somewha t mitigate the bias.\nAnother issue with collecting large-scale data is the priva cy concern. If LLMs are trained\non data from extensive sources, this potentially leads to ri sks regarding the exposure of sensitive\ninformation, such as intellectual property and personal da ta. This is particularly concerning given\nthe capacity of LLMs to represent patterns from the data they are trained on, which might in-\nadvertently involve memorizing and reproducing speciﬁc de tails. A simple approach to privacy\nprotection is to remove or anonymize sensitive information . For example, anonymization tech-\nniques can be applied to remove personally identiﬁable info rmation from training data to prevent\nLLMs from learning from such data. However, in practice, era sing or redacting all sensitive data\nis difﬁcult. Therefore, many LLMs, particularly those laun ched for public service, typically work\nwith systems that can detect the potential exposure of sensi tive data, or are ﬁne-tuned to reject\n\n58 Generative Models\ncertain requests that could lead to information leakage.\n2.2.2 Model Modiﬁcations\nTraining LLMs is difﬁcult. A commonly encountered problem i s that the training process be-\ncomes more unstable as LLMs get bigger. For example, one need s to choose a small learning rate\nto achieve stable training with gradient descent, but this i n turn results in much longer training\ntimes. Sometimes, even when the training conﬁguration is ca refully designed, training may di-\nverge at certain points during optimization. The training o f LLMs is generally inﬂuenced by many\nfactors, such as parameter initialization, batching, and r egularization. Here, we focus on common\nmodiﬁcations and improvements to the standard Transformer architecture, which are considered\nimportant in developing trainable LLMs.\n2.2.2.1 Layer Normalization with",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 51,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "e75c1f9b32023885",
    "content": "58 Generative Models\ncertain requests that could lead to information leakage.\n2.2.2 Model Modiﬁcations\nTraining LLMs is difﬁcult. A commonly encountered problem i s that the training process be-\ncomes more unstable as LLMs get bigger. For example, one need s to choose a small learning rate\nto achieve stable training with gradient descent, but this i n turn results in much longer training\ntimes. Sometimes, even when the training conﬁguration is ca refully designed, training may di-\nverge at certain points during optimization. The training o f LLMs is generally inﬂuenced by many\nfactors, such as parameter initialization, batching, and r egularization. Here, we focus on common\nmodiﬁcations and improvements to the standard Transformer architecture, which are considered\nimportant in developing trainable LLMs.\n2.2.2.1 Layer Normalization with Residual Connections\nLayer normalization is used to stabilize training for deep n eural networks. It is a process of\nsubtracting the mean and dividing by the standard deviation . By normalizing layer output in\nthis way, we can effectively reduce the covariate shift prob lem and improve the training stability.\nIn Transformers, layer normalization is typically used tog ether with residual connections. As\ndescribed in Section 2.1.1 , a sub-layer can be based on either the post-norm architectu re, in which\nlayer normalization is performed right after a residual blo ck, or the pre-norm architecture, in\nwhich layer normalization is performed inside a residual bl ock. While both of these architectures\nare widely used in Transformer-based systems [ Wang et al. ,2019 ], the pre-norm architecture has\nproven to be especially useful in training deep Transformer s. Given this, most LLMs are based on\nthe pre-norm architecture, expressed as output = LNorm( F(input)) + input .\nA widely-used form of the layer normalization function is gi ven by\nLNorm( h) =α·h−µ\nσ+ǫ+β (2.23)\nwhere his ad-dimensional real-valued vector, µis the mean of all the entries of h, andσis the\ncorresponding standard deviation. ǫis introduced for the sake of numerical stability. α∈Rdand\nβ∈Rdare the gain and bias terms.\nA variant of layer normalization, called root mean square (R MS) layer normalization, only\nre-scales the input vector but does not re-center it [ Zhang and Sennrich ,2019 ]. The RMS layer\nnormalization function is given by\nLNorm( h) =α·h\nσrms+ǫ+β (2.24)\nwhereσrmsis the root mean square of h, that is,σrms= (1\nd∑d\nk=1h2\nk)1\n2. This layer normalization\nfunction is used in LLMs like the LLaMA series.\n\n2.2 Training at Scale 59\n2.2.2.2 Activation Functions in FFNs\nIn Transformers, FFN sub-layers are designed to introduce n on-linearities into representation\nlearning, and are found to be useful for preventing the repre sentations learned by self-attention\nfrom degeneration8[Dong et al. ,2021 ]. A standard form of the FFNs used in these sub-layers can\nbe expressed as\nFFN( h) =σ(hW h+bh)Wf+bf (2.25)\nwhere Wh∈Rd×dh,bh∈Rdh,Wf∈Rdh×d, and bf∈Rdare the parameters, and dhis the\nhidden size. σ(·)is the activation function of the hidden layer. A common choi ce forσ(·)is the\nrectiﬁed linear unit (ReLU ), given by\nσrelu(h) = max(0 ,h) (2.26)\nIn practical implementations, increasing dhis helpful and thus it is often set to a larger number\nin LLMs. But a very large hidden size poses challenges for bot h training and deployment. In this\ncase, the design of the activation function plays a relative ly more important role in wide FFNs.\nThere are several alternatives to the ReLU in LLMs. One of the se is the gaussian error linear\nunit (GeLU ) which can be seen as a smoothed version of the ReLU. Rather th an controlling the\noutput by the sign of the input, the GeLU function weights its input by the percentile Pr(h≤h).\nHerehis ad-dimensional vector whose entries are drawn from the standa rd normal distribution\n",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 52,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "a4a6695703c887e5",
    "content": " the activation function of the hidden layer. A common choi ce forσ(·)is the\nrectiﬁed linear unit (ReLU ), given by\nσrelu(h) = max(0 ,h) (2.26)\nIn practical implementations, increasing dhis helpful and thus it is often set to a larger number\nin LLMs. But a very large hidden size poses challenges for bot h training and deployment. In this\ncase, the design of the activation function plays a relative ly more important role in wide FFNs.\nThere are several alternatives to the ReLU in LLMs. One of the se is the gaussian error linear\nunit (GeLU ) which can be seen as a smoothed version of the ReLU. Rather th an controlling the\noutput by the sign of the input, the GeLU function weights its input by the percentile Pr(h≤h).\nHerehis ad-dimensional vector whose entries are drawn from the standa rd normal distribution\nGaussian(0,1)9. Speciﬁcally, the GeLU function is deﬁned to be\nσgelu(h) = hPr(h≤h)\n=hΦ(h) (2.27)\nwhere Φ(h)is the cumulative distribution function of Gaussian(0,1), which can be implemented\nin convenient ways [ Hendrycks and Gimpel ,2016 ]. The GeLU function has been adopted in\nseveral LLMs, such as BERT, GPT-3, and BLOOM.\nAnother family of activation functions which is popular in L LMs is gated linear unit (GLU )-\nbased functions. The basic form of GLUs is given by\nσglu(h) =σ(hW 1+b1)⊙(W2+b2) (2.28)\nwhere W1∈Rd×d,b1∈Rd,W2∈Rd×d, and b2∈Rdare model parameters. Different choices\nofσ(·)result in different versions of GLU functions. For example, ifσ(·)is deﬁned to be the\nGeLU function, we will have the GeGLU function\nσgeglu(h) =σgelu(hW 1+b1)⊙(W2+b2) (2.29)\nThis activation function has been successfully applied in L LMs like Gemma.\nAs another example, consider σ(·)to be the Swish function σswish(h) =h⊙Sigmoid(ch)\n8Here degeneration refers to the phenomenon in which the rank of a matrix is reduced after some processing.\n9Pr(h≤h)is an informal notation. It refers to a vector, with each entr y representing the percentile for the\ncorresponding entry of h.\n\n60 Generative Models\n[Ramachandran et al. ,2017 ]. Then, the SwiGLU function is given by\nσswiglu (h) =σswish(hW 1+b1)⊙(W2+b2) (2.30)\nBoth the PaLM and LLaMA series are based on the SwiGLU functio n. For more discussions of\nGLUs, the reader can refer to Shazeer [2020 ]’s work.\n2.2.2.3 Removing Bias Terms\nAnother popular model design is to remove the bias terms in af ﬁne transformations used in LLMs.\nThis treatment can be applied to layer normalization, trans formations of the inputs to QKV atten-\ntion, and FFNs. For example, we can modify Eq. ( 2.25) to obtain an FFN with no bias terms\nFFN( h) =σ(hW h)Wf (2.31)\nChowdhery et al. [2022 ] report that removing bias terms helps improve the training stability\nof LLMs. This method has been used in several recent LLMs, suc h as LLaMA and Gemma.\n2.2.2.4 Other Issues\nMany LLMs also involve modiﬁcations to their positional emb edding models. For example, one\ncan replace sinusoidal positional encodings with rotary po sition embeddings so that the learned\nLLMs can handle long sequences better. These models will be d iscussed in Section 2.3.\nNote that while model modiﬁcations are common in training LL Ms, the stability of training\ncan be improved in many different ways. For example, increas ing the batch size as the training\nproceeds has been found to be useful for some LLMs. In general , achieving stable and efﬁcient\nlarge-scale LLM training requires carefully",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 53,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "8fa788e46efe568b",
    "content": "Chowdhery et al. [2022 ] report that removing bias terms helps improve the training stability\nof LLMs. This method has been used in several recent LLMs, suc h as LLaMA and Gemma.\n2.2.2.4 Other Issues\nMany LLMs also involve modiﬁcations to their positional emb edding models. For example, one\ncan replace sinusoidal positional encodings with rotary po sition embeddings so that the learned\nLLMs can handle long sequences better. These models will be d iscussed in Section 2.3.\nNote that while model modiﬁcations are common in training LL Ms, the stability of training\ncan be improved in many different ways. For example, increas ing the batch size as the training\nproceeds has been found to be useful for some LLMs. In general , achieving stable and efﬁcient\nlarge-scale LLM training requires carefully designed setu ps, including learning schedules, opti-\nmizer choices, training parallelism, mixed precision trai ning, and so on. Some of these issues are\nhighly engineered, and therefore, we typically need a numbe r of training runs to obtain satisfactory\nLLMs.\n2.2.3 Distributed Training\nTraining LLMs requires signiﬁcant amounts of computationa l resources. A common approach to\nimproving training efﬁciency is to use large-scale distrib uted systems. Fortunately, alongside the\nrise of neural networks in AI, deep learning-oriented softw are and hardware have been developed,\nmaking it easier to implement LLMs and perform computations . For example, one can now easily\nﬁne-tune an LLM using deep learning software frameworks and a machine with multiple GPUs.\nHowever, scaling up the training of LLMs is still challengin g, and requires signiﬁcant efforts in\ndeveloping hardware and software systems for stable and efﬁ cient distributed training.\nAn important consideration of distributed training is para llelism. There are several forms\nof parallelism: data parallelism, model parallelism, tens or parallelism, and pipeline parallelism.\nDespite different ways to distribute computations across d evices, these parallelism methods are\nbased on a similar idea: the training problem can be divided i nto smaller tasks that can be ex-\necuted simultaneously. The issue of parallelism in trainin g LLMs has been extensively studied\n\n2.2 Training at Scale 61\n[Narayanan et al. ,2021 ;Fedus et al. ,2022 ]. Here we sketch the basic concepts.\n•Data Parallelism . This method is one of the most widely used parallelism metho ds for\ntraining neural networks. To illustrate, consider the simp lest case where the standard delta\nrule is used in gradient descent\nθt+1=θt−lr·∂Lθt(Dmini)\n∂θt(2.32)\nwhere the new parameters θt+1is obtained by updating the latest parameters θtwith a small\nsteplrin the direction of the negative loss gradient.∂Lθt(Dmini)\n∂θtis the gradient of the loss\nwith respect to the parameters θt, and is computed on a minibatch of training sample Dmini.\nIn data parallelism, we divide Dmini intoNsmaller batches, denoted by {D1,...,DN}.\nThen, we distribute these batches to Nworkers, each with a corresponding batch. Once\nthe data is distributed, these workers can work at the same ti me. The gradient of the entire\nminibatch is obtained by aggregating the gradients compute d by the workers, like this\n∂Lθt(Dmini)\n∂θt=∂Lθt(D1)\n∂θt\nworker 1+∂Lθt(D2)\n∂θt\nworker 2+· · ·+∂Lθt(DN)\n∂θt\nworker N(2.33)\nIn ideal cases where the workers coordinate well and the comm unication overhead is small,\ndata parallelism can achieve nearly an N-fold speed-up for training.\n•Model Parallelism . Although data parallelism is simple and effective, it requ ires each\nworker to run the entire LLM and perform the complete forward and backward process.\nAs LLMs grow larger, it sometimes becomes unfeasible to load and execute an LLM on a\nsingle device. In this case, we can decouple the LLM into smal ler components and run these\ncomponents",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 54,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "372d629ecdb70bd6",
    "content": "∂θt\nworker 1+∂Lθt(D2)\n∂θt\nworker 2+· · ·+∂Lθt(DN)\n∂θt\nworker N(2.33)\nIn ideal cases where the workers coordinate well and the comm unication overhead is small,\ndata parallelism can achieve nearly an N-fold speed-up for training.\n•Model Parallelism . Although data parallelism is simple and effective, it requ ires each\nworker to run the entire LLM and perform the complete forward and backward process.\nAs LLMs grow larger, it sometimes becomes unfeasible to load and execute an LLM on a\nsingle device. In this case, we can decouple the LLM into smal ler components and run these\ncomponents on different devices. One simple way to do this is to group consecutive layers\nin the layer stack and assign each group to a worker. The worke rs operate in the order of\nthe layers in the stack, that is, in the forward pass we proces s the input from lower-level to\nupper-level layers, and in the backward pass we propagate th e error gradients from upper-\nlevel to lower-level layers. Consider, for example, a Trans former decoder with Lstacked\nblocks. To distribute the computation load, each block is as signed to a worker. See the\nfollowing illustration for a single run of the forward and ba ckward passes of this model.\nWorkerL BL(↑)BL(↓)\n... ... ...\nWorker 2 B2(↑) B2(↓)\nWorker 1B1(↑) B1(↓)\nHere Bldenotes the computation of block l, and the symbols ↑and↓denote the forward and\nbackward passes, respectively. Note that this parallelism method forces the workers to run\nin sequence, so a worker has to wait for the previous worker to ﬁnish their job. This results\nin the devices being idle for most of the time. In practical sy stems, model parallelism is\ngenerally used together with other parallelism mechanisms to maximize the use of devices.\n\n62 Generative Models\n•Tensor Parallelism . Parallelism can also be performed in a single computation s tep. A\ncommon example is splitting a large parameter matrix into ch unks, multiplying an input\ntensor with each of these chunks separately, and then concat enating the results of these\nmultiplications to form the output. For example, consider t he multiplication of the repre-\nsentation h∈Rdwith the parameter matrix Wh∈Rd×dhin an FFN sub-layer (see Eq.\n(2.25)). We can slice the matrix Wh∈Rd×dhvertically to a sequence of Msub-matrices\nWh=[\nW1\nhW2\nh...WM\nh]\n(2.34)\nwhere each sub-matrix Wk\nhhas a shape of d×dh\nM. The multiplication of hwithWhcan be\nexpressed as\nhW h=h[\nW1\nhW2\nh...WM\nh]\n=[\nhW1\nhhW2\nh...hWM\nh]\n(2.35)\nWe can perform matrix multiplications {hW1\nh,hW2\nh,...,hWM\nh}onMdevices separately.\nAs a result, we distribute a large matrix multiplication acr oss multiple devices, each of\nwhich may have relatively small memory. From the perspectiv e of the design of modern\nGPUs, tensor parallelism over GPUs provides a two-level, ti le-based approach to parallel\ncomputing. First, at a higher level, we decompose a matrix mu ltiplication into sub-matrix\nmultiplications that can directly ﬁt into the memory of GPUs . Then, at a lower level, we\nexecute these sub-matrix multiplications on GPUs using til e-based parallel algorithms that\nare speciﬁcally optimized for GPUs.\n•Pipeline Parallelism . Above, in model parallelism, we have described a simple app roach\nto spreading groups of model components across multiple dev ices. But this method is in-\nefﬁcient because only one device is activated at a time durin g processing. Pipeline par-\nallelism addresses this issue by introducing overlaps betw een computations on different\ndevices [ Harlap et al. ,2018 ;Huang et al. ,2019 ]. To do this, a batch of samples is divided\ninto a number of micro-batches, and then these micro-batche s are processed by each worker\nas usual. Once a micro-b",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 55,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "70023ae95cfb9244",
    "content": " we decompose a matrix mu ltiplication into sub-matrix\nmultiplications that can directly ﬁt into the memory of GPUs . Then, at a lower level, we\nexecute these sub-matrix multiplications on GPUs using til e-based parallel algorithms that\nare speciﬁcally optimized for GPUs.\n•Pipeline Parallelism . Above, in model parallelism, we have described a simple app roach\nto spreading groups of model components across multiple dev ices. But this method is in-\nefﬁcient because only one device is activated at a time durin g processing. Pipeline par-\nallelism addresses this issue by introducing overlaps betw een computations on different\ndevices [ Harlap et al. ,2018 ;Huang et al. ,2019 ]. To do this, a batch of samples is divided\ninto a number of micro-batches, and then these micro-batche s are processed by each worker\nas usual. Once a micro-batch is processed by a worker and pass ed to the next one, the\nfollowing micro-batch immediately occupies the same worke r. In other words, we create\na pipeline in which different computation steps can overlap if multiple jobs are given to\nthe pipeline. The following shows an illustration of pipeli ne parallelism for processing 3\nmicro-batches.\nWorkerL BL,1BL,2BL,3BL,1BL,2BL,3\n... ... ...\nWorker 2 B2,1B2,2B2,3 B2,1B2,2B2,3\nWorker 1B1,1B1,2B1,3 B1,1B1,2B1,3\nHere Bl,krepresents the processing of the k-th micro-batch by the l-th worker. Ideally we\nwould like to maximize the number of micro-batches, and thus minimize the idle time of the\n\n2.2 Training at Scale 63\nworkers. However, in practice, using small micro-batches o ften reduces GPU utilization and\nincreases task-switching costs. This may, in turn, decreas e the overall system throughput.\nThe ultimate goal of parallel processing is to achieve linea r growth in efﬁciency, that is, the\nnumber of samples that can be processed per unit of time incre ases linearly with the number of\ndevices. However, distributed training is complicated, an d inﬂuenced by many factors in addition\nto the parallelism method we choose. One problem, which is of ten associated with distributed\nsystems, is the cost of communication. We can think of a distr ibuted system as a group of net-\nworked nodes. Each of these nodes can perform local computat ion or pass data to other nodes. If\nthere are a large number of such nodes, it will be expensive to distribute and collect data across\nthem. Sometimes, the time savings brought about by parallel ism are offset by the communica-\ntion overhead of a large network. Another problem with large -scale distributed systems is that\nthe synchronization of nodes introduces additional costs. As is often the case, some nodes may\ntake longer to work, causing others to wait for the slowest on es. While we can use asynchronous\ntraining to handle heterogeneity in computational resourc es, this may lead to stale gradients and\nnon-guaranteed convergence. Moreover, as more nodes are ad ded to the network, there is more\nchance to have crashed nodes during training. In this case, w e need to ensure that the whole\nsystem is fault tolerant. In many practical settings, to inc rease scalability, one needs to take into\naccount additional issues, including architecture design , data transfer and computation overlap,\nload balancing, memory bandwidth and so on.\nTraining LLMs is so computationally expensive that, even th ough distributed training is al-\nready in use, researchers and engineers often still employ v arious model compression and speed-\nup methods to improve training efﬁciency [ Weng ,2021 ]. One example is mixed precision training,\nin which low precision data (such as FP16 and FP8 data) is used for gradient computation on each\nindividual node, and single or double precision data (such a s FP32/FP64 data) is used for updating\nthe model [ Micikevicius et al. ,2018 ]. A key operation in this approach is gradient accumulation\nwhere gradients need to be accumulated and synchronized acr oss nodes. However, due to the\nnon-associativity of ﬂoating-point addition, this can lea d to slight numerical differences in accu-\nmulated gradients on different nodes, which may affect mode l convergence and ﬁnal performance.\nThis problem is more obvious if there are a large number of nod es involved",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 56,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "ca183432b84a404c",
    "content": "ough distributed training is al-\nready in use, researchers and engineers often still employ v arious model compression and speed-\nup methods to improve training efﬁciency [ Weng ,2021 ]. One example is mixed precision training,\nin which low precision data (such as FP16 and FP8 data) is used for gradient computation on each\nindividual node, and single or double precision data (such a s FP32/FP64 data) is used for updating\nthe model [ Micikevicius et al. ,2018 ]. A key operation in this approach is gradient accumulation\nwhere gradients need to be accumulated and synchronized acr oss nodes. However, due to the\nnon-associativity of ﬂoating-point addition, this can lea d to slight numerical differences in accu-\nmulated gradients on different nodes, which may affect mode l convergence and ﬁnal performance.\nThis problem is more obvious if there are a large number of nod es involved in distributed training,\nespecially given that low-precision numerical computatio ns may encounter overﬂow and under-\nﬂow issues, as well as inconsistencies across different har dware devices. Therefore, the design of\ndistributed systems needs to consider these numerical comp utation issues to ensure satisfactory\nresults and convergence.\n2.2.4 Scaling Laws\nThe success of LLMs reveals that training larger language mo dels using more resources can lead\nto improved model performance. Researchers have explained this as scaling laws of LLMs. More\nspeciﬁcally, scaling laws describe the relationships betw een the performance of LLMs and the\nattributes of LLM training, such as the model size, the amoun t of computation used for training,\nand the amount of training data. For example, Hestness et al. [2017 ] show that the performance of\ndeep neural networks is a power-law-like function of the tra ining data size. In the beginning, when\nthe amount of training data is not large, the performance of t he model improves slowly. Afterward,\nwhen more training data is used, the model enters a phase of ra pid performance improvement, and\nthe performance curve resembles a power-law curve. Ultimat ely, the improvement in performance\n\n64 Generative Models\nSlow Reduction\nPhasePower-law Reduction\nPhaseConvergence\nPhase\n(Irreducible Error)\nTraining Dataset Size (Log-scale)Number of Test Errors (Log-scale)\nFig. 2.3: A scaling law of test error against a variable of interest (e. g., training dataset size) [ Hestness et al. ,2017 ]. The\ncurve of the scaling law can be divided into three phases. At t he beginning, the number of test errors decreases slowly\nwhen more training data is used, but this only lasts for a shor t period. In the second phase, the number of test errors\ndecreases drastically, and the curve becomes a power law cur ve. After that, the error reduction slows down again in the\nthird phase. Note that there are irreducible errors that can not be eliminated, regardless of the amount of training data .\nbecomes slow again, and more data does not lead to signiﬁcant gains. Figure 2.3shows an example\nof such curves.\nIn NLP, a traditional view holds that the performance gains w ill disappear at a certain point\nas the training is scaled up. However, recent results show th at, if we consider the problem on\na larger scale, scaling up training is still a very effective method for obtaining stronger LLMs.\nFor example, both closed-source and open-source LLMs can be neﬁt from more data, even though\ntrillions of tokens have already been used for training.\nWith the increase in the scale of model training, LLMs exhibi t new capabilities, known as the\nemergent abilities of LLMs. For example, Wei et al. [2022b ] studied the scaling properties of\nLLMs across different model sizes and amounts of computatio nal resources. Their work shows\nthat some abilities emerge when we scale the model size to cer tain level. The appearance of\nemergent abilities has demonstrated the role of scaled trai ning in enhancing the performance of\nLLMs, and it has also, to some extent, motivated researchers to continuously attempt to train larger\nmodels. As larger and stronger LMs continue to appear, our un derstanding of the scaling laws\ncontinues to mature. This helps researchers predict the per formance of LLMs during training and\nestimate the minimal computational resources required to a chieve a given level of performance.\nTo understand how model performance scales with various fac tors considered during training,\nit is common to express the model performance as a function of these factors. For example",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 57,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "64169cd451f1c551",
    "content": " capabilities, known as the\nemergent abilities of LLMs. For example, Wei et al. [2022b ] studied the scaling properties of\nLLMs across different model sizes and amounts of computatio nal resources. Their work shows\nthat some abilities emerge when we scale the model size to cer tain level. The appearance of\nemergent abilities has demonstrated the role of scaled trai ning in enhancing the performance of\nLLMs, and it has also, to some extent, motivated researchers to continuously attempt to train larger\nmodels. As larger and stronger LMs continue to appear, our un derstanding of the scaling laws\ncontinues to mature. This helps researchers predict the per formance of LLMs during training and\nestimate the minimal computational resources required to a chieve a given level of performance.\nTo understand how model performance scales with various fac tors considered during training,\nit is common to express the model performance as a function of these factors. For example, in\nthe simplest case, we can express the loss or error of an LLM as a function of a single variable of\ninterest. However, there are no universal scaling laws that can describe this relationship. Instead,\ndifferent functions are proposed to ﬁt the learning curves o f LLMs.\nLetxbe the variable of interest (such as the number of model param eters) and L(x)be the\nloss of the model given x(such as the cross-entropy loss on test data). The simplest f orm of L(x)\nis a power law\nL(x) =axb(2.36)\n\n2.2 Training at Scale 65\n1051071092.43.24.04.85.6\nNumber of ParametersTest LossL(N) = (N\n8.8·1013)−0.076\n1081092.733.33.63.94.2\nDataset SizeTest LossL(D) = (D\n5.4·1013)−0.095\nFig. 2.4: Test loss against model size ( N) and training dataset size ( D) (data points are plotted for illustrative purposes).\nWe plot test loss as a function of N, which is deﬁned as L(N) =(N\n8.8×1013)−0.076, and a function of D, which is\ndeﬁned as L(D) =(D\n5.4×1013)−0.095[Kaplan et al. ,2020 ].\nwhereaandbare parameters that are estimated empirically. Despite its simplicity, this func-\ntion has successfully interpreted the scaling ability of la nguage models and machine transla-\ntion systems in terms of model size (denoted by N) and training dataset size (denoted by D)\n[Gordon et al. ,2021 ;Hestness et al. ,2017 ]. For example, Kaplan et al. [2020 ] found that the per-\nformance of their language model improves as a power law of ei therNorDafter an initial\ntransient period, and expressed these relationships using L(N) =(N\n8.8×1013)−0.076andL(D) =\n(D\n5.4×1013)−0.095(see Figure 2.4).\nAn improvement to this scaling law is to add an irreducible error term to the power law. The\nform of L(x)is then given by\nL(x) =axb+ǫ∞ (2.37)\nwhereǫ∞is the irreducible error that accounts for the error due to un known variables, which is\npresent even as x→ ∞ . Eq. ( 2.37) is one of the most widely used forms for designing scaling\nlaws of LLMs. For example, Rosenfeld et al. [2020 ] developed a scaling law that involves both\nmodel scaling and dataset scaling, like this\nL(N,D ) =aNb+cDd+ǫ∞ (2.38)\nAn example of such formulation is the Chinchilla scaling law . It states that the test loss per\ntoken is the sum of the inverse proportion functions of NandD, with an additional irreducible\nerror term. Hoffmann et al. [2022 ] express this scaling law as\nL(N,D ) =406.4\nN0.34\nmodel scaling+410.7\nD0.28\ndataset scaling+ 1.69\nirreducible error(2.39)\nAll the scaling laws mentioned above are based on monotonic f unctions. So they cannot cover\n",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 58,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "f60a99636048b668",
    "content": " [2020 ] developed a scaling law that involves both\nmodel scaling and dataset scaling, like this\nL(N,D ) =aNb+cDd+ǫ∞ (2.38)\nAn example of such formulation is the Chinchilla scaling law . It states that the test loss per\ntoken is the sum of the inverse proportion functions of NandD, with an additional irreducible\nerror term. Hoffmann et al. [2022 ] express this scaling law as\nL(N,D ) =406.4\nN0.34\nmodel scaling+410.7\nD0.28\ndataset scaling+ 1.69\nirreducible error(2.39)\nAll the scaling laws mentioned above are based on monotonic f unctions. So they cannot cover\nfunctions with inﬂection points, such as double descent cur ves. In response, researchers have\nexplored more sophisticated functions to ﬁt the learning cu rves. Examples of such functions can\n\n66 Generative Models\nbe found in Alabdulmohsin et al. [2022 ] and Caballero et al. [2023 ]’s work.\nThe signiﬁcance of scaling laws lies in providing direction al guidance for LLM research: if\nwe are still in the region of the power law curve, using more re sources to train larger models is a\nvery promising direction. While this result “forces” big re search groups and companies to invest\nmore in computational resources to train larger models, whi ch is very expensive, scaling laws\ncontinuously push the boundaries of AI further away. On the o ther hand, understanding scaling\nlaws helps researchers make decisions in training LLMs. For example, given the computational\nresources at hand, the performance of LLMs may be predicted.\nOne last note on scaling laws in this section. For LLMs, a lowe r test loss does not always\nimply better performance on all downstream tasks. To adapt L LMs, there are several steps such\nas ﬁne-tuning and prompting that may inﬂuence the ﬁnal resul t. Therefore, the scaling laws for\ndifferent downstream tasks might be different in practice.\n2.3 Long Sequence Modeling\nWe have already seen that, in large-scale training, larger l anguage models can be developed by us-\ning more data and computational resources. However, scalin g up can also occur in other directions.\nFor instance, in many applications, LLMs are adapted to proc ess signiﬁcantly long sequences. An\ninteresting example is that we pre-train an LLM on extensive texts of normal length and then ap-\nply it to deal with very long token sequences, far beyond the l ength encountered in pre-training.\nHere we use Pr(y|x)to denote the text generation probability where xis the context and yis the\ngenerated text. There are broadly three types of long sequen ce modeling problems.\n•Text generation based on long context (i.e., xis a long sequence). For example, we\ngenerate a short summary for a very long text.\n•Long text generation (i.e., yis a long sequence). For example, we generate a long story\nbased on a few keywords.\n•Long text generation based on long context (i.e., both xandyare long sequences). For\nexample, we translate a long document from Chinese to Englis h.\nRecently, NLP researchers have been more interested in appl ying and evaluating LLMs on\ntasks where extremely long input texts are involved. Imagin e an LLM, which reads a C++ source\nﬁle containing tens of thousands of lines, and outlines the f unctionality of the program correspond-\ning to the source ﬁle. Such models, capable of handling exten sive textual contexts, are sometimes\ncalled long-context LLMs . In this section we will restrict ourselves to long-context LLMs, but\nthe methods discussed here can be applicable to other proble ms.\nFor Transformers, dealing with long sequences is computati onally expensive, as the computa-\ntional cost of self-attention grows quadratically with the sequence length. This makes it infeasible\nto train and deploy such models for very long inputs. Two stra nds of research have tried to adapt\nTransformers to long-context language modeling.\n• The ﬁrst explores efﬁcient training methods and model arch itectures to learn self-attention\nmodels from long-sequence data.\n\n2.3 Long Sequence Modeling 67\n• The other adapts pre-trained LLMs to",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 59,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "e173e33ef05d6714",
    "content": " tens of thousands of lines, and outlines the f unctionality of the program correspond-\ning to the source ﬁle. Such models, capable of handling exten sive textual contexts, are sometimes\ncalled long-context LLMs . In this section we will restrict ourselves to long-context LLMs, but\nthe methods discussed here can be applicable to other proble ms.\nFor Transformers, dealing with long sequences is computati onally expensive, as the computa-\ntional cost of self-attention grows quadratically with the sequence length. This makes it infeasible\nto train and deploy such models for very long inputs. Two stra nds of research have tried to adapt\nTransformers to long-context language modeling.\n• The ﬁrst explores efﬁcient training methods and model arch itectures to learn self-attention\nmodels from long-sequence data.\n\n2.3 Long Sequence Modeling 67\n• The other adapts pre-trained LLMs to handle long sequences with modest or no ﬁne-tuning\nefforts.\nHere, we will discuss the former brieﬂy since it can be found i n general discussions of efﬁcient\nTransformer architectures [ Tay et al. ,2020 ;Xiao and Zhu ,2023 ]. We will focus on the latter,\nhighlighting popular methods in recent LLMs. We will also di scuss the strengths and limitations\nof these long-sequence models.\n2.3.1 Optimization from HPC Perspectives\nWe begin our discussion by considering improvements to stan dard Transformer models from the\nperspectives of high-performance computing. Most of these improvements, though not speciﬁ-\ncally designed for LLMs, have been widely applied across var ious deep learning models [ Kim et al. ,\n2023 ]. A commonly used approach is to adopt a low-precision imple mentation of Transformers.\nFor example, we can use 8-bit or 16-bit ﬁxed-point data types for arithmetic operations, instead\nof 32-bit or 64-bit ﬂoating-point data types. Using these lo w-precision data types can increase\nthe efﬁciency and memory throughput, so that longer sequenc es can be processed more easily.\nAn alternative approach is to improve Transformers by using hardware-aware techniques. For\nexample, on modern GPUs, the efﬁciency of Transformers can b e improved by using IO-aware\nimplementations of the self-attention function [ Dao et al. ,2022 ;Kwon et al. ,2023 ].\nAnother way to handle long sequences is through sequence par allelism [ Li et al. ,2023b ;\nKorthikanti et al. ,2023 ]. Speciﬁcally, consider the general problem of attending t he query qi\nat the position ito the keys Kand values V. We can divide Kby rows and obtain a set of sub-\nmatrices {K[1],...,K[nu]}, each corresponding to a segment of the sequence. Similarly , we can\nobtain the sub-matrices of V, denoted by {V[1],...,V[nu]}. Then, we assign each pair of K[u]and\nV[u]to a computing node (e.g., a GPU of a GPU cluster). The assigne d nodes can run in parallel,\nthereby parallelizing the attention operation.\nRecall that the output of the self-attention model can be wri tten as\nAttqkv(qi,K,V) =m−1∑\nj=0αi,jvj (2.40)\nwhereαi,jis the attention weight between positions iandj. In Transformers, αi,jis obtained\nby normalizing the rescaled version of the dot product betwe enqiandkj. Letβi,jdenote the\nattention score between qiandkj. We have\nβi,j=qi·kj√\nd+ Mask(i,j) (2.41)\nwhere Mask(i,j)is the masking variable for (i,j). Then, we deﬁne the attention weight αi,jto\nbe\nαi,j= Softmax( βi,j)\n=exp(βi,j)∑\nj′exp(βi,j′)(2.42)\n\n68 Generative Models\nOn each computing node, we need to implement these equations . Given the keys and values\nassigned to this node, computing the numerator of the right- hand side of Eq. ( 2.42) (i.e., exp(βi,j))\nis straightforward, as all the required information is stor ed on the node. However, computing the\ndenominator of the right-hand side of Eq. ( 2.42) involves a sum of exp(βi,j′)over",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 60,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "1de0a9020815b2ee",
    "content": "attention score between qiandkj. We have\nβi,j=qi·kj√\nd+ Mask(i,j) (2.41)\nwhere Mask(i,j)is the masking variable for (i,j). Then, we deﬁne the attention weight αi,jto\nbe\nαi,j= Softmax( βi,j)\n=exp(βi,j)∑\nj′exp(βi,j′)(2.42)\n\n68 Generative Models\nOn each computing node, we need to implement these equations . Given the keys and values\nassigned to this node, computing the numerator of the right- hand side of Eq. ( 2.42) (i.e., exp(βi,j))\nis straightforward, as all the required information is stor ed on the node. However, computing the\ndenominator of the right-hand side of Eq. ( 2.42) involves a sum of exp(βi,j′)over allj′s, which\nrequires transferring data to and from other nodes. To illus trate, suppose that vjandkjare placed\non nodeu. We can rewrite Eq. ( 2.42) as\nαi,j\n=node u\nexp(βi,j)∑\nkj′∈K[1]exp(βi,j′)\n\nnode 1+· · ·+∑\nkj′∈K[u]exp(βi,j′)\n\nnode u+· · ·+∑\nkj′∈K[nu]exp(βi,j′)\n\nnode nu(2.43)\nwhere the notation kj′∈K[u]represents that kj′is a row vector of K[u]. In a straightforward\nimplementation, we ﬁrst perform the summations {∑\nkj′∈K[u]exp(βi,j′)}separately on the corre-\nsponding nodes. Then, we collect these summation results fr om different nodes to combine them\ninto a ﬁnal result. This corresponds to a collective operati on in the context of parallel processing.\nThere are many efﬁcient implementations of such operations , such as the all-reduce algorithms.\nHence the sum of all exp(βi,j)values can be computed using optimized routines in collecti ve\ncommunication toolkits.\nGiven the attention weights {αi,j}, we then compute the attention results using Eq. ( 2.40).\nThe problem can be re-expressed as\nAttqkv(qi,K,V)\n=∑\nvj′∈V[1]αi,j′vj′\n\nnode 1+· · ·+∑\nvj′∈V[u]αi,j′vj′\n\nnode u+· · ·+∑\nvj′∈V[nu]αi,j′vj′\n\nnode nu(2.44)\nLike Eq. ( 2.43), Eq. ( 2.44) can be implemented as a summation program in parallel proce ss-\ning. First, perform the weighted summations of values on dif ferent nodes simultaneously. Then,\nwe collect the results from these nodes via collective opera tions.\nNote that, although this section primarily focuses on long s equence modeling, much of the mo-\ntivation for sequence parallelism comes from the distribut ed training methods of deep networks,\nas discussed in Section 2.2.3 . As a result, the implementation of these methods can be base d on\nthe same parallel processing library.\n2.3.2 Efﬁcient Architectures\nOne difﬁculty of applying Transformers to long sequences is that self-attention has a quadratic\ntime complexity with respect to the sequence length. Moreov er, a key-value cache (orKV cache\nfor short) is maintained during inference, and its size incr eases as more tokens are processed. Al-\nthough the KV cache grows linearly with the sequence length, for extremely long input sequences,\nthe memory footprint becomes signiﬁcant and it is even infea sible to deploy LLMs for such tasks.\nAs a result, the model architecture of long-context LLMs gen erally moves away from the standard\n\n2.3 Long Sequence Modeling 69\nTransformer, turning instead to the development of more efﬁ cient variants and alternatives.\nOne",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 61,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "f5a198463776a063",
    "content": " in Section 2.2.3 . As a result, the implementation of these methods can be base d on\nthe same parallel processing library.\n2.3.2 Efﬁcient Architectures\nOne difﬁculty of applying Transformers to long sequences is that self-attention has a quadratic\ntime complexity with respect to the sequence length. Moreov er, a key-value cache (orKV cache\nfor short) is maintained during inference, and its size incr eases as more tokens are processed. Al-\nthough the KV cache grows linearly with the sequence length, for extremely long input sequences,\nthe memory footprint becomes signiﬁcant and it is even infea sible to deploy LLMs for such tasks.\nAs a result, the model architecture of long-context LLMs gen erally moves away from the standard\n\n2.3 Long Sequence Modeling 69\nTransformer, turning instead to the development of more efﬁ cient variants and alternatives.\nOne approach is to use sparse attention instead of standard s elf-attention. This family of\nmodels is based on the idea that only a small number of tokens a re considered important when\nattending to a given token, and so most of the attention weigh ts between tokens are close to zero.\nAs a consequence, we can prune most of the attention weights a nd represent the attention model\nin a compressed form. To illustrate, consider the self-atte ntion model\nAttqkv(Q,K,V) =α(Q,K)V (2.45)\nwhere the attention weight matrix α(Q,K)∈Rm×mis obtained by\nα(Q,K) = Softmax(QKT\n√\nd+Mask )\n=\nα0,0 0 0 ... 0\nα1,0α1,1 0... 0\nα2,0α2,1α2,2... 0\n...............\nαm−1,0αm−1,1αm−1,2... α m−1,m−1\n(2.46)\nEach row vector[\nαi,0... α i,i0...0]\ncorresponds to a distribution of attending the i-th\ntoken to every token of the sequence. Since language models p redict next tokens only based on\ntheir left-context, we normally write the output of the atte ntion model at position ias\nAttqkv(qi,K≤i,V≤i) =[\nαi,0... α i,i]\nv0\n...\nvi\n\n=i∑\nj=0αi,jvj (2.47)\nwhere K≤i=\nk0\n...\nki\nandV≤i=\nv0\n...\nvi\nare the keys and values up to position i.\nIn the original version of self-attention[\nαi,0... α i,i]\nis assumed to be dense, that is, most of\nthe values are non-zero. In sparse attention, some of the ent ries of[\nαi,0... α i,i]\nare considered\nnon-zero, and the remaining entries are simply ignored in co mputation. Suppose G⊆ {0,...,i}is\nthe set of indices of the non-zero entries. For language mode ls, the output of the sparse attention\nmodel at position iis given by\nAttsparse (qi,K≤i,V≤i) =∑\nj∈Gα′\ni,jvj (2.48)\nHere {α′\ni,j}are normalized over G. Hence their values are different from the original attenti on\nweights (in fact we have α′\ni,j>α i,j). The sparsity of the model is determined by how large Gis.\nSparse attention models differ in the way we deﬁne G. One simple approach is to deﬁne Gbased\n\n70 Generative Models\non heuristically designed patterns. For example, a widely- used pattern involves having Gcover a\nwindow of tokens located near position i[Parmar et al. ,2018 ].\nWhile sparse attention reduces the computation through the use of sparse operations, such\nmodels still have signiﬁcant limitations as we",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 62,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "b21d195411e37127",
    "content": ", the output of the sparse attention\nmodel at position iis given by\nAttsparse (qi,K≤i,V≤i) =∑\nj∈Gα′\ni,jvj (2.48)\nHere {α′\ni,j}are normalized over G. Hence their values are different from the original attenti on\nweights (in fact we have α′\ni,j>α i,j). The sparsity of the model is determined by how large Gis.\nSparse attention models differ in the way we deﬁne G. One simple approach is to deﬁne Gbased\n\n70 Generative Models\non heuristically designed patterns. For example, a widely- used pattern involves having Gcover a\nwindow of tokens located near position i[Parmar et al. ,2018 ].\nWhile sparse attention reduces the computation through the use of sparse operations, such\nmodels still have signiﬁcant limitations as we must keep the entire KV cache (i.e., K≤iandV≤i)\nduring inference. If the sequence is very long, storing this cache will become highly memory-\nintensive. To address this, we can consider a different form of attention models where the KV\ncache is not explicitly retained. Linear attention is one su ch approach [ Katharopoulos et al. ,2020 ].\nIt uses a kernel function φ(·)to project each query and key onto points q′\ni=φ(qi)andk′\ni=φ(ki),\nrespectively. By removing the Softmax function under such t ransformations10, the form of the\nresulting attention model is given by\nAttqkv(qi,K≤i,V≤i)≈Attlinear(q′\ni,K′\n≤i,V≤i)\n=q′\niµi\nq′\niνi(2.49)\nwhereµiandνiare variables that are computed in the recurrent forms\nµi=µi−1+k′T\nivi (2.50)\nνi=νi−1+k′T\ni (2.51)\nµiandνican be seen as representations of the history up to position i. A beneﬁt of this model is\nthat we need not keep all past queries and values. Instead onl y the latest representations µiand\nνiare used. So the computational cost of each step is a constant , and the model can be easily\nextended to deal with long sequences.\nIn fact, this sequential approach to long sequence modeling arises naturally when we adopt a\nviewpoint of recurrent models. Such models read one token (o r a small number of tokens) at a\ntime, update the recurrent state using these inputs, and the n discard them before the next token\narrives. The output at each step is generated based only on th e recurrent state, rather than on all the\nprevious states. The memory footprint is determined by the r ecurrent state which has a ﬁxed size.\nRecurrent models can be used in real-time learning scenario s where data arrives in a stream and\npredictions can be made at any time step. In NLP, applying rec urrent models to language mod-\neling is one of the earliest successful attempts to learn rep resentations of sequences. Although\nTransformer has been used as the foundational architecture in LLMs, recurrent models are still\npowerful models, especially for developing efﬁcient LLMs. More recently, recurrent models have\nstarted their resurgence in language modeling and have been reconsidered as a promising alterna-\ntive to Transformers [ Gu and Dao ,2023 ]. Figure 2.5shows a comparison of the models discussed\nin this subsection.\n10In the new space after this transformation, the Softmax norm alization can be transformed into the simple scaling\nnormalization.\n\n2.3 Long Sequence Modeling 71\nqi ki ki−1 ki−2 · · · k1 k0\nvi vi−1 vi−2 · · · v1 v0Att qkv(qi,K≤i,V≤i)\n(a) Standard Self-attention\nqi ki ki−1 ki−2 · · · k1 k0\nvi vi−1 vi−2 · · · v1 v0Att qkv(qi,{k1,ki},{v1,vi})\n(b) Sparse Attention\nqi ki ki−1 ki−2 · · · k1 k0\nvi vi−1 vi−2 · · · v1 v0νi=νi−1+k′T\niµi=µi−1+k′T\nivi\nνiµi ⇒\n⇒Att linear(qi,K≤i,V≤i) =q′\niµ",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 63,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "03c1be7c60822bef",
    "content": " simple scaling\nnormalization.\n\n2.3 Long Sequence Modeling 71\nqi ki ki−1 ki−2 · · · k1 k0\nvi vi−1 vi−2 · · · v1 v0Att qkv(qi,K≤i,V≤i)\n(a) Standard Self-attention\nqi ki ki−1 ki−2 · · · k1 k0\nvi vi−1 vi−2 · · · v1 v0Att qkv(qi,{k1,ki},{v1,vi})\n(b) Sparse Attention\nqi ki ki−1 ki−2 · · · k1 k0\nvi vi−1 vi−2 · · · v1 v0νi=νi−1+k′T\niµi=µi−1+k′T\nivi\nνiµi ⇒\n⇒Att linear(qi,K≤i,V≤i) =q′\niµi\nq′\niνi\n(c) Linear Attention\nhi hi−1 hi−2 hi−3 · · · h1 h0\ninputihi=f(hi−1,inputi)\n(d) Recurrent Models\nFig. 2.5: Illustrations of self-attention, sparse attention, linea r attention and recurrent models. Blue boxes = cached\nstates for producing the output at position i.f(·)= a recurrent cell.\n2.3.3 Cache and Memory\nLLMs based on the standard Transformer architecture are glo bal models. The inference for these\nmodels involves storing the entire left-context in order to make predictions for future tokens. This\nrequires a KV cache where the representations (i.e., keys an d values) of all previously-generated\n\n72 Generative Models\ntokens are kept, and the cost of caching grows as the inferenc e proceeds. Above, we have dis-\ncussed methods for optimizing this cache via efﬁcient atten tion approaches, such as sparse atten-\ntion and linear attention. Another idea, which may have over lap with the previous discussion, is\nto explicitly encode the context via an additional memory mo del.\n2.3.3.1 Fixed-size KV Cache\nA straightforward approach is to represent the keys and valu es using a ﬁxed-size memory model.\nSuppose we have a memory Mem which retains the contextual information. We can write the\nattention operation at position iin a general form\nAtt(qi,Mem) = Att qkv(qi,K≤i,V≤i) (2.52)\nIn this model, Mem is simply the KV cache, i.e., Mem = ( K≤i,V≤i). Thus the size of\nMem is determined by i. If we deﬁne Mem as a ﬁxed-size variable, then the cost of performing\nAtt(qi,Mem) will be ﬁxed. There are several alternative ways to design Mem .\n• One of the simplest methods is to consider a ﬁxed-size windo w of previous keys and values.\nMem is therefore given by\nMem = ( K[i−nc+1,i],V[i−nc+1,i]) (2.53)\nwherencdenotes the size of the window. The notation K[i−nc+1,i]andV[i−nc+1,i]denote\nthe keys and values over positions from i−nc+ 1toi.11This model can be seen as a type\nof local attention model.\n• It is also possible to deﬁne Mem as a pair of summary vectors, which leads to a more\ncompressed representation of the history. A simple way to su mmarize the previous keys\nand values is to use the moving average of them. For example, Mem can be deﬁned as the\nunweighted moving average of the previous nckeys and values\nMem =(∑i\nj=i−nc+1kj\nnc,∑i\nj=i−nc+1vj\nnc)\n(2.54)\nAlternatively, we can use a weighted version of moving avera ge\nMem =(∑i\nj=i−nc+1βj−i+nckj∑nc\nj=1βj,∑i\nj=i−nc+1βj−i+ncvj∑nc\nj=1βj)\n(2.55)\nHere {β1,...,β nc}are the coefﬁcients, which can be either learned as model par ameters\nor determined via heuristics. For example, they can be set to increasing coefﬁcients (i.e.,\nβ1<β 2<...<β nc−1<β nc) in order to give larger weight to positions that are closer t o\n11More formally",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 64,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "805b2954a8b52e47",
    "content": " and values\nMem =(∑i\nj=i−nc+1kj\nnc,∑i\nj=i−nc+1vj\nnc)\n(2.54)\nAlternatively, we can use a weighted version of moving avera ge\nMem =(∑i\nj=i−nc+1βj−i+nckj∑nc\nj=1βj,∑i\nj=i−nc+1βj−i+ncvj∑nc\nj=1βj)\n(2.55)\nHere {β1,...,β nc}are the coefﬁcients, which can be either learned as model par ameters\nor determined via heuristics. For example, they can be set to increasing coefﬁcients (i.e.,\nβ1<β 2<...<β nc−1<β nc) in order to give larger weight to positions that are closer t o\n11More formally, we write K[i−nc+1,i]=\nki−nc+1\n...\nki\nandV[i−nc+1,i]=\nvi−nc+1\n...\nvi\n. Sometimes we denote\nK[i−nc+1,i]by{ki−nc+1,...,ki}andV[i−nc+1,i]by{vi−nc+1,...,vi}for notation simplicity.\n\n2.3 Long Sequence Modeling 73\ni. We can extend the moving average to include all the position s up toi. This leads to the\ncumulative average of the keys and values, given in the form\nMem =(∑i\nj=0kj\ni+ 1,∑i\nj=0vj\ni+ 1)\n(2.56)\nIn general, the cumulative average can be written using a rec ursive formula\nMem i=(ki,vi) +i·Mem i−1\ni+ 1(2.57)\nwhere Mem iandMem i−1denote the cumulative averages of the current and previous p o-\nsitions, respectively. An advantage of this model is that we only need to store a single\nkey-value pair during inference, rather than storing all th e key-value pairs. Note that the\nabove memory models are related to recurrent models, and mor e advanced techniques have\nbeen used to develop alternatives to self-attention mechan isms in Transformers [ Ma et al. ,\n2023 ].\n• The memory Mem can also be a neural network. At each step, it takes both the pr evious\noutput of the memory and the current states of the model as inp ut, and produces the new\noutput of the memory. This neural network can be formulated a s the function\nMem = Update( Skv,Mem pre) (2.58)\nHere Mem andMem prerepresent the outputs of the memory at the current step and th e\nprevious step, respectively. Skvis a set of key-value pairs, representing the recent states o f\nthe model. This formulation is general and allows us to devel op various memory models by\nselecting different Update( ·)andSkvconﬁgurations. For example, if Skvonly contains the\nlatest key-value pair (ki,vi)andUpdate( ·)is deﬁned as a recurrent cell, then Eq. ( 2.58)\ncan be expressed as an RNN-like model\nMem =f((ki,vi),Mem pre) (2.59)\nwheref(·)is a recurrent cell. Recurrence can also be applied to segmen t-level modeling\nfor efﬁciency consideration. A simple approach is that we ca n divide the sequence into\nsegments, and treat Skvas a segment. Applying recurrent models to Update( ·)will result in\nmemory models that operate on segments. A special example is that we deﬁne Update( ·)as\nan FIFO function that adds Skvinto the memory and removes the oldest key-value segment\nfrom the memory, given by\nMem = FIFO( Skv,Mem pre) (2.60)\nConsider a memory which includes two segments, one for curre nt segment, and one for the\nprevious segment. In the attention operation, each positio n can access the history key-value\npairs in two closest consecutive segments. This essentiall y deﬁnes a local memory, but it\nand its variants have been widely used segment-level recurr ent models [ Dai et al. ,2019 ;\nHutchins et al. ,2022 ;Bulatov et al. ,2022 ].\n• The above memory models",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 65,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "87cf9beca71f6430",
    "content": " n divide the sequence into\nsegments, and treat Skvas a segment. Applying recurrent models to Update( ·)will result in\nmemory models that operate on segments. A special example is that we deﬁne Update( ·)as\nan FIFO function that adds Skvinto the memory and removes the oldest key-value segment\nfrom the memory, given by\nMem = FIFO( Skv,Mem pre) (2.60)\nConsider a memory which includes two segments, one for curre nt segment, and one for the\nprevious segment. In the attention operation, each positio n can access the history key-value\npairs in two closest consecutive segments. This essentiall y deﬁnes a local memory, but it\nand its variants have been widely used segment-level recurr ent models [ Dai et al. ,2019 ;\nHutchins et al. ,2022 ;Bulatov et al. ,2022 ].\n• The above memory models can be extended to involve multiple memories. An example\n\n74 Generative Models\nof this approach is compressive Transformer [ Rae et al. ,2019 ]. It employs two distinct\nﬁxed-size memories: one for modeling local context (denote d by Mem ), and the other for\nmodeling and compressing long-term history (denoted by CMem ). The KV cache in this\nmodel is the combination of Mem andCMem . The attention function can be written as\nAttcom(qi,Mem,CMem) = Att qkv(qi,[Mem,CMem]) (2.61)\nwhere [Mem,CMem] is a combined memory of Mem andCMem . As with other segment-\nlevel models, the compressive Transformer model operates o n segments of the sequence.\nEach segment is a sequence of nsconsecutive tokens, and we denote Sk\nkvas the key-value\npairs corresponding to the tokens of the k-th segment. When a new segment arrives, Mem\nis updated in an FIFO fashion: we append the nckey-value pairs in Sk\nkvtoMem , and then\npop thensoldest key-value pairs from Mem , which is given by\nMem = FIFO( Sk\nkv,Mem pre) (2.62)\nThe popped key-value pairs are then used to update the compre ssive memory CMem . These\nnskey-value pairs are compressed intons\nckey-value pairs via a compression network.\nCMem is an FIFO which appends the compressedns\nckey-value pairs to the tail of the\nqueue, and drops the ﬁrstns\nckey-value pairs of the queue. It is given by\nCMem = FIFO( Ck\nkv,CMem pre) (2.63)\nwhereCk\nkvrepresents the set of compressed key-value pairs. Implicit in the compressive\nTransformer model is that local context should be represent ed explicitly with minimal in-\nformation loss, while long-range context can be more compre ssed.\n• We have already seen that both global and local contexts are useful and can be mod-\neled using attention models. This view motivates the extens ion to attention models for\ncombining both local and long-term memories [ Ainslie et al. ,2020 ;Zaheer et al. ,2020 ;\nGupta and Berant ,2020 ]. A simple but widely-used approach is to involve the ﬁrst fe w to-\nkens of the sequence in attention, serving as global tokens. This approach is usually applied\nalong with other sparse attention models. An advantage of in corporating global tokens of\nthe sequence is that it helps smooth the output distribution of the Softmax function used in\nattention weight computation, and thus stabilizes model pe rformance when the context size\nis very large [ Xiao et al. ,2024 ]. One drawback, however, is that using a ﬁxed-size global\nmemory may result in information loss. When dealing with lon g sequences, we need to\nenlarge the KV cache for sufﬁcient representations of the co ntext, but this in turn increases\nthe computational cost.\nFigure 2.6shows illustrations of the above approaches. Note that, whi le we focus on optimiza-\ntion of the KV cache here, this issue is closely related to tho se discussed in the previous section.\nAll of the methods we have mentioned so far can broadly be cate gorized as efﬁcient attention\napproaches, which are widely used in various Transformer va riants.\n\n2.3 Long Sequence Modeling 75\n· · ·\n· · ·\ni i −1 i−2 i−3 i−4 i−5 i−6 i−7Keys\nValuesSize = 4 ×2Memory\n(a) Window-based Cache\n· · ·\n· · ·\ni i −",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 66,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "1c63cf1ee76d4f53",
    "content": " a ﬁxed-size global\nmemory may result in information loss. When dealing with lon g sequences, we need to\nenlarge the KV cache for sufﬁcient representations of the co ntext, but this in turn increases\nthe computational cost.\nFigure 2.6shows illustrations of the above approaches. Note that, whi le we focus on optimiza-\ntion of the KV cache here, this issue is closely related to tho se discussed in the previous section.\nAll of the methods we have mentioned so far can broadly be cate gorized as efﬁcient attention\napproaches, which are widely used in various Transformer va riants.\n\n2.3 Long Sequence Modeling 75\n· · ·\n· · ·\ni i −1 i−2 i−3 i−4 i−5 i−6 i−7Keys\nValuesSize = 4 ×2Memory\n(a) Window-based Cache\n· · ·\n· · ·\ni i −1 i−2 i−3 i−4 i−5 i−6 i−7Keys\nValuesSize = 1 ×2Memory ki−3+ki−2+ki−1+ki\n4⇒\nvi−3+vi−2+vi−1+vi\n4⇒\n(b) Moving Average-based Cache\n· · ·\n· · ·\ni i −1 i−2 i−3 i−4 i−5 i−6 i−7Keys\nValuesSize = 1 ×2MemoryMem = Update( Skv,Mem pre)⇒\n(c) Recurrent Network as Cache\n· · ·\n· · ·\ni i −1 i−2 i−3 i−4 i−5 i−6 i−7Keys\nValuesSize = 4 ×2Memory\nSize = 2 ×2MemoryCompressed\n(d) Hybrid Cache (Compressed Memory + Local Memory)\nFig. 2.6: Illustrations of ﬁxed-size KV caches in LLMs. Blue boxes rep resent the keys and values generated during\nLLM inference, green boxes represent the keys and values sto red or encoded in the primary memory, and orange boxes\nrepresent the keys and values stored or encoded in the compre ssed memory.\n\n76 Generative Models\n2.3.3.2 Memory-based Models\nThe modeling of memories discussed above was based on update s to the KV cache, and the re-\nsulting models are typically referred to as internal memories . We now consider another family\nof models, called external memories , which operate as independent models to access large-scale\ncontexts for LLMs. Many such models are based on memory-based methods which have been\nextensively discussed in machine learning [ Bishop ,2006 ]. A common example is nearest neigh-\nbor algorithms: we store context representations in a datas tore, and try to ﬁnd the most similar\nstored representations to match a given query. The retrieve d context representations are then used\nto improve attention for this query.\nHere, we consider the k-nearest neighbors (k-NN) method which is one of the most popular\nmemory-based methods. Since our focus is language modeling in this section, we deﬁne a sample\nin the datastore as a key-value pair corresponding to some co ntext state. Note that “context” is a\nbroad concept here, not just a sequence preﬁx in text generat ion. One might, for example, view\nthe entire dataset as the context for predicting tokens. Thi s allows us to retrieve the closest context\nsituation in a set of sequences, rather than a given sequence preﬁx. Although we will restrict\nourselves to context modeling for a single sequence, in this subsection, we discuss a relatively\nmore general case.\nSuppose we have a set of keys {kj}with corresponding values {vj}, and suppose we store\nthese key-value pairs in a vector database12. For each query qi, we ﬁnd itsknearest neighbours by\ngrowing the radius of the sphere centered as qiuntil it contains kdata points in {kj}. This results\nin a set ofkkeys along with their corresponding values, denoted by Mem knn. As before, we\ndenote Mem as the local memory for the query, such as the KV cache of neigh boring tokens. Our\ngoal is to attend query qito both the local memory Mem and the long-term memory Mem knn.\nThere are, of course, several ways to incorporate Mem andMem knninto the attention model.\nFor example, we might simply combine them to form a single KV c ache [Mem,Mem knn], and\nattend qito[Mem,Mem knn]via standard QKV attention. Or we might use Mem andMem knn\nin separate attention steps",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 67,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "8af5234c60bb7530",
    "content": " corresponding values {vj}, and suppose we store\nthese key-value pairs in a vector database12. For each query qi, we ﬁnd itsknearest neighbours by\ngrowing the radius of the sphere centered as qiuntil it contains kdata points in {kj}. This results\nin a set ofkkeys along with their corresponding values, denoted by Mem knn. As before, we\ndenote Mem as the local memory for the query, such as the KV cache of neigh boring tokens. Our\ngoal is to attend query qito both the local memory Mem and the long-term memory Mem knn.\nThere are, of course, several ways to incorporate Mem andMem knninto the attention model.\nFor example, we might simply combine them to form a single KV c ache [Mem,Mem knn], and\nattend qito[Mem,Mem knn]via standard QKV attention. Or we might use Mem andMem knn\nin separate attention steps. An example of such approaches i s the model developed by Wu et al.\n[2021 ]. It linearly combines the two types of attention, given by\nAtt(qi,Mem,Mem knn) = g⊙Attlocal+ (1 −g)⊙Attknn (2.64)\nAttlocal = Att( qi,Mem) (2.65)\nAttknn= Att( qi,Mem knn) (2.66)\nHere g∈Rdis the coefﬁcient vector, which can be the output of a learned gate.\nGiven thek-NN-based memory model described above, the remaining task is to determine\nwhich key-value pairs are retained in the datastore. For sta ndard language modeling tasks, we\nconsider the previously seen tokens in a sequence as the cont ext, so we can add the keys and\nvalues of all these tokens into the datastore. In this case, t he resulting k-NN-based attention\nmodel is essentially equivalent to a sparse attention model [Gupta et al. ,2021 ].\nAlternatively, we can extend the context from one sequence t o a collection of sequences.\nFor example, we might collect all key-value pairs across the sequences in a training dataset and\nadd them to the datastore to model a larger context. Thus, LLM s can predict tokens based on a\n12A vector database, or vector store, is a database that provid es highly optimized retrieval interfaces for ﬁnding stored\nvectors that closely match a query vector.\n\n2.3 Long Sequence Modeling 77\ngeneralized context. A problem with this approach is that th e computational cost would be large\nif many sequences are involved. Since these sequences are pa rt of our training data, we can build\nand optimize an index for the vectors in the datastore before running the LLMs. As a result, the\nretrieval of similar vectors can be very efﬁcient, as in most vector databases.\nIn fact, all the above-mentioned methods can be viewed as ins tances of a retrieval-based ap-\nproach. Instead of using retrieval results to improve atten tion, we can apply this approach in other\nways as well. One application of k-NN-based search is k-NN language modeling (ork-NN LM )\n[Khandelwal et al. ,2020 ]. The idea is that, although it is attempting to extend the co ntext used\nin self-attention by incorporating nearest neighbors in re presentation learning, in practice, similar\nhidden states in Transformers are often highly predictive o f similar tokens in subsequent positions.\nInk-NN LM, each item in the datastore is a key-value tuple (z,w), where zrepresents a hidden\nstate of the LLM at a position, and wrepresents the corresponding prediction. A typical way to\ncreate the datastore is to collect the output vector of the Tr ansformer layer stack and the corre-\nsponding next token for each position of each sequence in a tr aining dataset. During inference,\nwe have a representation higiven a preﬁx. Given this representation, we ﬁrst search the datastore\nforkclosest matching data items {(z1,w1),...,(zk,wk)}. Here {w1,...,w k}are thought of as\nreference tokens for prediction, and thus can be used to guid e the token prediction based on hi.\nOne common way to make use of reference tokens is to deﬁne a dis tribution over the vocabulary\nV,\nPrknn(·|hi) = Softmax([\n−d0· · · −d|V|]\n) (2.67)\nwheredvequals the distance between hiandzjifwjequals thev-th entry of V, and equals 0\notherwise. We use a linear function with a coefﬁcient",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 68,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "c1df6c100b71fa42",
    "content": " ansformer layer stack and the corre-\nsponding next token for each position of each sequence in a tr aining dataset. During inference,\nwe have a representation higiven a preﬁx. Given this representation, we ﬁrst search the datastore\nforkclosest matching data items {(z1,w1),...,(zk,wk)}. Here {w1,...,w k}are thought of as\nreference tokens for prediction, and thus can be used to guid e the token prediction based on hi.\nOne common way to make use of reference tokens is to deﬁne a dis tribution over the vocabulary\nV,\nPrknn(·|hi) = Softmax([\n−d0· · · −d|V|]\n) (2.67)\nwheredvequals the distance between hiandzjifwjequals thev-th entry of V, and equals 0\notherwise. We use a linear function with a coefﬁcient λthat interpolates between the retrieval-\nbased distribution Prknn(·|hi)and the LLM output distribution Prlm(·|hi)\nPr(·|hi) =λ·Prknn(·|hi) + (1 −λ)·Prlm(·|hi) (2.68)\nThen, as usual, we can choose the next token yby maximizing the probability Pr(y|hi).\nAs with information retrieval (IR) systems, the datastore c an also manage texts and provide\naccess to relevant texts for a query. For example, we can stor e a collection of text documents\nin a search engine with full-text indexing, and then search i t for documents that match a given\ntext-based query. Applying IR techniques to LLMs leads to a g eneral framework called retrieval-\naugmented generation (RAG ). The RAG framework works as follows. We use the context xas\nthe query and ﬁnd the kmost relevant document pieces {c1,...,ck}from the datastore via efﬁcient\nIR techniques13. These search results are combined with the original contex t via a prompting\n13In piratical applications, queries are typically generate d using a query generation system, which may expand it\nwith variations of tokens and query intent.\n\n78 Generative Models\ntemplateg(·)14, resulting in an augmented input for the LLM\nx′=g(c1,...,ck,x) (2.69)\nThen, we use x′as the context and predict the following text using the model Pr(y|x′). One\nadvantage of RAG is that we need not modify the architecture o f LLMs, but instead augment the\ninput to LLMs via an additional IR system. Figure 2.7shows a comparison of the use of different\nexternal memories in LLMs.\n2.3.3.3 Memory Capacity\nA memory model in LLMs, in the form of a simple key-value cache or a datastore, can broadly\nbe seen as an encoder of contextual information. Ideally, be fore we say that a memory model\nis representative of the entire context in token prediction , we need to make sure that the model\ncan accurately represent any part of the context. The standa rd KV cache is one such model that\ncompletely stores all past history. In this case, the model i s said to have adequate capacity for\nmemorizing the context. In many practical applications, ho wever, complete memorization is not\nrequired. Instead, the goal is to enable LLMs to access impor tant contextual information. As a\nresult, efﬁcient and compressed memory models are develope d, as described in this section. Note\nthat, the longer the sequence, the more difﬁcult it becomes f or a low-capacity memory model to\ncapture important contextual information. It is therefore common practice to simply increase the\nmodel capacity when processing long contexts.\nWhile high-capacity models are generally favorable, they a re difﬁcult to train and deploy. A\nchallenging scenario is that the tokens arrive in a stream an d the context continuously grows.\nDeveloping LLMs for such tasks is difﬁcult as we need to train Transformers on extremely long\nsequences. A possible way to address this difﬁculty is to use non-parametric methods, such as\nretrieval-based methods. For example, as discussed above, we can use a vector database to store\npreviously generated key-value pairs, and thus represent t he context by this external memory\nmodel. Although this approach side-steps the challenge of r epresenting long context in Trans-\nformers, building and updating external memory models are c omputationally expensive. These\nmodels are more often used in problems where the context is gi ven in advance",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 69,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "94c9e78c09a83cb3",
    "content": ". It is therefore common practice to simply increase the\nmodel capacity when processing long contexts.\nWhile high-capacity models are generally favorable, they a re difﬁcult to train and deploy. A\nchallenging scenario is that the tokens arrive in a stream an d the context continuously grows.\nDeveloping LLMs for such tasks is difﬁcult as we need to train Transformers on extremely long\nsequences. A possible way to address this difﬁculty is to use non-parametric methods, such as\nretrieval-based methods. For example, as discussed above, we can use a vector database to store\npreviously generated key-value pairs, and thus represent t he context by this external memory\nmodel. Although this approach side-steps the challenge of r epresenting long context in Trans-\nformers, building and updating external memory models are c omputationally expensive. These\nmodels are more often used in problems where the context is gi ven in advance and ﬁxed during\ninference, and hence unsuitable for streaming context mode ling.\nIn cases where the size of the context continuously grows, ap plying ﬁxed-size memory models\nis a commonly used approach. For example, in recurrent model s, a sequence of arbitrary length\ncan be summarized into a set of hidden states by which we have a ﬁxed computational cost per\nstep. While recurrent models were initially found to be not v ery good at handling long-distance\ndependencies in sequence modeling in early applications of deep learning to NLP, recent advance-\nments have shown that their variants are now effective in mod eling extremely long sequences.\n[Bulatov et al. ,2022 ;Hutchins et al. ,2022 ;Munkhdalai et al. ,2024 ;Ma et al. ,2024 ].\n14For example, the template could be:\nmessage = {* c1*} ... {* ck*}\ninput: {* x*}\noutput:\n\n2.3 Long Sequence Modeling 79\n· · ·\n· · ·\nDatastoreqi\nSearchKV Cache kNearest\nNeighborsAtt(qi,Mem) Att(qi,Mem knn)g⊙Att(qi,Mem) + (1 −g)⊙Att(qi,Mem knn)\nKeys/values in LLM\nKeys/values in Datastore\n(a)k-NN Search Augmented Attention\n· · ·\n· · ·\nDatastoreqi\nSearchKV Cache kNearest\nNeighborsAtt(qi,Mem) Att(qi,Mem)· · ·Distribution Pr(·)\nDistribution Prknn(·)Output Distribution\nKeys/values in LLM\nKeys in Datastore\nPredicted Tokens\n(b)k-NN Language Modeling\nInput Context:\nx=What is deep learning?DatastoreSearch· · ·c2=Machine learning is ...c1=Deep network is ...\nkNearest\nNeighborsWhat is deep learning?Message: deep network ... machine learning ...LLM\n(c) Retrieval-augmented Generation\nFig. 2.7: Illustrations of external memories (or datastores) for lan guage modeling.\n\n80 Generative Models\nThere is no general deﬁnition of memory capacity in LLMs. A si mple approach might consider\nhow much storage is used to retain contextual information. F or example, memory capacity could\nbe deﬁned by the size of the KV cache in Transformers or the vec tor database used in retrieval-\nbased methods. A related concept is model complexity. In mac hine learning, there are several\nways to deﬁne the model complexity of a model. One of the simpl est methods is by counting the\nnumber of parameters. However, it should be emphasized that the memory models discussed here\nprimarily serve to store information, rather than add train able parameters. Therefore, a model with\na large memory capacity is not necessarily more complex. Nev ertheless, in practice determining\nthe capacity of a memory model is not straightforward. In gen eral, we need to control the trade-off\nbetween maximizing the performance and controlling the mem ory footprint.\n2.3.4 Sharing across Heads and Layers\nIn Transformers, the KV cache is a data structure that can be d ynamically adjusted along multiple\ndimensions, such as heads, layers, and sequence length. For example, consider an LLM with L\nlayers. Each layer has τattention heads, and each head produces a dh-dimensional output. During\ninference, we store the keys and values for up to mtokens. The space complexity of this caching\nmechanism is O(L·τ·dh·m). As we have seen previously, this complexity can be reduced b y\ncaching the keys and values for fewer tokens. For example, in sliding window attention, a",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 70,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "64811edfa5a03f5b",
    "content": ", a model with\na large memory capacity is not necessarily more complex. Nev ertheless, in practice determining\nthe capacity of a memory model is not straightforward. In gen eral, we need to control the trade-off\nbetween maximizing the performance and controlling the mem ory footprint.\n2.3.4 Sharing across Heads and Layers\nIn Transformers, the KV cache is a data structure that can be d ynamically adjusted along multiple\ndimensions, such as heads, layers, and sequence length. For example, consider an LLM with L\nlayers. Each layer has τattention heads, and each head produces a dh-dimensional output. During\ninference, we store the keys and values for up to mtokens. The space complexity of this caching\nmechanism is O(L·τ·dh·m). As we have seen previously, this complexity can be reduced b y\ncaching the keys and values for fewer tokens. For example, in sliding window attention, a ﬁxed-\nsize window is used to cache the keys and values in local conte xt. And this model has a space\ncomplexity of O(L·τ·dh·mw), withmwbeing the size of the window.\nIn addition to reducing m, we can also decrease the size of the KV cache along other di-\nmensions. A widely-used approach is to enable sharing acros s heads in multi-head self-attention.\nRecall from Section 2.1.1 that multi-head self-attention uses multiple sets of queri es, keys, and\nvalues (each set is called a head), each performing the QKV at tention mechanism as usual. This\ncan be expressed as\nOutput = Merge(head 1,...,head τ)Whead(2.70)\nwhere head j∈Rdhis computed using the standard QKV attention function\nhead j= Att qkv(q[j]\ni,K[j]\n≤i,V[j]\n≤i) (2.71)\nHere, q[j]\ni,K[j]\n≤i, and V[j]\n≤iare the query, keys, and values that are projected onto the j-th feature\nsub-space. So this model can be interpreted as performing at tention on a group of feature sub-\nspaces in parallel (see Figure 2.8(b)). The KV cache needs to retain the keys and values for all\nthese heads, that is, {(K[1]\n≤i,V[1]\n≤i),...,(K[τ]\n≤i,V[τ]\n≤i)}.\nOne reﬁnement to the multi-head attention model, called multi-query attention (MQA ), is to\nshare keys and values across heads, while allowing queries t o be unique for each head [ Shazeer ,\n2019 ]. In MQA, there is a single set of keys and values (K≤i,V≤i). In addition, there are τ\nqueries {q[1]\ni,...,q[τ]\ni}, each corresponding to a different head. For each head, we ha ve\nhead j= Att qkv(q[j]\ni,K≤i,V≤i) (2.72)\nFigure 2.8(c) illustrates this model. By sharing keys and values, the s ize of the KV cache would\n\n2.3 Long Sequence Modeling 81\nquery key value\n(a) Single-head Attentionquery key value\n(b) Multi-head Attention\nquery key value\n(c) Multi-query Attentionquery key value\n(d) Grouped Query Attention\nquery key value\nLayerl\nLayerl−1Sharing\n(e) Cross-layer Multi-head Attention\nFig. 2.8: Illustration of QKV attention based on different multi-hea d and sharing mechanisms. (a) = single-head\nattention, and (b-e) = attention with multiple heads.\nbeO(L·dh·m).\nGrouped query attention (GQA ) is a natural extension to multi-head attention and MQA\n[Ainslie et al. ,2023 ]. In GQA, heads are divided into nggroups, each corresponding to a shared\nset of keys and values. Hence we have ngsets of keys and values {(K[1]\n≤i,V[1]\n≤i),...,(K[ng]\n≤i,V[ng]\n≤i)}.\nSee Figure 2.8(d) for an illustration. Let g(j)be the group id for the j-th head. The GQA model\ncan be expressed as\nhead j= Att qkv(q[j]\ni,K[g(j)]\n≤i,V[g(j)]\n≤i) (2.73)\nThe size of the KV cache of GQA is O(L·ng·dh·m). One beneﬁt of GQA is",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 71,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "4917cfe6e06a2c0b",
    "content": " heads.\nbeO(L·dh·m).\nGrouped query attention (GQA ) is a natural extension to multi-head attention and MQA\n[Ainslie et al. ,2023 ]. In GQA, heads are divided into nggroups, each corresponding to a shared\nset of keys and values. Hence we have ngsets of keys and values {(K[1]\n≤i,V[1]\n≤i),...,(K[ng]\n≤i,V[ng]\n≤i)}.\nSee Figure 2.8(d) for an illustration. Let g(j)be the group id for the j-th head. The GQA model\ncan be expressed as\nhead j= Att qkv(q[j]\ni,K[g(j)]\n≤i,V[g(j)]\n≤i) (2.73)\nThe size of the KV cache of GQA is O(L·ng·dh·m). One beneﬁt of GQA is that we can trade-off\nbetween computational efﬁciency and model expressiveness by adjusting ng. Whenng=τ, the\nmodel becomes the standard multi-head attention model. By c ontrast, when ng= 1, it becomes\n\n82 Generative Models\nthe GQA model.\nSharing can also be performed across layers. Such a method fa lls into the family of shared\nweight and shared activation methods, which have been exten sively used in Transformers [ Dehghani et al. ,\n2018 ;Lan et al. ,2020 ]. For example, one can share KV activations or attention wei ghts across\nlayers to reduce both computation and memory footprints [ Xiao et al. ,2019 ;Brandon et al. ,2024 ].\nFigure 2.8(e) shows an illustration of this method, where a query in a la yer directly accesses the\nKV cache of a lower-level layer.\n2.3.5 Position Extrapolation and Interpolation\nSince Transformer layers are order-insensitive to input, w e need some way to encode positional\ninformation in the input tokens. To do this, it is common to ad d positional embeddings to token\nembeddings, and then feed these combined embeddings into th e Transformer layer stack as input.\nIn this case, the embedding at position ican be expressed as\nei=xi+ PE(i) (2.74)\nwhere xi∈Rddenotes the token embedding, and PE(i)∈Rddenotes the positional embedding.\nIn general, the token embedding xiis a position-independent vector, and so the positional emb ed-\ndingPE(i)is used to encode the positional context. A straightforward approach is to treat PE(i)\nas a learnable variable and train it alongside other model pa rameters. In this way, we can learn\na unique representation for each position, and thus disting uish the tokens appearing at different\npositions of a sequence.\nRepresentations of positions using learned vectors can wor k well in tasks where the sequences\nat training and test times are of similar lengths. In practic e, however, we often impose length\nrestrictions on sequences during training to prevent exces sive computational costs, but wish to\napply the trained models to much longer sequences during inf erence. In this case, using learned\npositional embeddings has obvious drawbacks, as there are n o trained embeddings for positions\nthat are not observed in the training phase.\nAn alternative approach to modeling positional informatio n is to develop positional embed-\ndings that can generalize: once trained, the embedding mode l can be used to handle longer se-\nquences. Suppose that we train a positional embedding model on sequences with a maximum\nlength ofml, and we wish to apply the trained model to a sequence of length m(m >> m l). If\nthe embedding model is limited in the range of positions that we can observe from training data,\nthen this model will simply fail to deal with new data outside that range. See Figure 2.9(a) for\nan illustration where the learned embedding model cannot mo del data points outside the training\ndomain if it lacks the ability to extrapolate.\nThere are several approaches to making positional embeddin g models generalize. They can\nbe grouped into two classes.\n•Extrapolation . The model learned on observed data points (i.e., positions ) can be directly\nemployed to assign meaningful values to data points beyond t he original range. For ex-\nample, suppose we have a series of numbers 1,2,...,10, and we want to understand the\nmeaning of a new number, 15. Knowing that these numbers are na tural numbers used for\nordering, we can easily infer that 15 is a number that follows 10, even though 15 has not\n\n2.3 Long Sequence Modeling 83\n0 1 ,024 2",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 72,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "625b4b6e17f29e9d",
    "content": " data,\nthen this model will simply fail to deal with new data outside that range. See Figure 2.9(a) for\nan illustration where the learned embedding model cannot mo del data points outside the training\ndomain if it lacks the ability to extrapolate.\nThere are several approaches to making positional embeddin g models generalize. They can\nbe grouped into two classes.\n•Extrapolation . The model learned on observed data points (i.e., positions ) can be directly\nemployed to assign meaningful values to data points beyond t he original range. For ex-\nample, suppose we have a series of numbers 1,2,...,10, and we want to understand the\nmeaning of a new number, 15. Knowing that these numbers are na tural numbers used for\nordering, we can easily infer that 15 is a number that follows 10, even though 15 has not\n\n2.3 Long Sequence Modeling 83\n0 1 ,024 2 ,048−101\nSequence Length\n(a) Encoding with No GeneralizationValue\n0 1 ,024 2 ,048−101\nSequence Length\n(b) ExtrapolationValue\n0 1 ,024 2 ,048−101\nSequence Length\n(c) InterpolationValue\nFig. 2.9: Illustrations of different positional embedding methods f or a range of positions. Blue points represent the\npositions that have been observed during training, and red p oints represent the positions that are newly observed at tes t\ntime. In sub-ﬁgure (a), the encoding model only memorizes th e points seen during training, and cannot generalize. In\nsub-ﬁgures (b) and (c), the model can generalize through ext rapolation and interpolation.\nbeen observed before. Figure 2.9(b) shows an example of this approach, where a function\nis learned to ﬁt the data points within a speciﬁc range and the n applied to estimate the values\nof data points outside that range.\n•Interpolation . This approach maps a larger range of data points into the ori ginal obser-\nvation range. For example, suppose we have a model designed f or numbers in the range\n[1,10]. When given a new range of [1,20], we can scale this down by dividing every num-\nber by 2, thereby ﬁtting all numbers into [1,10]. This scaling allows us to use the model\ntrained on the range [1,10]to describe data points in the expanded range of [1,20]. See\nFigure 2.9(c) for an illustration of this approach.\nIn fact, positional embeddings in many systems have achieve d some level of generalization.\nFor example, sinusoidal encoding, the most common position al embedding method, employs sine\nand cosine functions that can naturally extend to sequences of any length. Although this approach\nmight seem direct and simple, it does not perform well when we signiﬁcantly extend the sequences\nfor processing. In this subsection, we will discuss several alternative methods based on either\nextrapolation or interpolation.\n\n84 Generative Models\n2.3.5.1 Attention with Learnable Biases\nOne problem with Eq. ( 2.74) is that the embedding model treats each token independentl y and\ntherefore ignores the distance between different tokens. A common improvement to this model,\ncalled relative positional embedding, is to consider the pa irwise relationship between tokens\n[Shaw et al. ,2018 ]. The general idea behind this is to obtain the offset betwee n any pair of posi-\ntions and incorporate it into the self-attention model. One of the simplest forms of self-attention\nwith relative positional embedding is given by\nAttqkv(qi,K≤i,V≤i) =i∑\nj=0α(i,j)vj (2.75)\nα(i,j) = Softmax(qikT\nj+PE(i,j)√\nd+ Mask(i,j)) (2.76)\nThe only difference between this model and the original self -attention model is that a bias term\nPE(i,j)is added to the query-key product in this new model. Intuitiv ely,PE(i,j)can be inter-\npreted as a distance penalty for the pair of positions iandj. Asimoves away from j, the value of\nPE(i,j)decreases.\nPE(i,j)can be deﬁned in several different ways. Here, we consider th e T5 version of relative\npositional embedding, called the T5 bias [ Raffel et al. ,2020 ]. For each pair of query qiand key\nkj, the offset between them is deﬁned to be15",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 73,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "51b7acbb3422c7d6",
    "content": "∑\nj=0α(i,j)vj (2.75)\nα(i,j) = Softmax(qikT\nj+PE(i,j)√\nd+ Mask(i,j)) (2.76)\nThe only difference between this model and the original self -attention model is that a bias term\nPE(i,j)is added to the query-key product in this new model. Intuitiv ely,PE(i,j)can be inter-\npreted as a distance penalty for the pair of positions iandj. Asimoves away from j, the value of\nPE(i,j)decreases.\nPE(i,j)can be deﬁned in several different ways. Here, we consider th e T5 version of relative\npositional embedding, called the T5 bias [ Raffel et al. ,2020 ]. For each pair of query qiand key\nkj, the offset between them is deﬁned to be15\nd(i,j) =i−j (2.77)\nA simple design for the bias PE(i,j)is to share the same learnable variable for all query-key\npairs with the same offset, i.e., PE(i,j) =ui−j, whereui−jis the variable corresponding to\nthe offseti−j. However, simply assigning a unique value to each offset wil l restrict this model\nto observed offsets. When i−jis larger than the maximum trained offset, the model cannot\ngeneralize.\nThe T5 bias instead adopts a generalization of this model. Ra ther than assigning each query-\nkey offset a unique bias term, it groups difference offsets i nto “buckets”, each corresponding to\none learnable parameter. More speciﬁcally, the bias terms f ornb+ 1buckets are given as follows.\n• For buckets 0 tonb+1\n2−1, each bucket corresponds to one offset, that is, bucket 0 ↔offset\n0, bucket 1 ↔offset 1, bucket 2 ↔offset 2, and so on. We express this as b(i−j) =i−j.\n• For bucketsnb+1\n2tonb, the size of each bucket increases logarithmically. For exa mple, the\nbucket number for a given offset i−j≥nb+1\n2can be deﬁned as\nb(i−j) =nb+ 1\n2+⌊log(i−j)−log(nb+1\n2)\nlog(dist max)−log(nb+1\n2)·nb+ 1\n2⌋ (2.78)\nwhere the parameter dist maxis typically set to a relatively large number to indicate the\n15For language modeling, a query is only allowed to attend to it s left-context, and so we have i−j≥0. In the more\ngeneral case of self-attention, where a token can attend to a ll tokens in the sequence, we may have negative offsets\nwheni<j .\n\n2.3 Long Sequence Modeling 85\n0 1 2 3· · · 14 15 16 17 18 · · · 32 Bucket\nOffset\n(i−j)0 1 2 3 14 15 16 ∼20 21 ∼26 27 ∼33 802 ∼ ∞ﬁxed bucket size logarithmically increased bucket size\nFig. 2.10: Illustration of distributing query-key offsets into bucke ts in the T5 model ( nb= 32 anddist max= 1024 ).\nBoxes represent buckets. In the ﬁrst half of the buckets, we u se a ﬁxed bucket size. In the second half of the buckets,\nwe increase the bucket size logarithmically. The last bucke t contains all the query-key offsets that are not covered by\nprevious buckets.\nmaximum offset we may encounter.\n• Wheni−j >dist max, we placei−jin the last bucket. In other words, bucket nbcontains\nall the offsets that are not assigned to the previous buckets .\nTogether, these can be expressed as the function\nb(i−j)\n=\n\ni−j 0≤i−j <nb+1\n2\nmin(nb,nb+1\n2+⌊log(i−j)−log(nb+1\n2)\nlog(dist max)−log(nb+1\n2)·nb+1\n2⌋)i−j≥nb+1\n2(2.79)\nFigure 2.10 shows an illustration of these buckets. We see that in the ﬁrs t half of the buckets,\neach bucket is associated",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 74,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "9d0074c1eca2b98e",
    "content": " all the query-key offsets that are not covered by\nprevious buckets.\nmaximum offset we may encounter.\n• Wheni−j >dist max, we placei−jin the last bucket. In other words, bucket nbcontains\nall the offsets that are not assigned to the previous buckets .\nTogether, these can be expressed as the function\nb(i−j)\n=\n\ni−j 0≤i−j <nb+1\n2\nmin(nb,nb+1\n2+⌊log(i−j)−log(nb+1\n2)\nlog(dist max)−log(nb+1\n2)·nb+1\n2⌋)i−j≥nb+1\n2(2.79)\nFigure 2.10 shows an illustration of these buckets. We see that in the ﬁrs t half of the buckets,\neach bucket is associated with only one value of i−j, while in the second half, the bucket size\nincreases as i−jgrows. The last bucket is designed to handle sequences of arb itrarily long\nlengths.\nAllPE(i,j)s in a bucket share the same bias term ub(i−j). Substituting PE(i,j) =ub(i−j)\ninto Eq. ( 2.76), the attention weight for qiandkjbecomes16\nα(i,j) = Softmax(qikT\nj+ub(i−j)√\nd+ Mask(i,j)) (2.81)\nThe parameters {u0,...,u nb}are learned as common parameters during training. It should\nbe emphasized that this model can generalize to long sequenc es. This is because PE(i,j)s with\nsimilar query-key offsets share the same parameter, and thi s sharing strategy is particularly im-\nportant for achieving good generalization, given that larg e query-key offsets are rare in training.\nIn practice, we often set nbto a moderate number, and thus it can help control the overﬁtt ing of\npositional embedding models.\n16Note that, in Raffel et al. [2020 ]’s T5 model, the rescaling operation for the query-key prod uct is removed. The\nattention weight α(i,j)is then given by\nα(i,j) = Softmax( qikT\nj+ub(i−j)+ Mask(i,j)) (2.80)\n\n86 Generative Models\n2.3.5.2 Attention with Non-learned Biases\nRelative positional embedding models are based on a set of le arned biases for the query-key prod-\nuct in self-attention. An alternative approach is to give th ese biases ﬁxed values via heuristics,\nrather than training them on a particular dataset. One beneﬁ t of this heuristics-based approach is\nthat it does not rely on a training process and thus can be dire ctly applied to any sequences once\nthe biases are set.\nOne example of such an approach is Press et al. [2022 ]’s approach, called attention with\nlinear biases orALiBi for short. In the ALiBi approach, the bias term is deﬁned as th e negative\nscaled query-key offset\nPE(i,j) = −β·(i−j)\n=β·(j−i) (2.82)\nwhereβis the scaling factor. Adding this term to the query-key prod uct, we obtain a new form of\nattention weights\nα(i,j) = Softmax(qikT\nj+β·(j−i)√\nd+ Mask(i,j)) (2.83)\nThis model can be interpreted as adding a ﬁxed penalty to qikT\njwheneverjmoves one step\naway fromi. So we do not need to adapt it to a range of sequence lengths, an d can employ it to\nmodel arbitrarily long sequences. See Figure 2.11 for a comparison of the T5 bias and the ALiBi\nbias.\nIn general, the scalar βshould be tuned on a validation dataset. However, Press et al. [2022 ]\nfound that setting βto values decreasing geometrically by a factor of1\n2afor multi-head attention\nperforms well on a variety of tasks. Speciﬁcally, for a self- attention sub-layer involving nhead\nheads, the scalar for the k-th head is given by\nβk=1\n28\nk(2.84)\nThe ALiBi approach provides a simple form of relative positi onal embeddings. There are\nother similar methods for designing query-key biases using the offseti−j. Table 2.4shows a\ncomparison of such biases",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 75,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "bfdc898e4c36a398",
    "content": "\naway fromi. So we do not need to adapt it to a range of sequence lengths, an d can employ it to\nmodel arbitrarily long sequences. See Figure 2.11 for a comparison of the T5 bias and the ALiBi\nbias.\nIn general, the scalar βshould be tuned on a validation dataset. However, Press et al. [2022 ]\nfound that setting βto values decreasing geometrically by a factor of1\n2afor multi-head attention\nperforms well on a variety of tasks. Speciﬁcally, for a self- attention sub-layer involving nhead\nheads, the scalar for the k-th head is given by\nβk=1\n28\nk(2.84)\nThe ALiBi approach provides a simple form of relative positi onal embeddings. There are\nother similar methods for designing query-key biases using the offseti−j. Table 2.4shows a\ncomparison of such biases. As an aside it is worth noting that the form of the right-hand side\nof Eq. ( 2.82) is very similar to length features used in conventional fea ture-based systems. For\nexample, in statistical machine translation systems, such features are widely used to model word\nreordering problems, resulting in models that can generali ze well across different translation tasks\n[Koehn ,2010 ].\n2.3.5.3 Rotary Positional Embedding\nAs with sinusoidal embeddings, rotary positional embeddin gs are based on hard-coded values for\nall dimensions of an embedding [ Su et al. ,2024 ]. Recall that in the sinusoidal embedding model,\npositions are represented as combinations of sine and cosin e functions with different frequencies.\nThese embeddings are then added to token embeddings to form t he inputs to the Transformer\n\n2.3 Long Sequence Modeling 87\nq0kT\n0\nq1kT\n0q1kT\n1\nq2kT\n0q2kT\n1q2kT\n2\nq3kT\n0q3kT\n1q3kT\n2q3kT\n3\nq4kT\n0q4kT\n1q4kT\n2q4kT\n3q4kT\n4\nq5kT\n0q5kT\n1q5kT\n2q5kT\n3q5kT\n4q5kT\n5\nq6kT\n0q6kT\n1q6kT\n2q6kT\n3q6kT\n4q6kT\n5q6kT\n6qikT\nj\nu0\nu1u0\nu2u1u0\nu2u2u1u0\nu3u2u2u1u0\nu3u3u2u2u1u0\nu3u3u3u2u2u1u0Bias ( ub(i−j))\n+\n(a) The T5 bias ( nb= 3anddist max= 5)\nq0kT\n0\nq1kT\n0q1kT\n1\nq2kT\n0q2kT\n1q2kT\n2\nq3kT\n0q3kT\n1q3kT\n2q3kT\n3\nq4kT\n0q4kT\n1q4kT\n2q4kT\n3q4kT\n4\nq5kT\n0q5kT\n1q5kT\n2q5kT\n3q5kT\n4q5kT\n5\nq6kT\n0q6kT\n1q6kT\n2q6kT\n3q6kT\n4q6kT\n5q6kT\n6qikT\nj\n0\n−1β0\n−2β−1β0\n−3β−2β−1β0\n−4β−3β−2β−1β0\n−5β−4β−3β−2β−β 0\n−6β−5β−4β−3β−2β−β 0Bias ( −β(i−j))\n+\n(b) The ALiBi bias\nFig. 2.11: Query-key products with biases (above = the T5 bias and below = the ALiBi bias). The color scale of the\nbiases ranges from light blue denoting small absolute value s to deep blue denoting large absolute values.\nlayer stack. Rotary positional embeddings instead model po sitional context as rotations to token\nembeddings",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 76,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "472201c11e0693bb",
    "content": "T\n2q6kT\n3q6kT\n4q6kT\n5q6kT\n6qikT\nj\n0\n−1β0\n−2β−1β0\n−3β−2β−1β0\n−4β−3β−2β−1β0\n−5β−4β−3β−2β−β 0\n−6β−5β−4β−3β−2β−β 0Bias ( −β(i−j))\n+\n(b) The ALiBi bias\nFig. 2.11: Query-key products with biases (above = the T5 bias and below = the ALiBi bias). The color scale of the\nbiases ranges from light blue denoting small absolute value s to deep blue denoting large absolute values.\nlayer stack. Rotary positional embeddings instead model po sitional context as rotations to token\nembeddings in a complex space. This leads to a model expresse d in the form of multiplicative\nembeddings\nei=xiR(i) (2.85)\nwhereR(i)∈Rd×dis the rotation matrix representing the rotations performe d on the token\nembedding xi∈Rd.\nFor simplicity, we will ﬁrst consider embeddings with only t wo dimensions and return to a\ndiscussion of the more general formulation later. Suppose w e have a 2-dimensional token embed-\nding x=[\nx1x2]\n. We can represent it as a vector in a plane, originating at the origin (0,0)\nand terminating at (x1,x2). A counterclockwise rotation of this vector refers to an ope ration of\n\n88 Generative Models\nEntry Query-Key Bias ( PE(i,j))\nT5 [Raffel et al. ,2020 ]ub(i−j)\nALiBi [ Press et al. ,2022 ] −β·(i−j)\nKerple [ Chi et al. ,2022 ] −β1(i−j)β2(power)\n−β1log(1 +β2(i−j))(logarithmic)\nSandwich [ Chi et al. ,2023 ]∑¯d/2\nk=1cos((i−j)/100002k/¯d)\nFIRE [ Li et al. ,2024 ]f(ψ(i−j)/ψ(max(mlen,i)))\nTable 2.4: Query-key biases as relative positional embeddings. β,β1,β2,¯d, andmlenare hyper-parameters. In the T5\nmodel,b(i−j)denotes the bucket assigned to i−j. In the FIRE model, ψ(·)is a monotonically increasing function\nsuch asψ(x) = log(cx+ 1) , andf(·)is an FFN.\nmoving the vector around the origin while maintaining its ma gnitude, as shown in Figure 2.12 (a).\nThe degree of rotation is usually deﬁned by a speciﬁc angle, d enoted byθ. The rotation can be\nexpressed mathematically in the form\nRo(x,θ) = xRθ\n=[\nx1x2][\ncosθsinθ\n−sinθcosθ]\n=[\ncosθ·x1−sinθ·x2sinθ·x1+ cosθ·x2]\n(2.86)\nwhereRθ=[\ncosθsinθ\n−sinθcosθ]\nis the rotation matrix. If two or more rotations are performe d on the\nsame vector, we can rotate the vector further. This follows f rom the fact that the composition of\nsuccessive rotations is itself a rotation. More formally, r otating a vector by an angle θforttimes\ncan be expressed as\nRo(x,tθ) = xRtθ\n=[\ncostθ·x1−sintθ·x2sintθ·x1+ costθ·x2]\n(2.87)\nIf we interpret tas the position of a token represented by xin a sequence, then we will ﬁnd\nthat the above equation deﬁnes a simple positional embeddin g model. As shown in Figure 2.12\n(b), we start moving the token from position 0. Each time we mo ve one step forward, the vector\nis rotated by the angle θ. Upon arriving at the position t, the representation of the token with\npositional context is given by Ro(x,iθ). As the rotations do not change the magnitude of the\nembedding, the original “meaning” of the token is retained. The positional information is injected\ninto the embedding, when it gets rotated.\nA popular way to understand vector",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 77,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "bf87a4e9952f7d42",
    "content": "forttimes\ncan be expressed as\nRo(x,tθ) = xRtθ\n=[\ncostθ·x1−sintθ·x2sintθ·x1+ costθ·x2]\n(2.87)\nIf we interpret tas the position of a token represented by xin a sequence, then we will ﬁnd\nthat the above equation deﬁnes a simple positional embeddin g model. As shown in Figure 2.12\n(b), we start moving the token from position 0. Each time we mo ve one step forward, the vector\nis rotated by the angle θ. Upon arriving at the position t, the representation of the token with\npositional context is given by Ro(x,iθ). As the rotations do not change the magnitude of the\nembedding, the original “meaning” of the token is retained. The positional information is injected\ninto the embedding, when it gets rotated.\nA popular way to understand vector rotation is to deﬁne it in c omplex spaces. It is easy\nto transform each vector x=[\nx1x2]\nin the 2D Euclidean space R2to a complex number\nx′=x1+ix2in the complex space Cvia a bijective linear map. Then, the rotation of xwith the\nangletθcorresponds to the multiplication by eitθ. Given that eitθ= costθ+isintθ, the rotation\n\n2.3 Long Sequence Modeling 89\nx1x2\nθvector x\nxRθrotated vector\n(a) Single-step Rotationx1x2\nθ\nθ\nθxxRθ\nxR2θ\nxR3θ\n(b) Multi-step Rotation\nx1x2\n7θ\n7θsleeping 4\nsleeping 11cat2\ncat9The 1cat2is3sleeping 4peacefully 5\nin6the7warm 8sunlight 9.10\nEvery 1afternoon 2,3you 4’ll5ﬁnd 6that 7\nthe8cat9is10sleeping 11on12my13bed 14.15\n(c) Angles between embeddings of two tokens at different pos itions\nFig. 2.12: Illustrations of vector rotations in a plane. Sub-ﬁgures (a ) and (b) show rotations of a vector in a single\nstep and multiple steps, respectively. Sub-ﬁgure (c) shows the embeddings of tokens catandsleeping in two different\nsentences. We show these sentences with a subscript afﬁxed t o each token to indicate its position. If we represent\ntokens as vectors, we can add positional information by rota ting these vectors. This rotation preserves the “distances ”\nbetween the vectors. For example, given that the distance be tween catandsleeping is the same in both sentences, the\nangle between their embeddings also remains the same during rotation.\noperation can be re-expressed in the form\nxRtθ↦→x′eitθ\n= (x1+ix2)(costθ+isintθ)\n= costθ·x1−sintθ·x2+i(sintθ·x1+ costθ·x2) (2.88)\nHere we denote the token representation x′eitθbyC(x,tθ). The inner product of the representa-\ntions of the tokens at positions tandscan be written as\n⟨C(x,tθ),C(y,sθ)⟩= (x′y′)ei(t−s)θ(2.89)\nwhere y′is the complex conjugate of y′. As can be seen, the result of this inner product involves\na termt−s, and so it can model the offset between the two tokens.\n\n90 Generative Models\nNow we go back to representations in the 2D Euclidean space. T he dot-product of Ro(x,tθ)\nandRo(y,sθ)is can be written as a function of (t−s)θ\nRo(x,tθ)[Ro( y,sθ)]T=xRtθ[yRsθ]T\n=xRtθ[Rsθ]TyT\n=xR(t−s)θyT(2.90)\nGiven this result, if we consider Ro(x,tθ)andRo(y,sθ)as the query and the key, then the self-\nattention operation will implicitly involve the modeling o f relative positional context.\nThis rotary positional embedding can be extended to multi-d imensional embeddings. For\nad-dimensional token embedding x=[\nx1x2... x d]\n, we can treat it as ad\n2-dimensional\ncomplex vector x′=[\nx",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 78,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "449c1bc3e72de2bd",
    "content": " and so it can model the offset between the two tokens.\n\n90 Generative Models\nNow we go back to representations in the 2D Euclidean space. T he dot-product of Ro(x,tθ)\nandRo(y,sθ)is can be written as a function of (t−s)θ\nRo(x,tθ)[Ro( y,sθ)]T=xRtθ[yRsθ]T\n=xRtθ[Rsθ]TyT\n=xR(t−s)θyT(2.90)\nGiven this result, if we consider Ro(x,tθ)andRo(y,sθ)as the query and the key, then the self-\nattention operation will implicitly involve the modeling o f relative positional context.\nThis rotary positional embedding can be extended to multi-d imensional embeddings. For\nad-dimensional token embedding x=[\nx1x2... x d]\n, we can treat it as ad\n2-dimensional\ncomplex vector x′=[\nx′\n1x′\n2... x′\nd/2]\n=[\nx1+ix2x3+ix4... x d−1+ixd]\n, where\neach consecutive pair of items forms a complex number. Then, the rotary positional embedding in\nthe complex space is given by\nC(x,tθ) =d/2∑\nk=1x′\nkeitθk⃗ ek (2.91)\nwhere⃗ ekis the standard basis vector with a single non-zero value in t hek-th coordinate and 0’s\nelsewhere [ Biderman et al. ,2021 ].\nAlthough this formula involves a complicated expression, i ts equivalent form in the d-dimensional\nEuclidean space is relatively easy to understand. We can wri te it as\nRo(x,tθ) =[\nx1x2... x d]\nRtθ1\nRtθ2\n...\nRtθd/2\n(2.92)\nwhereRtθk=[\ncostθksintθk\n−sintθkcostθk]\n.θ=[\nθ1,...,θ d/2]\nare the parameters for controlling the an-\ngles of rotations in different dimensions. Typically, θkis set to 10000−2(k−1)\nd, which is analogous\nto the setting in sinusoidal embeddings.\nIn a practical implementation, Eq. ( 2.92) can be rewritten into a form that relies solely on the\nelement-wise product and addition of vectors.\nRo(x,tθ) =\nx1\nx2\n...\nxd−1\nxd\nT\n⊙\ncostθ1\ncostθ1\n...\ncostθd/2\ncostθd/2\nT\n+\n−x2\nx1\n...\n−xd\nxd−1\nT\n⊙\nsintθ1\nsintθ1\n...\nsintθd/2\nsintθd/2\nT\n(2.93)\nFinally, we rewrite Eq. ( 2.85) to obtain the form of the embedding at position i\n\n2.3 Long Sequence Modeling 91\nei= Ro( xi,iθ) (2.94)\n2.3.5.4 Position Interpolation\nIn position interpolation, our goal is to map the positions i n the new sequence to match the ob-\nserved range in training. Suppose the sequence length for tr aining ranges from 0toml. When\nm>m lat test time, we represent the positions in [0,m]such that our representations ﬁt [0,ml].\nTo illustrate, consider the rotary positional embedding mo del described above. The embedding\nof each token is described",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 79,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "281185c9cdcc46a4",
    "content": "\nsintθ1\n...\nsintθd/2\nsintθd/2\nT\n(2.93)\nFinally, we rewrite Eq. ( 2.85) to obtain the form of the embedding at position i\n\n2.3 Long Sequence Modeling 91\nei= Ro( xi,iθ) (2.94)\n2.3.5.4 Position Interpolation\nIn position interpolation, our goal is to map the positions i n the new sequence to match the ob-\nserved range in training. Suppose the sequence length for tr aining ranges from 0toml. When\nm>m lat test time, we represent the positions in [0,m]such that our representations ﬁt [0,ml].\nTo illustrate, consider the rotary positional embedding mo del described above. The embedding\nof each token is described by a model Ro(xi,iθ)in whichθ=[\nθ1,...,θ d/2]\nare the parameters.\nRo(xi,iθ)can be cast in the form of a linear combination of two periodic functions (see Eq.\n(2.93))\ncosiθ=[\ncosiθ1...cosiθd/2]\n(2.95)\nsiniθ=[\nsiniθ1...siniθd/2]\n(2.96)\nθkis a exponential function of kand takes the form\nθk=b−2(k−1)\nd (2.97)\nwherebis the base. The period of cosiθkandsiniθkis\nTk= 2π·b2(k−1)\nd (2.98)\nThe key idea behind position interpolation is to adjust this period so that the new positions can\nbe encoded within the range [0,ml]. One way to achieve this is to scale up Tkbym\nml, given by\nT′\nk=m\nml·2π·b2(k−1)\nd (2.99)\nHence all points in [0,m]are compressed into [0,ml]. This linear scaling can be easily realized\nby modifying the input to the embedding model [ Chen et al. ,2023c ]. The new model with linear\npositional interpolation is given by\nRo′(xi,iθ) = Ro( xi,ml\nmiθ) (2.100)\nAnother method of positional interpolation is to scale the b ase17. Suppose that the base bis\nscaled byλ. We wish the period of this new model in the last dimension of θ(i.e., dimensiond\n2)\nto be equal to that of the linear positional interpolation mo del. This can be expressed as\n2π·(λb)2(d\n2−1)\nd =m\nml·2π·b2(d\n2−1)\nd (2.101)\n17This method was ﬁrst proposed in https://www.reddit.com/r/LocalLLaMA/comments/14lz7j 5/\nntkaware_scaled_rope_allows_llama_models_to_have/\n\n92 Generative Models\nSolving this equation, we obtain\nλ=(m\nml)d\n2(d\n2−1)\n=(m\nml)d\nd−2 (2.102)\nThis gives an embedding model\nRo′(xi,iθ) = Ro( xi,iθ′) (2.103)\nwhere\nθ′=[\n(λb)−0\nd,(λb)−2\nd,...,(λb)−d−2\nd]\n(2.104)\nNote that scaling the base provides a non-uniform method for scaling the periods across dif-\nferent dimensions of θ. This method has been found to be helpful for extending LLMs t o longer\nsequences, and several improvements have been developed [ Peng et al. ,2024 ;Ding et al. ,2024 ].\n2.3.6 Remarks\nIn this section, we have presented a variety of methods for lo ng-context language modeling. We\nclose this section by discussing some interesting issues re lated to these methods.\n2.3.6.1 Need for Long Context\nOne of the ultimate goals of long-context LLMs is that these m odels can precisely encode inﬁnite\ncontext. The so-called inﬁnite context refers more to the fa ct that an LLM can continuously read\nwords. This motivates LLMs that can handle extremely long co ntext or stream data. As discussed\nin Section 2.3.3 , it is common to use ﬁxed-size memory models to process conti nuously expanding\ncontext. Many such systems are based on recurrent architect ures or their variants, because they\nare",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 80,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "128bd80595d36302",
    "content": ", and several improvements have been developed [ Peng et al. ,2024 ;Ding et al. ,2024 ].\n2.3.6 Remarks\nIn this section, we have presented a variety of methods for lo ng-context language modeling. We\nclose this section by discussing some interesting issues re lated to these methods.\n2.3.6.1 Need for Long Context\nOne of the ultimate goals of long-context LLMs is that these m odels can precisely encode inﬁnite\ncontext. The so-called inﬁnite context refers more to the fa ct that an LLM can continuously read\nwords. This motivates LLMs that can handle extremely long co ntext or stream data. As discussed\nin Section 2.3.3 , it is common to use ﬁxed-size memory models to process conti nuously expanding\ncontext. Many such systems are based on recurrent architect ures or their variants, because they\nare inherently suited to model time series problems where th e effects of past inputs continue\nindeﬁnitely. Another way to achieve inﬁnite memory is to dev elop alternatives to self-attention\nmodels, for example, one can use continuous-space attentio n models to encode context, which\nremoves the dependency on context length [ Martins et al. ,2022 ].\nWhen studying long-context LLMs, it is natural to wonder wha t mechanisms may explain the\nuse of long context in language modeling. Can we compress the representation of inﬁnite context\ninto a relatively small-sized model? Are all context tokens useful for predicting next tokens? How\ndo LLMs prepare for token prediction when they see the contex t? Can we know in advance which\ncontextual information will be critical for prediction? Ge neral answers to all these questions\nare not obvious, but they inspire follow-on research of expl ainable models, and some interesting\nresults have been found. For example, Deletang et al. [2024 ] conducted extensive experiments\nto show that LLMs are powerful in-context compressors. Alth ough viewing predictive models\nas compression models has long been studied in machine learn ing, it also provides insights into\nour understanding of the LLM scaling laws. Pal et al. [2023 ] and Wu et al. [2024 ] investigated\nwhether the features learned up to the current step, though n ot intentionally, are already sufﬁcient\n\n2.3 Long Sequence Modeling 93\nfor predicting tokens at the following steps. Note that the n eed for long-context in language\nmodeling is highly dependent on the problem that we address. A related issue is where to apply\nLLMs and how to evaluate them. For example, in summarization tasks we may only need to distill\nand focus on a few key aspects of the text, while in retrieval- like tasks we need to “memorize”\nthe entire context so that the relevant information can be ac cessed. We will discuss the evaluation\nissue later in this subsection.\n2.3.6.2 Pre-training or Adapting LLMs?\nTraining LLMs requires signiﬁcant computational costs. Al though it is straightforward to train\nLLMs on long sequence data, the training becomes computatio nally unwieldy for large data sets. It\nis common practice to pre-train LLMs on general datasets, an d then adapt them with modest ﬁne-\ntuning effort. For example, LLMs with relative or rotary pos itional embeddings can be directly\ntrained on large-scale data in the pre-training phase. Whil e the resulting models may exhibit some\nabilities to extrapolate lengths in the inference phase, it may be more effective to ﬁne-tune them\non longer sequences.\nIdeally, we would like to pre-train LLMs with standard Trans former architectures and adapt\nthem to new tasks. This allows us to use many off-the-shelf LL Ms and efﬁciently adapt them to\nhandle long sequences. However, when new architectures are adopted, it seems inevitable that\nwe need to train these models from scratch. This poses practi cal difﬁculties for developing long-\ncontext LLMs, as we cannot leverage well-developed, pre-tr ained models and must instead train\nthem ourselves. On the other hand, ﬁne-tuning is still an eff ective way to adapt LLMs with certain\narchitectures that are different from those in pre-trainin g. An example is models augmented with\nexternal memories. In these models, the pre-trained LLMs ar e ﬁxed, and the focus is on how\nto make these LLMs collaborate with the memory models. In RAG , for instance, it is common",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 81,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "ae82a9ae7991b1c1",
    "content": "ain LLMs with standard Trans former architectures and adapt\nthem to new tasks. This allows us to use many off-the-shelf LL Ms and efﬁciently adapt them to\nhandle long sequences. However, when new architectures are adopted, it seems inevitable that\nwe need to train these models from scratch. This poses practi cal difﬁculties for developing long-\ncontext LLMs, as we cannot leverage well-developed, pre-tr ained models and must instead train\nthem ourselves. On the other hand, ﬁne-tuning is still an eff ective way to adapt LLMs with certain\narchitectures that are different from those in pre-trainin g. An example is models augmented with\nexternal memories. In these models, the pre-trained LLMs ar e ﬁxed, and the focus is on how\nto make these LLMs collaborate with the memory models. In RAG , for instance, it is common\nto ﬁne-tune LLMs to improve their use of retrieval-augmente d inputs. Another example of ﬁne-\ntuning LLMs for long-context modeling is that we train an LLM with full attention models, and\nthen replace them with sparse attention models in the ﬁne-tu ning phase. The pre-trained LLM\nprovides initial values of model parameters used in a differ ent model, and this model is then ﬁne-\ntuned as usual.\n2.3.6.3 Evaluating Long-context LLMs\nEvaluating long-context LLMs is important, but it is a new is sue in NLP. The general idea is that,\nif we input a long context to an LLM, then we can check from the o utput of the LLM whether it\nunderstands the entire context and makes use of it in predict ing following tokens. In conventional\nresearch of NLP, such evaluations are often aimed at examini ng the ability of NLP models in\nhandling long-range dependencies. However, the size of con texts used in recent LLMs is much\nlarger than that used in NLP systems a few years ago. This moti vates researchers to develop new\nevaluation benchmarks and metrics for long-context LLMs.\nOne approach is to use the perplexity metric. However, in spi te of its apparent simplicity, this\nmethod tends to reﬂect more on the LLMs’ ability to make use of local context rather than global\ncontext. It is therefore tempting to develop evaluation met hods that are speciﬁc to long-context\nLLMs. Popular methods include various synthetic tasks wher e artiﬁcially generated or modiﬁed\n\n94 Generative Models\ndata is used to evaluate speciﬁc capabilities of long-conte xt LLMs. In needle-in-a-haystack18and\npasskey retrieval tasks [ Mohtashami and Jaggi ,2024 ;Chen et al. ,2023c ], for instance, LLMs are\nrequired to identify and extract a small, relevant piece of i nformation from a large volume of given\ntext. The assumption here is that an LLM with sufﬁcient memor y should remember earlier parts\nof the text as it processes new information. This LLM can thus pick out the relevant details, which\nmight be sparse and hidden among much irrelevant informatio n, from the text. Alternatively,\nin copy memory tasks (or copy tasks for short), LLMs are used t o repeat the input text or a\nspeciﬁc segment multiple times. These tasks were initially proposed to test the extent to which\nrecurrent models can retain and recall previously seen toke ns [Hochreiter and Schmidhuber ,1997 ;\nArjovsky et al. ,2016 ], and have been adopted in evaluating recent LLMs [ Bulatov et al. ,2022 ;\nGu and Dao ,2023 ].\nAnother approach to evaluating long-context LLMs is to test them on NLP tasks that involve\nvery long input sequences. Examples include long-document or multi-document summarization,\nlong-document question answering, code completion, and so on. A beneﬁt of this approach is that\nit can align evaluations with user expectations.\nAlthough many methods have been developed, there is still no general way to evaluate long-\ncontext LLMs [ Liu et al. ,2024c ]. One problem is that most of these methods focus on speciﬁc\naspects of LLMs, rather than their fundamental ability to mo del very long contexts. Even though\nan LLM can pick out the appropriate piece of text from the inpu t, we cannot say that it truly un-\nderstands the entire context. Instead, it might",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 82,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "67c11a449a9a7944",
    "content": " ], and have been adopted in evaluating recent LLMs [ Bulatov et al. ,2022 ;\nGu and Dao ,2023 ].\nAnother approach to evaluating long-context LLMs is to test them on NLP tasks that involve\nvery long input sequences. Examples include long-document or multi-document summarization,\nlong-document question answering, code completion, and so on. A beneﬁt of this approach is that\nit can align evaluations with user expectations.\nAlthough many methods have been developed, there is still no general way to evaluate long-\ncontext LLMs [ Liu et al. ,2024c ]. One problem is that most of these methods focus on speciﬁc\naspects of LLMs, rather than their fundamental ability to mo del very long contexts. Even though\nan LLM can pick out the appropriate piece of text from the inpu t, we cannot say that it truly un-\nderstands the entire context. Instead, it might just rememb er some important parts of the context,\nor even simply recall the answer via the model learned in pre- training. Moreover, the data used\nin many tasks is small-scale and relatively preliminary, le ading to discrepancies between evalu-\nation results and actual application performance. A more in teresting issue is that the results of\nLLMs are inﬂuenced by many other factors and experimental se tups, for example, using different\nprompts can lead to very different outcomes. This makes eval uation even more challenging be-\ncause improvements may not solely result from better modeli ng of long contexts, and there is a\nrisk of overclaiming our results. Nevertheless, many open q uestions remain in the development\nand evaluation of long-context LLMs. For example, these mod els still suffer from limitations\nsuch as restricted context length and high latency. Studyin g these issues is likely to prove valuable\nfuture directions.\n2.4 Summary\nIn this chapter, we have discussed the concept of LLMs and rel ated techniques. This can be consid-\nered a general, though not comprehensive, introduction to L LMs, laying the foundation for further\ndiscussions on more advanced topics in subsequent chapters . Furthermore, we have explored two\nways to scale up LLMs. The ﬁrst focuses on the large-scale pre -training of LLMs, which is cru-\ncial for developing state-of-the-art models. The second fo cuses on methods for adapting LLMs to\nlong inputs, including optimizing attention models, desig ning more efﬁcient and compressed KV\ncaches, incorporating memory models, and exploring better positional embeddings.\nThe strength of LLMs lies in their ability to break the constr aints of training NLP models for\na limited number of speciﬁc tasks. Instead, LLMs learn from l arge amounts of text through the\nsimple task of token prediction — we predict the next token in a sentence given its prior tokens.\n18https://github.com/gkamradt/LLMTest_NeedleInAHaysta ck\n\n2.4 Summary 95\nA general view is that, by repeating this token prediction ta sk a large number of times, LLMs can\nacquire some knowledge of the world and language, which can t hen be applied to new tasks. As a\nresult, LLMs can be prompted to perform any task by framing it as a task of predicting subsequent\ntokens given prompts. This emergent ability in language mod els comes from several dimensions,\nsuch as scaling up training, model size, and context size. It is undeniable that scaling laws are\ncurrently the fundamental principle adopted in developing large language models, although sim-\nply increasing model size has yet to prove sufﬁcient for achi eving AGI. These continuously scaled\nLLMs have been found to show capabilities in general-purpos e language understanding, genera-\ntion, and reasoning. More recently, it has been found that sc aling up the compute at inference time\ncan also lead to signiﬁcant improvements in complex reasoni ng tasks [ OpenAI ,2024 ].\nGiven their amazing power, LLMs have attracted considerabl e interest, both in terms of tech-\nniques and applications. As a result, the explosion of resea rch interest in LLMs has also led to a\nvast number of new techniques and models. However, we do not a ttempt to provide a comprehen-\nsive literature review on all aspects of LLMs, given the rapi d evolution of the ﬁeld. Nevertheless,\none can still gain knowledge about LLMs from general reviews [Zhao et al. ,2023 ;Minaee et al. ,\n2024 ] or more focused discussions on speciﬁc topics [ R",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 83,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "a4b1841f26c4b8bb",
    "content": " in general-purpos e language understanding, genera-\ntion, and reasoning. More recently, it has been found that sc aling up the compute at inference time\ncan also lead to signiﬁcant improvements in complex reasoni ng tasks [ OpenAI ,2024 ].\nGiven their amazing power, LLMs have attracted considerabl e interest, both in terms of tech-\nniques and applications. As a result, the explosion of resea rch interest in LLMs has also led to a\nvast number of new techniques and models. However, we do not a ttempt to provide a comprehen-\nsive literature review on all aspects of LLMs, given the rapi d evolution of the ﬁeld. Nevertheless,\none can still gain knowledge about LLMs from general reviews [Zhao et al. ,2023 ;Minaee et al. ,\n2024 ] or more focused discussions on speciﬁc topics [ Ruan et al. ,2024 ].\n\nCHAPTER 3\nPrompting\nIn the context of LLMs, prompting refers to the method of providing an LLM with a speciﬁc input\nor cue to generate a desired output or perform a task. For exam ple, if we want the LLM to translate\na sentence from English to Chinese, we can prompt it like this\nTranslate the text from English to Chinese.\nText: The early bird catches the worm.\nTranslation:\nPrompting is crucial for LLMs because it directly inﬂuences how effectively these models under-\nstand and respond to user queries. A well-crafted prompt can guide an LLM to generate more\naccurate, relevant, and contextually appropriate respons es. Furthermore, this process can be iter-\natively reﬁned. By analyzing the responses of the LLM, users can adjust their prompts to align\nmore closely with their speciﬁc needs. Given the importance of prompting in applying LLMs,\nprompt design has become an essential skill for users and dev elopers working with LLMs. This\nleads to an active research area, called prompt engineering , in which we design effective prompts\nto make better use of LLMs and enhance their practical utilit y in real-world applications.\nAn important concept related to prompting is in-context learning . When prompting an LLM,\nwe can add new information to the context, such as demonstrat ions of problem-solving. This\nallows the LLM to learn from this context how to solve the prob lem. Here is an example of\nprompting LLMs with a few demonstrations of how to classify t ext based on sentiment polarity.\nHere are some examples of text classiﬁcation.\nExample 1: We had a delightful dinner together. →Label: Positive\nExample 2: I’m frustrated with the delays. →Label: Negative\nWhat is the label for “That comment was quite hurtful.”?\nLabel:\nIn-context learning is often seen as an emergent ability of L LMs that arises after pre-training.\nThough LLMs can be trained or tuned to perform new tasks, in-c ontext learning provides a very\nefﬁcient way to adapt these models without any training or tu ning effort. Perhaps this is one of\nthe most notable features of LLMs: they indeed learn general knowledge about the world and\nlanguage during pre-training, which we can easily apply to n ew challenges. Moreover, in-context\nlearning reﬂects the broader trend of making AI systems more generalizable and user-friendly.\nInstead of requiring specialized engineers to ﬁne-tune mod els for every unique task, users can\ninteract with LLMs in a more intuitive way, simply providing examples or adjusting the context\nas needed.\nIn this chapter, we focus on prompting techniques in LLMs. We begin by considering several\ninteresting prompt designs commonly used in prompt enginee ring. Then, we discuss a series of\n96\n\n3.1 General Prompt Design 97\nreﬁnements to these methods. Finally, we explore approache s for automating prompt design.\n3.1 General Prompt Design\nThis section presents basic concepts in prompt design, alon g with examples of how to prompt\nLLMs for various NLP tasks. Since the effectiveness of promp ting is highly dependent on the\nLLMs being used, prompts often vary across different LLMs, m aking it difﬁcult to provide a\ncomprehensive list of prompts for all LLMs and downstream ta sks. Therefore, this discussion is\nnot focused on any speciﬁc LLM. Instead, the goal is to provid e guiding principles for prompt\ndesign.\n3.1.1 Basics\nThe term prompt is used in many different ways. In this chapter we deﬁne a prom",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 84,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "02caae018b4f7b65",
    "content": " prompt designs commonly used in prompt enginee ring. Then, we discuss a series of\n96\n\n3.1 General Prompt Design 97\nreﬁnements to these methods. Finally, we explore approache s for automating prompt design.\n3.1 General Prompt Design\nThis section presents basic concepts in prompt design, alon g with examples of how to prompt\nLLMs for various NLP tasks. Since the effectiveness of promp ting is highly dependent on the\nLLMs being used, prompts often vary across different LLMs, m aking it difﬁcult to provide a\ncomprehensive list of prompts for all LLMs and downstream ta sks. Therefore, this discussion is\nnot focused on any speciﬁc LLM. Instead, the goal is to provid e guiding principles for prompt\ndesign.\n3.1.1 Basics\nThe term prompt is used in many different ways. In this chapter we deﬁne a prom pt as the input\ntext to an LLM, denoted by x. The LLM generates a text yby maximizing the probability Pr(y|x).\nIn this generation process, the prompt acts as the condition on which we make predictions, and it\ncan contain any information that helps describe and solve th e problem.\nA prompt can be obtained using a prompt template (or template for short) [ Liu et al. ,2023a ].\nA template is a piece of text containing placeholders or vari ables, where each placeholder can\nbe ﬁlled with speciﬁc information. Here are two templates fo r asking the LLM for weekend\nsuggestions.\nPlease give me some suggestions for a fun weekend.\nIf{∗premise ∗}, what are your suggestions for a fun weekend.\nIn the ﬁrst template, we simply instruct the LLM to return som e suggestions. So the template\nis just a piece of text with no variables. In the second templa te, the variable {∗premise ∗}needs to\nbe speciﬁed by the users to provide a premise for making sugge stions. For example, if we input\npremise = the weather is nice this weekend\nthen we can generate a prompt\nIf the weather is nice this weekend,\nwhat are your suggestions for a fun weekend.\nWe can also design a template with multiple variables. Here i s an example in which we\ncompare the two sentences in terms of their semantic similar ity.\n\n98 Prompting\nHere is a sentence\n{∗sentence1 ∗}\nHere is another sentence\n{∗sentence2 ∗}\nCompute the semantic similarity between the two sentences\nA popular way to format prompts is to write each input or outpu t in a “name:content” style.\nFor example, we can describe a conversation between two peop le, named John and David, and use\nthe LLM to continue the conversation. A template of such prom pts is given by\nJohn: {∗utterance1 ∗}\nDavid: {∗utterance2 ∗}\nJohn: {∗utterance3 ∗}\nDavid: {∗utterance4 ∗}\nJohn: {∗utterance5 ∗}\nDavid: {∗utterance6 ∗}\nJohn: {∗utterance7 ∗}\nDavid:\nThe “name:content” format can be used to deﬁne the task that w e want the LLM to perform.\nFor example, given that “Q” and “A” are commonly used abbrevi ations for “Question” and “An-\nswer”, respectively, we can use the following template to do question-answering.\nQ:{∗question ∗}\nA:\nThis format can be used to describe more complex tasks. For ex ample, the following is an\nexample of providing a speciﬁcation for a translation task\nTask: Translation\nSource language: English\nTarget language: Chinese\nStyle: Formal text\nTemplate: Translate the following sentence: {∗sentence ∗}\nIn practical systems, it is common to represent and store suc h data in key-value pairs, such as the\nJSON format1.\nWhen the problem is difﬁcult to describe in an attribute-bas ed manner, it is more common\nto instruct LLMs with a clear and detailed description. Ther e are many ways to do this. One\n1The JSON representation is\n\n3.1 General Prompt Design 99\nexample is to assign a role to LLMs and provide sufﬁcient cont ext. The following is a template\nthat instructs an LLM to act as an expert and answer questions from children.\nYou are a computer scientist with extensive knowledge in the ﬁeld\nof deep learning.\nPlease explain the following computer-related concept to a",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 85,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "672c8b8ed56457ba",
    "content": " speciﬁcation for a translation task\nTask: Translation\nSource language: English\nTarget language: Chinese\nStyle: Formal text\nTemplate: Translate the following sentence: {∗sentence ∗}\nIn practical systems, it is common to represent and store suc h data in key-value pairs, such as the\nJSON format1.\nWhen the problem is difﬁcult to describe in an attribute-bas ed manner, it is more common\nto instruct LLMs with a clear and detailed description. Ther e are many ways to do this. One\n1The JSON representation is\n\n3.1 General Prompt Design 99\nexample is to assign a role to LLMs and provide sufﬁcient cont ext. The following is a template\nthat instructs an LLM to act as an expert and answer questions from children.\nYou are a computer scientist with extensive knowledge in the ﬁeld\nof deep learning.\nPlease explain the following computer-related concept to a child\naround 10 years old, using simple examples whenever possibl e.\n{∗concept ∗}\nHere the text “You are a computer scientist ... deep learning . ” is sometimes called system\ninformation, and is provided to help the LLM understand the c ontext or constraints of the task it\nis being asked to perform.\n3.1.2 In-context Learning\nLearning can occur during inference. In-context learning i s one such method, where prompts\ninvolve demonstrations of problem-solving, and LLMs can le arn from these demonstrations how\nto solve new problems. Since we do not update model parameter s in this process, in-context\nlearning can be viewed as a way to efﬁciently activate and reo rganize the knowledge learned in\npre-training without additional training or ﬁne-tuning. T his enables quick adaptation of LLMs to\nnew problems, pushing the boundaries of what pre-trained LL Ms can achieve without task-speciﬁc\nadjustments.\nIn-context learning can be illustrated by comparing three m ethods: zero-shot learning, one-\nshot learning and few-shot learning. Zero-shot learning, a s its name implies, does not involve a\ntraditional “learning” process. It instead directly appli es LLMs to address new problems that were\nnot observed during training. In practice, we can repetitiv ely adjust prompts to guide the LLMs in\ngenerating better responses, without demonstrating probl em-solving steps or providing examples.\nConsider the following example. Suppose we want to use an LLM as an assistant that can help\ncorrect English sentences. A zero-shot learning prompt is g iven by\n{\n\"Task\": \"Translation\"\n\"Source language\": \"English\"\n\"Target language\": \"Chinese\"\n\"Style\": \"Formal text\"\n\"Template\": \"Translate the following sentence: {∗sentence ∗}\"\n}\n\n100 Prompting\nSYSTEM You are a helpful assistant, and are great at grammar correct ion.\nUSER You will be provided with a sentence in English. The task is\nto output the correct sentence.\nInput: She don’t like going to the park.\nOutput:\nHere the gray words are used to indicate different ﬁelds of th e prompt.\nIn one-shot learning, we extend this prompt by adding a demon stration of how to correct\nsentences, thereby allowing the LLM to learn from this newly -added experience.\nSYSTEM You are a helpful assistant, and are great at grammar correct ion.\nDEMO You will be provided with a sentence in English. The task is\nto output the correct sentence.\nInput: There is many reasons to celebrate.\nOutput: There are many reasons to celebrate.\nUSER You will be provided with a sentence in English. The task is\nto output the correct sentence.\nInput: She don’t like going to the park.\nOutput:\nFurthermore, we can add more demonstrations to enable few-s hot learning.\nSYSTEM You are a helpful assistant, and are great at grammar correct ion.\nDEMO1 You will be provided with a sentence in English. The task is\nto output the correct sentence.\nInput: There is many reasons to celebrate.\nOutput: There are many reasons to celebrate.\nDEMO2 You will be provided with a sentence in English. The task is\nto output the correct sentence.\nInput: Me and my friend goes to the gym every day.\nOutput: My friend and I go to the gym every day.\nUSER You will be provided with a sentence in English. The task is\nto output the correct sentence.\nInput: She don’t like going to the park.\nOutput:\nIn few-shot learning, we essentially provide a pattern that maps some inputs to the corre-\nsponding outputs. The LLM attempts to follow this pattern in making predictions, provided that\nthe prompt includes a sufﬁcient number of demonstrations, a lthough generally",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 86,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "de70c1f52bf5b605",
    "content": " to enable few-s hot learning.\nSYSTEM You are a helpful assistant, and are great at grammar correct ion.\nDEMO1 You will be provided with a sentence in English. The task is\nto output the correct sentence.\nInput: There is many reasons to celebrate.\nOutput: There are many reasons to celebrate.\nDEMO2 You will be provided with a sentence in English. The task is\nto output the correct sentence.\nInput: Me and my friend goes to the gym every day.\nOutput: My friend and I go to the gym every day.\nUSER You will be provided with a sentence in English. The task is\nto output the correct sentence.\nInput: She don’t like going to the park.\nOutput:\nIn few-shot learning, we essentially provide a pattern that maps some inputs to the corre-\nsponding outputs. The LLM attempts to follow this pattern in making predictions, provided that\nthe prompt includes a sufﬁcient number of demonstrations, a lthough generally small. It is also\n\n3.1 General Prompt Design 101\npossible to use simpler patterns to achieve this. For exampl e, one can use the following few-shot\nlearning prompt for translating words from Chinese to Engli sh.\nDEMO现在→ now\n来→ come\n去→ go\n男孩→ boy\nUSER女孩→\nIf the LLM is powerful enough, few-shot learning can enable i t to address complex prob-\nlems, such as mathematical reasoning. For example, conside r the following task of summing two\nnumbers and then dividing the sum by their product.\nDEMO 12 5 → (12 + 5)/(12×5) = 0.283\n3 1 → (3 + 1)/(3×1) = 1.33\n−9 4 → (−9 + 4)/(−9×4) = 0.138\n15 15 → (15 + 15)/(15×15) = 0.133\nUSER 19 73 →\nIn many practical applications, the effectiveness of in-co ntext learning relies heavily on the\nquality of prompts and the fundamental abilities of pre-tra ined LLMs. On one hand, we need a\nsigniﬁcant prompt engineering effort to develop appropria te prompts that help LLMs learn more\neffectively from demonstrations. On the other hand, strong er LLMs can make better use of in-\ncontext learning for performing new tasks. For example, sup pose we wish to use an LLM to\ntranslate words from Inuktitut to English. If the LLM lacks p re-training on Inuktitut data, its\nunderstanding of Inuktitut will be weak, and it will be difﬁc ult for the model to perform well in\ntranslation regardless of how we prompt it. In this case, we n eed to continue training the LLM\nwith more Inuktitut data, rather than trying to ﬁnd better pr ompts.\nIt might be interesting to explore how in-context learning e merges during pre-training and\nwhy it works during inference. One simple understanding is t hat LLMs have gained some knowl-\nedge of problem-solving, but there are many possible predic tions, which are hard to distinguish\nwhen the models confront new problems. Providing demonstra tions can guide the LLMs to fol-\nlow the “correct” paths. Furthermore, some researchers hav e tried to interpret in-context learn-\ning from several different perspectives, including Bayesi an inference [ Xie et al. ,2022 ], gradient\ndecent [ Dai et al. ,2023 ;V on Oswald et al. ,2023 ], linear regression [ Akyürek et al. ,2023 ], meta\nlearning [ Garg et al. ,2022 ], and so on.\n3.1.3 Prompt Engineering Strategies\nDesigning prompts is highly empirical. In general, there ar e many ways to prompt an LLM for\nperforming the same task, and we need to perform a number of tr ial-and-error runs to ﬁnd a\nsatisfactory prompt. To write good prompts more efﬁciently , one can follow certain strategies.\nExamples of common prompting principles include\n\n102 Prompting\n•Describing the task as clearly as possible . When we apply an LLM to solve a problem,\nwe need to provide a precise, speciﬁc, and clear description of the problem and instruct the\nLLM to perform as we expect. This is particularly important w hen we want the output of\nthe LLM to meet certain expectations. For example, suppose w e are curious about climate\nchange. A simple prompt for asking the LLM to provide some inf ormation is\nTell me about climate change.\nSince this instruction",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 87,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "2e3dcf568d0ea803",
    "content": "3.1.3 Prompt Engineering Strategies\nDesigning prompts is highly empirical. In general, there ar e many ways to prompt an LLM for\nperforming the same task, and we need to perform a number of tr ial-and-error runs to ﬁnd a\nsatisfactory prompt. To write good prompts more efﬁciently , one can follow certain strategies.\nExamples of common prompting principles include\n\n102 Prompting\n•Describing the task as clearly as possible . When we apply an LLM to solve a problem,\nwe need to provide a precise, speciﬁc, and clear description of the problem and instruct the\nLLM to perform as we expect. This is particularly important w hen we want the output of\nthe LLM to meet certain expectations. For example, suppose w e are curious about climate\nchange. A simple prompt for asking the LLM to provide some inf ormation is\nTell me about climate change.\nSince this instruction is too general, the LLM may generate a response that addresses any\naspect of climate change, which may not align with our speciﬁ c interests. In this case, we\ncan instead use prompts that are speciﬁc and detailed. One su ch example is\nProvide a detailed explanation of the causes and effects of c limate change,\nincluding the impact on global temperatures, weather patte rns, and sea\nlevels. Also, discuss possible solutions and actions being taken to mitigate\nthese effects.\nNow suppose we intend to explain climate change to a 10-year- old child. We can adjust the\nabove prompt further.\nExplain the causes and effects of climate change to a 10-year -old child.\nTalk about how it affects the weather, sea levels, and temper atures. Also,\nmention some things people are doing to help. Try to explain i n simple\nterms and do not exceed 500 words.\n•Guiding LLMs to think . LLMs have exhibited surprisingly good capabilities to “th ink”.\nA common example is that well-developed LLMs have achieved i mpressive performance\nin mathematical reasoning tasks, which are considered chal lenging. In prompt engineering,\nthe “thinking” ability of LLMs needs to be activated through appropriate prompting, espe-\ncially for problems that require signiﬁcant reasoning effo rts. In many cases, an LLM that\nis instructed to “think” can produce completely different r esults compared with the same\nLLM that is instructed to perform the task straightforwardl y. For example, Kojima et al.\n[2022 ] found that simply appending “Let’s think step by step” to th e end of each prompt\ncan improve the performance of LLMs on several reasoning tas ks. LLMs can be prompted\nto “think” in a number of ways. One method is to instruct LLMs t o generate steps for rea-\nsoning about the problem before reaching the ﬁnal answer. Fo r example, consider a task of\nsolving mathematical problems. See below for a simple promp t for this task.\n\n3.1 General Prompt Design 103\nYou are a mathematician. You will be provided with a math prob lem.\nPlease solve the problem.\nSince solving math problems requires a detailed reasoning p rocess, LLMs would probably\nmake mistakes if they attempted to work out the answer direct ly. So we can explicitly ask\nLLMs to follow a given reasoning process before coming to a co nclusion.\nYou are a mathematician. You will follow these detailed reas oning steps\nwhen solving math problems.\nStep 1: Problem Interpretation.\nThe mathematician carefully listens to your query and under stands the in-\ntricate details of the mathematical challenge you have pres ented.\nStep 2: Strategy Formulation.\nDrawing upon their extensive knowledge, the mathematician chooses the\nmost effective strategy tailored to the type of math problem , whether it is\nalgebra, calculus, or geometry.\nStep 3: Detailed Calculation.\nWith precision and expertise, the mathematician performs t he necessary\ncalculations step by step, adhering to all mathematical pri nciples.\nStep 4: Solution Review.\nBefore providing the ﬁnal answer, the mathematician meticu lously checks\nthe calculations for accuracy and offers a concise explanat ion or rationale\nfor the solution.\nYou will be provided with a math problem. Please solve the pro blem.\n{∗problem ∗}\nAnother method to guide LLMs to “think” is through multiple r ounds of interaction with\nLLMs. For example, as a ﬁrst step, we can instruct LLMs to solv e the problem directly\nYou will be provided with a math problem. Please solve the pro blem.\n{∗problem",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 88,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "c05ad0c2dcb77255",
    "content": " extensive knowledge, the mathematician chooses the\nmost effective strategy tailored to the type of math problem , whether it is\nalgebra, calculus, or geometry.\nStep 3: Detailed Calculation.\nWith precision and expertise, the mathematician performs t he necessary\ncalculations step by step, adhering to all mathematical pri nciples.\nStep 4: Solution Review.\nBefore providing the ﬁnal answer, the mathematician meticu lously checks\nthe calculations for accuracy and offers a concise explanat ion or rationale\nfor the solution.\nYou will be provided with a math problem. Please solve the pro blem.\n{∗problem ∗}\nAnother method to guide LLMs to “think” is through multiple r ounds of interaction with\nLLMs. For example, as a ﬁrst step, we can instruct LLMs to solv e the problem directly\nYou will be provided with a math problem. Please solve the pro blem.\n{∗problem ∗}\nNow we have an initial answer to the problem. As a second step, we prompt LLMs to\nevaluate the correctness of the answer and, if necessary, re work it to ﬁnd a better solution.\n\n104 Prompting\nYou will be provided with a math problem, along with a solutio n. Evaluate\nthe correctness of this solution, and identify any errors if present. Then,\nwork out your own solution.\nProblem: {∗problem ∗}\nSolution: {∗solution ∗}\nThe prompts presented here are closely related to a long line of research on reasoning prob-\nlems in LLMs. It is impossible to provide a complete discussi on of all related issues because\nthis topic covers a large family of methods. But we will see a r elatively more detailed dis-\ncussion on how to improve prompting through more reasoning i n Section 3.2.\n•Providing reference information . As discussed in the previous section, we can include\ndemonstrations in prompts and allow LLMs to in-context lear n from these demonstrations\nhow to perform the task. In fact, given the remarkable abilit y of language understanding of\nLLMs, we can add any type of text into the prompts and so these m odels can predict based\non enriched contexts. In many applications, we have various information that is relevant\nto user queries. Instead of using LLMs to make unconstrained predictions, we often want\nLLMs to produce outputs that are conﬁned to the relevant text . One such example is RAG,\nwhere the relevant text for the user query is provided by call ing an IR system, and we\nprompt LLMs to generate responses based on this provided rel evant text. The following\nprompt shows an example.\nYou are an expert that can generate answers to input queries. You have now\nbeen provided with a query and the corresponding context inf ormation.\nPlease generate an answer based on this context information . Note that\nyou need to provide the answer in your own words, not just copy from the\ncontext provided.\nContext information: {∗IR-result ∗}\nQuery: {∗query ∗}\nIf the context information is highly reliable, we can even re strict LLMs to answering using\nonly the provided text. An example prompt is shown as follows\n\n3.1 General Prompt Design 105\nYou are an expert tasked with generating answers from input q ueries. You\nhave been provided with a query and corresponding context in formation,\norganized in a table where each row represents a useful recor d. Please\ngenerate an answer using only this context information. Ens ure that you\nprovide the answer in your own words.\nContext information: {∗table ∗}\nQuery: {∗query ∗}\nWhen dealing with real-world problems, we often have prior k nowledge and additional\ninformation about the problems that help produce better ans wers. Considering such infor-\nmation in prompting is generally helpful in improving the re sult.\n•Paying attention to prompt formats . In general, the performance of LLMs is highly\nsensitive to the prompts we input. Sometimes a small modiﬁca tion to a prompt can lead to a\nbig change in model output. An interesting example is that ch anging the order of sentences\nin a prompt may cause LLMs to generate different results. To m ake prompts easy to read\nand reduce ambiguity, it is common to format them in a way that ensures clarity. One\nexample is that we deﬁne several ﬁelds for prompts and ﬁll dif ferent information in each\nﬁeld. Another example is we can use code-style prompts for LL Ms which can understand\nand generate both natural language and code. See the followi ng for a code-style prompt that\nperforms",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 89,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "b604fd9a5d71dd74",
    "content": " that help produce better ans wers. Considering such infor-\nmation in prompting is generally helpful in improving the re sult.\n•Paying attention to prompt formats . In general, the performance of LLMs is highly\nsensitive to the prompts we input. Sometimes a small modiﬁca tion to a prompt can lead to a\nbig change in model output. An interesting example is that ch anging the order of sentences\nin a prompt may cause LLMs to generate different results. To m ake prompts easy to read\nand reduce ambiguity, it is common to format them in a way that ensures clarity. One\nexample is that we deﬁne several ﬁelds for prompts and ﬁll dif ferent information in each\nﬁeld. Another example is we can use code-style prompts for LL Ms which can understand\nand generate both natural language and code. See the followi ng for a code-style prompt that\nperforms translation where one demonstration is presented .\n[English] = [I have an apple.]\n[German] = [Ich habe einen Apfel.]\n[English] = [I have an orange.]\n[German] =\nLLMs can receive text in various formats. This allows us to us e control characters, XML\ntags, and speciﬁc formatting to represent complex data. And it is useful to specify how the\ninput and output should be formatted or structured. For exam ple, we can delimit sections of\ntext using quotes and prompt LLMs accordingly (e.g., adding a sentence like “the input text\nis delimited by double quotes” to the prompt).\nAbove, we have discussed only a few strategies for writing go od prompts. There are, of course,\nmany such methods, and one needs to develop their own through practice. Interested readers can\nrefer to various online documents for more information, suc h as OpenAI’s manual on the GPT\nseries models2.\n2See https://platform.openai.com/docs/guides/prompt-engi neering/\nsix-strategies-for-getting-better-results .\n\n106 Prompting\n3.1.4 More Examples\nIn this subsection, we consider more examples of prompting L LMs to perform various NLP tasks.\nThe motivation here is not to give standard prompts for these tasks, but rather to use simple\nexamples to illustrate how LLMs can be prompted to deal with N LP problems.\n3.1.4.1 Text Classiﬁcation\nText classiﬁcation is perhaps one of the most common problem s in NLP. Many tasks can be\nbroadly categorized as assigning pre-deﬁned labels to a giv en text. Here we consider the polarity\nclassiﬁcation problem in sentiment analysis. We choose pol arity classiﬁcation for illustration be-\ncause it is one of the most popular and well-deﬁned text class iﬁcation tasks. In a general setup of\npolarity classiﬁcation, we are required to categorize a giv en text into one of three categories: neg-\native, positive, or neutral. Below is a simple prompt for doi ng this (for easy reading, we highlight\nthe task description in the prompt).\nAnalyze thepolarityofthefollowingtextandclassifyitaspositive, negative, or\nneutral.\nText:\nThe service at the restaurant was slower than expected, whic h was a bit frustrat-\ning.\nThe polarity of the text can be classiﬁed as positive.\nTo make the example complete, we show the response generated by the LLM (underlined text).\nAlthough the answer is correct, the LLM gives this answer not in labels but in text describing\nthe result. The problem is that LLMs are designed to generate text but not to assign labels to text\nand treat classiﬁcation problems as text generation proble ms. As a result, we need another system\nto map the LLM’s output to the label space (call it label mapping ), that is, we extract “positive”\nfrom “The polarity of the text can be classiﬁed as positive”. This is trivial in most cases because\nwe can identify label words via simple heuristics. But occas ionally, LLMs may not express the\nclassiﬁcation results using these label words. In this case , the problem becomes more complicated,\nas we need some way to map the generated text or words to predeﬁ ned label words.\nOne method to induce output labels from LLMs is to reframe the problem as a cloze task. For\nexample, the following shows a cloze-like prompt for polari ty classiﬁcation.\nAnalyze thepolarityofthefollowingtextandclassifyitaspositive, negative, or\nneutral",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 90,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "d4f4d02f7605ccdf",
    "content": "ﬁcation problems as text generation proble ms. As a result, we need another system\nto map the LLM’s output to the label space (call it label mapping ), that is, we extract “positive”\nfrom “The polarity of the text can be classiﬁed as positive”. This is trivial in most cases because\nwe can identify label words via simple heuristics. But occas ionally, LLMs may not express the\nclassiﬁcation results using these label words. In this case , the problem becomes more complicated,\nas we need some way to map the generated text or words to predeﬁ ned label words.\nOne method to induce output labels from LLMs is to reframe the problem as a cloze task. For\nexample, the following shows a cloze-like prompt for polari ty classiﬁcation.\nAnalyze thepolarityofthefollowingtextandclassifyitaspositive, negative, or\nneutral.\nText:\nThe service at the restaurant was slower than expected, whic h was a bit frustrat-\ning.\nThe polarity of the text is positive\n\n3.1 General Prompt Design 107\nWe can use LLMs to complete the text and ﬁll the blank with the m ost appropriate word. Ide-\nally, we wish the ﬁlled word would be positive ,negative , orneutral . However, LLMs are not\nguaranteed to generate these label words. One method to addr ess this problem is to constrain the\nprediction to the set of label words and select the one with th e highest probability. Then, the output\nlabel is given by\nlabel = arg max\ny∈YPr(y|x) (3.1)\nwhereydenotes the word ﬁlled in the blank, and Ydenotes the set of label words\n{positive,negative,neutral }.\nAnother method of using LLMs to generate labels is to constra in the output with prompts. For\nexample, we can prompt LLMs to predict within a controlled se t of words. Here is an example.\nAnalyze thepolarityofthefollowingtextandclassifyitaspositive, negative, or\nneutral.\nText:\nThe service at the restaurant was slower than expected, whic h was a bit frustrat-\ning.\nWhat is the polarity of the text?\nJust answer: positive, negative, or neutral.\nPositive\nSentiment analysis is a common NLP problem that has probably been well understood by\nLLMs through pre-training or ﬁne-tuning. Thus we can prompt LLMs using simple instructions\nto perform the task. However, for new classiﬁcation problem s, it may be necessary to provide\nadditional details about the task, such as the classiﬁcatio n standards, so that the LLMs can perform\ncorrectly. To do this, we can add a more detailed description of the task and/or demonstrate\nclassiﬁcation examples in the prompts. To illustrate, cons ider the following example.\n\n108 Prompting\nAnalyze thepolarityofthefollowingtextandclassifyitaspositive, negative, or\nneutral.Here’s what each category represents:\nPositive: This indicates thatthetextconveys apositiveemotionorattitude. For\nexample,texts express inghappiness, satisfaction, excitement, oradmirationare\nconsidered positive.\nNegative: This refers toatextthatexpresses anegativeemotion orattitude. It\nencompasses feelings ofsadness, anger, frustration, orcriticism.\nNeutral:Neutralsentiment isused todescribe texts thatdonotexhibit clear posi-\ntiveornegativeemotions butinstead conveyinformational, factual, orindifferent\ntones.\nText:\nThe service at the restaurant was slower than expected, whic h was a bit frustrat-\ning.\nWhat is the polarity of the text?\nPositive\nWhile it seems straightforward to use LLMs for classiﬁcatio n problems, there are still issues\nthat have not been well addressed. For example, when dealing with a large number of categories,\nit remains challenging to effectively prompt LLMs. Note tha t if we face a very difﬁcult classiﬁca-\ntion problem and have a certain amount of labeled data, ﬁne-t uning LLMs or adopting “BERT +\nclassiﬁer”-like architectures is also desirable.\n3.1.4.2 Information Extraction\nMany NLP problems can be regarded as information extraction problems, involving the identiﬁ-\ncation or extraction of speciﬁc pieces of information from u nstructured text. This information can\ninclude named entities, relationships, events, and other r elevant data points. The goal of infor-\nmation extraction is to",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 91,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "37fe7b390c24f771",
    "content": "-\ning.\nWhat is the polarity of the text?\nPositive\nWhile it seems straightforward to use LLMs for classiﬁcatio n problems, there are still issues\nthat have not been well addressed. For example, when dealing with a large number of categories,\nit remains challenging to effectively prompt LLMs. Note tha t if we face a very difﬁcult classiﬁca-\ntion problem and have a certain amount of labeled data, ﬁne-t uning LLMs or adopting “BERT +\nclassiﬁer”-like architectures is also desirable.\n3.1.4.2 Information Extraction\nMany NLP problems can be regarded as information extraction problems, involving the identiﬁ-\ncation or extraction of speciﬁc pieces of information from u nstructured text. This information can\ninclude named entities, relationships, events, and other r elevant data points. The goal of infor-\nmation extraction is to transform raw data into a format that can be easily analyzed and used in\nvarious downstream applications.\nAs information extraction covers a wide range of problems, w e cannot discuss them all here.\nInstead, we start with the task of named entity recognition — a task that has long been a concern\nin NLP. Named entity recognition is a process that detects an d classiﬁes key information in text\ninto speciﬁc groups. These key pieces of information, known as named entities, typically include\nproper names and are categorized into distinct classes such as people, locations, organizations,\ndates, monetary values, and percentages. Consider ﬁrst a si mple example of extracting person\nnames from a text3.\n3The text is from https://edition.cnn.com/travel\n\n3.1 General Prompt Design 109\nIdentifyallpersonnames intheprovided text.\nText:\nIs the UK really doing that badly or have travel trends shifte d, post-pandemic?\nFor Tom Jenkins, CEO of the European Tourism Organisation, i t’s the latter. “I\nthink the UK is doing perfectly well but we’ll see more people going to Europe,”\nhe says of 2024, adding that there’s “allegedly a slight plat eau of American de-\nmand for the UK.”\nThe person name identiﬁed in the provided text is: Tom Jenkin s\nThen, we can extract all named entities.\nIdentifyandclassifyallnamed entitiesintheprovided textintocategories such\naspersonnames, locations, dates, andorganizations. Listeach entitywith itstype\nononeline.\nText:\nIs the UK really doing that badly or have travel trends shifte d, post-pandemic?\nFor Tom Jenkins, CEO of the European Tourism Organisation, i t’s the latter. “I\nthink the UK is doing perfectly well but we’ll see more people going to Europe,”\nhe says of 2024, adding that there’s “allegedly a slight plat eau of American de-\nmand for the UK.”\n1. Tom Jenkins - Person Name\n2. European Tourism Organisation - Organization\n3. UK - Location\n4. Europe - Location\n5. 2024 - Date\nGiven these identiﬁed named entities, we can further proces s the text using other information\nextraction tasks. For example, we can identify the relation ships between these named entities (call\nitrelation extraction ). The corresponding prompt is shown as follows.\n\n110 Prompting\nGiven atextandalistofnamed entitiesidentiﬁedwithin it,analyze anddescribe\ntherelationships between these entities. Explain how each entityiscontextually\nrelated toothers.\nText:\nIs the UK really doing that badly or have travel trends shifte d, post-pandemic?\nFor Tom Jenkins, CEO of the European Tourism Organisation, i t’s the latter. “I\nthink the UK is doing perfectly well but we’ll see more people going to Europe,”\nhe says of 2024, adding that there’s “allegedly a slight plat eau of American de-\nmand for the UK.”\nNamed Entity:\nTom Jenkins, European Tourism Organisation, UK, Europe, 20 24\nTom Jenkins -European Tourism Organisation: Tom Jenkins istheCEO ofthe\nEuropean Tourism Organisation. This indicates aprofessional association where\nTom Jenkins likely hasasigniﬁcant roleindirecting theorganization’s activities,\nespecially those related totourism inEurope.\n...\nIf LLMs have been ﬁne-tuned with instruction following for i nformation extraction tasks, it\nis relatively easy to perform various information extracti on tasks. For example, the following is a\nprompt template for information extraction.\nYouwillbeprovided with atext. Your taskisto{∗task-description ∗}\nText: {",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 92,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "a493a088d902003b",
    "content": " the latter. “I\nthink the UK is doing perfectly well but we’ll see more people going to Europe,”\nhe says of 2024, adding that there’s “allegedly a slight plat eau of American de-\nmand for the UK.”\nNamed Entity:\nTom Jenkins, European Tourism Organisation, UK, Europe, 20 24\nTom Jenkins -European Tourism Organisation: Tom Jenkins istheCEO ofthe\nEuropean Tourism Organisation. This indicates aprofessional association where\nTom Jenkins likely hasasigniﬁcant roleindirecting theorganization’s activities,\nespecially those related totourism inEurope.\n...\nIf LLMs have been ﬁne-tuned with instruction following for i nformation extraction tasks, it\nis relatively easy to perform various information extracti on tasks. For example, the following is a\nprompt template for information extraction.\nYouwillbeprovided with atext. Your taskisto{∗task-description ∗}\nText: {∗text∗}\nHere task-description is the instruction for the task. Examples of the instruction include\nextract the keywords from the text.\nextract the key events from the text.\ndetect the coreference and anaphoric links between entitie s from the text.\nand so on. Of course, we can provide a more detailed descripti on of the task to prompt LLMs\nmore accurately.\n3.1.4.3 Text Generation\nIn NLP, the term text generation is used broadly to cover a wide range of different problems, m any\nof which focus on generating text based on given text. These p roblems can be roughly categorized\ninto two classes: 1) continual writing based on the input tex t (call it text completion ), and 2)\ntransformation of the input text into another text (call it text transformation ). Here, we illustrate\nthe prompts for the former and consider the prompts for the la tter in the following discussion.\n\n3.1 General Prompt Design 111\nA simple example of text completion is continuing to write a s tory starting with a few given\nsentences.\nYou areawriter who isgreat atwritingstories. You areprovided with afew\nsentences tostart with. Please continue thestory from theprovided sentences.\nStart of the story:\nOne sunny afternoon, Lily discovered a tiny, golden key hidd en under a rock in\nher backyard. Curious, she picked it up and wondered what it m ight unlock. Just\nthen, her cat, Whiskers, began to meow excitedly, leading he r towards the old\noak tree at the edge of the garden.\nLily followed Whiskers, herheart pounding with excitement and atouch of\nnervousness. Theoldoaktreehadalways seemed mysterious toher,itsgnarled\nbranches casting long shadows even onthebrightest days. Asthey reached the\ntree, Whiskers darted ahead andstarted pawing atsomething buried beneath a\npileofautumn leaves ...\nLikewise, we can create a conversation given the conversati onal context.\nYou areprovided with aconversationbetween atourist andataxidriver. Please\ncomplete theconversation.\nTourist: Could you take me to the downtown museum, please?\nTaxi Driver: Of course! First time in the city?\nTourist: Yes, any must-see places around?\nTaxi Driver: Deﬁnitely check out the botanical gardens afte r the museum.\nThey’re nearby and quite beautiful.\nTourist: Great, thanks forthetip!\nTaxi Driver: Noproblem. Enjoy your visit!\nAnother application of text completion is generating text b ased on user requirements. The\nrequirements can include style, tone, length, and any speci ﬁc content that the text should contain.\nFor example, users may need a formal report, a creative story , or a piece of programming code.\nHere is a prompt for generating a ﬁve-character regulated Ch inese poem.\n\n112 Prompting\nYouareapoet. Please write atraditional Chinese poem. Here istherequire ment.\nTheme: Spring’s rejuvenation\nStructure: Five-character regulated poem\nEmotion: Convey a sense of happiness and renewal through viv id spring imagery.\n春意盎然\n翠柳风中舞，\n红花雨后新。\n山明水又绿，\n天宽鸟自频。\nIf the LLM is trained to generate language and code, we can pro mpt it to perform code com-\npletion tasks. Here is an example.\nPlease write aPython functiontocalculatetheaverageofalistofnumbers.\ndefcalculate_average(numbers):\nifnumbers: #Check ifthelistisnotempty\nreturn sum(numbers) /len(numbers)\nelse:\nreturn",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 93,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "e52d4706f4cb196f",
    "content": " creative story , or a piece of programming code.\nHere is a prompt for generating a ﬁve-character regulated Ch inese poem.\n\n112 Prompting\nYouareapoet. Please write atraditional Chinese poem. Here istherequire ment.\nTheme: Spring’s rejuvenation\nStructure: Five-character regulated poem\nEmotion: Convey a sense of happiness and renewal through viv id spring imagery.\n春意盎然\n翠柳风中舞，\n红花雨后新。\n山明水又绿，\n天宽鸟自频。\nIf the LLM is trained to generate language and code, we can pro mpt it to perform code com-\npletion tasks. Here is an example.\nPlease write aPython functiontocalculatetheaverageofalistofnumbers.\ndefcalculate_average(numbers):\nifnumbers: #Check ifthelistisnotempty\nreturn sum(numbers) /len(numbers)\nelse:\nreturn 0#Return 0ifthelistisempty\n3.1.4.4 Text Transformation\nTypical text transformation tasks include machine transla tion, summarization, text style transfer,\nand so on. The prompts used to perform these tasks are not sign iﬁcantly different from the ones\npresented previously. Here, we present a few examples to sho wcase these prompts.\nTrans latethefollowingtextfrom English toSpan ish.\nText:\nThe quick brown fox jumps over the lazy dog.\nEl rápido zorro marrón salta sobre el perro perezoso.\n\n3.1 General Prompt Design 113\nSummarizethefollowingthisarticleinnomore than 50words:\nArticle:\nIn recent years, urban areas have been facing signiﬁcant cha llenges related to sus-\ntainability and environmental management. Cities are expe riencing higher rates\nof pollution, increased trafﬁc congestion, and greater dem ands on infrastructure.\nThis growth has led to numerous environmental issues, inclu ding elevated lev-\nels of air and water pollution, increased waste production, and strained public\nservices ...\nUrban areas aregrappling with sustainability challenges, such asrising pollution,\ntrafﬁc congestion, andinfrastructure demands ...\nRewrite thistextinaformaltone.\nText:\nHey, what’s up? Long time no see!\nHello, how have youbeen? Ithasbeen quite some time since welastmet!\n3.1.4.5 Question Answering\nThe question-answering format is inherently simple. For a g iven question, there is an answer that\ncorresponds to it. For example, in open-domain question ans wering, we expect the system to\nreturn an answer in response to a user-submitted question. P rompt templates for general-purpose\nquestion answering can be:\n{∗question ∗}\nQuestion: {∗question ∗}\nAnswer:\nQuestion answering is important in NLP because many problem s can be framed as question-\nanswering tasks. In particular, many recent reasoning task s are deﬁned in the form of question\nanswering. For example, in the MMLU benchmark [ Hendrycks et al. ,2021 ], each example con-\nsists of a multiple-choice question, and LLMs are required t o select the correct answer. See the\nfollowing for an example prompt for answering a question in t his dataset.\n\n114 Prompting\n(Z,∗)is a group with a∗b=a+b+ 1for alla,binZ. The inverse of ais\n(A) 0\n(B)−2\n(C)a−2\n(D) (2 +a)∗ −1\nD\nAnother widely-used benchmark is the GSM8K dataset [ Cobbe et al. ,2021 ]. It consists of\nthousands of grade school math word problems. For each probl em, we use LLMs to return a\nsolution in natural language. For example, the following is a prompt used for solving a problem\nin GSM8K.\nDEMO Q: Jerry’s two daughters play softball on different teams. T hey each have\n8 games this season. Each team practices 4 hours for every gam e they\nplay. If each game lasts for 2 hours, how many hours will Jerry spend at\nthe ﬁeld watching his daughters play and practice altogethe r?\nA: Jerry will spend 8games * 2hours per game = ≪8∗2 = 16 ≫16\nhours watching one daughter play her games. He will spend 16∗2 =≪\n16∗2 = 32 ≫32hours watching both daughters play their games. He\nwill spend 8games * 4hours of practice = ≪8∗4 =",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 94,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "a15f83388f84cfe4",
    "content": " school math word problems. For each probl em, we use LLMs to return a\nsolution in natural language. For example, the following is a prompt used for solving a problem\nin GSM8K.\nDEMO Q: Jerry’s two daughters play softball on different teams. T hey each have\n8 games this season. Each team practices 4 hours for every gam e they\nplay. If each game lasts for 2 hours, how many hours will Jerry spend at\nthe ﬁeld watching his daughters play and practice altogethe r?\nA: Jerry will spend 8games * 2hours per game = ≪8∗2 = 16 ≫16\nhours watching one daughter play her games. He will spend 16∗2 =≪\n16∗2 = 32 ≫32hours watching both daughters play their games. He\nwill spend 8games * 4hours of practice = ≪8∗4 = 32 ≫32hours\nwatching one daughter practice. He will spend 32∗2 =≪32∗2 = 64 ≫\n64hours watching both daughters practice. He will spend a tota l of32\nhours watching games + 64hours watching practice =≪32 + 64 =\n96≫96hours. #### 96\nQ: Mary bought six apples from the store. From the apples she b ought,\nfor each that Mary ate, she planted two trees from the remaini ng ones.\nHow many apples did Mary eat?\nA: She planted eight trees. This means she used half of that am ount of\napples, which is 8trees/2trees/apple =≪8/2 = 4 ≫4apples. That\nmeans that she planted four of the six apples she bought, leav ing only 6\napples −4apples =≪6−4 = 2 ≫2apples to be eaten. #### 2\nUSER Q: Boris has 100 apples. Beck has 23 fewer apples than Boris. I f Boris\ngives Beck 10 apples, how many fewer apples does Beck have tha n Boris\nnow?\nA:Boris starts with 100 apples and gives Beck 10apples, leaving\nhim with 100 apples −10apples =≪100−10 = 90 ≫90apples.\nInitially, Beck has23fewer apples than Boris, giving him a\nstarting count of100−23 =≪100−23 = 77 ≫77apples. After\nreceiving 10apples from Boris, Beck’s total comes to77apples +10\napples =≪77 + 10 = 87 ≫87apples. Consequently, Beck now has\n90−87 =≪90−87 = 3 ≫3fewer apples than Boris. #### 3\n\n3.2 Advanced Prompting Methods 115\nHere a few-shot prompt is adopted. The LLM learns from these d emonstrations of problem-\nsolution pairs not only the way of problem-solving but also t he way of formatting the output. For\nexample, the ﬁnal result of calculation follows the #### tok en, and ≪...≫annotates the detailed\ncalculation steps (called calculation annotation )4.\n3.2 Advanced Prompting Methods\nSo far in this chapter, we have introduced the basic concepts related to LLM prompting and pre-\nsented a number of prompts for NLP tasks. We now consider seve ral techniques for enhancing the\neffectiveness of prompting.\n3.2.1 Chain of Thought\nWe have encountered the concept of chain of thought (CoT ) several times in this chapter and\nprevious ones [ Wei et al. ,2022c ;Chowdhery et al. ,2022 ]. CoT methods provide a simple way\nto prompt LLMs to generate step-by-step reasoning for compl ex problems, thereby approaching\ntasks in a more human-like manner. Rather than coming to a con clusion directly, the CoT methods\ninstruct LLMs to generate reasoning steps or to learn from de monstrations of detailed reasoning\nprocesses provided in the prompts. To illustrate CoT, we con sider the problem of algebraic calcu-\nlation, as commonly described in the literature. Suppose we are given a algebraic problem\nCalculate the average of the numbers 2, 4, and 6.\nWe can consider it as the question and prompt an LLM to answer i t.\nQ: Please calculate the average of the numbers 2,4, and 9.\nA: The answer is 6.\nIt seems difﬁcult for the LLM to directly give a correct answe r. A simple improvement is to\nadd demonstrations of",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 95,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "6b280933c9d41740",
    "content": "2 ]. CoT methods provide a simple way\nto prompt LLMs to generate step-by-step reasoning for compl ex problems, thereby approaching\ntasks in a more human-like manner. Rather than coming to a con clusion directly, the CoT methods\ninstruct LLMs to generate reasoning steps or to learn from de monstrations of detailed reasoning\nprocesses provided in the prompts. To illustrate CoT, we con sider the problem of algebraic calcu-\nlation, as commonly described in the literature. Suppose we are given a algebraic problem\nCalculate the average of the numbers 2, 4, and 6.\nWe can consider it as the question and prompt an LLM to answer i t.\nQ: Please calculate the average of the numbers 2,4, and 9.\nA: The answer is 6.\nIt seems difﬁcult for the LLM to directly give a correct answe r. A simple improvement is to\nadd demonstrations of similar problems in the prompt, and th us the LLM can learn from these\ndemonstrations.\nQ: Please calculate the average of the numbers 1,3,5, and 7.\nA: The answer is 4.\nQ: Please calculate the average of the numbers 2,4, and 9.\nA: The answer is 7.\nThe problem here is that, although we have shown a similar que stion-answer pair, it remains\ndifﬁcult for the LLM to reason out the correct answer. In CoT, not only can LLMs learn from the\n4During prediction, a calculator is used when we see ≪...≫. More speciﬁcally, once the LLM encounters “=”\nin a≪...≫, then the calculator calculates the expression on the left- hand side of “=”. This method helps reduce the\ncalculation errors made by LLMs.\n\n116 Prompting\ncorrespondence between questions and answers but they may g ain more from detailed problem-\nsolving steps that used to derive the answers. To do this, we c an incorporate some reasoning steps\ninto the prompt to obtain a CoT prompt.\nQ: Please calculate the mean square of the numbers 1, 3, 5, and 7.\nA:Calculatethesquare ofeach number:12= 1,32= 9,52= 25 ,and72= 49 .\nSum thesquares, 1 + 9 + 25 + 49 = 84 .There are4numbers intotal.Divide\nthesum bythenumberofitems, 84/4 = 21 . The answer is 21.\nQ: Please calculate the average of the numbers 2,4, and 9.\nA:Calculate2 + 4 + 9 ,which equals 15.There arethree numbers. Divide the\ntotalsum bythecount, resultingin15/3 = 5 . The answer is 5.\nHere we highlight the reasoning steps in green. By providing a detailed reasoning process for\na similar problem, the LLM learns to reason and thereby gener ates a problem-solving path that\nleads to the correct answer.\nThere are several beneﬁts of using CoT prompting. First, CoT allows LLMs to decom-\npose complex problems into smaller, sequential reasoning s teps. This somewhat mirrors human\nproblem-solving behaviors, making it particularly effect ive for tasks requiring detailed, multi-step\nreasoning. Second, CoT makes the reasoning process more tra nsparent and interpretable. Since all\nreasoning steps are visible, we can understand and interpre t how a conclusion was reached. Third,\nif users can follow the logic behind the reasoning process, t hey will be more likely to trust the pre-\ndictions of an LLM. This is particularly important when appl ying LLMs in ﬁelds like medicine,\neducation, and ﬁnance. Fourth, CoT is an in-context learnin g approach, and thus, it is applicable\nto most well-trained, off-the-shelf LLMs. Moreover, CoT pr ovides efﬁcient ways to adapt LLMs\nto different types of problems. It can even inspire more crea tive solutions by exploring various\nalternative reasoning paths, which might not be obvious whe n arriving at a conclusion directly.\nThe method described above requires providing one or more ex amples of CoT reasoning, typ-\nically called the few-shot CoT method. By contrast, the zero -shot CoT method does not require\nsuch examples. It instead prompts LLMs to reason step-by-st ep by incorporating speciﬁc instruc-\ntions in prompts. For example, below is a zero-shot CoT promp t.\nQ: Please calculate the average of",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 96,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "e7c1f5c16cf6fbea",
    "content": "ing LLMs in ﬁelds like medicine,\neducation, and ﬁnance. Fourth, CoT is an in-context learnin g approach, and thus, it is applicable\nto most well-trained, off-the-shelf LLMs. Moreover, CoT pr ovides efﬁcient ways to adapt LLMs\nto different types of problems. It can even inspire more crea tive solutions by exploring various\nalternative reasoning paths, which might not be obvious whe n arriving at a conclusion directly.\nThe method described above requires providing one or more ex amples of CoT reasoning, typ-\nically called the few-shot CoT method. By contrast, the zero -shot CoT method does not require\nsuch examples. It instead prompts LLMs to reason step-by-st ep by incorporating speciﬁc instruc-\ntions in prompts. For example, below is a zero-shot CoT promp t.\nQ: Please calculate the average of the numbers 2,4, and 9.\nA:Let’s think step-by-step.\nWe have three numbers: 2,4,and 9.Add these numbers together,\n2 + 4 + 9 = 15 .Determine how many numbers there are, which inthiscase\nisthree. The average iscalculated bydividing thetotal sum bythenumber of\nelements. Completing thedivision gives 15/3 = 5 .Sotheanswer is5.\nFollowing the instruction “Let’s think step by step”, the LL M is prompted to generate detailed\nreasoning steps. As discussed in Kojima et al. [2022 ]’s work, prompting with such instructions\nmay result in LLMs generating only the reasoning steps witho ut a clear conclusion. In this case,\na second round of prompting can be used to extract the answer f rom these reasoning steps. For\nexample, Kojima et al. [2022 ] create a second prompt which combines both the input and out put\n\n3.2 Advanced Prompting Methods 117\nin the ﬁrst round of prompting. Using this combined input, th e LLM can continue its reasoning\nprocess and then generate the correct answer. Furthermore, it is possible to prompt LLMs to\nreason using instructions other than “Let’s think step by st ep”, such as “Let’s think logically” and\n“Please show me your thinking steps ﬁrst”.\nWhile we have illustrated CoT methods using an algebraic rea soning problem, these methods\ncan be applied to a variety of different problems. Typical pr oblem-solving scenarios for CoT\ninclude mathematical reasoning, logical reasoning, commo nsense reasoning, symbolic reasoning,\ncode generation, and so on. See Figure 3.1for more examples of applying CoT in various tasks.\nCoT today is one of the most active ﬁelds of prompt engineerin g. This has not only led to im-\nproved performance for LLM prompting but has opened the door to a wide range of methods for\nstudying and verifying reasoning capabilities of LLMs. Alt hough we have focused on the basic\nidea of CoT in this section, it can be improved in several ways . For example, we can consider the\nreasoning process as a problem of searching through many pos sible paths, each of which may con-\nsist of multiple intermediate states (i.e., reasoning step s). In general, we wish the search space to\nbe well-deﬁned and sufﬁciently large, so that we are more lik ely to ﬁnd the optimal result. For this\nreason, an area of current LLM research is aimed at designing better structures for representing\nreasoning processes, allowing LLMs to tackle more complex r easoning challenges. These struc-\ntures include tree-based structures [ Yao et al. ,2024 ], graph-based structures [ Besta et al. ,2024 ],\nand so on. By using these compact representations of reasoni ng paths, LLMs can explore a wider\nrange of decision-making paths, analogous to System 2 think ing5. Another line of research fo-\ncuses on prompting LLMs with multi-round interactions. Thi s involves decomposing complex\nproblems into sub-problems, verifying and reﬁning model ou tputs, employing model ensembling,\nand so on. Note that these methods and the issues involved are not limited to CoT. In fact, they\nare often used as more general approaches to improving LLMs, while CoT can be seen as a way\nto test the capabilities of LLMs. We will see discussions of s ome of these issues in the following\nsubsections.\nBefore leaving our discussion of CoT, we should consider its practical limitations. One of them\nis the",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 97,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "ee1933649207c5a2",
    "content": " et al. ,2024 ], graph-based structures [ Besta et al. ,2024 ],\nand so on. By using these compact representations of reasoni ng paths, LLMs can explore a wider\nrange of decision-making paths, analogous to System 2 think ing5. Another line of research fo-\ncuses on prompting LLMs with multi-round interactions. Thi s involves decomposing complex\nproblems into sub-problems, verifying and reﬁning model ou tputs, employing model ensembling,\nand so on. Note that these methods and the issues involved are not limited to CoT. In fact, they\nare often used as more general approaches to improving LLMs, while CoT can be seen as a way\nto test the capabilities of LLMs. We will see discussions of s ome of these issues in the following\nsubsections.\nBefore leaving our discussion of CoT, we should consider its practical limitations. One of them\nis the need for detailed, multi-step reasoning demonstrati ons in few-shot CoT scenarios, which\nmay be difﬁcult to obtain, either automatically or manually . Also, there is no standard method for\nbreaking down complex problems into simpler problem-solvi ng steps. This often heavily depends\non the user’s experience. In addition, errors in intermedia te steps can also affect the accuracy of\nthe ﬁnal conclusion. For further discussion on the pros and c ons of CoT, the interested reader can\nrefer to recent surveys on this topic [ Chu et al. ,2023 ;Yu et al. ,2023 ;Zhang et al. ,2023a ].\n3.2.2 Problem Decomposition\nWe have seen that LLMs can beneﬁt from solving a complex probl em by breaking it down into\nsimpler problem-solving tasks. Such an approach can be seen as an example of a broader paradigm\nknown as problem decomposition , which has been extensively explored and discussed in psy-\nchology and computer science. From the psychological persp ective, complex problem-solving\nrefers to a process of addressing a problem using knowledge t hat helps overcome the barriers of\n5System 1 and System 2 thinking, as described by Kahneman [2011 ], represent two different modes of cognitive\nprocessing. System 1 is fast, automatic, intuitive, and emo tional. This mode of thinking operates effortlessly and\nquickly, and is often what guides our daily decisions, judgm ents, and impressions. System 2 is slow, deliberate, and\nanalytical. It is activated when we need to perform complex c omputations.\n\n118 Prompting\nQ: What home entertainment equipment re-\nquires cable?\nAnswer Choices: (a) radio shack (b) substa-\ntion (c) television (d) cabinet\nA:The answer must require cable. Ofthe\nabove choices, only televisionrequires cable.\nSo the answer is (c).\nQ: What type of person typically contracts\nillness?\nAnswer Choices: (a) hospital (b) head (c)\nsick person (d) elderly person (e) doctor’s of-\nﬁce\nA: Theanswer must beapersonsuscepti -\nbletoillness. Ofthechoices, \"elderly per\nson\" iscorrectbecause they typically have\nweaker immune systems.\nSo the answer is (d).Q: Do hamsters provide food for any ani-\nmals?\nA:Ham sters areprey animals. Prey arefood\nforpreda tors. Thus, hamsters provide food\nforsome animals. So the answer is yes.\nQ: Can you use Microsoft Ofﬁce without in-\nternet?\nA:Yes, youcanuseMicrosoft Ofﬁceappli-\ncations ,although features likecloud stor-\nageandonlinecollaborationrequire inter-\nnet.So the answer is yes.\n(a) CSQA (b) StrategyQA\nQ: Complete the rest of the sequence, mak-\ning sure that the parentheses are closed prop-\nerly. Input: [{\nA:Let’s think stepbystep.\n0: empty stack\n1:[; stack: [\n2:{; stack: [{\nSo the answer is }].\nQ: Complete the rest of the sequence, mak-\ning sure that the parentheses are closed prop-\nerly. Input:<[ [\nA:Let’s think stepbystep.\n0: empty stack\n1:<; stack:<\n2:[; stack:<[\n3:[; stack:<[ [\nSo the answer is ] ]>.Q: Take the last letters of the words in \"Elon\nMusk\" and concatenate them.\nA:The lastletterof\"Elon\" is\"n\". The last\nletterof\"Musk\" is\"k\".Concatenatingthem\nis\"nk\". The",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 98,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "bb221c7503031567",
    "content": "-\nnet.So the answer is yes.\n(a) CSQA (b) StrategyQA\nQ: Complete the rest of the sequence, mak-\ning sure that the parentheses are closed prop-\nerly. Input: [{\nA:Let’s think stepbystep.\n0: empty stack\n1:[; stack: [\n2:{; stack: [{\nSo the answer is }].\nQ: Complete the rest of the sequence, mak-\ning sure that the parentheses are closed prop-\nerly. Input:<[ [\nA:Let’s think stepbystep.\n0: empty stack\n1:<; stack:<\n2:[; stack:<[\n3:[; stack:<[ [\nSo the answer is ] ]>.Q: Take the last letters of the words in \"Elon\nMusk\" and concatenate them.\nA:The lastletterof\"Elon\" is\"n\". The last\nletterof\"Musk\" is\"k\".Concatenatingthem\nis\"nk\". The answer is nk.\nQ: Take the last letters of each words in \"Re-\nnata Mariela Mona Kristin\" and concatenate\nthem.\nA:Thelastletterof\"Renata\" is\"a\".The\nlastletterof\"Mariela\" is\"a\".Thelastlet-\nterof\"Mona\" is\"a\"\". Thelastletterof\n\"Kristin\" is\"n\".Concatenatingthem is\n\"aaan\". The answer is aaan.\n(c) Dyck languages (d) Last Letter Concatenation\nFig. 3.1: CoT in four different reasoning tasks, including CSQA, Stra tegyQA, Dyck languages, and Last Letter Con-\ncatenation. The CoT parts are highlighted in green.\nthe problem6. There are generally no standard or clear paths to a solution for a complex prob-\nlem. However, it is often advantageous to employ strategies that decompose the problem, thereby\nmaking it easier to tackle the corresponding sub-problems w ith less effort. For example, consider\nwriting a blog about the risks of AI. If we simply prompt an LLM with the instruction “Please\nwrite a blog about the risks of AI”, the LLM may generate a blog with arbitrary structures and\n6A relatively formal deﬁnition can be found in Frensch and Funke [2014 ]’s book: complex problem-solving occurs\nto overcome barriers between a given state and a desired goal state by means of behavioral and/or cognitive, multi-step\nactivities .\n\n3.2 Advanced Prompting Methods 119\nwriting styles. A better method, instead, could be to outlin e the blog and provide more detailed\ninformation about each section. Consider the following pro mpt\nYou are a blog writer. Please follow the provided outline bel ow to write a blog\nabout the risks of AI.\n• Introduction\nIntroduce AI, its relevance, and the importance of understa nding its risks for youth.\n• Privacy Concerns\nDiscuss how AI might compromise personal privacy through in teractions online.\n• Misinformation\nExplore AI’s role in spreading misinformation and inﬂuenci ng young people’s deci-\nsions.\n• Cyberbullying\nHighlight how AI tools can be utilized in cyberbullying and t he impact on mental\nhealth.\n• Tips for Safe AI Use\nOffer guidelines for responsible AI usage and promote criti cal thinking.\n• Conclusion\nRecap main points and encourage proactive engagement with A I ethics.\nHere we give the title and major points for each section. Then , the LLM can use this structure to\nbreak down the writing task by ﬁlling in content for these sec tions. Note that the way to structure\nthe blog can be provided by humans or even generated automati cally. For example, we can use\nthe LLM to ﬁrst generate the outline, and then ask it to follow this outline to complete the writing.\nIn computer science, decomposing complex problems is a comm only used strategy in software\nand hardware system design. A well-known example is the divi de-and-conquer paradigm, which\nis often used to design algorithms for computation problems that can be reduced to simpler, more\nmanageable problems. For example, consider a problem of det ermining whether a document\ndiscusses the risks of AI. We can instruct the LLM with the fol lowing prompt.\nYou are provided with a text. Please determine whether it dis cusses the risks of\nAI.\n{∗document ∗}\nIf the document is long, the computation will be expensive. A lternatively, we can divide\nthe document into relatively short segments and perform the same task on each segment. These\nsegments can be processed in parallel to further reduce the c omputational cost. Next, we determine\n\n120 Prompting\nthe relevancy of each",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmp_5y6zuhc.pdf",
      "filename": "tmp_5y6zuhc.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-14T16:37:24.727257",
      "modification_date": "2025-03-14T16:37:24.732515",
      "processing_date": "2025-03-14T16:37:39.938741",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-14T16:37:39.940476",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 99,
      "total_chunks": 205
    }
  }
]