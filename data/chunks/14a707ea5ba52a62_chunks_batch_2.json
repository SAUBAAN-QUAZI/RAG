[
  {
    "chunk_id": "ea9926ab6818e727",
    "content": " models. In Findings\nof the Association for Computational Linguistics: EMNLP 20 22, pages 5621–5634, 2022.\n[Wu et al., 2024] Wilson Wu, John X Morris, and Lionel Levine. Do language models plan for future\ntokens? arXiv preprint arXiv:2404.00859 , 2024.\n[Wu et al., 2021] Yuhuai Wu, Markus Norman Rabe, DeLesley Hut chins, and Christian Szegedy. Memo-\nrizing transformers. In Proceedings of International Conference on Learning Repre sentations , 2021.\n[Wu et al., 2023] Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu,\nNoah A. Smith, Mari Ostendorf, and Hannaneh Hajishirzi. Fin e-grained human feedback gives better\nrewards for language model training. In Thirty-seventh Conference on Neural Information Processi ng\nSystems , 2023.\n[Xia et al., 2024] Mengzhou Xia, Sadhika Malladi, Suchin Gur urangan, Sanjeev Arora, and Danqi Chen.\nLess: Selecting inﬂuential data for targeted instruction t uning. arXiv preprint arXiv:2402.04333 , 2024.\n[Xiao et al., 2024] Guangxuan Xiao, Yuandong Tian, Beidi Che n, Song Han, and Mike Lewis. Efﬁcient\nstreaming language models with attention sinks. In Proceedings of The Twelfth International Conference\non Learning Representations , 2024.\n[Xiao and Zhu, 2023] Tong Xiao and Jingbo Zhu. Introduction t o transformers: an nlp perspective. arXiv\npreprint arXiv:2311.17633 , 2023.\n[Xiao et al., 2013] Tong Xiao, Jingbo Zhu, and Tongran Liu. Ba gging and boosting statistical machine\ntranslation systems. Artiﬁcial Intelligence , 195:496–527, 2013.\n\n4.5 Summary 221\n[Xiao et al., 2019] Tong Xiao, Yinqiao Li, Jingbo Zhu, Zhengt ao Yu, and Tongran Liu. Sharing attention\nweights for fast transformer. In Proceedings of the Twenty-Eighth International Joint Conf erence on\nArtiﬁcial Intelligence (IJCAI-19) , pages 5292–5298, 2019.\n[Xie et al., 2022] Sang Michael Xie, Aditi Raghunathan, Perc y Liang, and Tengyu Ma. An explanation\nof in-context learning as implicit bayesian inference. In Proceedings of International Conference on\nLearning Representations , 2022.\n[Xin et al., 2020] Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and Jimmy Lin. Deebert: Dynamic early\nexiting for accelerating bert inference. In Proceedings of the 58th Annual Meeting of the Association fo r\nComputational Linguistics , pages 2246–2251, 2020.\n[Xu et al., 2024] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao,\nQingwei Lin, and Daxin Jiang. Wizardlm: Empowering large pr e-trained language models to follow\ncomplex instructions. In The Twelfth International Conference on Learning Represen tations , 2024.\n[Yang et al., 2024] An Yang, Baosong Yang, Beichen Zhang, Bin yuan Hui, Bo Zheng, Bowen Yu,\nChengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwe n2. 5 technical report. arXiv preprint\narXiv:2412.15115 , 2024.\n[Yang et al., 2019] Zhilin Yang, Zihang Dai, Yiming Yang, Jai me Carbonell, Russ R Salakhutdinov, and\nQuoc V Le. Xlnet: Generalized autoregressive pretraining f or language understanding. Advances in\nneural information processing systems , 32, 2019.\n[Yao et al., 2024] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Sh afran, Tom Grifﬁths, Yuan Cao, and Karthik\nNarasimhan. Tree of thoughts: Deliberate problem solving w ith large language models. Advances in\nNeural Information Processing Systems",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpej5m2drm.pdf",
      "filename": "tmpej5m2drm.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T23:14:52.052821",
      "modification_date": "2025-03-15T23:14:52.057494",
      "processing_date": "2025-03-15T23:15:08.115294",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "processing_status": "processing",
      "upload_time": "2025-03-15T23:14:52.058073",
      "source_document_id": "tmpej5m2drm.pdf",
      "timestamp": "2025-03-15T23:15:08.116347",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 200,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "109dff619aff5cce",
    "content": "Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwe n2. 5 technical report. arXiv preprint\narXiv:2412.15115 , 2024.\n[Yang et al., 2019] Zhilin Yang, Zihang Dai, Yiming Yang, Jai me Carbonell, Russ R Salakhutdinov, and\nQuoc V Le. Xlnet: Generalized autoregressive pretraining f or language understanding. Advances in\nneural information processing systems , 32, 2019.\n[Yao et al., 2024] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Sh afran, Tom Grifﬁths, Yuan Cao, and Karthik\nNarasimhan. Tree of thoughts: Deliberate problem solving w ith large language models. Advances in\nNeural Information Processing Systems , 36, 2024.\n[Yarowsky, 1995] David Yarowsky. Unsupervised word sense d isambiguation rivaling supervised methods.\nInProceedings of the 33rd annual meeting of the association fo r computational linguistics , pages 189–\n196, 1995.\n[Yu et al., 2023] Zihan Yu, Liang He, Zhen Wu, Xinyu Dai, and Ji ajun Chen. Towards better chain-of-\nthought prompting strategies: A survey. arXiv preprint arXiv:2310.04959 , 2023.\n[Zaheer et al., 2020] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, C. Alberti,\nS. Ontañón, Philip Pham, Anirudh Ravula, Qifan Wang, L. Yang , and A. Ahmed. Big bird: Transformers\nfor longer sequences. Advances in neural information processing systems , 33:17283–17297, 2020.\n[Zellers et al., 2018] Rowan Zellers, Yonatan Bisk, Roy Schw artz, and Yejin Choi. Swag: A large-scale\nadversarial dataset for grounded commonsense inference. I nProceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing , pages 93–104, 2018.\n[Zhang and Sennrich, 2019] Biao Zhang and Rico Sennrich. Roo t mean square layer normalization. Ad-\nvances in Neural Information Processing Systems , 32, 2019.\n[Zhang et al., 2023] Zhuosheng Zhang, Yao Yao, Aston Zhang, X iangru Tang, Xinbei Ma, Zhiwei He,\nYiming Wang, Mark Gerstein, Rui Wang, Gongshen Liu, and Hai Z hao. Igniting language intelli-\ngence: The hitchhiker’s guide from chain-of-thought reaso ning to language agents. arXiv preprint\narXiv:2311.11797 , 2023a.\n[Zhang et al., 2023] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought\nprompting in large language models. In The Eleventh International Conference on Learning Represe n-\ntations , 2023b.\n[Zhao et al., 2024] Hao Zhao, Maksym Andriushchenko, France sco Croce, and Nicolas Flammarion. Long\nis more for alignment: A simple but tough-to-beat baseline f or instruction ﬁne-tuning. arXiv preprint\narXiv:2402.04833 , 2024.\n[Zhao et al., 2023] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,\nYingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifa n Du, Chen Yang, Yushuo Chen, Z. Chen,\nJinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jianyun Nie, and Ji rong Wen.\n\n222 Alignment\nA survey of large language models. arXiv preprint arXiv:2303.18223 , 2023.\n[Zhou et al., 2023] Chunting Zhou, Pengfei Liu, Puxin Xu, Sri ni Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma,\nAvia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike L ewis, Luke Z",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpej5m2drm.pdf",
      "filename": "tmpej5m2drm.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T23:14:52.052821",
      "modification_date": "2025-03-15T23:14:52.057494",
      "processing_date": "2025-03-15T23:15:08.115294",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "processing_status": "processing",
      "upload_time": "2025-03-15T23:14:52.058073",
      "source_document_id": "tmpej5m2drm.pdf",
      "timestamp": "2025-03-15T23:15:08.116347",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 201,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "8d3e6b21142e4545",
    "content": "3] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,\nYingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifa n Du, Chen Yang, Yushuo Chen, Z. Chen,\nJinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jianyun Nie, and Ji rong Wen.\n\n222 Alignment\nA survey of large language models. arXiv preprint arXiv:2303.18223 , 2023.\n[Zhou et al., 2023] Chunting Zhou, Pengfei Liu, Puxin Xu, Sri ni Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma,\nAvia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike L ewis, Luke Zettlemoyer, and Omer\nLevy. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206 , 2023a.\n[Zhou et al., 2023] Denny Zhou, Nathanael Schärli, Le Hou, Ja son Wei, Nathan Scales, Xuezhi Wang, Dale\nSchuurmans, Claire Cui, Olivier Bousquet, Quoc V . Le, and Ed H. Chi. Least-to-most prompting enables\ncomplex reasoning in large language models. In Proceedings of The Eleventh International Conference\non Learning Representations , 2023b.\n[Zhou et al., 2020] Wangchunshu Zhou, Canwen Xu, Tao Ge, Juli an McAuley, Ke Xu, and Furu Wei. Bert\nloses patience: Fast and robust inference with early exit. Advances in Neural Information Processing\nSystems , 33:18330–18341, 2020.\n[Zhou et al., 2023] Yongchao Zhou, Andrei Ioan Muresanu, Ziw en Han, Keiran Paster, Silviu Pitis, Harris\nChan, and Jimmy Ba. Large language models are human-level pr ompt engineers. In The Eleventh\nInternational Conference on Learning Representations , 2023c.\n[Zoph and Le, 2016] Barret Zoph and Quoc Le. Neural architect ure search with reinforcement learning. In\nProceedings of International Conference on Learning Repre sentations , 2016.\n[Zoph et al., 2020] Barret Zoph, Golnaz Ghiasi, Tsung-Yi Lin , Yin Cui, Hanxiao Liu, Ekin Dogus Cubuk,\nand Quoc Le. Rethinking pre-training and self-training. Advances in neural information processing\nsystems , 33:3833–3845, 2020.\n\nIndex\nk-NN, 74\nk-NN LM, 76\nk-NN language modeling, 76\nk-nearest neighbors, 74\nA2C, 175\naction-value function, 171\nadvantage, 175\nadvantage actor-critic, 175\nAgent, 47\nALiBi, 85\nalignment, 46\nattention with linear biases, 85\nautomated machine learning, 137\nautomatic prompt design, 137\nAutoML, 137\nautonomous agents, 134\nBART, 19\nBERT, 1\nBest-of-Nsampling, 197\nBoN sampling, 197\nBradley-Terry model, 178\ncalculation annotation, 113\ncatastrophic forgetting, 34\ncausal language modeling, 9\nchain of thought, 113\nchain-of-thought prompting, 53\ncompletion, 6\ncompositional generalization, 122\nCoT, 113\nCOT prompting, 53\ncross-lingual language models, 28\ncumulative reward, 172\ndeliberate-then-generate, 126\ndemonstrations, 6\ndirect preference optimization, 190\nDocument Rotation, 20\nDPO, 190\nDTG, 126\nemergent abilities, 63\nexternal memories, 74\nExtrapolation, 81few-shot COT prompting, 54\ngated linear unit, 58\ngaussian error linear unit, 58\nGeLU, 58\nGLU, 58\nGPT, 1\nGQA, 80\nGrouped query attention, 80\nhard prompts, 140\nhuman preference alignment, 152\nICL, 53\nICT, 6\nimportance sampling, 180\nin-context learning, 6,53,95\n",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpej5m2drm.pdf",
      "filename": "tmpej5m2drm.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T23:14:52.052821",
      "modification_date": "2025-03-15T23:14:52.057494",
      "processing_date": "2025-03-15T23:15:08.115294",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "processing_status": "processing",
      "upload_time": "2025-03-15T23:14:52.058073",
      "source_document_id": "tmpej5m2drm.pdf",
      "timestamp": "2025-03-15T23:15:08.116347",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 202,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "e0ee82cac3403f9c",
    "content": "ization, 122\nCoT, 113\nCOT prompting, 53\ncross-lingual language models, 28\ncumulative reward, 172\ndeliberate-then-generate, 126\ndemonstrations, 6\ndirect preference optimization, 190\nDocument Rotation, 20\nDPO, 190\nDTG, 126\nemergent abilities, 63\nexternal memories, 74\nExtrapolation, 81few-shot COT prompting, 54\ngated linear unit, 58\ngaussian error linear unit, 58\nGeLU, 58\nGLU, 58\nGPT, 1\nGQA, 80\nGrouped query attention, 80\nhard prompts, 140\nhuman preference alignment, 152\nICL, 53\nICT, 6\nimportance sampling, 180\nin-context learning, 6,53,95\ninput inversion, 163\ninstruction alignment, 152\ninstruction ﬁne-tuning, 43,154\ninterference, 30\ninternal memories, 74\nInterpolation, 82\nirreducible error, 63\nkey-value cache, 68\nKV cache, 68\nlabel mapping, 105\nLearning from Human Feedback, 47\nleast-to-most prompting, 119\nlong-context LLMs, 66\nmasked language modeling, 1,9\nmBERT, 28\nmemory-based methods, 74\nMQA, 79\nmulti-lingual BERT, 28\nmulti-query attention, 79\nNAS, 137\nneural architecture search, 137\nnext sentence prediction, 13\nNSP, 13\nofﬂine reinforcement learning, 193\none-shot COT prompting, 54\nOutcome-based Approaches, 195\n223\n\n224 Alignment\noveroptimization problem, 189\nPerformance Estimation, 137\nperformance function, 172\nperformance gap recovered, 167\npermuted language modeling, 11\nPGR, 167\nPlackett-Luce model, 184\nPPO, 50,181\npreﬁx ﬁne-tuning, 144\npreﬁx language modeling, 16\nproblem decomposition, 116\nProcess-based Approaches, 195\nprompt embeddings, 148\nprompt engineering, 95\nprompt optimization, 137\nPrompt Search Space, 137\nprompting engineering, 51\nproximal policy optimization, 50,181\nQ-value function, 171\nRAG, 76\nratio function, 180\nrectiﬁed linear unit, 58\nreinforcement learning from human feedback,\n47,153\nrejection sampling, 198\nrelation extraction, 108\nReLU, 58\nretrieval-augmented generation, 76\nreturn, 172\nreward gaming, 189\nreward hacking, 189\nReward Model, 47\nRLHF, 47,153\nRoBERTa, 26\nsample efﬁcient, 164\nscaling laws, 63\nself-consistency, 129\nself-instruct, 160\nself-supervised learning, 3\nself-training, 3\nSentence Reordering, 19\nSequence Encoding Models, 3\nSequence Generation Models, 3\nSFT, 47,152single-round prediction, 155\nsoft prompts, 140\nSpan Masking, 19\nstate-value function, 171\nStrong Ceiling Performance, 167\nSub-problem Generation, 118\nSub-problem Solving, 118\nsuperﬁcial alignment hypothesis, 164\nSupervised Fine-tuning, 47\nsupervised ﬁne-tuning, 152\nsupervised learning, 2\nsurrogate objective, 180\nT5,15\nTD,176\ntemporal difference, 176\ntext completion, 109\ntext transformation, 109\nToken Deletion, 19\nToken Masking, 19\nTransformers, 1\ntranslation language modeling, 29\ntrust regions, 181\nunsupervised learning, 2\nWeak Performance, 167\nweak-to-strong generalization, 166\nWeak-to-strong Performance, 167\nXLMs, 28\nzero-shot COT, 54\nzero-shot learning, 45",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpej5m2drm.pdf",
      "filename": "tmpej5m2drm.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T23:14:52.052821",
      "modification_date": "2025-03-15T23:14:52.057494",
      "processing_date": "2025-03-15T23:15:08.115294",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "processing_status": "processing",
      "upload_time": "2025-03-15T23:14:52.058073",
      "source_document_id": "tmpej5m2drm.pdf",
      "timestamp": "2025-03-15T23:15:08.116347",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 203,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "d2ff15abada3cba8",
    "content": " learning, 2\nsurrogate objective, 180\nT5,15\nTD,176\ntemporal difference, 176\ntext completion, 109\ntext transformation, 109\nToken Deletion, 19\nToken Masking, 19\nTransformers, 1\ntranslation language modeling, 29\ntrust regions, 181\nunsupervised learning, 2\nWeak Performance, 167\nweak-to-strong generalization, 166\nWeak-to-strong Performance, 167\nXLMs, 28\nzero-shot COT, 54\nzero-shot learning, 45",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpej5m2drm.pdf",
      "filename": "tmpej5m2drm.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T23:14:52.052821",
      "modification_date": "2025-03-15T23:14:52.057494",
      "processing_date": "2025-03-15T23:15:08.115294",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "processing_status": "processing",
      "upload_time": "2025-03-15T23:14:52.058073",
      "source_document_id": "tmpej5m2drm.pdf",
      "timestamp": "2025-03-15T23:15:08.116347",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 204,
      "total_chunks": 205
    }
  }
]