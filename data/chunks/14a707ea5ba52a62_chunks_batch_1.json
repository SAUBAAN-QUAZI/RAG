[
  {
    "chunk_id": "e452555ffdf2f0be",
    "content": " it to follow this outline to complete the writing.\nIn computer science, decomposing complex problems is a comm only used strategy in software\nand hardware system design. A well-known example is the divi de-and-conquer paradigm, which\nis often used to design algorithms for computation problems that can be reduced to simpler, more\nmanageable problems. For example, consider a problem of det ermining whether a document\ndiscusses the risks of AI. We can instruct the LLM with the fol lowing prompt.\nYou are provided with a text. Please determine whether it dis cusses the risks of\nAI.\n{∗document ∗}\nIf the document is long, the computation will be expensive. A lternatively, we can divide\nthe document into relatively short segments and perform the same task on each segment. These\nsegments can be processed in parallel to further reduce the c omputational cost. Next, we determine\n\n120 Prompting\nthe relevancy of each segment to the topic of AI risks. The ﬁna l output is then generated using\nanother prompt.\nYour task is to determine whether a text discusses the risks o f AI. This text has\nbeen divided into segments, and you have obtained the releva ncy of each segment\nto the topic of AI risks. Based on this, please provide your ﬁn al result.\nSegment 1: {∗relevancy-to-the-topic1 ∗}\nSegment 2: {∗relevancy-to-the-topic2 ∗}\nSegment 3: {∗relevancy-to-the-topic3 ∗}\n...\nNow let us return to a more general discussion of problem deco mposition in prompting. While\nproblem decomposition can be applied to various NLP problem s, it has been more extensively\ndiscussed and tested in reasoning tasks recently. For compl ex reasoning tasks, we often need\na multi-step reasoning path to reach a correct conclusion. W e can use LLMs to achieve this in\nthree different ways. First, LLMs can directly reach the con clusion. In other words, they can\npredict without explicit reasoning processes, and there is a hidden and uninterpretable reasoning\nmechanism. Second, LLMs are prompted to generate a multi-st ep reasoning path that leads to the\nconclusion, like CoT. However, we run LLMs just once, and all intermediate steps in reasoning\nare generated in a single prediction. Third, we break down th e original problem into a number of\nsub-problems, which are either addressed in separate runs o f LLMs or tackled using other systems.\nHere we focus our attention on the third approach, which is cl osely related to problem decompo-\nsition. Note, however, that a more comprehensive discussio n could cover all these approaches,\nwhile the ﬁrst two have been discussed to some extent in this c hapter.\nA general framework for problem decomposition involves two elements.\n•Sub-problem Generation . This involves decomposing the input problem into a number o f\nsub-problems.\n•Sub-problem Solving . This involves solving each sub-problem and deriving inter mediate\nand ﬁnal conclusions through reasoning.\nThese two issues can be modeled in different ways, leading to various problem decomposition\nmethods. One approach is to treat them as separate steps in a t wo-step process. For example,\nconsider the blog writing task described at the beginning of this subsection. In the ﬁrst step, we\ndecompose the entire problem into sub-problems all at once ( i.e., outline the blog). In the second\nstep, we solve the sub-problems either sequentially or in an other order (i.e., ﬁll in content for\neach section as needed). The ﬁnal output of this process comb ines the results from solving each\nsub-problem. While this method is simple and straightforwa rd, it assumes that the problem is\ncompositional, making it more suitable for tasks like writi ng and code generation.\nHowever, many real-world problems require complex reasoni ng. One key characteristic of\nthese problems is that the reasoning steps may not be ﬁxed. Th e reasoning path can vary for\ndifferent problems, and each step of reasoning may depend on the outcomes of prior steps. In\n\n3.2 Advanced Prompting Methods 121\nsuch cases, it is undesirable to use ﬁxed sub-problem genera tion in advance. Instead, sub-problems\nshould be generated dynamically based on the input problem, and, if possible, generated on the\nﬂy during the reasoning process. This makes problem decompo sition more challenging compared\nwith designing divide-and-conquer algorithms. Ideally, w e would like to jointly design both the\nsystems for sub-problem generation and sub-problem solvin g. But",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 100,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "325e2894240babfe",
    "content": " is simple and straightforwa rd, it assumes that the problem is\ncompositional, making it more suitable for tasks like writi ng and code generation.\nHowever, many real-world problems require complex reasoni ng. One key characteristic of\nthese problems is that the reasoning steps may not be ﬁxed. Th e reasoning path can vary for\ndifferent problems, and each step of reasoning may depend on the outcomes of prior steps. In\n\n3.2 Advanced Prompting Methods 121\nsuch cases, it is undesirable to use ﬁxed sub-problem genera tion in advance. Instead, sub-problems\nshould be generated dynamically based on the input problem, and, if possible, generated on the\nﬂy during the reasoning process. This makes problem decompo sition more challenging compared\nwith designing divide-and-conquer algorithms. Ideally, w e would like to jointly design both the\nsystems for sub-problem generation and sub-problem solvin g. But a more practical and widely\nused approach is to adopt separate models for these tasks. A s traightforward way to achieve this\nis to adapt an LLM for these tasks by either prompting or tunin g the model.\nHere we consider a method based on the above idea, called least-to-most prompting [Zhou et al. ,\n2023b ]. The motivation for this method arises from the challenges of solving difﬁcult reasoning\nproblems — those that cannot be addressed by simply generali zing from a few examples. For\nthese problems, a more effective problem-solving strategy is to follow a progressive sequence of\nsub-problems that systematically lead to the conclusion. M ore speciﬁcally, in the least-to-most\nprompting method, sub-problem generation is performed by p rompting an LLM with instructions\nand/or demonstrations. For example, below is a 2-shot promp t for sub-problem generation in\nleast-to-most prompting.\nTASK Your task is to decompose a problem into several sub-problem s. You will\nbe given a few examples to illustrate how to achieve this.\nDEMO Q: In a community, 5% of the population are infants, 15% are ch ildren,\n40% are adults, and 40% are seniors. Which group makes up the l argest\nportion of the population?\nA: To answer the question “Which group makes up the largest po rtion of the\npopulation?”, we need to know: “ How many percent areinfants?”, “ How\nmany percent arechildren?”, “ How many percent areadults?”, “ How many\npercent areseniors?”.\nQ: Alice, Bob, and Charlie brought beads for their group proj ect in their\ncraft class. Alice has twice as many beads as Bob, and Bob has ﬁ ve times\nas many beads as Charlie. If Charlie has 6 beads, how many bead s can they\nuse for their craft project?\nA: To answer the question “How many beads can they use for thei r craft\nproject?”, we need to know: “ How many beads does Bob have?”, “ How\nmany beads does Alicehave?”.\nUSER Q: The environmental study conducted from 2015 to 2020 revea led that the\naverage temperature in the region increased by 2.3 degrees C elsius. What\nwas the duration of the environmental study?\nA: To answer the question “ What wasthedurationoftheenvironmental\nstudy?”, we need to know: “ When didtheenvironmentalstudy start?”,\n“When didtheenvironmentalstudy end?”.\nBy learning from the examples, the LLM can generate two sub-p roblems for answering the\nnew problem “What was the duration of the environmental stud y?” (highlighted in blue and\norange). Given these sub-problems, we solve them sequentia lly. For each sub-problem, we take\nall previously-generated QA pairs as context, and then prod uce the answer. For the example above,\n\n122 Prompting\nwe need to answer the ﬁrst sub-problem by prompting the LLM, l ike this\nThe environmental study conducted from 2015 to 2020 reveale d that\nthe average temperature in the region increased by 2.3 degre es Celsius.\nSUB-PROB1 Q:When didtheenvironmentalstudy start?\nA: The environmental study started in 2015.\nOnce we have the answer to the ﬁrst sub-problem, we proceed to the second one. This time,\nwe include both the ﬁrst sub-problem and its corresponding a nswer in the input.\nThe environmental study conducted from 2015 to 2020 reveale d that\nthe average temperature in the region increased by 2.3",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 101,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "be74094bfac26739",
    "content": ", we solve them sequentia lly. For each sub-problem, we take\nall previously-generated QA pairs as context, and then prod uce the answer. For the example above,\n\n122 Prompting\nwe need to answer the ﬁrst sub-problem by prompting the LLM, l ike this\nThe environmental study conducted from 2015 to 2020 reveale d that\nthe average temperature in the region increased by 2.3 degre es Celsius.\nSUB-PROB1 Q:When didtheenvironmentalstudy start?\nA: The environmental study started in 2015.\nOnce we have the answer to the ﬁrst sub-problem, we proceed to the second one. This time,\nwe include both the ﬁrst sub-problem and its corresponding a nswer in the input.\nThe environmental study conducted from 2015 to 2020 reveale d that\nthe average temperature in the region increased by 2.3 degre es Celsius.\nSUB-PROB1 Q:When didtheenvironmentalstudy start?\nA: The environmental study started in 2015.\nSUB-PROB2 Q:When didtheenvironmentalstudy end?\nA: The environmental study started in 2020.\nFinally, we use the LLM to solve the original problem given th e answers to all the sub-\nproblems.\nThe environmental study conducted from 2015 to 2020 reveale d that\nthe average temperature in the region increased by 2.3 degre es Celsius.\nSUB-PROB1 Q:When didtheenvironmentalstudy start?\nA: The environmental study started in 2015.\nSUB-PROB2 Q:When didtheenvironmentalstudy end?\nA: The environmental study started in 2020.\nFINAL Q:What wasthedurationoftheenvironmentalstudy?\nA: The duration of the environmental study was 5years .\nThe least-to-most method offers a basic approach to prompti ng LLMs to generate and solve\nsub-problems separately. We can improve it in several ways. One simple improvement is to apply\nvarious advanced prompting techniques, which do not requir e changes to the problem decom-\nposition framework. For example, we can incorporate CoT int o the prompting to enhance the\nreasoning performance of sub-problem generation and solvi ng.\nAnother improvement is to explore methods for better decomp osing problems and organizing\nproblem-solving paths. To describe these approaches, we wi ll use the symbol p0to denote the\n\n3.2 Advanced Prompting Methods 123\ninput problem, and use the symbols {p1,...,p n}to denote the sub-problems corresponding to p0.\nFor least-to-most prompting, we decompose p0into{p1,...,p n}, given by\n{p1,...,p n}=G(p0) (3.2)\nwhereG(·)denotes the function of sub-problem generation. Then, we so lve the sub-problems\n{p1,...,p n}sequentially, resulting in a sequence of answers {a1,...,a n}. For answering the i-th\nsub-problem pi, we include both the original problem p0and all previously-seen problem-answer\npairs in the context for prediction. The answer aiis given by\nai=Si(pi,{p0,p<i,a<i}) (3.3)\nwherep<i={p1,...,p i−1}anda<i={a1,...,a i−1}.Si(·)denotes the function that solves the\nsub-problem pigiven the context {p0,p<i,a<i}. The last step is to generate the answer to the\noriginal problem p0, which can be expressed in a similar manner to Eq. ( 3.3).\na0=S0(p0,{p≤n,a≤n}) (3.4)\nOne way to reﬁne this model is to modify the G(·)function so that the model can dynamically\ngenerate answers. Instead of generating all sub-problems a t one time, we can generate each of\nthem during problem-solving [ Dua et al. ,2022 ]. To do this, we can replace Eq. ( 3.2) with\npi=Gi(p0,{p<i,a<i}) (3.5)\nHence we obtain a sub-problem generation model that operate s in a step-by-step manner. At each\nstepi, we ﬁrst generate the sub-problem piby prompting an LLM with the original problem p0\nand the problem-solving history {p<i,a<i}. We then generate the answer aifor this sub-problem\nusing the same or a different LLM, based on the same contextua l information (see",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 102,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "9de77bd261f34d5c",
    "content": ",{p≤n,a≤n}) (3.4)\nOne way to reﬁne this model is to modify the G(·)function so that the model can dynamically\ngenerate answers. Instead of generating all sub-problems a t one time, we can generate each of\nthem during problem-solving [ Dua et al. ,2022 ]. To do this, we can replace Eq. ( 3.2) with\npi=Gi(p0,{p<i,a<i}) (3.5)\nHence we obtain a sub-problem generation model that operate s in a step-by-step manner. At each\nstepi, we ﬁrst generate the sub-problem piby prompting an LLM with the original problem p0\nand the problem-solving history {p<i,a<i}. We then generate the answer aifor this sub-problem\nusing the same or a different LLM, based on the same contextua l information (see Eq. ( 3.3)). This\nmethod effectively expands the reasoning capacity of LLMs b y allowing them to dynamically\ngenerate and solve sub-problems in intermediate reasoning steps. As a result, the reasoning paths\nare not ﬁxed in advance, and the models can choose and adapt th eir reasoning strategies during\nproblem-solving.\nAnother way to improve the above model is to focus on developi ng better sub-problem solvers.\nIn our previous discussion, we restricted Si(·)to LLMs that are prompted to solve the sub-problem\npi. In fact, we can expand this function to any system that is cap able of addressing the sub-\nproblem. For example, Si(·)could make calls to IR systems, thereby allowing us to access a\nbroader range of data for problem-solving. Another example is usingSi(·)as a calculator to\naccurately compute results in mathematical problem-solvi ng. If the sub-problem piis complex\nand requires multiple intermediate problem-solving steps , it is also possible to further decompose\npiinto smaller sub-problems. For example, Si(·)can be deﬁned as a recursive program that\ngenerates and solves sub-problems. This incorporates recu rsion into problem-solving and allows\nus to address problems by iteratively decomposing them. As a result, we can deﬁne a hierarchical\nstructure for problem-solving [ Khot et al. ,2023 ].\nIf we generalize the above formulation a bit further, we can c onsider it as a reinforcement\nlearning problem. A typical method is to model a problem-sol ving process as a decision making\nprocess. In each step of this process, an action is taken base d on the current state. These actions\n\n124 Prompting\ncan include all functions for sub-problem generation and so lving (i.e.,Gi(·)andSi(·)). Thus,\nthe action sequence corresponds to a problem-solving path. Since the discussion of reinforcement\nlearning problems is beyond the scope of this chapter, we ski p the precise description of this\nlearning task. Nevertheless, developing an agent or contro ller to determine when and how to\ngenerate and solve a sub-problem is also a natural choice.\nIn NLP, problem decomposition is related to a long line of res earch on multi-hop question\nanswering [ Mavi et al. ,2024 ]. This task requires the system to gather and combine inform ation\nfrom multiple pieces of text to provide an accurate answer to a complex question. For example,\nto answer the question “What is the capital of the country whe re Albert Einstein was born?”, we\nneed to know “Where Albert Einstein was born?” and “What’s th e capital of Germany?”. Earlier\nwork in this area and related ones has investigated the issue of problem decomposition, though the\nmethods might not be based on LLMs. For example, a popular met hod is to develop an additional\nneural model to generate simpler questions that address dif ferent aspects of the original question\n[Andreas et al. ,2016 ;Talmor and Berant ,2018 ;Min et al. ,2019 ]. This question generator can\ncreate questions in a batch or sequential manner.\nBroadly speaking, problem decomposition is also related to the compositionality issue in NLP\n[Drozdov et al. ,2022 ;Press et al. ,2023 ]. For example, in semantic parsing, we map natural lan-\nguage sentences into structured meaning representations b y breaking them down into constituent\nparts and understanding the sentences based on the meanings of these parts and the rules used to\ncombine them. In early studies of this ﬁeld, highly composit ional sentences were considered easier\nfor testing systems, as it is relatively straightforward to decompose such sentences and",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 103,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "3671f121c71a7504",
    "content": " not be based on LLMs. For example, a popular met hod is to develop an additional\nneural model to generate simpler questions that address dif ferent aspects of the original question\n[Andreas et al. ,2016 ;Talmor and Berant ,2018 ;Min et al. ,2019 ]. This question generator can\ncreate questions in a batch or sequential manner.\nBroadly speaking, problem decomposition is also related to the compositionality issue in NLP\n[Drozdov et al. ,2022 ;Press et al. ,2023 ]. For example, in semantic parsing, we map natural lan-\nguage sentences into structured meaning representations b y breaking them down into constituent\nparts and understanding the sentences based on the meanings of these parts and the rules used to\ncombine them. In early studies of this ﬁeld, highly composit ional sentences were considered easier\nfor testing systems, as it is relatively straightforward to decompose such sentences and compose\nthe meanings of their parts. However, the task becomes much m ore difﬁcult when more gener-\nalization is required for modeling compositionality in new data. In this case, we want systems\nto have improved abilities of compositional generalization . In more recent research on LLMs,\nthis issue has been frequently discussed in compositional r easoning tasks, such as SCAN7, as it\nis considered an important aspect of testing the language un derstanding and reasoning abilities\nof LLMs. This also presents new tasks for developing and exam ining problem decomposition\nmethods.\nIn LLMs, one interesting application of problem decomposit ion is tool use. In some cases,\nit is necessary to integrate external tools into LLMs to acce ss accurate data not available during\ntraining or ﬁne-tuning. For example, LLMs can integrate wit h APIs to fetch real-time data such\nas weather updates, stock market prices, or news feeds, enab ling them to provide up-to-date re-\nsponses to user queries. When using tools, LLM predictions m ight include markers that indicate\nwhere and how to call external APIs. This requires decomposi ng the problem into sub-problems,\nwith some handled by the LLMs and others by external tools. Mo re detailed discussions on this\ntopic will be presented in Section 3.2.5 .\n3.2.3 Self-reﬁnement\nIn many cases, predictions of LLMs can be inaccurate or incor rect. Given that current LLMs can\nperform tasks like reﬁnement and correction, it makes sense to explore methods for these models\nto self-reﬁne their outputs. Self-reﬁnement is a common phe nomenon in human psychological\n7The SCAN tasks (Simpliﬁed versions of the CommAI Navigation tasks) are designed to evaluate the ability of\nLLMs to perform compositional generalization [ Lake and Baroni ,2018 ]. They involve translating natural language\ncommands into a sequence of actions. For example, a command “ jump opposite left and walk thrice” can be translated\ninto the action sequence “LTURN LTURN JUMP WALK WALK WALK”.\n\n3.2 Advanced Prompting Methods 125\nactivities and daily behavior. For example, when designing a product, a designer might ﬁrst create\na basic prototype, then reﬁne the design after evaluation an d testing to enhance user experience\nand functionality. The reﬁnement can be iterated several ti mes until the design is satisfactory. The\nidea of predict-then-reﬁne can also be found in NLP. One earl y example is Brill’s tagger [ Brill,\n1992 ], where an initial POS tagging result of a sentence can be ite ratively reﬁned using a rule-\nbased system. In the era of deep learning, a good deal of work o n sequence-to-sequence problems,\nsuch as grammar correction and text rewriting, can also be se en as examples on this theme.\nWe can prompt LLMs to do self-reﬁnement. Consider a simple ex ample where we ﬁrst use\nan LLM to translate a text from Chinese to English and then use the same model to reﬁne the\ntranslation. Here is how we can prompt the model to translate :\nTranslate the following text from Chinese to English:\nChinese:\n一系列考古发现奠定红山文化在中华文明起源研究中的重要地位。最新公\n布的研究成果认为，大约从距今5800年开始，中华大地上各个区域相继出\n现较为明显的社会分",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 104,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "2abebacfa5ee9443",
    "content": " era of deep learning, a good deal of work o n sequence-to-sequence problems,\nsuch as grammar correction and text rewriting, can also be se en as examples on this theme.\nWe can prompt LLMs to do self-reﬁnement. Consider a simple ex ample where we ﬁrst use\nan LLM to translate a text from Chinese to English and then use the same model to reﬁne the\ntranslation. Here is how we can prompt the model to translate :\nTranslate the following text from Chinese to English:\nChinese:\n一系列考古发现奠定红山文化在中华文明起源研究中的重要地位。最新公\n布的研究成果认为，大约从距今5800年开始，中华大地上各个区域相继出\n现较为明显的社会分化，进入文明起源的加速阶段。\nEnglish:\nAseries ofdiscoveries have cemented thesigniﬁcant role oftheHongshan\nculture instudies ontheorigins ofChinese civilization. The latest research\nﬁndings suggests that, starting from around 5800 years ago, many regions\nacross China began toexperience noticeable social differentiations, entering an\naccelerated phase intheorigin ofcivilization.\nThen, we can prompt it to reﬁne the translation:\nNiuTrans\n\n126 Prompting\nYou are provided with a Chinese text along with its English tr anslation. Please\nreﬁne the translation to improve its accuracy and ﬂuency if n ecessary.\nChinese:\n一系列考古发现奠定红山文化在中华文明起源研究中的重要地位。最新公\n布的研究成果认为，大约从距今5800年开始，中华大地上各个区域相继出\n现较为明显的社会分化，进入文明起源的加速阶段。\nEnglish:\nA series of discoveries have cemented the signiﬁcant role of the Hongshan cul-\nture in studies on the origins of Chinese civilization. The l atest research ﬁndings\nsuggests that, starting from around 5800 years ago, many reg ions across China\nbegan to experience noticeable social differentiations, e ntering an accelerated\nphase in the origin of civilization.\nReﬁnement:\nA series of archaeological discoveries have cemented the signiﬁcant role\nof the Hongshan culture in studies on the origins of Chinese c ivilization.\nThe latest research ﬁndings suggest that, starting from around 5800 years\nago, various regions across China began to experience noticeable soc ial\ndifferentiations, entering an accelerated phase in the ori gin of civilization.\nHere modiﬁcations to the input translation are highlighted in green. This process can be iterative:\nat each step, we use the translation from the previous step as input, and prompt the LLM to produce\na potentially better translation.\nIn the above example, the reﬁnement of the initial model outp ut was achieved by prompting the\nLLM with a simple instruction like “Please reﬁne it!”. Howev er, the reﬁnement is based solely on\nthe instruction-following ability of the LLM, and there is n o guidance or supervision on how and\nwhere to improve the model output. A more effective approach would be to consider feedback on\nspeciﬁc aspects that need reﬁnement. For example, we can pro mpt the LLM with “Please correct\nall the grammatical errors in the translation”, so that the m odel can focus more on grammatical\nerror correction during reﬁnement.\nA general framework of self-reﬁnement with LLMs involves th ree steps [ Madaan et al. ,2024 ].\n•Prediction . We use an LLM to produce the initial model output.\n•Feedback Collection . We obtain feedback on the model output.\n•Reﬁnement . We use the LLM to reﬁne the model output based on the feedback .\nThe last two steps can be repeated multiple times, which lead s to an iterative self-reﬁnement\nprocess. In this framework, receiving accurate and detaile d feedback is critical as it guides the sub-\nsequent reﬁnement step. In general, feedback can be obtaine d in various ways [ Fernandes et al. ,\n2023 ;Pan et al. ,2024 ]. We can manually examine the model",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 105,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "fc5fdc8c04fb6032",
    "content": " correct\nall the grammatical errors in the translation”, so that the m odel can focus more on grammatical\nerror correction during reﬁnement.\nA general framework of self-reﬁnement with LLMs involves th ree steps [ Madaan et al. ,2024 ].\n•Prediction . We use an LLM to produce the initial model output.\n•Feedback Collection . We obtain feedback on the model output.\n•Reﬁnement . We use the LLM to reﬁne the model output based on the feedback .\nThe last two steps can be repeated multiple times, which lead s to an iterative self-reﬁnement\nprocess. In this framework, receiving accurate and detaile d feedback is critical as it guides the sub-\nsequent reﬁnement step. In general, feedback can be obtaine d in various ways [ Fernandes et al. ,\n2023 ;Pan et al. ,2024 ]. We can manually examine the model’s predictions to identi fy issues. Also,\nwe can use a feedback model to automatically generate feedba ck. For instance, a reward model\ntrained on some labeled data can help evaluate the model outp ut and return a score indicating its\nquality.\n\n3.2 Advanced Prompting Methods 127\nSince the focus of this chapter is on LLM prompting, we consid er use LLMs to provide feed-\nback. To illustrate, let us consider an example of generatin g good responses to user questions:\n• First, we prompt an LLM to answer the input question, as usua l.\nGenerate a response to the question: “What are some major env ironmental\nconcerns today?”\nResponse:\nMajor environmental concerns today include climate change, airandwater\npollution, deforestation, lossofbiodiversity, andocean degradation.\n• Then, using the following prompt, we prompt the same LLM aga in to get feedback on the\ngenerated response.\nYou will be provided with a question and its corresponding re sponse. Eval-\nuate the accuracy and depth of this response, and identify an y factual in-\naccuracies or areas where additional details could enhance understanding.\nQuestion:\nWhat are some major environmental concerns today?\nResponse:\nMajor environmental concerns today include climate change , air and water\npollution, deforestation, loss of biodiversity, and ocean degradation.\nFeedback:\nThe response provided isaccurate and highlights key environmental\nconcerns. However, itwould beneﬁt from more speciﬁc and detailed\ndescriptions ofthese concerns. Here aresuggestions forimprovement:\n1.Include speciﬁc examples ofenvironmental issues.\n2.Add information about major pollutants.\n3.Specify theregions most affected.\n• Given the feedback the LLM produces, we prompt it to reﬁne th e response.\n\n128 Prompting\nYou will be provided with a question, its corresponding resp onse, and\nsome feedback on this response.\nQuestion:\nWhat are some major environmental concerns today?\nResponse:\nMajor environmental concerns today include climate change , air and water\npollution, deforestation, loss of biodiversity, and ocean degradation.\nFeedback:\nThe response provided is accurate and highlights key enviro nmental con-\ncerns. However, it would beneﬁt from more speciﬁc and detail ed descrip-\ntions of these concerns. Here are suggestions for improveme nt:\n1. Include speciﬁc examples of environmental issues.\n2. Add information about major pollutants.\n3. Specify the regions most affected.\nNow reﬁne the response based on the provided feedback.\nReﬁned Response:\nKey environmental concerns include climate change, evidenced byrising\nglobal temperatures and severe weather patterns; signiﬁcant airand\nwater pollution inurban andindustrial areas; rampant deforestation in\nregions liketheAmazon, causing biodiversity loss; andocean degradation,\nhighlighted bycoral reefbleaching andwidespread overﬁshing.\nIdeally, if a strong LLM is adopted, we would like to have it pe rform all three steps without\nextra training. On the other hand, if we have enough labeled d ata for the task of interest, we can\nenhance the performance of the LLM using supervised learnin g. For example, we can ﬁne-tune\nthe LLM to better adapt it to reﬁnement tasks, or alternative ly, use task-speciﬁc models, which\nmay not necessarily be based on LLMs [ Welleck et al. ,2023 ;Schick et al. ,2023 ]. In a broader\nsense, improving LLMs for self-reﬁnement tasks can be seen a s an alignment issue. For example,\nit has been found that some self-correction abilities can be activated through RLHF [",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 106,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "2c1d689c0c6fe7cb",
    "content": "ocean degradation,\nhighlighted bycoral reefbleaching andwidespread overﬁshing.\nIdeally, if a strong LLM is adopted, we would like to have it pe rform all three steps without\nextra training. On the other hand, if we have enough labeled d ata for the task of interest, we can\nenhance the performance of the LLM using supervised learnin g. For example, we can ﬁne-tune\nthe LLM to better adapt it to reﬁnement tasks, or alternative ly, use task-speciﬁc models, which\nmay not necessarily be based on LLMs [ Welleck et al. ,2023 ;Schick et al. ,2023 ]. In a broader\nsense, improving LLMs for self-reﬁnement tasks can be seen a s an alignment issue. For example,\nit has been found that some self-correction abilities can be activated through RLHF [ Ganguli et al. ,\n2023 ]. However, discussing these issues is beyond the scope of th is chapter. Further discussion\ncan be found in Chapter 4.\nIn LLMs, self-reﬁnement is related to several concepts that reveal the psychological aspects\nof these models, such as the ability to self-reﬂect. A view is that if LLMs are capable of self-\nreﬂection, their predictions can become more accurate and e ven possess self-correcting capabili-\nties. This self-reﬂection can be activated in various ways, for example, by prompting these LLMs\nto engage in more in-depth and careful thinking, or by provid ing examples from which the models\ncan learn and reﬂect. To illustrate, we consider here the deliberate-then-generate (DTG ) method\npresented in Li et al. [2023a ]’s work, where LLMs are prompted to deliberate. In DTG, we ar e\ngiven an initial model output which may contain errors. LLMs are then prompted to identify the\nerror types of this model output and provide an improved outp ut. Below is a template of DTG\nprompting for Chinse-to-English translation tasks.\n\n3.2 Advanced Prompting Methods 129\nGiven the Chinese sentence: {∗source ∗}\nThe English translation is: {∗target ∗}\nPlease ﬁrst detect the type of error, and then reﬁne the trans lation.\nError Type:\nWe aim to ﬁrst predict the error type (red), and then produce a reﬁned translation (blue). This\nprocess of deliberation is guided by the instruction “Pleas e ﬁrst detect the type of error, and then\nreﬁne the translation”. It encourages LLMs to initially eng age in thoughtful analysis and then give\nbetter results. Since error type prediction and reﬁnement a re performed in a single run of LLMs,\nthis method incorporates both steps of feedback and reﬁneme nt into one process.\nIn the above prompts, we assume that the LLM we use is able to re view the input translation\nand correctly identify its error types. However, this raise s new difﬁculties as the model may not\nbe good at ﬁnding errors in translations. This will in turn re sult in extra ﬁne-tuning or prompt-\ning engineering efforts. So a simpler method is to reduce the burden of error identiﬁcation and\nuse LLMs for deliberation only. To do this, we can replace the input translation with a random\ntranslation and assign a default error type. An example of su ch a prompt is shown below.\nGiven the Chinese sentence:\n一系列考古发现奠定红山文化在中华文明起源研究中的重要地位。\nThe English translation is:\nAvarietyofinnovativetechniques have redeﬁned theimportance ofmodernart\nincontemporary cultural studies.\nPlease ﬁrst detect the type of error, and then reﬁne the trans lation.\nError Type: IncorrectTrans lation\nIn this example, the input translation is not generated by LL Ms but is instead randomly sam-\npled from the dataset. So it is simply an incorrect translati on for the source sentence, and we can\nset the error type accordingly. The LLMs then generate a new t ranslation by taking both the source\nsentence and the incorrect translation as input. The design of this prompt can also be considered as\nactivating the learning capabilities of LLMs through “nega tive evidence” [ Marcus ,1993 ], thereby\nenabling them to re�",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 107,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "40470a6e040cafbc",
    "content": "文化在中华文明起源研究中的重要地位。\nThe English translation is:\nAvarietyofinnovativetechniques have redeﬁned theimportance ofmodernart\nincontemporary cultural studies.\nPlease ﬁrst detect the type of error, and then reﬁne the trans lation.\nError Type: IncorrectTrans lation\nIn this example, the input translation is not generated by LL Ms but is instead randomly sam-\npled from the dataset. So it is simply an incorrect translati on for the source sentence, and we can\nset the error type accordingly. The LLMs then generate a new t ranslation by taking both the source\nsentence and the incorrect translation as input. The design of this prompt can also be considered as\nactivating the learning capabilities of LLMs through “nega tive evidence” [ Marcus ,1993 ], thereby\nenabling them to reﬂect and produce better outcomes through contrastive analysis. Nevertheless,\nthis method does not rely on any feedback and can enhance the p erformance of a single LLM\nprediction via simple prompting.\nNote that while DTG is non-iterative, iterative learning an d reﬁnement are commonly used\nin NLP. An advantage of these iterative approaches are that t hey mimics human learning and\nproblem-solving, where continuous feedback and adjustmen ts lead to progressively improved out-\ncomes. Iterative methods can be applied to a range of LLM prom pting problems. For example, in\nproblem decomposition, one can incorporate new sub-proble ms and their solutions into the con-\ntext at each step, and thus LLMs can progressively approach t he solution of the original problem.\n\n130 Prompting\nOn the other hand, iterative methods raise several issues th at are absent in non-iterative meth-\nods, for example, errors in earlier steps may negatively imp act subsequent problem-solving, and\ndetermining when to stop iterating often requires addition al engineering effort.\n3.2.4 Ensembling\nModel ensembling for text generation has been extensively d iscussed in the NLP literature. The\nidea is to combine the predictions of two or more models to gen erate a better prediction. This\ntechnique can be directly applicable to LLMs. For example, w e can collect a set of LLMs and run\neach of them on the same input. The ﬁnal output is a combined pr ediction from these models.\nFor LLM prompting, it is also possible to improve performanc e by combining predictions\nbased on different prompts. Suppose we have an LLM and a colle ction of prompts that address\nthe same task. We can run this LLM with each of the prompts and t hen combine the predictions.\nFor example, below are three different prompt templates for text simpliﬁcation.\nMake this text simpler.\n{∗text∗}\nCondense and simplify this text.\n{∗text∗}\nRewrite for easy reading.\n{∗text∗}\nEach of these prompts will lead to a different prediction, an d we can consider all three predictions\nto generate the ﬁnal one.\nFormally, let {x1,...,xK}beKprompts for performing the same task. Given an LLM Pr(·|·),\nwe can ﬁnd the best prediction for each xiusing ˆyi= arg maxyiPr(yi|xi). These predictions\ncan be combined to form a “new” prediction:\nˆy= Combine(ˆ y1,...,ˆyK) (3.6)\nHere Combine( ·)is the combination model, which can be designed in several di fferent ways. For\nexample, we can select the best prediction by voting or by ide ntifying the one that overlaps the\nmost with others. Another method for model combination is to perform model averaging during\ntoken prediction. Let ˆyjbe the predicted token at the j-th step for model combination. The\n\n3.2 Advanced Prompting Methods 131\nprobability of predicting ˆyjis given by\nˆyj= arg max\nyjK∑\nk=1log Pr(yj|xk,ˆy1,...,ˆyj−1) (3.7)\nIn ensembling for LLM prompting, it is generally advantageo us to use diverse prompts so that\nthe combination can capture a broader range of potential res ponses. This practice is common in\nensemble learning, as diversity helps average out biases an d errors that may be speciﬁc to any\nsingle model or conﬁguration. From the Bayesian viewpoint, we can treat the prompt xas a latent\nvariable, given the problem of",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 108,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "4698ee620f41f823",
    "content": " overlaps the\nmost with others. Another method for model combination is to perform model averaging during\ntoken prediction. Let ˆyjbe the predicted token at the j-th step for model combination. The\n\n3.2 Advanced Prompting Methods 131\nprobability of predicting ˆyjis given by\nˆyj= arg max\nyjK∑\nk=1log Pr(yj|xk,ˆy1,...,ˆyj−1) (3.7)\nIn ensembling for LLM prompting, it is generally advantageo us to use diverse prompts so that\nthe combination can capture a broader range of potential res ponses. This practice is common in\nensemble learning, as diversity helps average out biases an d errors that may be speciﬁc to any\nsingle model or conﬁguration. From the Bayesian viewpoint, we can treat the prompt xas a latent\nvariable, given the problem of interest, p. This allows the predictive distribution of ygivenpto\nbe written as the distribution Pr(y|x)marginalized over all possible prompts\nPr(y|p) =∫\nPr(y|x) Pr(x|p)dx (3.8)\nThe integral computes the total probability of yby considering all possible values of x, weighted\nby their likelihoods given p. Here Pr(y|x)is given by the LLM, and Pr(x|p)is the prior distri-\nbution of prompts for the problem. This is a good model becaus e the integral effectively accounts\nfor the uncertainty in the choice of x, ensuring that the ﬁnal predictive distribution Pr(y|p)is\nrobust and encompasses all potential variations and biases in the prompts. However, computing\nthis integral directly can be computationally infeasible d ue to the potentially inﬁnite space of x.\nOne approach to addressing this issue is to employ methods li ke Monte Carlo sampling, which\napproximate the integral using a manageable, ﬁnite number o f prompts.\nWhile the Bayesian treatment is mathematically well-deﬁne d, it is common practice in NLP\nto assume a non-informative or uniform prior and focus inste ad on constructing a set of diverse\nprompts. Consequently, the output can be computed using a st raightforward combination model,\nas described in Eq. ( 3.6). The issue of creating high-quality, diverse prompts has b een studied in\nCoT and other in-context learning areas. Most of the researc h focuses on incorporating a variety\nof demonstration examples across different prompts. Here, we list some of these methods.\n• Given a problem, we manually create a number of demonstrati ons and use different ones\nfor different prompts.\n• Given a problem, we use LLMs to automatically generate demo nstrations and prompts.\n• Given a prompt, we create different prompts by changing the order of demonstrations in the\nprompt.\n• Given a prompt, we use LLMs to generate a number of similar pr ompts.\n• Given a prompt, we transform it into other forms, e.g., tran slating it into other languages.\nOf course, in practice, we can combine these methods to achie ve greater diversity. An underly-\ning assumption here is that diverse prompts can lead to diver se model outputs. This is particularly\nthe case when the problem we deal with is relatively new and di fﬁcult. For stronger and more ro-\nbust LLMs, the variance in output for similar prompts might n ot be large. In this case, the beneﬁt\nof involving multiple prompts can be modest.\n\n132 Prompting\nIn addition to providing diverse prompts for LLMs, another a pproach is to make use of the\ninherent variance in the outputs of LLMs. One simple way to ge nerate multiple outputs is to\nsample outputs from the hypothesis space. This is straightf orward for LLMs because their search\nalgorithms are naturally suited to producing multiple resu lts. For example, if beam search is\nused, we can easily collect all the complete hypotheses in th e beam to generate multiple output\ncandidates. Also, we can modify the search algorithms to imp lement more powerful sampling\nmethods and explore a wider range of high-quality hypothese s.\nThe next question is how to combine diverse model prediction s. This leads us back to the\nmodel combination issue as described in Eq. ( 3.6). Here we consider the self-consistency method,\nwhich outputs not the prediction with the highest probabili ty, but rather the one that best aligns\nwith other predictions [ Wang et al. ,2022a ;2023b ]. First, an LLM is prompted with CoT as\nusual and generates multiple reasoning paths",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 109,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "6b5f986b9507bc43",
    "content": " to ge nerate multiple outputs is to\nsample outputs from the hypothesis space. This is straightf orward for LLMs because their search\nalgorithms are naturally suited to producing multiple resu lts. For example, if beam search is\nused, we can easily collect all the complete hypotheses in th e beam to generate multiple output\ncandidates. Also, we can modify the search algorithms to imp lement more powerful sampling\nmethods and explore a wider range of high-quality hypothese s.\nThe next question is how to combine diverse model prediction s. This leads us back to the\nmodel combination issue as described in Eq. ( 3.6). Here we consider the self-consistency method,\nwhich outputs not the prediction with the highest probabili ty, but rather the one that best aligns\nwith other predictions [ Wang et al. ,2022a ;2023b ]. First, an LLM is prompted with CoT as\nusual and generates multiple reasoning paths by sampling. T hen, the frequency of each answer is\ncounted across these reasoning paths. The ﬁnal output is the answer that has the highest count.\nBelow shows three different predictions by prompting an LLM with CoT.\nPrediction 1 (Correct):\nQ: Three friends play a game where they ﬂip a fair coin. Each fr iend ﬂips the\ncoin once. What is the probability that exactly one of them ﬂi ps heads?\nA: Let’s think step by step.\nEach coin ﬂipisanindependent event with a50% chance oflanding heads\n(H)and50% tails (T). Consider thepossible outcomes forthree ﬂips: HHH,\nHHT, HTH, HTT, THH, THT, TTH, TTT. Outofthese, only HHH, HHT, HTH,\nandTHH arerelevant. Weareonly interested inscenarios with exactly one\nhead: HTT, THT, andTTH. There are3such outcomes outof8total, giving a\nprobability of3/8.\nPrediction 2 (Correct):\nQ: Three friends play a game where they ﬂip a fair coin. Each fr iend ﬂips the\ncoin once. What is the probability that exactly one of them ﬂi ps heads?\nA: Let’s think step by step.\nUsing abinomial probability formula, wherep(success orheads) is0.5,and\neach ofthe3trials (ﬂips) isindependent, welook forexactly one success\n(head). The formula forexactlyksuccesses inntrials isgiven bythe\nbinomial coefﬁcient formula(n\nk)pk(1−p)n−k.Plugging inthevalues, weget(3\n1)(0.5)1(0.5)3−1= 3·0.5·0.25 = 0.375.\nPrediction 3 (Wrong):\n\n3.2 Advanced Prompting Methods 133\nQ: Three friends play a game where they ﬂip a fair coin. Each fr iend ﬂips the\ncoin once. What is the probability that exactly one of them ﬂi ps heads?\nA: Let’s think step by step.\nSince each coin ﬂiphasa50% chance oflanding heads anda50% chance of\nlanding tails, andwearelooking forthecase where only oneofthethree ﬂips\nresults inheads, wemight initially think thisissimply amatter ofpicking which\nﬂipistheheads. There arethree ﬂips, soonemight incorrectly assume thatthe\nchance ofpicking onespeciﬁc outcome likethiswould be1outof3.Thus, they\nmight conclude thattheprobability ofexactly onehead is1/3 = 33.3%.\nPredictions 1 and 2 correctly identify the three cases where exactly one head is ﬂipped, both\nobtaining a probability of 37.5%. The reasoning in Predicti on 3 fails to account for the total num-\nber of outcomes possible with three coin ﬂips, thus giving a w rong answer of 33.3%. Therefore,\nwe select 37.5% as the ﬁnal answer because it is the consensus .\nSelf-consistency provides a criterion for determining the best prediction in a pool of can-\ndidates. Since the prompt and the model are ﬁxed in this metho d, it is not strictly a prompt\nensembling method. Instead, it can be seen as an instance of o utput ensembling methods, also\nknown as hypothesis selection methods, which have long been explored in N",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 110,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "1125175c23222037",
    "content": "\nmight conclude thattheprobability ofexactly onehead is1/3 = 33.3%.\nPredictions 1 and 2 correctly identify the three cases where exactly one head is ﬂipped, both\nobtaining a probability of 37.5%. The reasoning in Predicti on 3 fails to account for the total num-\nber of outcomes possible with three coin ﬂips, thus giving a w rong answer of 33.3%. Therefore,\nwe select 37.5% as the ﬁnal answer because it is the consensus .\nSelf-consistency provides a criterion for determining the best prediction in a pool of can-\ndidates. Since the prompt and the model are ﬁxed in this metho d, it is not strictly a prompt\nensembling method. Instead, it can be seen as an instance of o utput ensembling methods, also\nknown as hypothesis selection methods, which have long been explored in NLP, particularly for\ntext generation problems [ Xiao et al. ,2013 ]. In these methods, multiple outputs are generated by\nvarying model architectures or parameters. Each output is t hen assigned a score by some criterion,\nand the outputs are re-ranked based on these scores. There ar e various ways to deﬁne the scoring\nfunction, such as measuring the agreement between an output and others, and using a stronger\nmodel to rescore each output8. Figure 3.2shows a comparison of different ensembling methods\nfor LLMs.\nNow, let us brieﬂy review the methods we have discussed so far in this section, such as problem\ndecomposition and self-reﬁnement. It is apparent that thes e methods enhance decision-making by\nintroducing more “choices” into the reasoning process. To s ome extent, they all involve evalu-\nating and providing feedback on the results of LLMs. For exam ple, in self-reﬁnement, we need\nto offer suggestions for improving the prediction of LLMs, a nd in output ensembling, we select\nthe optimal output from a pool of candidates. In this sense, t hese methods fall under the broader\ncategory of predict-then-verify approaches, where predic tions are initially made, then veriﬁed and\nreﬁned. The fundamental problem here involves verifying an d evaluating the reasoning results\nor intermediate steps. This issue is somewhat related to the problem of training reward models\nin RLHF, although RLHF addresses a different aspect. In fact , the development of veriﬁers has\nbeen explored and implemented in reasoning with LLMs. Most w ork, rather than developing\nheuristic-based inference-time algorithms, focuses on le arning veriﬁers in a supervised manner. A\nstraightforward method is to train veriﬁers as binary class iﬁers, such as classifying an answer as\n8An interpretation of self-consistency is to view it as a mini mum Bayes risk search process. It searches for the best\noutput by minimizing the Bayes risk. More speciﬁcally, a ris k functionR(y,yr)is deﬁned on each pair of outputs\n(denoted by (y,yr)), representing the cost of replacing ywith yr. Given a set of outputs Ω, the risk of an output\ny∈Ωis given by\nRisk( y) =Eyr∼Pr(yr|x)R(y,yr)\n=∑\nyr∈ΩR(y,yr)·Pr(yr|x) (3.9)\n\n134 Prompting\nLLM2\nLLM1LLM2\nPrompt Prediction2\nPrediction1Prediction3Combine/Select\nFinal\nPrediction\n(a) Model Ensembling\nLLM Prompt2\nPrompt1Prompt3\nPrediction2\nPrediction1Prediction3Combine/Select\nFinal\nPrediction\n(b) Prompt Ensembling\nLLM Prompt Prediction2\nPrediction1Prediction3Combine/Select\nFinal\nPredictionSample\n(c) Output Ensembling\nFig. 3.2: Ensembling methods for LLMs. In standard model ensembling ( a), multiple LLMs varying in architectures or\nparameters are used. Each LLM receives the same prompt and pr oduces a prediction. These predictions are combined\nto generate the ﬁnal prediction. In prompt ensembling (b), w e have one LLM and multiple prompts. The LLM produces\na prediction for each prompt, and these predictions are comb ined as usual. In output ensembling (c), the LLM samples\nmultiple predictions over the prediction space given a prom pt. It can be seen as a method to boost the performance\nof the LLM itself. Note that these ensembling",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 111,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "3a3cd5a14aeebdbf",
    "content": "\nPrompt1Prompt3\nPrediction2\nPrediction1Prediction3Combine/Select\nFinal\nPrediction\n(b) Prompt Ensembling\nLLM Prompt Prediction2\nPrediction1Prediction3Combine/Select\nFinal\nPredictionSample\n(c) Output Ensembling\nFig. 3.2: Ensembling methods for LLMs. In standard model ensembling ( a), multiple LLMs varying in architectures or\nparameters are used. Each LLM receives the same prompt and pr oduces a prediction. These predictions are combined\nto generate the ﬁnal prediction. In prompt ensembling (b), w e have one LLM and multiple prompts. The LLM produces\na prediction for each prompt, and these predictions are comb ined as usual. In output ensembling (c), the LLM samples\nmultiple predictions over the prediction space given a prom pt. It can be seen as a method to boost the performance\nof the LLM itself. Note that these ensembling methods can be c ombined to increase the diversity of predictions. For\nexample, we can use both prompt ensembling and output ensemb ling to obtain more diverse predictions.\ncorrect or incorrect, although these veriﬁers are typicall y used as scoring models. Given a reason-\ning path for a problem, the veriﬁers can be used to score eithe r the entire path (called outcome-\nbased approaches) [ Cobbe et al. ,2021 ], or each individual reasoning step (called process-based\napproaches) [ Uesato et al. ,2022 ;Lightman et al. ,2024 ].\n3.2.5 RAG and Tool Use\nRAG is generally employed when standard LLMs, which rely sol ely on pre-trained knowledge,\nlack accuracy and depth in the generated text. By drawing fro m external databases and documents,\n\n3.2 Advanced Prompting Methods 135\nRAG can signiﬁcantly improve the quality of responses, ensu ring they are both contextually rel-\nevant and factually correct. Such an approach is particular ly useful in scenarios that require high\nfactual accuracy and up-to-date information, such as compl ex question answering.\nThe concept of RAG has been mentioned several times in the pre vious sections and chapters.\nFor completeness, we outline the key steps involved in RAG he re.\n• We prepare a collection of texts which are treated as an addi tional source of knowledge we\ncan access.\n• We retrieve relevant texts for a given query.\n• We input both the retrieved texts and the query into an LLM, w hich is then prompted to\nproduce the ﬁnal prediction.\nSteps 1 and 2 can be implemented by using an external informat ion retrieval system. For\nexample, we can store the collection of texts in a vector data base and then retrieve the most similar\ntexts through vector-based search techniques. Since infor mation retrieval is not the focus of this\nchapter, we will assume that such systems are available off- the-shelf and use them directly.\nHere we present how to prompt LLMs to make use of retrieved tex ts. To illustrate, consider\nan example of using LLMs to answer the following question.\nWhere will the 2028 Olympics be held?\nWe can simply input this question into an online search engin e. It will then return the relevant\npieces of text found on the internet, for example,\n(Wikipedia)\nThe 2028 Summer Olympics, ofﬁcially the Games of the XXXIV Ol ympiad and com-\nmonly known as Los Angeles 2028 or LA28, is an upcoming intern ational multi-sport\nevent scheduled to take place from July 14-30, 2028, in the Un ited States. ...\n(The Sporting News)\nIn 2028, Los Angeles will become the third city, following Lo ndon and Paris respectively,\nto host three Olympics after hosting the Summer Games in 1932 and 1984. It will also\nbe the ﬁrst time the United States has hosted an Olympic Games since the 2002 Winter\nGames in Salt Lake City. ...\n...\nWe can use these retrieved texts as additional context, and p rompt an LLM to generate a\nresponse based on these texts. Below is an example RAG prompt .\n\n136 Prompting\nYour task is to answer the following question. To help you wit h this, relevant\ntexts are provided. Please base your answer on these texts.\nQuestion:\nWhere will the 2028 Olympics be held?\nRelevant Text 1:\nThe 2028 Summer Olympics, ofﬁcially the Games of the XXXIV Ol ympiad and\ncommonly known as Los Angeles 2028 or LA28 ...\nRelevant Text 2:\nIn 2028, Los",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 112,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "ca76fd7177e31b2f",
    "content": " become the third city, following Lo ndon and Paris respectively,\nto host three Olympics after hosting the Summer Games in 1932 and 1984. It will also\nbe the ﬁrst time the United States has hosted an Olympic Games since the 2002 Winter\nGames in Salt Lake City. ...\n...\nWe can use these retrieved texts as additional context, and p rompt an LLM to generate a\nresponse based on these texts. Below is an example RAG prompt .\n\n136 Prompting\nYour task is to answer the following question. To help you wit h this, relevant\ntexts are provided. Please base your answer on these texts.\nQuestion:\nWhere will the 2028 Olympics be held?\nRelevant Text 1:\nThe 2028 Summer Olympics, ofﬁcially the Games of the XXXIV Ol ympiad and\ncommonly known as Los Angeles 2028 or LA28 ...\nRelevant Text 2:\nIn 2028, Los Angeles will become the third city, following Lo ndon and Paris\nrespectively, to host three Olympics after ...\n...\nThe 2028 Olympics will be held in Los Angeles.\nThis prompt assumes that the provided texts are relevant to t he question and expects the LLM\nto generate a faithful response using these texts. However, the information retrieval system may\nsometimes provide irrelevant or incorrect texts, which may lead the LLM to produce an incorrect\nanswer. One straightforward way to address this issue is to i mprove the accuracy of the informa-\ntion retrieval system. Nevertheless, as with most AI system s, errors may still occur. Therefore, it\nis also necessary to enhance the robustness of the LLM, so tha t it can make reasonable predictions\neven when the input is inaccurate. Below is a new prompt that e nables the LLM to be more faith-\nful to the facts, and allows it to choose not to answer questio ns when the information provided is\ninaccurate.\nYour task is to answer the following question. To help you wit h this, relevant\ntexts are provided. Please base your answer on these texts.\nPlease note thatyour answers need tobeasaccurateaspossibleandfaithfulto\nthefacts. Iftheinformationprovided isinsufﬁcient foranaccurateresponse, you\nmay simplyoutput\"No answer!\".\nQuestion:\nWhere will the 2028 Olympics be held?\nRelevant Text 1:\nThe 2024 Summer Olympics, ofﬁcially the Games of the XXXIII O lympiad and\nbranded as Paris 2024, were an international multi-sport ev ent ...\n...\nNoanswer!\nIn this example, the LLM refuses to answer because the provid ed information is insufﬁcient and\nirrelevant to the question.\nBoth RAG and ﬁne-tuning are common methods for adapting LLMs using task-speciﬁc data.\nStandard RAG is training-free and can be directly applied to LLMs. To further improve RAG, it\n\n3.2 Advanced Prompting Methods 137\nis also possible to ﬁne-tune LLMs, though this will require s ome training effort. For example, we\ncan ﬁne-tune LLMs using human-labelled data to supervise th em in learning to refuse to answer.\nNote that, while the examples shown above seem simple, RAG is not trivial. From the prompt\nengineering perspective, different use cases may require d ifferent prompts, though our somewhat\n“greedy” goal is to develop a universal prompting strategy t hat can adapt to different tasks. In\nmany cases, we need to control how much we depend on the retrie ved context to make predictions.\nSometimes, LLMs must derive responses strictly from the pro vided texts, while at other times,\nthey may need to generate responses using their pre-trained knowledge if the provided texts are\ninsufﬁcient. There are many aspects of RAG, such as improvem ents to the retrieval systems, that\ncannot be covered in this chapter. Interested readers can re fer to surveys of RAG techniques for\nmore information [ Li et al. ,2022 ;Gao et al. ,2023c ].\nOne reason we discuss RAG here is that it can be broadly regard ed as an instance of the\ngeneral problem decomposition framework (see Section 3.2.2 ). RAG divides problem-solving\ninto two steps. In the ﬁrst step, we collect relevant and supp orting information for a given query\nfrom various knowledge sources. In the second step, we use LL Ms to generate responses based\non the collected information. If we extend the concept of pro blem decomposition further, we\nwill ﬁnd that many tasks requiring the use of external system s or tools can be treated as similar\n",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 113,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "d7ab3a8ebd2ac6f6",
    "content": " knowledge if the provided texts are\ninsufﬁcient. There are many aspects of RAG, such as improvem ents to the retrieval systems, that\ncannot be covered in this chapter. Interested readers can re fer to surveys of RAG techniques for\nmore information [ Li et al. ,2022 ;Gao et al. ,2023c ].\nOne reason we discuss RAG here is that it can be broadly regard ed as an instance of the\ngeneral problem decomposition framework (see Section 3.2.2 ). RAG divides problem-solving\ninto two steps. In the ﬁrst step, we collect relevant and supp orting information for a given query\nfrom various knowledge sources. In the second step, we use LL Ms to generate responses based\non the collected information. If we extend the concept of pro blem decomposition further, we\nwill ﬁnd that many tasks requiring the use of external system s or tools can be treated as similar\nproblems. One such example is tool use in LLMs. In many applic ations, LLMs need to employ\nexternal databases, APIs, and even simulation tools to gene rate accurate responses. For example,\nLLMs can access real-time data from ﬁnancial markets to prov ide up-to-date investment advice or\nintegrate with healthcare databases to offer personalized medical insights. This integration extends\nthe capabilities of LLMs by allowing them to interact with, a nd in some contexts, inﬂuence or\ncontrol external systems. Consequently, LLMs function mor e asautonomous agents rather than\nmere text generators [ Franklin and Graesser ,1996 ].\nThe issue of tool use is broad and vast. Here we narrow our disc ussion to tasks that can be fa-\ncilitated by calling external APIs to solve some of the sub-p roblems [ Parisi et al. ,2022 ;Gao et al. ,\n2023b ]. Consider again the example of asking an LLM to answer “Wher e will the 2028 Olympics\nbe held?”. Suppose the LLM can access a web search tool. We can then prompt the LLM to\nanswer the question with web search, like this\nYour task is to answer the following question. You may use ext ernal tools, such\nas web search, to assist you.\nQuestion:\nWhere will the 2028 Olympics be held?\nThe information regarding this question is given as follows :\n{tool: web-search, query: \"2028 Olympics\"}\nSo the answer is: Los Angeles\nHere {tool: web-search, query: \"2028 Olympics\"} indicates a request to the web search\nsystem using the query “2028 Olympics”. When the LLM sees thi s string, it executes a web search\nand uses the result to replace the string. Then, in subsequen t steps of prediction, the LLM uses\nthis web search result as context to produce the correct answ er.\nConsider another example where we ask the LLM to solve a mathe matical problem.\n\n138 Prompting\nProblem:\nA swimming pool needs to be ﬁlled with water. The pool measure s 10 meters\nin length, 4 meters in width, and 2 meters in depth. Calculate the volume of the\npool in cubic meters and then determine how many liters of wat er are needed to\nﬁll it (considering 1 cubic meter equals 1000 liters).\nSolution:\nTo solve this problem, the LLM needs to ﬁrst calculate the vol ume of the\npool by using the formula for the volume of a rectangular pris m: Length ×\nWidth ×Depth.Therefore, The volume is 10m×4m×2m={tool:\ncalculator, expres sion: 10*4*2}m3. Next, to ﬁnd out how\nmany liters of water are needed, the LLM multiplies the volum e in cubic\nmeters by 1000 (since 1 cubic meter equals 1000 liters). Thus ,80×1000\n={tool: calculator, expres sion: 80*1000} liters.\nHere the string {tool: calculator, expression: 10 * 4 * 2} triggers the invocation\nof a mathematical interpreter to calculate the result of the expression. Note that the result (i.e.,\n80) will replace {tool: calculator, expression: 10 * 4 * 2} and can be referred to\nin the following token predictions. For example, in the last step of problem-solving, 80 is used\ninstead of {tool: calculator, expression: 10 * 4 * 2} .\nA key difference between the tool use examples here and the pr eviously discussed RAG ex-\namples is that in tool use, external functions can be called",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 114,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "dc947222493e2ad0",
    "content": " liters of water are needed, the LLM multiplies the volum e in cubic\nmeters by 1000 (since 1 cubic meter equals 1000 liters). Thus ,80×1000\n={tool: calculator, expres sion: 80*1000} liters.\nHere the string {tool: calculator, expression: 10 * 4 * 2} triggers the invocation\nof a mathematical interpreter to calculate the result of the expression. Note that the result (i.e.,\n80) will replace {tool: calculator, expression: 10 * 4 * 2} and can be referred to\nin the following token predictions. For example, in the last step of problem-solving, 80 is used\ninstead of {tool: calculator, expression: 10 * 4 * 2} .\nA key difference between the tool use examples here and the pr eviously discussed RAG ex-\namples is that in tool use, external functions can be called d uring inference. In contrast, in RAG,\nthe retrieved texts are provided before the prediction proc ess begins. However, from the language\nmodeling perspective, they are actually doing the same thin g: before generating the ﬁnal result,\nwe use external tools, either manually or automatically, to obtain sufﬁcient and relevant context. A\nhigh-level interpretation of these approaches is that they both rely on an “agent” that can determine\nwhere and how to call external functions to generate the cont ext necessary for prediction.\nAn issue with tool use is that the original LLMs are not traine d to generate the necessary\nmarkers for tool use. Therefore, we need to ﬁne-tune the LLMs to adapt them for these tasks\n[Schick et al. ,2024 ]. As this chapter focuses on prompting, we will not present t he details of this\nﬁne-tuning process. To put it simply, we ﬁrst need to annotat e data. For each ﬁne-tuning example,\nwe replace parts of the output that require the use of externa l tools with predeﬁned commands\nor markers. Then, we use this labeled data to ﬁne-tune the par ameters of the LLM as usual. As\na result, the LLM can gain the ability to generate commands fo r calling external tools. During\ninference, we can execute these tool use commands in the mode l outputs to get assistance from\nexternal tools.\n3.3 Learning to Prompt\nSo far in this chapter, we have considered several basic prom pting strategies and various reﬁne-\nments to them. However, all the prompts we have discussed wer e designed manually. This leads\nto a number of problems: First, designing high-quality prom pts is inherently difﬁcult and requires\nsubstantial manual effort. For example, extensive experim entation with different prompts is often\nneeded to identify the most effective ones. Since different LLMs may respond better to certain\n\n3.3 Learning to Prompt 139\ntypes of prompts, developing universally effective prompt s can be even more resource-intensive.\nSecond, manual prompt design relies heavily on human expert ise, which can limit the diversity\nof approaches and overlook potentially effective prompts t hat are not immediately obvious to hu-\nmans. Third, prompts created by humans can be complex and red undant, leading to longer inputs\nfor LLMs and higher computational costs.\nIn this section, we discuss techniques for automated prompt ing. These methods aim to auto-\nmatically create, optimize, and represent prompts so that t he downstream tasks can be addressed\nmore effectively and efﬁciently. In particular, we conside r three issues here.\n• How can we automate the process of designing and optimizing prompts for LLMs?\n• Are there other forms of representing prompts beyond strin gs, and how can we learn such\nrepresentations?\n• How can we make prompts more concise and compact, thereby re ducing their complexity\nand length?\nNote that there are many settings in which we can investigate these issues. For example, we\nmight specify that prompts are developed speciﬁcally for a p articular LLM, or that the develop-\nment is independent of the LLM used. These settings can lead t o different methods and application\nscenarios, but these methods may overlap in some ways. In the following discussion, we will cover\nseveral different scenarios and discuss the connections be tween various methods.\n3.3.1 Prompt Optimization\nGiven that prompt design is difﬁcult and labor-intensive, i t is desirable to use machine learning\nmodels to discover the optimal prompt for a speciﬁc task (cal l itautomatic prompt design or\nprompt optimization ). This approach can broadly be regarded as an instance of automated ma-\nchine learning (AutoML",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 115,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "488eaa3a2f52831d",
    "content": " we learn such\nrepresentations?\n• How can we make prompts more concise and compact, thereby re ducing their complexity\nand length?\nNote that there are many settings in which we can investigate these issues. For example, we\nmight specify that prompts are developed speciﬁcally for a p articular LLM, or that the develop-\nment is independent of the LLM used. These settings can lead t o different methods and application\nscenarios, but these methods may overlap in some ways. In the following discussion, we will cover\nseveral different scenarios and discuss the connections be tween various methods.\n3.3.1 Prompt Optimization\nGiven that prompt design is difﬁcult and labor-intensive, i t is desirable to use machine learning\nmodels to discover the optimal prompt for a speciﬁc task (cal l itautomatic prompt design or\nprompt optimization ). This approach can broadly be regarded as an instance of automated ma-\nchine learning (AutoML ), which aims to reduce or eliminate the need for expert-driv en manual\ndesign of machine learning models. Although our focus here i s on the design of prompts, prompts\nthemselves are discrete structures. Therefore, designing prompts is very similar to designing ma-\nchine learning models, such as discrete model architecture s. Perhaps one of the most related ﬁelds\nisneural architecture search (NAS ), where the most optimal neural networks are identiﬁed by\nexploring a space of possible neural networks [ Zoph and Le ,2016 ;Elsken et al. ,2019 ]. If we con-\nsider prompt optimization as a search process, then we can de scribe a general prompt optimization\nframework involving the following components:\n•Prompt Search Space . This deﬁnes all possible prompts that the algorithms can ex plore.\nFor example, one can edit some seed prompts to generate a set o f diverse candidate prompts.\n•Performance Estimation . Once an prompt is chosen, it needs to be evaluated. For examp le,\na straightforward way is to input it to an LLM and measure its p erformance on a validation\nset.\n•Search Strategy . The search process is generally the same as that used in many AI sys-\ntems. At each step, the system explores a set of promising pro mpts in the search space and\n\n140 Prompting\nevaluates them. This process continues as more prompts are e xplored. The outcome of the\nsearch is the best-performing prompt observed until the sea rch stops.\nThis is a very general framework, and different prompt optim ization systems can vary in their\ndesign of each component. A widely-used approach is to use LL Ms as the basis to develop these\ncomponents. Initially, a few prompts are provided. Then, th e following process is iterated until\na stopping criterion is met: 1) the prompts are evaluated on a validation set; 2) a candidate pool\nis maintained by keeping only the most promising prompts; an d 3) new prompts are created by\nemploying LLMs to infer similar prompts from this candidate pool. One beneﬁt of this approach is\nthat it allows us to use off-the-shelf LLMs to perform the tas ks mentioned above without the need\nfor substantial system development. To achieve this, we can prompt or ﬁne-tune LLMs to adapt\nthem to these tasks. Here we consider Zhou et al. [2023c ]’s method for illustrating LLM-based\nprompt optimization. It involves the following steps.\n•Initialization . LetCrepresent the pool of the candidate prompts we intend to expl ore. The\nﬁrst step is to add initial prompts into C. We can do this in several ways. A simple method is\nto create such prompts by hand for a given task. However, in ma ny cases where humans have\nlimited knowledge about how to write effective prompts for t he task, developing prompts\nbecomes challenging. In these cases, it is desirable to use L LMs to generate prompts.\nFor example, we can directly instruct LLMs to produce prompt s, providing them with a\ndescription of the task.\nYou are given a task to complete using LLMs. Please write a pro mpt to\nguide the LLMs.\n{∗task-descripion ∗}\nThis method is straightforward, but it still requires a huma n-provided description of the task.\nAn alternative method is to use LLMs to generate prompts give n examples of the input and\noutput of the task. Here is a prompt template.\nYou are provided with several input-output pairs for a task. Please write\nan instruction for performing this task.\nInput: {∗input1 ∗}Output: {∗output1 ∗}\nInput: {∗input2 �",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 116,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "430935d58e60f1e7",
    "content": "\nlimited knowledge about how to write effective prompts for t he task, developing prompts\nbecomes challenging. In these cases, it is desirable to use L LMs to generate prompts.\nFor example, we can directly instruct LLMs to produce prompt s, providing them with a\ndescription of the task.\nYou are given a task to complete using LLMs. Please write a pro mpt to\nguide the LLMs.\n{∗task-descripion ∗}\nThis method is straightforward, but it still requires a huma n-provided description of the task.\nAn alternative method is to use LLMs to generate prompts give n examples of the input and\noutput of the task. Here is a prompt template.\nYou are provided with several input-output pairs for a task. Please write\nan instruction for performing this task.\nInput: {∗input1 ∗}Output: {∗output1 ∗}\nInput: {∗input2 ∗}Output: {∗output2 ∗}\n...\nAs such, LLMs can infer the corresponding instruction for th e task from the provided inputs\nand outputs.\n•Evaluation . Once we obtain the candidate pool C, we need to evaluate the prompts in C.\nOne method is to feed each prompt into an LLM and assess the res ults on the downstream\n\n3.3 Learning to Prompt 141\ntask. For example, we can evaluate the output of the LLM given an input using a pre-deﬁned\nmetric, or alternatively, use the log-likelihood of the out put as a measure of the quality of\nthe prompt.\n•Pruning . IfCcontains a large number of prompts, it is reasonable to prune the unpromising\nprompts within it, thus reducing the computational burden i n subsequent steps. This is a\nstandard pruning problem. Given the evaluation score for ea ch prompt, a simple method is\nto keep only a certain percentage of the prompts and discard t he rest.\n•Expansion . Expansion is a key operation in search algorithms used to ex plore different\nstates in the search space. The expansion operation here can be deﬁned as a function\nC′= Expand( C,f) (3.10)\nwhereC′is the set of new prompts generated from Cusing the model f. If we consider f\nas an LLM, we can perform the expansion operation by instruct ingfto generate new and\nrelevant prompts based on C. Below is an example.\nBelow is a prompt for an LLM. Please provide some new prompts t o per-\nform the same task.\nInput: {∗prompt ∗}\nThen, we replace CwithC′. The steps of evaluation, pruning and expansion can be re-\npeated, and so we can gradually explore a wider range of promp ts.\nIn prompt optimization, the expansion step plays a key role, as it deﬁnes how we explore\nthe search space, and our goal is to ﬁnd optimal results with m inimal effort. One improvement\nto this step is to treat the problem as a paraphrasing task. A s imple method is to apply off-the-\nshelf paraphrasing systems, either based on LLMs or other mo dels, to transform input prompts\ninto semantically equivalent forms [ Jiang et al. ,2020 ]. Alternatively, we can deﬁne speciﬁc edit\noperations, such as insertions and modiﬁcations, for each t oken. A given prompt can be edited\ninto new prompts by applying these operations [ Prasad et al. ,2023 ]. Also, further evaluation and\npruning can be applied to ﬁlter out low-quality prompts. In a ddition to framing prompt generation\nas a paraphrasing problem, we can improve the quality of prom pts during expansion by learning\nfrom feedback [ Pryzant et al. ,2023 ]. This approach is somewhat related to the self-reﬁnement\nissue discussed in Section 3.2.3 . An LLM can be used to generate feedback on an input prompt,\nwhich is then revised based on this feedback. This feedback- and-revision cycle can be repeated\nmultiple times until the result converges or the desired out come is achieved.\nAnother approach to prompt optimization is to apply classic optimization techniques. For\nexample, the problem can be framed as an evolutionary comput ation problem, where prompts\nare treated as candidates that evolve generation by generat ion as the optimization progresses\n[Guo et al. ,2024 ]. Since many powerful optimization algorithms have been de veloped in related\nﬁelds, they can be directly applied to this problem.\n\n142 Prompting\nIn practice, we might be tempted to use existing LLM APIs to im plement the steps described\nabove. Such an approach,",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 117,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "f503bc5617a67695",
    "content": " [ Pryzant et al. ,2023 ]. This approach is somewhat related to the self-reﬁnement\nissue discussed in Section 3.2.3 . An LLM can be used to generate feedback on an input prompt,\nwhich is then revised based on this feedback. This feedback- and-revision cycle can be repeated\nmultiple times until the result converges or the desired out come is achieved.\nAnother approach to prompt optimization is to apply classic optimization techniques. For\nexample, the problem can be framed as an evolutionary comput ation problem, where prompts\nare treated as candidates that evolve generation by generat ion as the optimization progresses\n[Guo et al. ,2024 ]. Since many powerful optimization algorithms have been de veloped in related\nﬁelds, they can be directly applied to this problem.\n\n142 Prompting\nIn practice, we might be tempted to use existing LLM APIs to im plement the steps described\nabove. Such an approach, however, would be strongly depende nt on the inference and in-context\nlearning abilities of the LLMs. If these LLMs are not strong a nd lack adaptation to the tasks, they\nmay introduce errors into search, for example, generating i ncorrect prompts during expansion. In\nsuch cases, it is preferable to train models that are better s uited to the tasks. One approach in\nthis research direction appeals to reinforcement learning , which has been widely used in solving\ndiscrete decision making and optimization problems. For ex ample, Deng et al. [2022 ] developed\na prompt generator by integrating an FFN-based adaptor into an LLM. The prompt generator is\ntrained as a typical policy network, but only the parameters of the adaptor are updated while the\nremaining parameters of the model are kept unchanged. Durin g training, the reward is obtained by\ntesting the generated prompts using another LLM, similar to the evaluation method as discussed\nabove. Once the training is complete, the prompt generator i s then employed to generate new\nprompts.\nNote that, in our discussion here, prompts are simply seen as sequences of tokens, and the out-\nput of prompt optimization is such a sequence. However, in a s trict sense, prompts have complex\nstructures and include different ﬁelds such as user input, i nstruction, and demonstration. While\nour discussed approaches are mostly general, much work in pr ompt optimization has focused on\nlearning better instructions for prompting. Speciﬁcally, the goal is to generate instructions that\neffectively guide LLMs based on a given task. Of course, the c oncept of prompt optimization\ncan also be extended to learning other parts of prompts. For e xample, there has been substan-\ntial research interest in learning to select or generate dem onstrations in CoT [ Liu et al. ,2022 ;\nRubin et al. ,2022 ;Zhang et al. ,2023b ]. One of the differences between learning instructions\nand learning demonstrations is that generating high-quali ty demonstrations using LLMs is rela-\ntively easy and the focus of learning demonstrations is typi cally on how to sample appropriate\ndemonstrations from a pool of candidates. In contrast, the d ifﬁculty in learning instructions is\npartly because pre-trained LLMs are not suited to predict th e quality of instructions, and testing\nthese instructions on downstream tasks is computationally expensive. This makes the optimization\nmethods costly to apply, and exploring a wide variety of inst ructions poses signiﬁcant challenges.\n3.3.2 Soft Prompts\nAlthough developing natural language prompts, either manu ally or automatically, is a straight-\nforward and widely applied approach, it presents some probl ems. One problem is that natural\nlanguage prompts can be complex and lengthy, resulting in si gniﬁcant computational burdens\nwhen processed via LLMs. In many applications, users may nee d to perform a task repeatedly,\nand inputting the same long prompt into the LLMs a large numbe r of times is clearly inefﬁcient.\nAnother problem is that while prompts are typically represe nted as discrete token sequences (call\nthem hard prompts ) in regular LLM input, the LLMs encode them as low-dimension al real-\nvalued vectors. This raises the question of whether there ar e more compact and efﬁcient ways to\nrepresent prompts.\nIn this subsection, we introduce the concept of soft prompts , which can be viewed as hidden,\ndistributed representations of prompts. When prompting LL Ms, we are concerned with commu-\nnicating tasks or questions to elicit the desired responses . We can deﬁne hard prompts as explicit,\npredeﬁned text sequences that users input directly into LLM",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 118,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "3a27c5e8aba6c903",
    "content": " si gniﬁcant computational burdens\nwhen processed via LLMs. In many applications, users may nee d to perform a task repeatedly,\nand inputting the same long prompt into the LLMs a large numbe r of times is clearly inefﬁcient.\nAnother problem is that while prompts are typically represe nted as discrete token sequences (call\nthem hard prompts ) in regular LLM input, the LLMs encode them as low-dimension al real-\nvalued vectors. This raises the question of whether there ar e more compact and efﬁcient ways to\nrepresent prompts.\nIn this subsection, we introduce the concept of soft prompts , which can be viewed as hidden,\ndistributed representations of prompts. When prompting LL Ms, we are concerned with commu-\nnicating tasks or questions to elicit the desired responses . We can deﬁne hard prompts as explicit,\npredeﬁned text sequences that users input directly into LLM s to guide the responses. In contrast,\nwe can think of soft prompts as implicit, adaptable promptin g patterns embedded within LLMs.\nUnlike hard prompts, which are expressed in natural languag e and should be understandable for\n\n3.3 Learning to Prompt 143\n...Translate this into Chinese . I have a cat . ...Transformer... hjhj+1hj+2hj+3hj+4hj+5hj+6hj+7hj+8hj+9 ...... ... ... ... ... ... ... ... ... ... ... ...\nHard Prompt (Instruction)Soft Prompt\nFig. 3.3: Illustration of hard and soft prompts. Here the hard prompt i s the instruction we input to the LLM for\nperforming the task. The LLM encodes this instruction as usu al, and the intermediate representations corresponding to\nthe instruction can be viewed as some sort of soft prompt.\nhumans, soft prompts are encoded in a format that is more comp rehensible to the model rather\nthan to humans. To illustrate, consider a simple prompt\nTranslate the sentence into Chinese.\nConsider it done!\nHere, the instruction “Translate the sentence into Chinese ” can be seen as a hard prompt, denoted\nby the token sequence c1...c5. By feeding these tokens into an LLM, they are transformed in to a\nsequence of real-valued vectors h1...h5, each corresponding to a token. We can roughly think of\nh1...h5as a soft prompt, as illustrated in Figure 3.3.\nWhile the above example shows that soft prompts can be genera ted by transforming hard\nprompts, there is not necessarily a direct correspondence b etween them. In fact, we do not even\nneed to interpret soft prompts using meaningful text. They a re instead simply hidden states in\nLLMs and can be learned as standard parameters of the models t hrough continuous optimiza-\ntion. Such a treatment allows us to explore prompting method s beyond text. As another beneﬁt,\nsoft prompts provide dense, low-dimensional, and learnabl e representations for encoding how\nwe guide LLMs to generate speciﬁc outputs. The training and a pplication of these representa-\ntions require signiﬁcantly lower computational costs than those required for processing long hard\nprompts. This approach would be of great practical value in L LM inference applications where\nthe same prompt is repeatedly used.\n3.3.2.1 Adapting LLMs with Less Prompting\nOne obvious way to adapt an LLM for a particular task is to simp ly ﬁne-tune the model using\nlabeled data. This leads to a variety of LLM alignment method s, such as supervised ﬁne-tuning,\nwhich update the model parameters by aligning the responses to given prompts with supervision\nsignals. Fine-tuned LLMs embed task-related information i n model parameters, and thus these\n\n144 Prompting\nmodels can response correctly when dealing with similar pro mpts with those in ﬁne-tuning.\nIf we take this idea further, we can expect LLMs to absorb the k nowledge about prompting\nof a task as much as possible during ﬁne-tuning. Consequentl y, the prompting information is\npartially captured in the model parameters, and the ﬁne-tun ed LLMs can perform the task with\nless prompting. Here we consider a simple form of prompt, whe re only an instruction (denoted by\nc) and an user input (denoted by z) are included. A prompt can be expressed using the following\ntuple\nx= (c,z) (3.11)\nGiven a set of prompt-response pairs D={(x,y)}, the objective of ﬁne-tuning is to minimize\nthe total loss incurred over",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 119,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "44a3d111a18a9ad5",
    "content": "uned LLMs embed task-related information i n model parameters, and thus these\n\n144 Prompting\nmodels can response correctly when dealing with similar pro mpts with those in ﬁne-tuning.\nIf we take this idea further, we can expect LLMs to absorb the k nowledge about prompting\nof a task as much as possible during ﬁne-tuning. Consequentl y, the prompting information is\npartially captured in the model parameters, and the ﬁne-tun ed LLMs can perform the task with\nless prompting. Here we consider a simple form of prompt, whe re only an instruction (denoted by\nc) and an user input (denoted by z) are included. A prompt can be expressed using the following\ntuple\nx= (c,z) (3.11)\nGiven a set of prompt-response pairs D={(x,y)}, the objective of ﬁne-tuning is to minimize\nthe total loss incurred over this set. A popular method is to m inimize the negative log-likelihood\n(i.e., maximize the log-likelihood) with respect to the mod el parameters θ:\nˆθ= arg max\nθ∑\n(x,y)∈Dlog Pr θ(y|x)\n= arg max\nθ∑\n(x,y)∈Dlog Pr θ(y|c,z) (3.12)\nwhere Prθ(·|·)is the probability predicted by an LLM with the parameters θ9.\nIn general, the instruction in each ﬁne-tuning example shou ld follow the guideline of prompt\ndesign, for example, a good instruction should be as clear as possible and provide a detailed\ndescription of the task. However, the method described in th e above equation does not restrict the\ninstruction to any particular form. This ﬂexibility allows us to instruct LLMs in any way that we\nwant. Consider an example where we intend to instruct LLMs to translate an English sentence\ninto Chinese. Of course, as mentioned earlier in this chapte r, we can prompt LLMs using the\ninstruction\nTranslate the following sentence from English to Chinese.\nIf we want the instruction to be simpler, we may rephrase it in to a simpler form\nTranslate this into Chinese.\nEven, we can deﬁne the instruction as a single phrase\nTranslate!\nWith certain ﬁne-tuning effort, we can adapt LLMs to follow a ny of these instructions. From\na efﬁcient prompting perspective, there are computational advantages in simplifying instructions\nin prompting. For example, we can use simple instructions li ke “Translate!” to perform tasks\nthat would typically require more complex and detailed inst ructions. This can make subsequent\n9In practice, we initialize θwith the parameters obtained from pre-training, and then ad justθmoderately to ensure\nthat the results after ﬁne-tuning do not deviate too much fro m the pre-trained results.\n\n3.3 Learning to Prompt 145\nFull Context User Input + Prt(y|c,z)\nc z yTeacher Model:\nSimpliﬁed Context User Input + Prs(y|c′,z)\nc′ z yStudent Model:Loss\nFig. 3.4: Illustration of context distillation [ Snell et al. ,2022 ]. The teacher model is a standard LLM, which takes both\nthe context and the user input as model input and produces a pr ediction as model output. Then, we simplify the context\n(e.g., simplifying the instruction in prompting) and use th e student model to make predictions based on the simpliﬁed\ncontext and the user input. The student model is trained by mi nimizing the loss between the predictions produced by\nthe two models.\nprompting during inference much easier. On the other hand, ﬁ ne-tuning LLMs with overly simpli-\nﬁed instructions may be harmful to the generalization of the models. Since simpliﬁed instructions\ncan lead to a loss of information, it is more likely that the LL Ms will overﬁt the ﬁne-tuning data\nand fail to generalize beyond those instructions. In scenar ios involving both complex and simpli-\nﬁed instructions for ﬁne-tuning, this problem is more sever e because the labeled data available for\nﬁne-tuning is usually limited, and accommodating a variety of instructions is costly.\nAn alternative way to adapt LLMs for simpliﬁed instructions is through knowledge distillation.\nAs an example, we consider the context distillation method [ Snell et al. ,2022 ]. The goal of this\nmethod is to learn a student model that can make use of simpliﬁ ed instructions from a well-trained",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 120,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "641109ead310a518",
    "content": " On the other hand, ﬁ ne-tuning LLMs with overly simpli-\nﬁed instructions may be harmful to the generalization of the models. Since simpliﬁed instructions\ncan lead to a loss of information, it is more likely that the LL Ms will overﬁt the ﬁne-tuning data\nand fail to generalize beyond those instructions. In scenar ios involving both complex and simpli-\nﬁed instructions for ﬁne-tuning, this problem is more sever e because the labeled data available for\nﬁne-tuning is usually limited, and accommodating a variety of instructions is costly.\nAn alternative way to adapt LLMs for simpliﬁed instructions is through knowledge distillation.\nAs an example, we consider the context distillation method [ Snell et al. ,2022 ]. The goal of this\nmethod is to learn a student model that can make use of simpliﬁ ed instructions from a well-trained\ninstruction-following teacher model. Figure 3.4shows an illustration of this approach. Building\nthe teacher model follows a standard ﬁne-tuning process: we ﬁrst collect a certain amount of\ndata that includes instructions, user inputs, and correct r esponses, and then we continue to train a\npre-trained model with this dataset. For building the stude nt model, we need to construct a new\ndataset D′where each sample is a tuple consisting of an instruction, a c orresponding simpliﬁed\ninstruction, and a user input, denoted by x′= (c,c′,z). Knowledge distillation is performed by\nminimizing a loss function deﬁned on the outputs of the teach er and student models\nˆθ= arg min\nθ∑\nx′∈D′Loss(Prt(·|·),Prs\nθ(·|·),x′) (3.13)\nwhere Prt(·|·)denotes the pre-trained teacher model, and Prs\nθ(·|·)denotes the student model with\nthe parameters θ. To keep the notation simple we will write Loss(Prt(·|·),Prs\nθ(·|·),x)asLoss\nfor short. A commonly-used loss is the sequence-level loss, which has the basic form:\nLoss =∑\nyPrt(y|c,z) log Prs\nθ(y|c′,z) (3.14)\nBut this function is computationally infeasible because it requires summing over an exponen-\ntially large number of outputs. A variant of this method is to train the student model using outputs\ngenerated by the teacher model. For each sample, we use the te acher model to produce an output\n\n146 Prompting\nˆy= arg maxylog Prt(y|c,z). Then we consider ˆyas the target for learning, and the loss function\nis given by\nLoss = log Prs\nθ(ˆy|c′,z) (3.15)\nAlternatively, we can minimize the distances between the pr obability distributions outputted\nby the two models [ Askell et al. ,2021 ]. For example, the loss function can be deﬁned as the KL\ndivergence between the two output distributions\nLoss = KL(Pt||Ps\nθ) (3.16)\nwhere\nPt= Prt(·|c,z) (3.17)\nPs\nθ= Prs\nθ(·|c′,z) (3.18)\nAlthough we have restricted ourselves to knowledge distill ation for instructions, the approaches\ndiscussed here are general. By learning from the outputs of t he teacher model, the knowledge in\nprompting can be distilled into the parameters of the studen t model. Therefore, the distilled model\ncan be considered as encoding some sort of soft prompt. This m ethod can be applied to many\nother problems in prompt learning, such as compressing long contexts and learning soft prompts\nas speciﬁc components of LLMs.\n3.3.2.2 Learning Soft Prompts for Parameter-efﬁcient Fine -tuning\nUpdating all parameters is a common method for adapting LLMs to tasks of interest. Although\nﬁne-tuning is considered computationally cheaper than pre -training, it is still costly to apply in\npractice. This issue motivates the development of paramete r-efﬁcient ﬁne-tuning methods, which\naim to minimize the number of parameters that need to be updat ed.\nOne approach, known as preﬁx ﬁne-tuning , is to append a series of trainable vectors, or\npreﬁxes, at",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 121,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "ad4555b7e1435338",
    "content": " the studen t model. Therefore, the distilled model\ncan be considered as encoding some sort of soft prompt. This m ethod can be applied to many\nother problems in prompt learning, such as compressing long contexts and learning soft prompts\nas speciﬁc components of LLMs.\n3.3.2.2 Learning Soft Prompts for Parameter-efﬁcient Fine -tuning\nUpdating all parameters is a common method for adapting LLMs to tasks of interest. Although\nﬁne-tuning is considered computationally cheaper than pre -training, it is still costly to apply in\npractice. This issue motivates the development of paramete r-efﬁcient ﬁne-tuning methods, which\naim to minimize the number of parameters that need to be updat ed.\nOne approach, known as preﬁx ﬁne-tuning , is to append a series of trainable vectors, or\npreﬁxes, at the beginning of the input of each Transformer la yer [ Li and Liang ,2021 ]. These\npreﬁxes can be thought of as soft prompts that serve as additi onal context to guide the behavior\nof the model under speciﬁc tasks. During ﬁne-tuning, we need only to learn the preﬁxes for\nembedding task-speciﬁc knowledge. Thus, this method is efﬁ cient because it only modiﬁes a\nsmall part of the model rather than adjusting the entire set o f model parameters.\nSpeciﬁcally, let the input of a layer at depth lbe denoted by Hl=hl\n0hl\n1...hl\nm. The output of\nthe layer can be expressed as\nHl+1= Layer( Hl) (3.19)\nIn preﬁx ﬁne-tuning, we extend the sequence hl\n0hl\n1...hl\nmby adding a few vectors at beginning,\nwhich we denote as pl\n0pl\n1...pl\nn. Hence Hlcan be written in the form\nHl=pl\n0pl\n1...pl\nn\ntrainablehl\n0hl\n1...hl\nm\nprevious layer output(3.20)\n\n3.3 Learning to Prompt 147\nThe output of the layer is the last m+ 1representations.\nHl+1= Layer( Hl)[−m−1 :]\n=hl+1\n0hl+1\n1...hl+1\nm (3.21)\nwhere [−m−1 :]denotes the slicing operation that extracts the last m+1elements of a sequence.\nGiven Hl+1, the input of the next layer can be expressed in the same form o f Eq. ( 3.20):\nHl+1=pl+1\n0pl+1\n1...pl+1\nnHl+1\n=pl+1\n0pl+1\n1...pl+1\nnhl+1\n0hl+1\n1...hl+1\nm (3.22)\nHere each pi∈Rdcan be seen as a learnable parameter. During training, pl\n0pl\n1...pl\nnare trained\nas usual, and the parameters of the original Transformer mod el are kept ﬁxed.\nFigure 3.5shows an illustration of preﬁx ﬁne-tuning for a translation task. Here, only the preﬁx\nvectors pl\n0andpl\n1are updated by receiving the error gradients from the output (i.e., the Chinese\ntranslation). By adjusting these vectors for the translati on task, the model adapts accordingly. This\nmakes pl\n0andpl\n1serve as prompts which activate the LLM to perform the task wi thout needing\nexplicit input prompts like “Translate the following sente nce from English to Chinese”. At test\ntime, we prepend the optimized pl\n0andpl\n1to the layer, and the LLM will then translate the input\nsentence. Note that preﬁx ﬁne-tuning introduces additiona lL×n×dparameters, where Lis the\nnumber of layers, nis the number of preﬁxes, and dis the dimensionality of each preﬁx. However,\nthis number is much smaller compared to the total number of pa rameters in the LLM, making the\nﬁne-tuning process highly efﬁcient.\nWhile preﬁx ﬁne-tuning is simple, it still requires modiﬁca tions to LLMs. Alternatively, sep-\narating",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 122,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "d50e9cf298281336",
    "content": " accordingly. This\nmakes pl\n0andpl\n1serve as prompts which activate the LLM to perform the task wi thout needing\nexplicit input prompts like “Translate the following sente nce from English to Chinese”. At test\ntime, we prepend the optimized pl\n0andpl\n1to the layer, and the LLM will then translate the input\nsentence. Note that preﬁx ﬁne-tuning introduces additiona lL×n×dparameters, where Lis the\nnumber of layers, nis the number of preﬁxes, and dis the dimensionality of each preﬁx. However,\nthis number is much smaller compared to the total number of pa rameters in the LLM, making the\nﬁne-tuning process highly efﬁcient.\nWhile preﬁx ﬁne-tuning is simple, it still requires modiﬁca tions to LLMs. Alternatively, sep-\narating soft prompts from the LLMs allows us to preserve the o riginal model architecture, making\nit more efﬁcient for deployment across different tasks with out the need to adjust the core model.\nOne such method is prompt tuning [ Lester et al. ,2021 ]. Like preﬁx ﬁne-tuning, prompt tuning\nincorporates trainable vectors so that LLMs can adapt to giv en tasks by adjusting these vectors.\nHowever, prompt tuning differs in that it modiﬁes only the em bedding layer.\nRecall that in LLMs each input token ziis represented by an embedding ei. These embeddings\nare generally learned through a token embedding model and ar e then used as the real inputs to the\nLLMs, replacing the symbolically represented tokens. In pr ompt tuning, a number of pseudo\nembeddings p0...pnare added at the beginning of the token embedding sequence. S o the actual\ninput to the LLMs can be expressed as\np0p1...pn\ntrainablee0e1...em\ntoken embeddings\nNote that a pseudo embedding needs not to correspond to any to ken in natural language. Instead\nthese embeddings can be seen as “soft prompt embeddings” tha t serve to condition the LLMs.\nBy training soft prompt embeddings on task-speciﬁc data, th ey learn to interact adaptively with\nthe token embeddings e0...emand guide the behavior of LLMs. Since prompt tuning does not\nchange the underlying parameters of pre-trained LLMs, it is considered a lightweight and efﬁcient\nmethod of ﬁne-tuning, improving task-speciﬁc performance while maintaining their generalization\ncapabilities. See Figure 3.6for an illustration of prompt tuning.\n\n148 Prompting\npl\n0 pl\n1 hl\n0 hl\n1 hl\n2 hl\n3 hl\n4Layer l\npl−1\n0 pl−1\n1 hl−1\n0 hl−1\n1 hl−1\n3 hl−1\n4 hl−1\n5Layer l−1pl+1\n0 pl+1\n1 hl+1\n0 hl+1\n1 hl+1\n3 hl+1\n4 hl+1\n5Layer l+ 1· · · · · · · · · · · · · · ·Loss Loss\n· · · · · · · · · · · · · · ·\nLook out !小心 !trainable preﬁxes\nUser Input LLM Prediction Soft Prompt\nFig. 3.5: Illustration of preﬁx ﬁne-tuning for a translation task (Lo ok out! →小心!). For each layer, we add two\npreﬁxes pl\n0andpl\n1at the beginning. The LLM is trained to minimize the loss on th e predictions given the input.\nDuring this process, only the preﬁxes are optimized while th e rest of the parameters remain ﬁxed. Therefore, the model\ncan adapt to the given task in a very efﬁcient manner. At infer ence time, the LLM works with optimized preﬁxes, and\ncan perform the task without the need of explicit hard prompt s.\nSince p0p1...pnis itself a sequence, we can employ sequence models to better represent\nit. For example, a Transformer model can encode this sequenc e, and the resulting representation\ncan then be used as the input to the LLM. In other words, we can d evelop an additional model\nfor encoding soft prompts. Another way to improve prompting is by combining soft and hard\nprompts, thereby taking advantage of both types [ Liu et al. ,",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 123,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "4acbbfe2c9e747d5",
    "content": "0andpl\n1at the beginning. The LLM is trained to minimize the loss on th e predictions given the input.\nDuring this process, only the preﬁxes are optimized while th e rest of the parameters remain ﬁxed. Therefore, the model\ncan adapt to the given task in a very efﬁcient manner. At infer ence time, the LLM works with optimized preﬁxes, and\ncan perform the task without the need of explicit hard prompt s.\nSince p0p1...pnis itself a sequence, we can employ sequence models to better represent\nit. For example, a Transformer model can encode this sequenc e, and the resulting representation\ncan then be used as the input to the LLM. In other words, we can d evelop an additional model\nfor encoding soft prompts. Another way to improve prompting is by combining soft and hard\nprompts, thereby taking advantage of both types [ Liu et al. ,2023b ]. In the embedding sequence,\nwe can arrange or intersperse these prompts. This would resu lt in different prompt patterns. For\nexample, a simple pattern that uses both two types of prompt i s\np0p1· · · pnq0q1· · · qm′e0 e1· · · em\nc0c1· · ·cm′z0z1· · ·zmSoft Prompt Hard Prompt User Input and Response\nwherec0...cm′denotes the hard prompt and q0...qm′denotes the corresponding embedding se-\nquence.\nHere we have considered methods for inserting soft prompts i n LLMs. But we skip the details\nof training these soft prompts and assume that the reader is f amiliar with the standard supervised\nlearning process, that is, maximizing the likelihood of the correct model output given the model\n\n3.3 Learning to Prompt 149\nLayerl−1LayerlLayerl+ 1\n· · · · · · · · · · · · · · · · · · · · ·\np0 p1 e0 e1 e2 e3 e4· · · · · · · · · · · · · · · · · · · · ·Loss Loss\nLook out !小心 !trainable prompt\nembeddings\nUser Input LLM Prediction Soft Prompt\nFig. 3.6: Illustration of prompt tuning for a translation task (Look o ut!→小心!). Instead of using ﬁxed textual\nprompts, soft prompts are learnable embeddings that are add ed at the beginning of the embedding sequence. During\nﬁne-tuning, only these prompt embeddings are optimized to e fﬁciently adapt the LLM to the given task. Once opti-\nmized, the prompt embeddings are used to instruct the LLM to p erform the task as new data arrives.\ninput. In fact, learning soft prompts can be related to many i ssues in LLM ﬁne-tuning. For exam-\nple, if we consider it as a context compression problem, we ca n apply the knowledge distillation\nmethods described previously. In Mu et al. [2024 ]’s work, prompts are compressed and repre-\nsented as a few pseudo tokens, which are appended to each inpu t sequence. The embeddings of\nthese pseudo tokens are optimized to mimic the predictions o f a standard-prompted model. In\nother words, the prompting knowledge is distilled from a tea cher model into the pseudo tokens.\nBroadly speaking, many parameter-efﬁcient ﬁne-tuning met hods can be thought of as learning\nsome sort of soft prompt [ Lialin et al. ,2023 ]. When we ﬁne-tune a part of an LLM for a task, this\nprocess can essentially be seen as injecting task-related p rompting information into that speciﬁc\npart of the model. Another widely-used approach to paramete r-efﬁcient ﬁne-tuning is to add an\nadaptor layer between the existing model layers. This appro ach allows us to ﬁne-tune only the\nadaptor layer on speciﬁc tasks without altering the underly ing architecture or retraining the entire\nmodel. In this sense, adaptor layers can be viewed as soft pro mpts that encode prompting and task-\nrelated information and interact with the original LLM to he lp it adapt. To summarize, Figure 3.7\nshows a comparison of different methods of using soft prompt s in LLMs.\n3.3.2.3 Learning Soft Prompts with Compression\nAnother approach to learning soft prompts is from the perspe ctive of compression. As a simple\nexample, consider the problem of approximating a long conte xt using a continuous representation\n[W",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 124,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "81730a637244b751",
    "content": " rompting information into that speciﬁc\npart of the model. Another widely-used approach to paramete r-efﬁcient ﬁne-tuning is to add an\nadaptor layer between the existing model layers. This appro ach allows us to ﬁne-tune only the\nadaptor layer on speciﬁc tasks without altering the underly ing architecture or retraining the entire\nmodel. In this sense, adaptor layers can be viewed as soft pro mpts that encode prompting and task-\nrelated information and interact with the original LLM to he lp it adapt. To summarize, Figure 3.7\nshows a comparison of different methods of using soft prompt s in LLMs.\n3.3.2.3 Learning Soft Prompts with Compression\nAnother approach to learning soft prompts is from the perspe ctive of compression. As a simple\nexample, consider the problem of approximating a long conte xt using a continuous representation\n[Wingate et al. ,2022 ]. Suppose we have a user input zand its context c(such as long instructions\nand demonstrations). Now we want to develop a compressed rep resentation of the context, denoted\n\n150 Prompting\nLLM\n(a) Soft Prompts as PreﬁxesLLM\n(b) Soft Prompts as Inputs (Embeddings)\nLLMLayer\n(c) Fine-tuning Parts of the ModelLLM\nAdaptor\n(d) Fine-tuning the Adaptor\nFig. 3.7: Illustrations of using soft prompts in LLMs. Here tunable so ft prompts are shown in blue, and components\nwhose parameters are ﬁxed during ﬁne-tuning are shown in gra y. In sub-ﬁgure (a), soft prompts are preﬁxes appended\nto each layer of the LLM. In sub-ﬁgure (b), soft prompts are us ed as input embeddings for the LLM. In sub-ﬁgures (c)\nand (d), soft prompts are broadly treated as components of th e model that are ﬁne-tuned for task adaptation.\nbyσ, such that the prediction based on zandσis as close as possible to the prediction based on z\nandc. This goal can be expressed in the form\nˆσ= arg min\nσs(ˆy,ˆyσ) (3.23)\nwhere ˆy= arg maxyPr(y|c,z)andˆyσ= arg maxyσPr(y|σ,z)are the LLM predictions given\nthe full context and the compressed context, respectively. The function s(·,·)typically represents\na loss or similarity measure, aiming to minimize the differe nce in predictions between the two\ncontext representations.\nOne general framework for achieving this is knowledge disti llation, where ˆyandˆyσcan be\nseen as the predictions of the teacher model and the student m odel, respectively. This formal-\nization links our discussion to the context distillation pr oblem discussed earlier. The training\nobjective can be obtained by analogy with Eqs. ( 3.15) and ( 3.16). For example, a simple training\nobjective is given by\nˆσ= arg max\nσlog Pr(ˆ y|σ,z) (3.24)\nAlternatively, we can minimize the KL divergence between th e output distributions, giving\nˆσ= arg min\nσKL(Pr( ·|c,z)||Pr(·|σ,z)) (3.25)\nThe difference with the models in Eqs. ( 3.15) and ( 3.16) is that here the compressed context is\nrepresented as real-valued vectors (call them prompt embeddings ), rather than as normal tokens.\nBy applying the above methods, we distill the context from th e token sequence cinto the embed-\ndingsσ. Note that the teacher model Pr(·|c,z)and the student model Pr(·|σ,z)may not share\nthe same architecture or model settings. In practice, we gen erally wish for the teacher model to be\n\n3.3 Learning to Prompt 151\nσ<i\n1σ<i\n2 ei\n1 ei\n2 ei\n3 ei\n4 ⟨e1⟩ ⟨e2⟩\nzi\n1zi\n2zi\n3zi\n4 ⟨g1⟩ ⟨ g2⟩Transformer Layersh<i\n1 h<i\n1 h1 h2 h3 h4 σ<i+1\n1σ<i+1\n2\nSoft Prompts\nat Stepi−1Soft Prompts\nat the Current Step\nFig. 3.8: Illustration of compressing a context segment into soft pro mpts (κ",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 125,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "b3affbeec890b824",
    "content": " the above methods, we distill the context from th e token sequence cinto the embed-\ndingsσ. Note that the teacher model Pr(·|c,z)and the student model Pr(·|σ,z)may not share\nthe same architecture or model settings. In practice, we gen erally wish for the teacher model to be\n\n3.3 Learning to Prompt 151\nσ<i\n1σ<i\n2 ei\n1 ei\n2 ei\n3 ei\n4 ⟨e1⟩ ⟨e2⟩\nzi\n1zi\n2zi\n3zi\n4 ⟨g1⟩ ⟨ g2⟩Transformer Layersh<i\n1 h<i\n1 h1 h2 h3 h4 σ<i+1\n1σ<i+1\n2\nSoft Prompts\nat Stepi−1Soft Prompts\nat the Current Step\nFig. 3.8: Illustration of compressing a context segment into soft pro mpts (κ= 2 andmi= 4). The input to the\nLLM includes the soft prompts from the previous step ( σ<i\n1andσ<i\n2), the tokens of the segment ( z1,z2,z3,andz4),\nand the summary tokens ( ⟨g1⟩and⟨g2⟩). Given these, the LLM operates as usual. We then extract the outputs at the\nlast Transformer layer that correspond to the summary token s. These outputs can be viewed as the soft prompts that\naccumulated up to this segment.\nstronger, while the student model should be smaller and more efﬁcient.\nWhile compressing full context into continuous representa tions is a straightforward approach\nto learning soft prompts, it requires a teacher model that ca n deal with long input sequences. In\nmany cases, however, the context is so long that applying an L LM is too costly or infeasible.\nModeling long input sequences can fall under the broad famil y of efﬁcient methods for long-\ncontext LLMs. Many techniques have been developed to addres s this issue. For example, one can\nuse a ﬁxed-size KV cache to store the past information at each step during inference. Efﬁcient\nTransformer architectures and long-context LLMs have been intensively discussed in this book.\nFor more detailed discussions of these topics, interested r eaders can refer to Chapter 2.\nThere are also methods speciﬁcally designed to compress lon g context into soft prompts. Here\nwe consider Chevalier et al. [2023 ]’s method as an example. The basic idea is that we learn\nsoft prompts gradually by accumulating the ﬁxed-size conte xt representation over the context\nsequence. Given a long context, we ﬁrst divide it into a numbe r of segments z1,...,zK. We\nthen process these segments in sequence, each time generati ng a representation of the context we\nhave processed so far, denoted by σ<i+1. To do this, a few summary tokens ⟨g1⟩,...,⟨gκ⟩are\nintroduced. At each step, we take a segment zi=zi\n1...zi\nmi, along with the previous context rep-\nresentationσ<iand the summary tokens ⟨g1⟩,...,⟨gκ⟩as input, and use an LLM to produce the\ncorresponding hidden representation sequence at the last T ransformer layer. An example of this\nprocess is illustrated in Figure 3.8.\nHereσ<iis essentially a memory. The model operates in an RNN fashion . Each time we take\na segment and update this memory by encoding both the previou s memory state and the segment.\nTherefore, the σ<iproduced at the last segment is a representation of the entir e context sequence.\nThe Transformer model for learning these representations c an be a standard LLM but we need to\nﬁne-tune it to adapt to this context representation task.\nNote that here we simply consider prompt andcontext as similar terms, even though they are\nnot the same. Although we are somewhat “misusing” the concep tprompt , we can often view it as\na type of context. From this perspective, the methods discus sed here can be applied to general text\ncompression problems.\n\n152 Prompting\n3.3.3 Prompt Length Reduction\nWhile soft prompts provide dense, hidden representations, they are not directly interpretable. The\nlack of interpretability can be a signiﬁcant barrier for use rs trying to understand how their inputs\ninﬂuence LLM outputs. Moreover, although soft prompts are e fﬁcient for ﬁne-tuning and de-\nployment, they are inﬂexible and do not allow for easy adjust ments without extensive",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 126,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "42d9e784e30b63e1",
    "content": " c an be a standard LLM but we need to\nﬁne-tune it to adapt to this context representation task.\nNote that here we simply consider prompt andcontext as similar terms, even though they are\nnot the same. Although we are somewhat “misusing” the concep tprompt , we can often view it as\na type of context. From this perspective, the methods discus sed here can be applied to general text\ncompression problems.\n\n152 Prompting\n3.3.3 Prompt Length Reduction\nWhile soft prompts provide dense, hidden representations, they are not directly interpretable. The\nlack of interpretability can be a signiﬁcant barrier for use rs trying to understand how their inputs\ninﬂuence LLM outputs. Moreover, although soft prompts are e fﬁcient for ﬁne-tuning and de-\nployment, they are inﬂexible and do not allow for easy adjust ments without extensive ﬁne-tuning\nor modiﬁcation. This inﬂexibility can limit their utility i n dynamic environments where prompt\nchanges are frequently needed.\nOne alternative way to develop efﬁcient prompts is to simpli fy the text used for prompting.\nFor example, below is a prompt for answering questions on hea lthcare and ﬁnance.\nThe task involves developing a language model capable of und erstanding and\nresponding to user inquiries across various domains, with a particular emphasis\non healthcare and ﬁnance. Considering the broad range of pot ential queries,\nfrom the speciﬁcs of medical diagnoses to the nuances of ﬁnan cial regulations,\nthe model must ensure a comprehensive understanding and acc urate responses.\nQuestion:\nWhat are the best practices for using artiﬁcial intelligenc e in diagnosing cardio-\nvascular diseases?\nWe can simplify the task description by deleting the unimpor tant parts.\nThe task involves developing a language model capable of und erstanding\nand responding to user inquiries across various domains, with aparticular\nemphasis on healthcare and ﬁnance. Considering thebroad range ofpotential\nqueries, from thespeciﬁcs ofmedical diagnoses tothenuances ofﬁnancial\nregulations, The model must ensure a comprehensive understanding and ac-\ncurate responses.\nWe can also paraphrase it as a shorter text.\nThe task involves developing a language model focused on hea lthcare and\nﬁnance, capable of understanding and accurately respondin g to a wide range\nof user inquiries.\nThis problem can be viewed as a classic NLP issue — text simpli ﬁcation. So the methods\nused can be general and not restricted to the problem of simpl ifying prompts. There are many\nways to achieve this. One simple method is to deﬁne some heuri stics and identify redundant\nwords that can be eliminated without losing essential infor mation. For example, we can examine\neach token in a sequence in terms of its contribution to the ov erall meaning and remove those that\nprovide minimal value [ Li et al. ,2023c ;Jiang et al. ,2023b ]. Another method involves framing\nthe problem as a sequence-to-sequence task. With labeled da ta for text simpliﬁcation, we can\ntrain an encoder-decoder model to transform each input text into its simpliﬁed form. In addition,\n\n3.4 Summary 153\ngiven that many LLMs have been ﬁne-tuned and aligned to perfo rm text simpliﬁcation tasks, it is\nstraightforward to use these models to simplify prompts. Fo r example, we can prompt an LLM to\nsimplify a text under certain constraints, such as limiting the length of the simpliﬁed text.\n3.4 Summary\nIn this chapter, we have discussed a variety of issues relate d to LLM prompting. Our discussion\nhas focused mainly on two aspects:\n• How to design basic prompts to guide the predictions of LLMs and reﬁne these prompts for\nmore effective and efﬁcient problem-solving?\n• How to automate the design and representation of prompts?\nSolutions to these issues involve both general prompt desig ns and more advanced techniques, such\nas CoT and prompt learning, which have been explored extensi vely in recent research.\nIn NLP, prompting can be viewed as a technology that has evolv ed along with LLMs, and\nin a sense, it has opened the door to the practical applicatio n of these models in an impressive\nrange of problem domains. In fact, if we expand the concept of prompts to some extent, it can be\ntraced back to the early days of machine learning and NLP. For example, many NLP systems use",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 127,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "767b9fa66e71bcd3",
    "content": " this chapter, we have discussed a variety of issues relate d to LLM prompting. Our discussion\nhas focused mainly on two aspects:\n• How to design basic prompts to guide the predictions of LLMs and reﬁne these prompts for\nmore effective and efﬁcient problem-solving?\n• How to automate the design and representation of prompts?\nSolutions to these issues involve both general prompt desig ns and more advanced techniques, such\nas CoT and prompt learning, which have been explored extensi vely in recent research.\nIn NLP, prompting can be viewed as a technology that has evolv ed along with LLMs, and\nin a sense, it has opened the door to the practical applicatio n of these models in an impressive\nrange of problem domains. In fact, if we expand the concept of prompts to some extent, it can be\ntraced back to the early days of machine learning and NLP. For example, many NLP systems use\nhand-crafted features and templates to “prompt” speciﬁc ta sks. Imagine developing a feature to\nindicate whether a text is formal or informal. We can feed thi s feature into a machine translation\nsystem to condition the translation on the type of the input t ext.\nThe widespread use of the modern concept of prompts began wit h the rise of large pre-trained\nmodels in the ﬁeld of NLP. Initially, these models, such as BE RT, were adapted to speciﬁc down-\nstream tasks mainly through ﬁne-tuning. However, research ers soon discovered that by designing\nspeciﬁc \"prompts\" — adding certain words or sentences to the input — the models could be\ntriggered to respond to speciﬁc tasks without extensive ﬁne -tuning. This motivated the NLP com-\nmunity to develop and apply universal foundation models tha t can be prompted to address various\ntasks without changing the underlying architecture and the pre-training procedure.\nPrompting approaches were ﬁrst experimented with smaller m odels and later demonstrated\nimpressive capabilities with large models like GPT-3, whic h could generate high-quality text in\nresponse to simple prompts across various tasks. As prompti ng technology evolved, prompt en-\ngineering emerged as a critical area of research. As discuss ed in this chapter, it broadly involves\ndesigning effective prompts to maximize model performance , encompassing both hand-crafted\nand automatically generated prompts. More recent research has explored how to enhance the\neffectiveness of prompting through techniques like few-sh ot learning, zero-shot learning, and\nCoT reasoning, enabling LLMs to work effectively across a wi de range of scenarios. A gen-\neral discussion of prompting can be very broad, and we cannot cover all details in this chapter.\nFor more advanced techniques of prompting, the reader can re fer to recent surveys. Topics in-\nclude in-context learning [ Li,2023 ;Dong et al. ,2022 ], CoT [ Chu et al. ,2023 ;Yu et al. ,2023 ;\nZhang et al. ,2023a ], efﬁcient prompting [ Chang et al. ,2024 ], and general prompt engineering\n[Liu et al. ,2023c ;Chen et al. ,2023a ].\n\n154 Prompting\nNote that although we would ideally like to develop general p rompting methods without ad-\njusting model architectures and parameters, the results of prompting generally depend heavily\non the quality and size of the given LLMs. For stronger models , such as commercialized online\nLLMs, simple prompts may be sufﬁcient to instruct these mode ls to perform tasks correctly. In\nthis case, prompt engineering is relatively easy, though we still need certain efforts to make LLMs\nwork properly. By contrast, if the LLMs are not powerful enou gh, we may need to carefully design\nthe prompts to achieve the desired results. In many cases, ﬁn e-tuning is still necessary to adapt\nthe models to sophisticated prompting strategies.\n\nCHAPTER 4\nAlignment\nAlignment is not a new concept in NLP, but its meaning varies a cross different domains and over\ntime. In traditional NLP, the term alignment typically refers to the tasks that link corresponding\nelements in two sets, such as aligning words between a Chines e sentence and an English sentence.\nAs LLMs become increasingly important in NLP research, this term is more broadly used to refer\nto aligning model outputs with human expectations. The prob lem that alignment addresses is\nthat the output of a model may not align with the speciﬁc goals or contexts intended by users.\nFor example, pre-trained LLMs may not be able to follow user i nstructions because they were\nnot trained to do",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 128,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "85f05cf6b92cd81e",
    "content": "s are not powerful enou gh, we may need to carefully design\nthe prompts to achieve the desired results. In many cases, ﬁn e-tuning is still necessary to adapt\nthe models to sophisticated prompting strategies.\n\nCHAPTER 4\nAlignment\nAlignment is not a new concept in NLP, but its meaning varies a cross different domains and over\ntime. In traditional NLP, the term alignment typically refers to the tasks that link corresponding\nelements in two sets, such as aligning words between a Chines e sentence and an English sentence.\nAs LLMs become increasingly important in NLP research, this term is more broadly used to refer\nto aligning model outputs with human expectations. The prob lem that alignment addresses is\nthat the output of a model may not align with the speciﬁc goals or contexts intended by users.\nFor example, pre-trained LLMs may not be able to follow user i nstructions because they were\nnot trained to do so. Another example is that LLMs may generat e harmful content or perpetuate\nbiases inherent in their training data. This poses new chall enges in ensuring that LLM outputs are\nnot only accurate and relevant, but also ethically sound and non-discriminatory.\nSimply pre-training LLMs can result in a variety of alignmen t problems. Our ultimate goal\nis to resolve or mitigate all these problems to ensure LLMs ar e both accurate and safe. There\nis an interesting issue here: since large language models ar e trained on vast amounts of data,\nwe have reason to believe that if we have sufﬁcient data cover ing a variety of tasks and aligned\nwith human preferences, pre-training could make LLMs accur ate and safe enough, perhaps even\neliminating the need for alignment. However, the reality is that it is nearly impossible to gather\ndata that encompasses all tasks or adequately represents hu man preferences. This makes it difﬁcult\nto achieve model alignment through pre-training alone, or a t least, at this stage, alignment remains\na very necessary and critical step in the development of LLMs .\nIn this chapter, we will focus on alignment methods for LLMs. We will begin by discussing the\ngeneral alignment tasks. Then we will consider two widely-u sed approaches, known as instruc-\ntion alignment andhuman preference alignment , respectively. The former resorts to supervised\nﬁne-tuning techniques and guides the LLMs to generate outpu ts that adhere closely to user instruc-\ntions. On the other hand, the latter typically relies on rein forcement learning techniques, where\nthe LLMs are trained based on feedback from humans. While the se methods are motivated by\ndifferent goals, they are commonly used together to develop well-aligned LLMs.\n4.1 An Overview of LLM Alignment\nAlignment can be achieved in several different ways. We need different methods for LLM align-\nment because this problem is itself complicated and multifa ceted, requiring a blend of technical\nconsiderations. Here we consider three widely-used approa ches to aligning LLMs.\nThe ﬁrst approach is to ﬁne-tune LLMs with labeled data. This approach is straightforward\nas it simply extends the pre-existing training of a pre-trai ned LLM to adapt it to speciﬁc tasks.\nAn example of this is supervised ﬁne-tuning (SFT), in which the LLM is further trained on a\ndataset comprising task-speciﬁc instructions paired with their expected outputs. The SFT dataset\nis generally much smaller compared to the original training set, but this data is highly specialized.\nThe result of SFT is that the LLM can learn to execute tasks bas ed on user instructions. For\nexample, by ﬁne-tuning the LLM with a set of question-answer pairs, the model can respond to\nspeciﬁc questions, even if not directly covered in the SFT da taset. This method proves particularly\n155\n\n156 Alignment\nPre-training PromptingInstruction\nAlignment\n(e.g., SFT)Human Preference\nAlignment\n(e.g., RLHF)Pre-training\nStageAlignment\nStage\nTraining & Fine-tuning Inference\nFig. 4.1: Schematic illustration of the pre-train-then-align metho d for developing LLMs. In the pre-training stage, we\ntrain an LLM on vast amounts of data using next token predicti on. Then, in the alignment stage, we align the LLM\nto user instructions, intents, and preferences. This inclu des instruction alignment, human preference alignment, an d\nprompting.\nuseful when it is relatively easy to describe the input-outp ut relationships and straightforward to\nannotate the data.\nThe second approach is to ﬁne-t",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 129,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "e34263eb08edf894",
    "content": ", the model can respond to\nspeciﬁc questions, even if not directly covered in the SFT da taset. This method proves particularly\n155\n\n156 Alignment\nPre-training PromptingInstruction\nAlignment\n(e.g., SFT)Human Preference\nAlignment\n(e.g., RLHF)Pre-training\nStageAlignment\nStage\nTraining & Fine-tuning Inference\nFig. 4.1: Schematic illustration of the pre-train-then-align metho d for developing LLMs. In the pre-training stage, we\ntrain an LLM on vast amounts of data using next token predicti on. Then, in the alignment stage, we align the LLM\nto user instructions, intents, and preferences. This inclu des instruction alignment, human preference alignment, an d\nprompting.\nuseful when it is relatively easy to describe the input-outp ut relationships and straightforward to\nannotate the data.\nThe second approach is to ﬁne-tune LLMs using reward models. One difﬁculty in alignment\nis that human values and expectations are complex and hard to describe. In many cases, even for\nhumans themselves, articulating what is ethically correct or culturally appropriate can be challeng-\ning. As a result, collecting or annotating ﬁne-tuning data i s not as straightforward as it is with SFT.\nMoreover, aligning LLMs is not just a task of ﬁtting data, or i n other words, the limited samples\nannotated by humans are often insufﬁcient to comprehensive ly describe these behaviors. What we\nreally need here is to teach the model how to determine which o utputs are more in line with human\npreferences, for example, we not only want the outputs to be t echnically accurate but also to align\nwith human expectations and values. One idea is to develop a r eward model analogous to a human\nexpert. This reward model would work by rewarding the LLM whe never it generates responses\nthat align more closely with human preferences, much like ho w a teacher provides feedback to a\nstudent. To obtain such a reward model, we can train a scoring function from human preference\ndata. The trained reward model is then used as a guide to adjus t and reﬁne the LLM. This frames\nthe LLM alignment task as a reinforcement learning task. The resulting methods, such as rein-\nforcement learning from human feedback (RLHF ), have been demonstrated to be particularly\nsuccessful in adapting LLMs to follow the subtleties of huma n behavior and social norms.\nThe third approach is to perform alignment during inference rather than during training or\nﬁne-tuning. From this perspective, prompting in LLMs can al so be seen as a form of alignment,\nbut it does not involve training or ﬁne-funing. So we can dyna mically adapt an LLM to various\ntasks at minimal cost. Another method to do alignment at infe rence time is to rescore the outputs\nof an LLM. For example, we could develop a scoring system to si mulate human feedback on the\noutputs of the LLM (like a reward model) and prioritize those that receive more positive feedback.\nThe three methods mentioned above are typically used in sequ ence once the pre-training is\ncomplete: we ﬁrst perform SFT, then RLHF, and then prompt the LLM in some way during\ninference. This roughly divides the development of LLMs int o two stages — the pre-training stage\nand the alignment stage. Figure 4.1shows an illustration of this. Since prompting techniques h ave\nbeen intensively discussed in the previous chapter, we will focus on ﬁne-tuning-based alignment\nmethods in the rest of this chapter.\n\n4.2 Instruction Alignment 157\n4.2 Instruction Alignment\nOne feature of LLMs is that they can follow the prompts provid ed by users to perform various\ntasks. In many applications, a prompt consists of a simple in struction and user input, and we want\nthe LLM to follow this instruction to perform the task correc tly. This ability of LLMs is also\ncalled the instruction-following ability. For example, be low is a prompt where we want the LLM\nto extract key points and provide a concise summary for a leng thy article.\nInstruction Summarize this text in three sentences.\nInput Daylight Savings Time (DST) - the process of moving clocks fo rward\nby one hour in the summer - was started in Germany in 1916 ...\nOutput\nThis task requires the LLM to understand the instruction “Su mmarize this text in three sentences”\nand perform the summarization accordingly. However, LLMs a re typically trained for next-token\nprediction rather than",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 130,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "08c6b4309756521d",
    "content": " Instruction Alignment\nOne feature of LLMs is that they can follow the prompts provid ed by users to perform various\ntasks. In many applications, a prompt consists of a simple in struction and user input, and we want\nthe LLM to follow this instruction to perform the task correc tly. This ability of LLMs is also\ncalled the instruction-following ability. For example, be low is a prompt where we want the LLM\nto extract key points and provide a concise summary for a leng thy article.\nInstruction Summarize this text in three sentences.\nInput Daylight Savings Time (DST) - the process of moving clocks fo rward\nby one hour in the summer - was started in Germany in 1916 ...\nOutput\nThis task requires the LLM to understand the instruction “Su mmarize this text in three sentences”\nand perform the summarization accordingly. However, LLMs a re typically trained for next-token\nprediction rather than for generating outputs that follow i nstructions. Applying a pre-trained LLM\nto the above example would likely result in the model continu ing to write the input article instead\nof summarizing the main points. The goal of instruction alig nment (or instruction ﬁne-tuning ) is\nto tune the LLM to accurately respond to user instructions an d intentions. The rest of this section\nwill discuss some issues related to instruction alignment, including ﬁne-tuning LLMs to follow\ninstructions, generating or collecting instruction data, and generalizing instruction alignment.\n4.2.1 Supervised Fine-tuning\nOne straightforward approach to adapting LLMs to follow ins tructions is to ﬁne-tune these models\nusing annotated input-output pairs [ Ouyang et al. ,2022 ;Wei et al. ,2022a ]. Unlike standard lan-\nguage model training, here we do not wish to maximize the prob ability of generating a complete\nsequence, but rather maximizing the probability of generat ing the rest of the sequence given its\npreﬁx. This approach makes instruction ﬁne-tuning a bit dif ferent from pre-training. The SFT\ndata is a collection of such input-output pairs (denoted by S), where each output is the correct\nresponse for the corresponding input instruction. For exam ple, below is an SFT dataset\nx(instruction + user input) y(output)\nSummarize the following article. {∗summary ∗}\nArticle: In recent years, solar energy has seen\nunprecedented growth, becoming the fastest-growing ...\nExtract the main ﬁnancial ﬁgures from the following Revenue: $10 million,\nearnings report. Proﬁt Margin: 15%\nReport: The company reported a revenue of $10 million\nin the ﬁrst quarter with a proﬁt margin of 15% ...\nClassify the following email as spam or not spam. Spam\nText: Congratulations! You’ve won a $500 gift card.\nClick here to claim now.\nProvide a solution to the following technical issue. First, check for ...\nIssue: my computer is running slow and often freezes.\n\n158 Alignment\nwhere the instructions are highlighted. This dataset conta ins instructions and the corresponding\noutputs for several different NLP problems, and so we can ﬁne -tune an LLM to handle multiple\ntasks simultaneously.\nLetx=x0...xmbe an input sequence (e.g., instruction + user input) and y=y1...ynbe the\ncorresponding output sequence. In SFT, we aim to maximize th e probability of the output ygiven\nthe input x. Consider an LLM with pre-trained parameters ˆθ. The ﬁne-tuning objective can then\nbe formulated as:\n˜θ= arg max\nˆθ+∑\n(x,y)∈Dlog Pr ˆθ+(y|x) (4.1)\nwhere ˜θdenotes the parameters optimized via ﬁne-tuning, and ˆθ+represents an adjustment to ˆθ.\nHere we will omit the superscript +and useθto represent ˆθ+to keep the notation uncluttered. But\nthe reader should keep in mind that the ﬁne-tuning starts fro m the pre-trained parameters rather\nthan randomly initialized parameters.\nThe objective function log Pr θ(yi|x,y<i)is computed by summing the log-probabilities of\nthe tokens in y, conditional on the input xand all the previous tokens y<i:\nlog Pr θ(y|x) =n∑\ni=1log Pr θ(yi|x,y<i) (4.2)\nThis formulation is equivalent to minimizing the cross-ent ropy loss.\nNote that minimizing the conditional log-probability log Pr �",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 131,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "f652be68dd575e0c",
    "content": "ˆθ+(y|x) (4.1)\nwhere ˜θdenotes the parameters optimized via ﬁne-tuning, and ˆθ+represents an adjustment to ˆθ.\nHere we will omit the superscript +and useθto represent ˆθ+to keep the notation uncluttered. But\nthe reader should keep in mind that the ﬁne-tuning starts fro m the pre-trained parameters rather\nthan randomly initialized parameters.\nThe objective function log Pr θ(yi|x,y<i)is computed by summing the log-probabilities of\nthe tokens in y, conditional on the input xand all the previous tokens y<i:\nlog Pr θ(y|x) =n∑\ni=1log Pr θ(yi|x,y<i) (4.2)\nThis formulation is equivalent to minimizing the cross-ent ropy loss.\nNote that minimizing the conditional log-probability log Pr θ(y|x)is not a standard language\nmodel training problem. If we concatenate xandyas a single sequence, a more general form of\nlanguage modeling is based on the joint log-probability log Pr θ(x,y), that is, we minimize the\nloss over all tokens of the sequence seqx,y= [x,y]. We can write the probability of this sequence\nusing the chain rule\nlog Pr θ(seqx,y) = log Pr θ(x,y)\n= log Pr θ(x)\nset to 0+ log Pr θ(y|x)\nloss computation(4.3)\nThere are two terms on the right-hand side of the equation. We can simply set the ﬁrst term\nlog Pr θ(x)to 0, focusing solely on the second term log Pr θ(y|x)for loss computation. As a\nresult, the training can be implemented using standard LLMs . For the sequence seqx,y, we ﬁrst\nrun the forward pass as usual. Then, during the backward pass , we force the loss corresponding\ntoxto be zero. Figure 4.2shows an illustration of this process.\nBy taking log Pr θ(seqx,y)as the objective function, we can describe SFT using a regula r form\nof language model training:\n˜θ= arg max\nθ∑\n(x,y)∈Dlog Pr θ(seqx,y) (4.4)\nThe problem we considered above is fundamentally a single-round prediction problem, where\nthe LLM generates a response based on a single input without a ny further interaction or feedback\nfrom the user. The input is processed, and the output is gener ated in one go. This is typical in\n\n4.2 Instruction Alignment 159\nx0x1x2x3y1x1x2x3y1y2Input Output\n(a) Forward Passx0x1x2x3y1x1x2x3y1y2Loss = 0 Loss ̸= 0\n(b) Backward Pass\nFig. 4.2: Illustration of supervised ﬁne-tuning for LLMs. We concate nate the input and the output into a single se-\nquence. During the forward pass, we run the LLM as usual. Duri ng the backward pass, we compute the loss only for\nthe output part and simply set the loss for the input part to 0.\nscenarios where a single question is asked, and a single answ er is provided, with no follow-up\nquestions or clariﬁcations. However, in practice, we somet imes have to handle multi-round pre-\ndiction problems, for example, an LLM engages in a dialogue o ver multiple turns. In this setting,\nthe LLM not only generates responses based on the initial inp ut but also incorporates subsequent\ninputs that might reﬁne or expand on earlier interactions. F or example, we can use the LLM to act\nas a healthcare assistance chatbot and have a conversation w ith the user, like this\nUser I’ve been feeling very tired lately.\nChatbot I’msorry tohear that. Besides feeling tired, have younoticed any\nother symptoms?\nUser Yes, I’m also experiencing headaches frequently.\nChatbot How long have these symptoms been going on?\nUser About a week now.\nChatbot Itmight begood tocheck inwith ahealthcare professional. Would\nyoulikehelp setting upanappointment?\nUser Yes, please. Can it be after work hours?\nChatbot Sure, Icanarrange that. There areslots available next Wednesday\nandThursday after 5PM. Which dayworks",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 132,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "05415652cfd8bd0f",
    "content": " LLM engages in a dialogue o ver multiple turns. In this setting,\nthe LLM not only generates responses based on the initial inp ut but also incorporates subsequent\ninputs that might reﬁne or expand on earlier interactions. F or example, we can use the LLM to act\nas a healthcare assistance chatbot and have a conversation w ith the user, like this\nUser I’ve been feeling very tired lately.\nChatbot I’msorry tohear that. Besides feeling tired, have younoticed any\nother symptoms?\nUser Yes, I’m also experiencing headaches frequently.\nChatbot How long have these symptoms been going on?\nUser About a week now.\nChatbot Itmight begood tocheck inwith ahealthcare professional. Would\nyoulikehelp setting upanappointment?\nUser Yes, please. Can it be after work hours?\nChatbot Sure, Icanarrange that. There areslots available next Wednesday\nandThursday after 5PM. Which dayworks better foryou?\n...\nIn this task, there are several rounds of conversation, each involving the generation of a re-\nsponse based on the user’s request or question and the conver sational history. Suppose we have\nKrounds of conversation, denoted by {x1,y1,x2,y2,...,xK,yK}. Here xkandykdenote the\nuser request and the response, respectively, for each round k. The log-probability of generating\nthe response can be written as log Pr θ(yk|x1,y1,...,xk). Our goal is then to maximize the sum\nof these log-probabilities\n˜θ= arg max\nθK∑\nk=1log Pr θ(yk|x1,y1,...,xk) (4.5)\n\n160 Alignment\nA straightforward implementation of this involves calcula ting the conditional probability for\neachk. However, it requires running the LLM Ktimes, each time with an increased conversa-\ntional history to make predictions. A more efﬁcient method i s to perform loss computation of all\nresponses in a single run of the LLM. To do this, we represent t he conversation as a sequence\nseqx1,y1,...,xK,yK= [x1,y1,...,xK,yK](orseqfor short). The log-probability of this sequence\nis given by\nlog Pr θ(seq) = log Pr θ(x1,y1,...,xK,yK)\n= log Pr θ(x1)\nset to 0+ log Pr θ(y1|x1)\nloss computation+· · ·+\nlog Pr θ(xK|x1,y1,...,yK−1)  \nset to 0+\nlog Pr θ(yK|x1,y1,...,xK) \nloss computation(4.6)\nThe trick here is that we ignore the loss for generating user i nputs, as illustrated in Figure 4.3.\nHence we only compute the probabilities of generating the re sponses given their conversational\nhistories, in other words, the value on the right-hand side o f Eq. ( 4.6) is actually equal to the value\non the right-hand side of Eq. ( 4.5). As with Eq. ( 4.4), the training of this multi-round prediction\nmodel can be achieved by maximizing the log likelihood over a training dataset D:\n˜θ= arg max\nθ∑\nseq∈Dlog Pr θ(seq) (4.7)\nWhile implementing the SFT methods introduced above seems t rivial as they are fundamen-\ntally the same as regular language model training, there are still issues that need to be considered\nin practice. For example,\n• SFT requires labeled data. This makes SFT quite different f rom pre-training, where raw text\nis used as training data and is readily available. As in other supervised machine learning\nproblems, data annotation and selection in SFT are not simpl e tasks. In general, we wish\nto develop SFT data that is both substantial in quantity and h igh in quality, and this data\nshould be highly relevant to the tasks the LLM will perform. O n the other hand, there is\na need to ﬁne-tune LLMs with less data to minimize computatio nal and data construction\ncosts. Often, the quality of LLMs is highly dependent on the d ata used in SFT. Thus, such\ndata must be carefully developed and",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 133,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "091271aade481ab6",
    "content": "FT methods introduced above seems t rivial as they are fundamen-\ntally the same as regular language model training, there are still issues that need to be considered\nin practice. For example,\n• SFT requires labeled data. This makes SFT quite different f rom pre-training, where raw text\nis used as training data and is readily available. As in other supervised machine learning\nproblems, data annotation and selection in SFT are not simpl e tasks. In general, we wish\nto develop SFT data that is both substantial in quantity and h igh in quality, and this data\nshould be highly relevant to the tasks the LLM will perform. O n the other hand, there is\na need to ﬁne-tune LLMs with less data to minimize computatio nal and data construction\ncosts. Often, the quality of LLMs is highly dependent on the d ata used in SFT. Thus, such\ndata must be carefully developed and examined. As we will see in later subsections, SFT\ncan be more efﬁcient and effective through more advanced tec hniques for data construction.\n• SFT is still computationally expensive for LLMs due to thei r big size. As a result, main-\ntaining and updating such models is resource-intensive. Fo r example, applying gradient up-\ndates to billions of parameters within an LLM requires signi ﬁcant computational power and\nmemory. This often requires high-performance computing en vironments, which are costly\nto operate. To address these challenges, various optimizat ion strategies, such as pruning,\nquantization, and the use of more efﬁcient training algorit hms, have been explored. In par-\nticular, there has been signiﬁcant interest in parameter-e fﬁcient ﬁne-tuning methods which\nare designed to maintain state-of-the-art performance wit hout the need for extensive compu-\ntational resources. We have seen in Chapter 3 that applying t echniques like soft prompts can\n\n4.2 Instruction Alignment 161\nUser: I’ve been feeling very tired lately.\nChatbot: I’m sorry to hear that. Besides feeling tired,\nhave you noticed any other symptoms?\nUser: Yes, I’m also experiencing headaches frequently.\nChatbot: How long have these symptoms been going on?\n...x1 y1x2 y2 · · ·Prθ(x1) Prθ(y1|x1) Prθ(x2|x1,y1) Prθ(y2|x1,y1,x2)Loss = 0 Loss ̸= 0 Loss = 0 Loss ̸= 0\nFig. 4.3: Illustration of supervised ﬁne-tuning for conversational models. Here the LLM acts as a chatbot to respond to\neach request based on the conversational history. The conve rsation progresses by alternating between the user and the\nchatbot. In SFT, we treat the entire conversation as a sequen ce, just like in standard LLMs, but compute the loss only\nfor the responses of the LLM.\nmake the ﬁne-tuning process more efﬁcient. For further disc ussion on parameter-efﬁcient\nmethods, the reader can refer to related papers on this issue [Houlsby et al. ,2019 ;Hu et al. ,\n2022 ;Han et al. ,2024 ].\n• SFT can be regarded as a post-training step following pre-t raining. It is a separate training\nphase designed to preserve the advantages of the initial pre -training while incorporating new\nadjustments. This may seem paradoxical because updating a p re-trained LLM with further\ndata potentially causes the model to forget some of its prior knowledge. Imagine a scenario\nwhere we have a large amount of SFT data and extensively ﬁne-t une the LLM. In this\ncase, the LLM could overﬁt the data, which in turn may reduce g eneralization performance\nor cause catastrophic forgetting. A common strategy to miti gate this issue is to employ\nregularization and early stopping techniques. Another pra ctical approach is to use a smaller\nlearning rate to gently adjust the weights of the LLM. In addi tion, ﬁne-tuning with data from\ndiverse sources and problem domains can also be beneﬁcial. N evertheless, in practice, the\nSFT step is often carefully examined and requires substanti al engineering and experimental\nefforts to optimize.\n4.2.2 Fine-tuning Data Acquisition\nFine-tuning data is so important that much recent work in LLM has focused on developing various\ndatasets for instruction ﬁne-tuning. As with most work in ma chine learning, there",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 134,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "c9b4ac320e2595a6",
    "content": " of SFT data and extensively ﬁne-t une the LLM. In this\ncase, the LLM could overﬁt the data, which in turn may reduce g eneralization performance\nor cause catastrophic forgetting. A common strategy to miti gate this issue is to employ\nregularization and early stopping techniques. Another pra ctical approach is to use a smaller\nlearning rate to gently adjust the weights of the LLM. In addi tion, ﬁne-tuning with data from\ndiverse sources and problem domains can also be beneﬁcial. N evertheless, in practice, the\nSFT step is often carefully examined and requires substanti al engineering and experimental\nefforts to optimize.\n4.2.2 Fine-tuning Data Acquisition\nFine-tuning data is so important that much recent work in LLM has focused on developing various\ndatasets for instruction ﬁne-tuning. As with most work in ma chine learning, there are generally\ntwo approaches to data acquisition — manual data generation and automatic data generation.\n\n162 Alignment\n4.2.2.1 Manually Generated Data\nOne straightforward method is to recruit human annotators t o create input-output pairs for the\ntasks of interest. Unlike data annotation in conventional N LP, such as text classiﬁcation, where\nannotators simply assign labels to collected texts accordi ng to guidelines, creating ﬁne-tuning data\nfor LLMs requires more steps and effort, making it thus more c hallenging. Suppose we want to\nobtain ﬁne-tuning data for the English-to-Chinese machine translation task. The ﬁrst step is to\nwrite a prompt template to describe the task and format the pr oblem clearly. For example,\nInstruction Translate the text from English to Chinese.\nUser Input {∗text∗}\nOutput {∗translation ∗}\nThen, we collect pairs of source and target texts (i.e., Chin ese texts and the corresponding\ntranslations), and replace the variables {∗text∗}and{∗translation ∗}to generate the ﬁne-tuning\nsamples. For example, given a pair of English and Chinese sen tences\nHow’s the weather today? →今天天气怎么样？\n{∗text∗} {∗ translation ∗}\nwe can generate a ﬁne-tuning sample using the prompt templat e, like this\nInstruction Translate the text from English to Chinese.\nUser Input How’s the weather today?\nOutput今天天气怎么样？\nThat is,\nx=Translate the text from English to Chinese. \\n How’s the weather today?\ny=今天天气怎么样？\nWe can use this (x,y)pair to ﬁne-tune the LLM, as described in the previous subsec tion.\nOne difﬁculty here is that there are many, many different way s to write prompt templates\nfor the same task, and different people may produce prompt te mplates with varying qualities\nand complexities. Sometimes, we may write prompt templates with overly complex or verbose\ninstructions. Sometimes, we may not even know exactly what t he target task is and how to de-\nscribe it. A widely-adopted strategy is to create prompt tem plates for existing NLP tasks, given\nthat there have been so many well-established NLP problems a nd benchmarks [ Bach et al. ,2022 ;\nWang et al. ,2022b ;Mishra et al. ,2022 ]. In this case, annotators can be given the original task\ndescription and many examples. Then, they can use their own w ays to express how to prompt the\nLLM to perform the tasks. Note that, while such a method can ea se the process of creating and\nwriting prompts, we still need annotation frameworks and cr owdsourcing systems to manage the\nwork and conduct quality control. For example, we generally need to design annotation guidelines\n\n4.2 Instruction Alignment 163\nand a uniﬁed format for writing prompt templates, especiall y when many annotators are contribut-\ning to the same task. One advantage of inducing prompts from e xisting NLP tasks is that, once the\nprompt templates have been developed, it is easy to generate prompts using the annotated samples\nin the original tasks. For example, given a bilingual datase t for English-to-Chinese translation, we\ncan easily create a number of ﬁne-tuning examples by ﬁlling t he slots in the above template with\nthe sentence pairs in this dataset.\nAnother approach is to directly use the naturally existing d ata available on the internet. A\ncommon example is by collecting question-and-answer pairs from QA websites to ﬁne-tune LLMs",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 135,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "010060a3e2347ad0",
    "content": ", we still need annotation frameworks and cr owdsourcing systems to manage the\nwork and conduct quality control. For example, we generally need to design annotation guidelines\n\n4.2 Instruction Alignment 163\nand a uniﬁed format for writing prompt templates, especiall y when many annotators are contribut-\ning to the same task. One advantage of inducing prompts from e xisting NLP tasks is that, once the\nprompt templates have been developed, it is easy to generate prompts using the annotated samples\nin the original tasks. For example, given a bilingual datase t for English-to-Chinese translation, we\ncan easily create a number of ﬁne-tuning examples by ﬁlling t he slots in the above template with\nthe sentence pairs in this dataset.\nAnother approach is to directly use the naturally existing d ata available on the internet. A\ncommon example is by collecting question-and-answer pairs from QA websites to ﬁne-tune LLMs\nfor open-domain QA tasks [ Joshi et al. ,2017 ]. Many benchmarks in QA are built in this way\nbecause there are so many types of questions that it is imposs ible to think of them all by a small\ngroup of people. Instead, using data from those websites can ensure that the LLM ﬁne-tuning data\nis at a good or acceptable level in terms of quantity and quali ty.\nIn addition to employing existing resources, another strai ghtforward way to develop a ﬁne-\ntuning dataset is to crowdsource the data. A simple approach is to allow users to input any ques-\ntion, after which responses are either manually given or aut omatically generated by an LLM and\nthen manually annotated and corrected. It is thus possible t o capture real user behavior and conse-\nquently gather inputs and outputs for a large number of “new” problems that traditional NLP tasks\ndo not cover.\nAn issue related to the construction of the ﬁne-tuning datas ets is that we usually want the\ndata to be as diverse as possible. Many studies have found tha t increasing the diversity of ﬁne-\ntuning data can improve the robustness and generalization a bility of LLMs. For this reason, there\nhas been considerable interest in involving more diverse pr ompts and tasks in LLM ﬁne-tunining\ndatasets. We will give further discussions on the generaliz ation of ﬁne-tuning in Section 4.2.4 .\n4.2.2.2 Automatically Generated Data\nOne limitation of manual data generation is that the quality and diversity largely depend on human\nexperience and creativity. Therefore, if we want LLMs to han dle a broad range of tasks, that\nis, to effectively execute any instruction, relying on huma n-annotated data for LLM ﬁne-tuning\nis often inefﬁcient. Moreover, the coverage of such data can be limited, and the data may even\ncontain biases introduced by the annotators themselves. An alternative approach is to generate data\nautomatically. For example, we can collect a number of quest ions through crowdsourcing, and\nemploy a well-tuned LLM to generate answers to the questions . These question-answer pairs are\nthen used as ﬁne-tuning samples as usual. This method, thoug h very simple, has been extensively\napplied to generate large-scale ﬁne-tuning data for LLMs.\nThe above way of producing synthetic ﬁne-tuning data is simi lar to those used in data aug-\nmentation for NLP. If we have an LLM, we can produce a predicti on in response to any input.\nRepeating this process for different inputs allows us to cre ate a sufﬁcient number of ﬁne-tuning\nsamples. Such a method is particularly useful for ﬁne-tunin g new LLMs using a well-tuned LLM.\nHowever, one disadvantage of this approach is that it relies on human-crafted or collected inputs\nfor data generation, which may turn out to be inappropriate f or generalizing LLMs. In many LLM\napplications, a signiﬁcant challenge arises from the broad range of users’ questions and requests,\nmany of which are not covered in existing NLP tasks and datase ts. In these cases, it becomes\nnecessary to generate not only the predictions but also the i nputs themselves.\nHere we consider self-instruct as an example to illustrate how to generate LLM ﬁne-tuning\n\n164 Alignment\nInitializationInitialize the task pool with a number of instructions\nand corresponding input-output samples.\nSample 1: (Instruction ,User -input ,Output)\nSample 2: (Instruction ,User -input ,Output)\n· · ·Task Pool\nSamplingDraw a few instructions",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 136,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "0a75ccbb03100cc9",
    "content": " ﬁne-tunin g new LLMs using a well-tuned LLM.\nHowever, one disadvantage of this approach is that it relies on human-crafted or collected inputs\nfor data generation, which may turn out to be inappropriate f or generalizing LLMs. In many LLM\napplications, a signiﬁcant challenge arises from the broad range of users’ questions and requests,\nmany of which are not covered in existing NLP tasks and datase ts. In these cases, it becomes\nnecessary to generate not only the predictions but also the i nputs themselves.\nHere we consider self-instruct as an example to illustrate how to generate LLM ﬁne-tuning\n\n164 Alignment\nInitializationInitialize the task pool with a number of instructions\nand corresponding input-output samples.\nSample 1: (Instruction ,User -input ,Output)\nSample 2: (Instruction ,User -input ,Output)\n· · ·Task Pool\nSamplingDraw a few instructions from the pool\nInstruction a\nInstruction b\nInstruction cTask Poolsampling\nInstruction\nGenerationPrompt the LLM to generate a new instruction based on\nthe drawn instructions.\nYou are provided several different instructions for perfor ming\nsome tasks. Please generate an instruction based on these.\nTask 1: Instruction a\nTask 2: Instruction b\nTask 3: Instruction c\nNew Task: Instruction new\nSample\nGenerationGiven the newly-generated instruction and a few\ninput-output samples, generate a new sample.\nYou are provided with a set of input-output samples tasks,\neach composed of an instruction, a user input, and an output.\nPlease generate a new sample based on these.\nSample 1: Samplea\nSample 2: Sampleb\nNew Sample: Instruction new User -inputnew Outputnew\nFilteringFilter out invalid and low-quality samples.\nAdd the remaining samples into the pool.\nFig. 4.4: Illustration of self-instruct [ Wang et al. ,2023b ]. This method maintains a pool of instructions and correspo nd-\ning input-output samples. Initially, the pool contains a nu mber of hand-crafted instructions and samples. Each time,\nwe draw a few instructions from the pool. An LLM is then prompt ed to generate new instructions and samples based\non those drawn. Finally, the newly-generated instructions and samples are ﬁltered and added to the pool.\nsamples [ Wang et al. ,2023d ;Honovich et al. ,2023 ]. The idea is that we can prompt an LLM to\ncreate a new instruction by learning from other instruction s. Given this instruction, the LLM can\nthen ﬁll in other ﬁelds (such as the user input) and produce th e predictions. Figure 4.4shows a\nschematic illustration of self-instruct. Here we give a bri ef outline of the key steps involved.\n• The self-instruct algorithm maintains a pool of tasks. Ini tially it contains a number of seed\nhand-crafted tasks, each with an instruction and input-out put sample. As the algorithm\nproceeds, LLM-generated instructions and samples will be a dded to this pool.\n\n4.2 Instruction Alignment 165\n• At each step, a small number of instructions are drawn from t he instruction pool. For ex-\nample, we can randomly select a few human-written instructi ons and a few LLM-generated\ninstructions to ensure diversity.\n• The selected instructions are then used as demonstration e xamples. Thus, the LLM can\nin-context learn from these examples and produce a new instr uction. Below is an example\ntemplate for prompting the LLM.\nYou are provided several different instructions for perfor ming some tasks.\nPlease generate an instruction based on these.\nTask 1: {instruction1 }\nTask 2: {instruction2 }\nTask 3: {instruction3 }\nTask 4: {instruction4 }\nNew Task:\n• Given the generated instruction, the LLM is then prompted t o complete the sample by ﬁlling\nin the remaining input ﬁelds and generating the correspondi ng output. Below is a prompt\ntemplate.\nYou are provided with a set of input-output samples, each com posed of\nan instruction, a user input, and an output. Please generate a new sample\nbased on these.\nSample 1: {instruction1 }\nInput: {user-input1 }\nOutput: {output1 }\nSample 2: {instruction2 }\nInput: {user-input2 }\nOutput: {output2 }\nNew Sample: {new-instruction }\n• This newly-generated sample is examined by some heuristic rules (such as ﬁltering out\nsamples or instructions that are similar to those already in the pool). If it passes, the sample\nand instruction are added to the pool.\nThis generation process can be repeated many times to",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 137,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "3cec5aa7540ed2f8",
    "content": " 4: {instruction4 }\nNew Task:\n• Given the generated instruction, the LLM is then prompted t o complete the sample by ﬁlling\nin the remaining input ﬁelds and generating the correspondi ng output. Below is a prompt\ntemplate.\nYou are provided with a set of input-output samples, each com posed of\nan instruction, a user input, and an output. Please generate a new sample\nbased on these.\nSample 1: {instruction1 }\nInput: {user-input1 }\nOutput: {output1 }\nSample 2: {instruction2 }\nInput: {user-input2 }\nOutput: {output2 }\nNew Sample: {new-instruction }\n• This newly-generated sample is examined by some heuristic rules (such as ﬁltering out\nsamples or instructions that are similar to those already in the pool). If it passes, the sample\nand instruction are added to the pool.\nThis generation process can be repeated many times to obtain a sufﬁcient number of ﬁne-\ntuning samples. Note that, above, we just show simple prompt templates for generating instruction\nand ﬁne-tuning samples. Of course, we can develop better tem plates to generate more diverse and\naccurate instruction and ﬁne-tuning samples. For example, for certain tasks like text classiﬁcation,\n\n166 Alignment\nthe LLM may tend to produce biased predictions, for example, most generated samples belong to\na single class. In such cases, we can adjust the order of gener ation of different ﬁelds. More\nspeciﬁcally, we can specify the output (i.e., the class) wit h some prior, and prompt the LLM\nto generate user input given both the instruction and the out put. This method resembles input\ninversion , where the LLM generates the input based on the speciﬁed outp ut [Longpre et al. ,2023 ].\nUsing LLM-generated instructions and ﬁne-tuning samples h as been a common method for\ndeveloping LLMs, especially given that manually developin g such data is so expensive that most\nresearch groups cannot afford it. In several well-tuned LLM s, their ﬁne-tuning datasets include\na certain amount of synthetic data, which has proved useful [ Ouyang et al. ,2022 ;Taori et al. ,\n2023 ;Chiang et al. ,2023 ]. There have been further studies on synthetic data generat ion for LLM\nﬁne-tuning. For example, one can generate more diverse inst ructions by introducing evolutionary\nalgorithms [ Xu et al. ,2024 ], or use synthetic data as supervision signals in a more adva nced ﬁne-\ntuning process [ Chen et al. ,2024b ]. More recently, there has also been considerable interest in\nusing synthetic data in the pre-training stage [ Gunasekar et al. ,2023 ;Allal et al. ,2024 ].\nIn many applications, a real-world scenario is that, given a task, we can collect or annotate a\nrelatively small amount of ﬁne-tuning data, for example, we can recruit experts to create questions\nfor QA tasks in a speciﬁc domain. But the quantity and diversi ty of this data are in general not\nsufﬁcient. In this case, we can use self-instruct technique s to generate more diverse question-\nanswer pairs, and thus augment the ﬁne-tuning data. This pro vides a way of bootstrapping the\nLLM starting from a seed set of ﬁne-tuning samples. Note that using self-generated data is a com-\nmon practice and has long been applied in NLP. For example, th is approach has been successfully\nused in parsing and machine translation [ Charniak ,1997 ;Sennrich et al. ,2016 ].\n4.2.3 Fine-tuning with Less Data\nWith the increasing prominence of instruction ﬁne-tuning, there has been a surge in demand for\nlarge-scale, high-quality ﬁne-tuning data. For example, t he FLAN ﬁne-tuning dataset, which\nis compiled from 1,836 tasks, contains 15 million samples [ Longpre et al. ,2023 ]. Fine-tuning\nLLMs with such large datasets is typically a computationall y expensive task, especially given\nthat updating the large number of parameters in LLMs is resou rce-intensive. One approach for\nmitigating this issue is to explore efﬁcient model training methods, for example, one can use\nparameter-efﬁcient methods to update only a small portion o f the model. However, many ﬁne",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 138,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "d5fccb3d32ee3ff7",
    "content": " parsing and machine translation [ Charniak ,1997 ;Sennrich et al. ,2016 ].\n4.2.3 Fine-tuning with Less Data\nWith the increasing prominence of instruction ﬁne-tuning, there has been a surge in demand for\nlarge-scale, high-quality ﬁne-tuning data. For example, t he FLAN ﬁne-tuning dataset, which\nis compiled from 1,836 tasks, contains 15 million samples [ Longpre et al. ,2023 ]. Fine-tuning\nLLMs with such large datasets is typically a computationall y expensive task, especially given\nthat updating the large number of parameters in LLMs is resou rce-intensive. One approach for\nmitigating this issue is to explore efﬁcient model training methods, for example, one can use\nparameter-efﬁcient methods to update only a small portion o f the model. However, many ﬁne-\ntuning datasets contain a large amount of synthetic data, wh ere errors and biases are still inevitable.\nAnother approach to efﬁcient ﬁne-tuning is to consider only the most relevant and impactful\nexamples for ﬁne-tuning. We can thus reduce the amount of dat a that needs to be processed while\nstill maintaining the quality of the model updates. There ar e several methods to achieve this. For\nexample, Zhou et al. [2023a ] built an instruction-following dataset containing only 1 ,000 sam-\nples by carefully crafting the prompts and collecting sampl es from a variety of NLP tasks. They\nshowed that the LLaMa 65B model ﬁne-tuned with this dataset c ould be competitive with or even\nbetter than models with much more ﬁne-tuning effort. This su ggests that LLMs can be adapted\nto respond to diverse tasks without necessarily needing ﬁne -tuning on all types of instruction-\nfollowing data. Chen et al. [2024a ] developed a system based on the GPT-3.5 model to assess\nthe quality of each instruction-following sample. Therefo re, they could select high-quality sam-\nples from existing datasets, showing better ﬁne-tuning per formance with fewer ﬁne-tuning sam-\nples. Researchers have also developed methods to either sel ect or ﬁlter out data using heuristics\n\n4.2 Instruction Alignment 167\n[Zhao et al. ,2024 ;Ge et al. ,2024 ], or to prioritize data that more signiﬁcantly inﬂuences th e ﬁne-\ntuning process [ Xia et al. ,2024 ]. In fact, most of these methods can be seen as instances of la rger\nfamilies of data selection and ﬁltering methods. And it is of ten the case that using higher quality\n(but maybe less) data is beneﬁcial for training NLP models.\nThe discoveries in instruction ﬁne-tuning somewhat differ from traditional views in NLP: the\nability of models to handle complex problems can be activate d with a small amount of annotated\ndata, rather than requiring massive amounts of supervised d ata for extensive training. One pos-\nsible explanation is that the ability of generating correct responses given instructions has been\nlearned during pre-training, but such instruction-respon se mappings are not with high probabil-\nities during inference. Fine-tuning can slightly adjust th e models to get them to follow instruc-\ntions, requiring signiﬁcantly less training effort than pr e-training. This is closely related to what\nis known as the superﬁcial alignment hypothesis , which suggests that learning primarily occurs\nduring pre-training, and the subsequent ﬁne-tuning or alig nment phase does not signiﬁcantly con-\ntribute to the underlying knowledge base of an LLM [ Zhou et al. ,2023a ]. Since the core abilities\nand knowledge of the model are already established from pre- training, effective ﬁne-tuning for\nalignment with user needs can be achieved with relatively sm all training ﬁne-tuning effort. This\nimplies the possibility of ﬁne-tuning LLMs with very little data. In another direction, it may not\nbe necessary to restrict ﬁne-tuning to paired instruction- response data. For example, Hewitt et al.\n[2024 ] found that instruction-following can be implicitly achie ved by ﬁne-tuning LLMs only on\nresponses, without corresponding instructions.\nA concept related to the discussion here is sample efﬁciency . A machine learning method is\ncalled sample efﬁcient if it can learn effectively from a small number of training ex amples. In",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 139,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "18be1afd4ad0115b",
    "content": " signiﬁcantly con-\ntribute to the underlying knowledge base of an LLM [ Zhou et al. ,2023a ]. Since the core abilities\nand knowledge of the model are already established from pre- training, effective ﬁne-tuning for\nalignment with user needs can be achieved with relatively sm all training ﬁne-tuning effort. This\nimplies the possibility of ﬁne-tuning LLMs with very little data. In another direction, it may not\nbe necessary to restrict ﬁne-tuning to paired instruction- response data. For example, Hewitt et al.\n[2024 ] found that instruction-following can be implicitly achie ved by ﬁne-tuning LLMs only on\nresponses, without corresponding instructions.\nA concept related to the discussion here is sample efﬁciency . A machine learning method is\ncalled sample efﬁcient if it can learn effectively from a small number of training ex amples. In this\nsense, instruction ﬁne-tuning is sample efﬁcient compared with pre-training. From the perspective\nof machine learning, sample-efﬁcient methods can be seen as efﬁcient ways to sample the space\nof data, and are advantageous as they make optimal use of scar ce data. Therefore, sampling-based\nlearning techniques, such as many reinforcement learning a lgorithms, can beneﬁt from these sam-\nple efﬁcient approaches. For example, in human preference a lignment, we can either efﬁciently\nsample preference data via reward models [ Liu et al. ,2024b ] or improve sampling efﬁciency in\npolicy learning [ Wang et al. ,2024 ].\n4.2.4 Instruction Generalization\nIn many machine learning and NLP problems, training a model t o generalize is a fundamental\ngoal. For example, in text classiﬁcation, we expect our mode l to correctly classify new texts that\nwere not seen during training. However, generalization pos es additional challenges in instruction\nﬁne-tuning. We expect instruction-ﬁne-tuned LLMs to not on ly generate appropriate responses for\ndifferent inputs within a task but also to accurately perfor m various tasks as described by different\ninstructions. To illustrate this issue, consider an LLM Pr(y|c,z), where cis an instruction, z\nis a user input, and yis the corresponding model output (i.e., the response). Sup pose that the\nperformance of this model is evaluated in terms of a metric, w ritten as Performance(Pr( y|c,z))\norP(c,z,y)for short. Informally, when we say this model can generalize within a given task\n(indicated by the instruction c∗), we mean that there may be a value ǫsuch that the average\nperformance on new inputs is above this value:\n1\n|Z|∑\nz′∈ZP(c∗,z′,y′)>ǫ (4.8)\n\n168 Alignment\nwhere Zis the set of new inputs, and z′andy′are an input in this set and the corresponding\noutput, respectively.\nLikewise, we can say that this model can generalize across ta sks if the average performance\nover all instruction-input pairs is above some ǫ:\n1\n|D|∑\n(c′,z′)∈DP(c′,z′,y′)>ǫ (4.9)\nwhere Dis the set of new instruction-input pairs.\nHere, we need to deal with variations in two dimensions: inst ruction and user input. This\nmakes the generalization problem very complex, because, in tuitively, a model needs to learn from\na vast number of tasks and different input-output pairs asso ciated with each task to achieve good\ngeneralization. As we have discussed several times in this b ook, achieving such generalization\nincurs much lower cost than pre-training. In general, ﬁne-t uning LLMs with instruction-response\ndata to some extent can lead to models yielding instruction f ollowing on new tasks. Nevertheless,\nit is typically believed that certain efforts are still need ed to adapt LLMs to make them understand\nand execute instructions broadly.\nOne way to generalize instruction ﬁne-tuning is to increase the diversity of the ﬁne-tuning\ndata. In earlier studies on instruction ﬁne-tuning, resear chers developed many datasets, covering a\nwide variety of NLP tasks and different instructions for eac h task [ Wang et al. ,2022b ;Sanh et al. ,\n2022 ;Longpre et al. ,2023 ]. By transforming these tasks into a uniﬁed format, one can ﬁ ne",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 140,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "bae7a9e7f6785fb6",
    "content": " achieve good\ngeneralization. As we have discussed several times in this b ook, achieving such generalization\nincurs much lower cost than pre-training. In general, ﬁne-t uning LLMs with instruction-response\ndata to some extent can lead to models yielding instruction f ollowing on new tasks. Nevertheless,\nit is typically believed that certain efforts are still need ed to adapt LLMs to make them understand\nand execute instructions broadly.\nOne way to generalize instruction ﬁne-tuning is to increase the diversity of the ﬁne-tuning\ndata. In earlier studies on instruction ﬁne-tuning, resear chers developed many datasets, covering a\nwide variety of NLP tasks and different instructions for eac h task [ Wang et al. ,2022b ;Sanh et al. ,\n2022 ;Longpre et al. ,2023 ]. By transforming these tasks into a uniﬁed format, one can ﬁ ne-\ntune an LLM with a sufﬁciently large number of samples, for ex ample, there have been several\ninstruction ﬁne-tuning datasets that involve over 100 NLP t asks and 1M samples. However, these\nearly datasets mostly focus on existing academic problems, but not those that users want to deal\nwith in real-world applications. Much recent work has shift ed focus to addressing new and more\npractical problems. For example, there has been considerab le interest in constructing datasets\nthat contain large and complicated demonstrations and resp onses from SOTA models to real user\nqueries [ Wang et al. ,2023c ;Teknium ,2023 ].\nPerhaps the use of large and diverse ﬁne-tuning datasets has its origins in attempts to scale\nLLMs in different dimensions. Indeed, scaling laws have bee n used broadly to motivate the de-\nvelopment of a wide range of different instruction-ﬁne-tun ed LLMs. And it is reasonable to scale\ninstruction ﬁne-tuning to make an LLM follow broad instruct ions. From the perspective of LLM\nalignment, however, scaling instruction ﬁne-tuning might not be efﬁcient to achieve generaliza-\ntion.\nOne problem is that instruction ﬁne-tuning relies on superv ised learning that learns to gener-\nalize and perform tasks based on instruction-response mapp ings. However, such an approach does\nnot capture subtle or complex human preferences (e.g., tone , style, or subjective quality) because\nthese are hard to encode as explicit instruction-response d ata. Moreover, the generalization per-\nformance is bounded by the diversity and quality of the instr uction-response dataset. Given these\nlimitations, we would instead like to employ preference mod els as an additional ﬁne-tuning step\nfollowing instruction ﬁne-tuning, so the LLMs can generali ze further (see Section 4.3).\nAnother view is that some instruction-response mappings ma y already be learned during pre-\ntraining, and so the pre-trained LLMs have encoded such mapp ings. However, since we often do\nnot know exactly what data is used in the pre-training, it is h ard to judge whether we need to learn\nsuch mappings in the ﬁne-tuning. A related question is wheth er out-of-distribution generalization\n\n4.2 Instruction Alignment 169\nis primarily achieved during pre-training or ﬁne-tuning. W hile directly answering this question is\nbeyond the scope of this chapter, it has been shown that pre-t raining on large and diverse datasets\nis effective in improving out-of-distribution performanc e [Hendrycks et al. ,2020 ;Radford et al. ,\n2021 ;Gunasekar et al. ,2023 ]. This raises an interesting problem: if an LLM has been well pre-\ntrained at scale, ﬁne-tuning may not be as essential for out- of-distribution generalization, since the\nmodel may have already encountered sufﬁcient distribution al variation. This prompts researchers\nto ﬁne-tune LLMs with modest effort or to explore new methods to achieve instruction-following.\nAs discussed in the previous sub-section, for example, inst ruction following can be yielded by\nﬁne-tuning on a small number of carefully selected instruct ion-response pairs [ Zhou et al. ,2023a ],\nor even by using methods that are not explicitly designed to d o so [ Kung and Peng ,2023 ].\nThe above discussion provides two different strategies: on e requires scaling up ﬁne-tuning\ndatasets for larger diversity, the other requires small but necessary ﬁne-tuning datasets for efﬁcient\nLLM adaptation. However, in practice, involving diverse in structions often helps",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 141,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "aedc0af091b02119",
    "content": "LM has been well pre-\ntrained at scale, ﬁne-tuning may not be as essential for out- of-distribution generalization, since the\nmodel may have already encountered sufﬁcient distribution al variation. This prompts researchers\nto ﬁne-tune LLMs with modest effort or to explore new methods to achieve instruction-following.\nAs discussed in the previous sub-section, for example, inst ruction following can be yielded by\nﬁne-tuning on a small number of carefully selected instruct ion-response pairs [ Zhou et al. ,2023a ],\nor even by using methods that are not explicitly designed to d o so [ Kung and Peng ,2023 ].\nThe above discussion provides two different strategies: on e requires scaling up ﬁne-tuning\ndatasets for larger diversity, the other requires small but necessary ﬁne-tuning datasets for efﬁcient\nLLM adaptation. However, in practice, involving diverse in structions often helps. In many cases,\nwe need to adapt our LLM for speciﬁc purposes. But the LLM, whi ch has possibly encoded broad\ninstruction-following mappings during pre-training, mig ht tend to behave as a general-purpose\ninstruction executor even with modest ﬁne-tuning. An inter esting phenomenon is that when ﬁne-\ntuning on math data, the resulting LLM might not specialize i n math outputs. Instead, this model\nmight respond normally to general instructions, for exampl e, it could generate poetry if instructed\nto do so [ Hewitt ,2024 ]. This is not a bad thing, but it shows that LLMs may not easily change their\nnature of following general instructions. In this case, add itional adaptations with more diverse\ndata may help adjust the way the LLM follows instructions, pa rticularly for those tasks we aim to\naddress.\n4.2.5 Using Weak Models to Improve Strong Models\nSo far we have explored a variety of instruction ﬁne-tuning m ethods based on labeled data. One\nof the limitations of many such methods is that they require t he data to be annotated by humans or\ngenerated by strong LLMs, which can provide accurate superv ision signals in ﬁne-tuning. How-\never, in many cases, the LLM we have in hand is already strong ( or at least is advantageous in\nspeciﬁc aspects of problem solving), and thus it is not easy t o ﬁnd a superior model for supervi-\nsion. Even for human experts, when the problem becomes compl ex, providing correct and detailed\nanswers might be difﬁcult, or sometimes infeasible. For exa mple, when faced with an extremely\nlong document, the experts would ﬁnd it challenging to ident ify any inconsistencies, subtle biases,\nor missing key points without conducting an exhaustive and t ime-consuming review.\nOne may ask at this point: can we use weak LLMs to supervise str ong LLMs? This seems\nto be a signiﬁcant challenge, but it may reﬂect a future scena rio where we need to supervise AI\nsystems that are smarter than humans or any other AI systems [ Burns et al. ,2023b ]. The problem\nof using smaller, less complex models to improve the trainin g of larger, more complex models\nis also called the weak-to-strong generalization problem. While there have not been mature\napproaches to weak-to-strong generalization, using small er models to assist stronger models has\nindeed proven useful in several areas of LLMs.\nFor instruction ﬁne-tuning, one of the simplest ways of appl ying weak LLMs is to use these\nmodels to generate synthetic ﬁne-tuning data. Suppose we ha ve a collection of inputs X, where\neach input includes an instruction and a user input if necess ary. For each x∈X, we use a weak\nLLM Prw(·)to generate a prediction ˆy= arg maxyPrw(y|x). Then, the strong LLM Prs\nθ(·)can\n\n170 Alignment\nbe trained on these generated predictions (see Eq. ( 4.1)):\n˜θ= arg max\nθ∑\nx∈Xlog Prs\nθ(ˆy|x) (4.10)\nwhereθis the model parameters.\nThe above form transforms the ﬁne-tuning problem into a know ledge distillation problem, in\nother words, we distill knowledge from the weak model to the s trong model. Consequently, we\ncan employ various knowledge distillation methods to achie ve this goal. However, explaining\nweak-to-strong ﬁne-tuning from the perspective of knowled ge distillation is",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 142,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "4364b2960bd88856",
    "content": " where\neach input includes an instruction and a user input if necess ary. For each x∈X, we use a weak\nLLM Prw(·)to generate a prediction ˆy= arg maxyPrw(y|x). Then, the strong LLM Prs\nθ(·)can\n\n170 Alignment\nbe trained on these generated predictions (see Eq. ( 4.1)):\n˜θ= arg max\nθ∑\nx∈Xlog Prs\nθ(ˆy|x) (4.10)\nwhereθis the model parameters.\nThe above form transforms the ﬁne-tuning problem into a know ledge distillation problem, in\nother words, we distill knowledge from the weak model to the s trong model. Consequently, we\ncan employ various knowledge distillation methods to achie ve this goal. However, explaining\nweak-to-strong ﬁne-tuning from the perspective of knowled ge distillation is not straightforward.\nA major concern is that the strong model may merely imitate or overﬁt the errors of the weak\nmodel and fail to generalize. For example, the ﬁne-tuned str ong model still cannot solve difﬁcult\nproblems that the weak model cannot accurately predict. For tunately, preliminary experiments in\nthis line of research have shown positive and promising resu lts. For example, Burns et al. [2023a ]\nfound that ﬁne-tuning the strong pre-trained GPT-4 model wi th GPT-2-level supervision could\nimprove generalization across several NLP tasks. To measur e how the weak model improves the\ngeneralization of the strong model, we deﬁne the following t erms:\n•Weak Performance (Pweak). This is the test-set performance of the weak model, which\ncan be regarded as the baseline performance.\n•Weak-to-strong Performance (Pweak →strong ). This is the test-set performance of the strong\nmodel that is ﬁne-tuned with the weak model.\n•Strong Ceiling Performance (Pceiling ). This is the test-set performance of the strong model\nthat is ﬁne-tuned with ground truth data. For example, we ﬁne -tune the strong model with\nhuman-annotated predictions and take the resulting model a s a ceiling.\nThen, the performance gap recovered (PGR ) can be deﬁned as\nPGR = max{\n0,Pweak →strong −Pweak\nPceiling −Pweak}\n(4.11)\nThis metric measures how much of the performance gap between the ceiling model and the\nweak model can be recovered by the weak-to-strong model. A PG R of 1 indicates that the weak-\nto-strong ﬁne-tuning can completely closes the performanc e gap, whereas a PGR of 0 indicates\nno improvement. In Burns et al. [2023a ]’s work, it is shown that PGR can be around 0.8on 22\nNLP classiﬁcation tasks. It should be noted that, while the p otential of weak-to-strong ﬁne-tuning\nis promising, achieving substantial weak-to-strong gener alization remains a challenging goal that\nneeds further investigation [ Aschenbrenner ,2024 ].\nFine-tuning LLMs with weak supervision is just one choice fo r using small models to improve\nlarge models. Although this section primarily focuses on ﬁn e-tuning LLMs, we also mention\nother methods here to give a more complete discussion (see Fi gure 4.5for illustrations of these\nmethods).\n• Instead of using small models to generate synthetic data, i t is also straightforward to in-\ncorporate knowledge distillation loss based on these model s. For example, a simple loss\n\n4.2 Instruction Alignment 171\nLarge Model\nxˆyCompute Loss & Train\nDataset=⇒ xˆySmall Model\nInput Predict\n(a) Fine-tuning on data generated by a small model\n(weak-to-strong generalization)Large Model\nxyCompute Loss & Train\nDataset=⇒ x ySmall ModelKD LossLM Loss\n(b) Fine-tuning with KD Loss from a small model\n(weak-to-strong generalization)\nLarge Model\nxyCompute Loss & Train\nDataset=⇒ x yDataset\nSmall ModelData\nSelection\n(c) Data selection with a small modelSmall Model 2 Small Model 1 Small Model 3\nx x xCombination Modely\n(d) Ensemble of multiple small models\nLarge Model\nxy2\nSmall Model\n(e) Cascading (at inference time)xy1Step 2\n(expensive)Step 1\n(cheap)If Step 1 is not satisfactory, go to Step ",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 143,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "86833fa7acb508ce",
    "content": " 171\nLarge Model\nxˆyCompute Loss & Train\nDataset=⇒ xˆySmall Model\nInput Predict\n(a) Fine-tuning on data generated by a small model\n(weak-to-strong generalization)Large Model\nxyCompute Loss & Train\nDataset=⇒ x ySmall ModelKD LossLM Loss\n(b) Fine-tuning with KD Loss from a small model\n(weak-to-strong generalization)\nLarge Model\nxyCompute Loss & Train\nDataset=⇒ x yDataset\nSmall ModelData\nSelection\n(c) Data selection with a small modelSmall Model 2 Small Model 1 Small Model 3\nx x xCombination Modely\n(d) Ensemble of multiple small models\nLarge Model\nxy2\nSmall Model\n(e) Cascading (at inference time)xy1Step 2\n(expensive)Step 1\n(cheap)If Step 1 is not satisfactory, go to Step 2\nFig. 4.5: Illustrations of using small models to improve large models in LLMs. One approach involves using smaller\nmodels for the ﬁne-tuning or pre-training of larger models. This includes generating synthetic data (a), incorporatin g\nauxiliary loss (b), and selecting appropriate data (c). Ano ther approach involves combining small models and large\nmodels. This includes learning a strong model by aggregatin g multiple small models (d), and cascading small models\nwith large models (e).\nfunction that measures the difference between the small and large models can be deﬁned as:\nLoss kd= KL(Prw(·|x)||Prs\nθ(·|x)) (4.12)\nThen, we can add this loss to the original loss of language mod eling, and yield the following\ntraining objective\n˜θ= arg max\nθ∑\n(x,y)∈Dlog Prs\nθ(y|x)−λ·Loss kd (4.13)\n\n172 Alignment\nwhere Dis the set of input and output pairs, and λis the coefﬁcient of the interpolation. This\nmethod can be employed in either the pre-training or ﬁne-tun ing phase. We can adjust λto\ncontrol how much the small model inﬂuences the training. For example, we can gradually\ndecreaseλto make the training rely more on the original language model ing loss as the\nlarge model becomes more capable.\n• Another approach to involving small models in LLM pre-trai ning and ﬁne-tuning is to use\nthem to do data selection or ﬁltering. Given a sequence, we ca n compute the likelihood\nor cross-entropy using a small model. These quantities can t hen be used as criteria for\nselecting or ﬁltering data. For example, sequences with low likelihood or high cross-entropy\nmight be excluded from the training set, as they are less alig ned with the small model’s\nlearned distribution. Conversely, sequences with high lik elihood or low cross-entropy can\nbe prioritized, ensuring that the training focuses on more r elevant or high-quality data.\n• Ensemble learning is a simple and effective way to build a st rong model by combining mul-\ntiple weak models. Applying this technique to LLMs is straig htforward. We can aggregate\ndistributions predicted by multiple small models or specia lized submodels, and derive the\nﬁnal prediction from the aggregated results. This aggregat ion can be done using methods\nsuch as majority voting, weighted averaging, or stacking.\n• Small models can also be employed at inference time to impro ve overall efﬁciency. Suppose\nwe have a large model that is slow but more accurate, and a smal l model that is fast but\nless accurate. In model cascading, the small model ﬁrst proc esses the input data, quickly\ngenerating preliminary results. If these results meet cert ain pre-deﬁned criteria, they can be\ndirectly used. However, if the initial results are not sufﬁc iently good, the input is then passed\nto the larger, more accurate model to produce a better result . This approach signiﬁcantly\nreduces computational costs and latency, as the small model can effectively handle many\ninputs without access to the large model.\n4.3 Human Preference Alignment: RLHF\nSo far in this chapter, we have focused on ﬁne-tuning LLMs usi ng input-output paired labeled data.\nThis approach allows us to adapt LLMs for instruction-follo wing via supervised learning. In many\napplications, however, LLMs are required not only to follow instructions but also to act in ways\nthat are more aligned",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 144,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "b26a36910b4c92ca",
    "content": " accurate. In model cascading, the small model ﬁrst proc esses the input data, quickly\ngenerating preliminary results. If these results meet cert ain pre-deﬁned criteria, they can be\ndirectly used. However, if the initial results are not sufﬁc iently good, the input is then passed\nto the larger, more accurate model to produce a better result . This approach signiﬁcantly\nreduces computational costs and latency, as the small model can effectively handle many\ninputs without access to the large model.\n4.3 Human Preference Alignment: RLHF\nSo far in this chapter, we have focused on ﬁne-tuning LLMs usi ng input-output paired labeled data.\nThis approach allows us to adapt LLMs for instruction-follo wing via supervised learning. In many\napplications, however, LLMs are required not only to follow instructions but also to act in ways\nthat are more aligned with human values and preferences. Con sider a scenario where a user asks an\nLLM how to hack into a computer system. If the LLM is not approp riately aligned, it may respond\nby providing details on how to perform this illegal activity . Instead, a more desirable response\nmight be to advise the user against engaging in illegal activ ities and offer a general overview of\nthe consequences of such actions. The difﬁculty in achievin g this is that the ethical nuances and\ncontextual considerations required for an LLM to respond ap propriately in such scenarios are not\nalways straightforward to encode into a ﬁne-tuning dataset . What’s even more challenging is that,\noften, humans themselves cannot precisely express their ow n preferences.\nIn this section, we discuss an alternative LLM ﬁne-tuning me thod, called reinforcement learn-\ning from human feedback or RLHF for short [ Christiano et al. ,2017 ;Stiennon et al. ,2020 ]. The\nbasic idea behind RLHF is that LLMs can learn from comparison s of model outputs using reward\n\n4.3 Human Preference Alignment: RLHF 173\nLLM\nxy\nPredicted Token Distributions\n(ntoken distributions)Gold-standard Predictions\n(none-hot distributions)Objective (MLE):\nmax Pr( y|x)\nwhere\nx: input\ny: gold-standard output\n(a) Supervised ﬁne-tuning (maximizing the prediction prob ability given the input)\nLLM\nxy1y2Generate multiple\noutputs via sampling\nPrediction y2Prediction y1Objective (RL Loss Minimization):\nminL(x,{y1,y2},r)Reward Model Human preference datatrain\nwhere\nL(·): loss function\nr(·): reward model\n(b) Reinforcement Learning from Human Feedback\nFig. 4.6: Supervised ﬁne-tuning vs. reinforcement learning from hum an feedback. In supervised ﬁne-tuning, we\noptimize the LLM by maximizing the probability of the predic tion given the input. In reinforcement learning from\nhuman feedback, we ﬁrst train a reward model on human prefere nce data (on each pair of predictions, evaluators are\nasked to choose which one they prefer). Then, we use this rewa rd model to supervise the LLM during ﬁne-tuning.\nmodels (see Figure 4.6). To do this, we can recruit human experts who indicate their preferences\nbetween pairs of outputs generated by the LLM. This preferen ce data is used to train a reward\nmodel that can predict the perceived quality of LLM outputs. Once trained, the reward model\nprovides feedback by assigning scores to new outputs that th e LLM generates in response to the\ninputs. The LLM uses these scores to update its parameters th rough reinforcement learning algo-\nrithms. In the rest of this section, we will ﬁrst introduce th e basic knowledge of reinforcement\nlearning to facilitate the discussion, and then discuss met hods for training reward models and\naligning LLMs with these models.\n4.3.1 Basics of Reinforcement Learning\nWe begin by looking at some basic concepts of reinforcement l earning. Note that the notation used\nhere slightly differs from that used in the previous section s and chapters because we want to make\nour description more consistent with those in the reinforce ment learning literature. Nevertheless,\nwe will show how this notation corresponds to the language mo deling notation. The reader who\nis already familiar with reinforcement learning technique s may skip or skim this subsection.\nA general reinforcement learning framework describes how a n agent interacts with a dynamic\nenvironment. This interaction is modeled as a sequence of ac tions taken by the agent in response\nto the state of the environment. At each time step, the agent o bserves the current",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 145,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "48d937fabf83124a",
    "content": ". In the rest of this section, we will ﬁrst introduce th e basic knowledge of reinforcement\nlearning to facilitate the discussion, and then discuss met hods for training reward models and\naligning LLMs with these models.\n4.3.1 Basics of Reinforcement Learning\nWe begin by looking at some basic concepts of reinforcement l earning. Note that the notation used\nhere slightly differs from that used in the previous section s and chapters because we want to make\nour description more consistent with those in the reinforce ment learning literature. Nevertheless,\nwe will show how this notation corresponds to the language mo deling notation. The reader who\nis already familiar with reinforcement learning technique s may skip or skim this subsection.\nA general reinforcement learning framework describes how a n agent interacts with a dynamic\nenvironment. This interaction is modeled as a sequence of ac tions taken by the agent in response\nto the state of the environment. At each time step, the agent o bserves the current state, chooses an\naction based on its policy, performs the action, and then rec eives feedback from the environment\nin the form of a reward and a new state. This sequence of observ e-act-receive feedback is repeated\n\n174 Alignment\nuntil the agent achieves its goal.\nA reinforcement learning system involves several componen ts\n•Agent . This is the learner or decision-maker in reinforcement lea rning. In the context of\nLLMs, it can be seen as the LLM itself.\n•Environment . This includes everything external to the agent with which t he agent interacts.\nBut the environment in LLMs is less about a physical or virtua l space and more about the\nframework within which the agent (e.g., an LLM) receives fee dback and learns.\n•State (s). A state represents the current situation of the environme nt. Given a sequence of\ntokens for language modeling, a state at a time step can be vie wed as the tokens we observed\nso far, that is, the context tokens we take to predict the next token. For example, we can\ndeﬁne (x,y<t)as the state when predicting the next token at the time step t.\n•Action (a). Actions represent possible decisions the agent can make. We can see them as\npossible predicted tokens in the vocabulary.\n•Reward (R). The reward is the feedback from the environment that evalu ates the success\nof an action. For example, r(s,a,s′)denotes the reward the agent receives for taking the\nactionaat the statesand moving to the next state s′. If the state-action sequence is given,\nwe can denote the reward at the time step tasrt=r(st,at,st+1). Also note that if the\ndecision-making process is deterministic, we can omit st+1because it is can be determined\nbystandat. In such cases, we can use r(st,at)as shorthand for r(st,at,st+1).\n•Policy (π). For an LLM, a policy is deﬁned as the probability distribut ion over the tokens\nthat the LLM predicts, given the preceding context tokens. F ormally, this can be expressed\nas\nπ(a|s) = Pr(yt|x,y<t) (4.14)\nwhereacorresponds to the token y, andscorresponds to the context (x,y<t). Figure 4.7\nillustrates how an LLM can be treated as a policy in the reinfo rcement learning framework.\n•Value Function (VandQ). A state-value function (or value function, for short) assesses\nthe expected discounted return (i.e., accumulated rewards ) for an agent starting from a par-\nticular state sand following a speciﬁc policy π. It is deﬁned as:\nV(s) =E[\nr(s0,a0,s1) +γr(s1,a1,s2) +γ2r(s2,a2,s3) +· · ·⏐⏐s0=s,π]\n=E[\nr0+γr1+γ2r2+· · ·⏐⏐s0=s,π]\n=E[∞∑\nt=0γtrt⏐⏐s0=s,π]\n(4.15)\nwhereγ∈[0,1]is the discount factor that adjusts the importance of future rewards,s0=s\nindicates that the agent starts with the state s, and the expectation Eis performed over all\npossible trajectories (i.e., state-action sequences). Si milarly, an action-value function (or\n\n4.3 Human Preference Alignment: RLHF 175\nx0 x1 ... xm y1 ... yt−1Policy (",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 146,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "6b97c91c24677a78",
    "content": "(s) =E[\nr(s0,a0,s1) +γr(s1,a1,s2) +γ2r(s2,a2,s3) +· · ·⏐⏐s0=s,π]\n=E[\nr0+γr1+γ2r2+· · ·⏐⏐s0=s,π]\n=E[∞∑\nt=0γtrt⏐⏐s0=s,π]\n(4.15)\nwhereγ∈[0,1]is the discount factor that adjusts the importance of future rewards,s0=s\nindicates that the agent starts with the state s, and the expectation Eis performed over all\npossible trajectories (i.e., state-action sequences). Si milarly, an action-value function (or\n\n4.3 Human Preference Alignment: RLHF 175\nx0 x1 ... xm y1 ... yt−1Policy (LLM)y1 y2 ... yt\nState st(xandy<t)Action at\nReward Model\nR(st,at)Value Functions\nV(st)andQ(st,at)Feedback\nFig. 4.7: LLM as policy in reinforcement learning. At each step t, the LLM predicts a token ytgiven the model\ninput xand the previously-generated tokens y<t. This process can be framed as a reinforcement learning prob lem,\nwhereytserves as the action, (x,y<t)as the state, and the predicted distribution Pr(yt|x,y<t)as the policy. Once\nytis predicted, the LLM inputs both (x,y<t)andytto the reward model, which evaluates how effectively the cho sen\ntoken contributes to achieving the desired textual outcome . This evaluation generates reward scores which are used to\ncompute the value functions V(st)andQ(st,at). These functions then provide feedback to the LLM and guide t he\npolicy training.\nQ-value function ) measures the expected return starting from a state staking an action a\nand thereafter following a policy π, given by\nQ(s,a) =E[∞∑\nt=0γtrt⏐⏐s0=s,a0=a,π]\n(4.16)\nwherea0=aindicates that the action taken at the initial state is a.\nThe goal of reinforcement learning is to learn a policy that m aximizes the cumulative re-\nward (orreturn ) the agent receives over the long run. Given a state-action s equenceτ=\n{(s1,a1),...,(sT,aT)}1, the cumulative reward over this sequence can be written as\nR(τ) =T∑\nt=1rt (4.17)\nThe expectation of this cumulative reward over a space of sta te-action sequences is given in\nthe form\nJ(θ) =E\nτ∼D[\nR(τ)⏐⏐πθ]\n=∑\nτ∈DPrθ(τ)R(τ)\n=∑\nτ∈DPrθ(τ)T∑\nt=1rt (4.18)\n1We assume the state-action sequence begins with s1anda1, rather than s0anda0, to align with the notation\ncommonly used in this chapter, where the prediction ytypically starts from y1. Of course, it is also common to denote\na state-action sequence as {(s0,a0),...,(sT,aT)}or{(s0,a0),...,(sT−1,aT−1)}in the literature. But this variation\nin notation does not affect the discussion of the models pres ented here.\n\n176 Alignment\nwhereτ∼ D indicates that τis drawn from the state-action sequence space D, and the subscript\nθindicates the parameters of the policy. J(θ)is also called the performance function .\nThen the training objective is to maximize J(θ):\n˜θ= arg max\nθJ(θ) (4.19)\nNow, we have a simple reinforcement learning approach: 1) we sample a number of state-\naction sequences; then, 2) we evaluate each sequence using t he performance function; then, 3) we\nupdate the model to maximize this performance function. If w e take Eq. ( 4.18) and use gradient\ndescent to optimize the policy, this approach would constit utes a form of policy gradient methods\n[Williams ,1992 ].\nNote that in many NLP problems, such as machine translation, rewards are typically sparse.\nFor instance, a reward is only received at the end of a complet e sentence. This means that rt= 0\nfor allt < T , andrtis non",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 147,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "3aa317f1e30b7ede",
    "content": " subscript\nθindicates the parameters of the policy. J(θ)is also called the performance function .\nThen the training objective is to maximize J(θ):\n˜θ= arg max\nθJ(θ) (4.19)\nNow, we have a simple reinforcement learning approach: 1) we sample a number of state-\naction sequences; then, 2) we evaluate each sequence using t he performance function; then, 3) we\nupdate the model to maximize this performance function. If w e take Eq. ( 4.18) and use gradient\ndescent to optimize the policy, this approach would constit utes a form of policy gradient methods\n[Williams ,1992 ].\nNote that in many NLP problems, such as machine translation, rewards are typically sparse.\nFor instance, a reward is only received at the end of a complet e sentence. This means that rt= 0\nfor allt < T , andrtis non-zero only when t=T. Ideally, one might prefer feedback to\nbe immediate and frequent (dense), and thus the training of t he policy can be easier and more\nefﬁcient. While several methods have been proposed to addre ss sparse rewards, such as reward\nshaping, we will continue in our discussion to assume a spars e reward setup, where the reward is\navailable only upon completing the prediction.\nThe model described in Eqs. ( 4.17-4.19) establishes a basic form of reinforcement learning,\nand many variants and improvements of this model have been de veloped. Before showing those\nmore sophisticated models, let us take a moment to interpret the objective function J(θ)from the\nperspective of policy gradient. In gradient descent, we nee d to compute the gradient of J(θ)with\nrespect toθ:\n∂J(θ)\n∂θ=∂∑\nτ∈DPrθ(τ)R(τ)\n∂θ\n=∑\nτ∈D∂Prθ(τ)\n∂θR(τ)\n=∑\nτ∈DPrθ(τ)∂Prθ(τ)/∂θ\nPrθ(τ)R(τ)\n=∑\nτ∈DPrθ(τ)∂log Pr θ(τ)\n∂θR(τ) (4.20)\nIn some cases, we will assume that every sequence in Dis equally probable (i.e., Prθ(τ) =\n1/|D|). In this case we can simplify Eq. ( 4.20) and need only consider the terms∂log Pr θ(τ)\n∂θand\nR(τ):\n∂J(θ)\n∂θ=1\nm∑\nτ∈D∂log Pr θ(τ)\n∂θR(τ) (4.21)\nOne advantage of this result is that R(τ)does not need to be differentiable, which means that we\ncan use any type of reward function in reinforcement learnin g.\nBy treating the generation of the sequence τas a Markov decision process, we can further\n\n4.3 Human Preference Alignment: RLHF 177\nderive∂log Pr θ(τ)\n∂θ, and obtain:\n∂log Pr θ(τ)\n∂θ=∂\n∂θlogT∏\nt=1πθ(at|st) Pr(st+1|st,at)\n=∂\n∂θT∑\nt=1logπθ(at|st)\npolicy+∂\n∂θT∑\nt=1log Pr(st+1|st,at)\ndynamics(4.22)\nwhere the gradient is decomposed into two parts: the policy g radient and the dynamics gradient.\nThe policy component, logπθ(at|st), determines the log-probability of taking action atgiven\nstatest, and it is parameterized by θ. The dynamics component, log Pr(st+1|st,at), represents\nthe log-probability of transitioning to state st+1from statestafter taking action at. In typical\nreinforcement learning settings, the dynamics are not dire ctly inﬂuenced by the policy parameters\nθ, and thus, their derivatives are often zero. In this case, th erefore, Eq. ( 4.22) can be simpliﬁed to:\n∂log Pr θ(τ)\n∂θ=∂\n∂θT∑\nt=1logπθ(at",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 148,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "73cd00d0e32f7b46",
    "content": "��\ndynamics(4.22)\nwhere the gradient is decomposed into two parts: the policy g radient and the dynamics gradient.\nThe policy component, logπθ(at|st), determines the log-probability of taking action atgiven\nstatest, and it is parameterized by θ. The dynamics component, log Pr(st+1|st,at), represents\nthe log-probability of transitioning to state st+1from statestafter taking action at. In typical\nreinforcement learning settings, the dynamics are not dire ctly inﬂuenced by the policy parameters\nθ, and thus, their derivatives are often zero. In this case, th erefore, Eq. ( 4.22) can be simpliﬁed to:\n∂log Pr θ(τ)\n∂θ=∂\n∂θT∑\nt=1logπθ(at|st) (4.23)\nIn other words, we only concentrate on optimizing the policy without concerning ourselves with\nthe underlying dynamics.\nSubstituting Eq. ( 4.23) into Eq. ( 4.21), and expanding R(τ), we then obtain\n∂J(θ)\n∂θ=1\n|D|∑\nτ∈D∂\n∂θ(T∑\nt=1logπθ(at|st)T∑\nt=1rt)\n(4.24)\nWhile this policy gradient approach is straightforward, it suffers from the problem that the\nvariance of the estimated gradients can be very high, making the learning process noisy and inef-\nﬁcient. One reason for this high variance problem is that rew ards can vary greatly across different\nsteps or scenarios. Imagine that in a sequence of action deci sions, the reward model tends to assign\nsmall rewards to good actions (e.g., Rt= 2) and large penalties to poor actions (e.g., Rt=−50).\nSuch varying reward scales for good and poor actions can resu lt in a very low total reward for the\nentire sequence, even if it includes good actions.\nOne simple method for reducing the variance of the gradient i s to set a baseline band subtract\nit from∑T\nt=1rt, resulting in∑T\nt=1rt−b.2Here, the baseline can be interpreted as a reference\npoint. By centering the rewards around this baseline, we rem ove systematic biases in the reward\nsignal, making the updates more stable and less sensitive to extreme ﬂuctuations in individual\nrewards.\n2In fact, the use of a baseline bdoes not change the variance of the total rewards∑T\nt=1rt. However, it is important\nto note that while introducing a baseline does not alter the o verall variance of the rewards, it helps reduce the variance\nof the gradient estimates. This is because subtracting the b aseline from the total rewards effectively reduces ﬂuctuat ions\naround their mean, which makes the gradient estimates more s table. In general, the operation∑T\nt=1rt−bcenters the\nrewards around zero (e.g., bis deﬁned as the expected value of∑T\nt=1rt), which can lead to reduced variance in the\nproduct∑T\nt=1logπθ(at|st)(∑T\nt=1rt−b).\n\n178 Alignment\nThis policy gradient model with a baseline can be given by\n∂J(θ)\n∂θ=1\n|D|∑\nτ∈D∂\n∂θ(T∑\nt=1logπθ(at|st))( T∑\nt=1rt−b)\n=1\n|D|∑\nτ∈D∂\n∂θ[T∑\nt=1logπθ(at|st)(T∑\nk=1rk−b)]\n=1\n|D|∑\nτ∈D∂\n∂θ[T∑\nt=1logπθ(at|st)(t−1∑\nk=1rk+T∑\nk=trk−b)]\n(4.25)\nHere we write∑T\nk=1rkas the sum of two terms∑t−1\nk=1rkand∑T\nk=trkto distinguish between the\nrewards accrued before and after the action at time step t. Note that in Markov decision processes,\nthe future is independent of the past given the present. Ther efore, the action taken at time step t\ncannot inﬂuence the",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 149,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "8d9bbaf9d7907e22",
    "content": "−b)\n=1\n|D|∑\nτ∈D∂\n∂θ[T∑\nt=1logπθ(at|st)(T∑\nk=1rk−b)]\n=1\n|D|∑\nτ∈D∂\n∂θ[T∑\nt=1logπθ(at|st)(t−1∑\nk=1rk+T∑\nk=trk−b)]\n(4.25)\nHere we write∑T\nk=1rkas the sum of two terms∑t−1\nk=1rkand∑T\nk=trkto distinguish between the\nrewards accrued before and after the action at time step t. Note that in Markov decision processes,\nthe future is independent of the past given the present. Ther efore, the action taken at time step t\ncannot inﬂuence the rewards received before t, or in other words, the rewards prior to tare already\n“ﬁxed” by the time the action at tis chosen. The term∑t−1\nk=1rkdoes not contribute to the gradient\nand can be omitted, leading to a simpliﬁed version of Eq. ( 4.25)\n∂J(θ)\n∂θ=1\n|D|∑\nτ∈D∂\n∂θ[T∑\nt=1logπθ(at|st)(T∑\nk=trk−b)]\n(4.26)\nAlso note that removing∑T\nk=trkcan further reduce the variance of the gradient.\nThere are many ways to deﬁne the baseline b. Here we consider the value function of the state\nst, that is, the estimated value of being in state st:V(st) =E(rt+rt+1+· · ·+rT). Hence we\nhave\nA(st,at) =T∑\nk=trk−b\n=T∑\nk=trk−V(st) (4.27)\nwhere∑T\nk=trkrepresents the actual return received, and V(st)represents the expected return.\nA(st,at)(orAtfor short) is called the advantage at time step t, which quantiﬁes the relative\nbeneﬁt of the action atcompared to the expected value of following the policy from t he statest\nonward.\nBy using the advantage function A(st,at), the gradient of J(θ)can be written in the form\n∂J(θ)\n∂θ=1\n|D|∑\nτ∈D∂\n∂θ(T∑\nt=1logπθ(at|st)A(st,at))\n(4.28)\nThis optimization objective corresponds to the advantage actor-critic (A2C ) method in re-\ninforcement learning [ Mnih et al. ,2016 ]. In this method, the actor aims at learning a policy. It\nupdates the policy parameters using Eq. ( 4.28) to help focus more on actions that are likely to\nimprove performance. The critic, on the other hand, updates its estimation of the value function,\nwhich is used to calculate the advantage function A(st,at), thus serving as the evaluator of the\n\n4.3 Human Preference Alignment: RLHF 179\npolicy being learned by the actor.\nIn the A2C method, A(st,at)is typically expressed as the difference of the action-valu e func-\ntionQ(st,at)and the state-value function V(st)\nA(st,at) =Q(st,at)−V(st) (4.29)\nAt ﬁrst glance, this model may seem challenging to develop be cause it requires two separate sub-\nmodels to calculate Q(st,at)andV(st)respectively. Fortunately, considering that Q(st,at)can\nbe deﬁned as the return rt+V(st+1), we can rewrite Eq. ( 4.29) as\nA(st,at) =rt+V(st+1)−V(st) (4.30)\nor alternatively, introduce the discount factor γto obtain a more general form\nA(st,at) =rt+γV(st+1)−V(st) (4.31)\nA(st,at) =rt+γV(st+1)−V(st)is also called the temporal difference (TD) error. What\nwe need is to train a critic network for the value function V(st), and then use it to compute the\nadvantage function3.\nUp to this point, we have spent considerable space discuss",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 150,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "b3b3ae869eb17f2e",
    "content": ", this model may seem challenging to develop be cause it requires two separate sub-\nmodels to calculate Q(st,at)andV(st)respectively. Fortunately, considering that Q(st,at)can\nbe deﬁned as the return rt+V(st+1), we can rewrite Eq. ( 4.29) as\nA(st,at) =rt+V(st+1)−V(st) (4.30)\nor alternatively, introduce the discount factor γto obtain a more general form\nA(st,at) =rt+γV(st+1)−V(st) (4.31)\nA(st,at) =rt+γV(st+1)−V(st)is also called the temporal difference (TD) error. What\nwe need is to train a critic network for the value function V(st), and then use it to compute the\nadvantage function3.\nUp to this point, we have spent considerable space discussin g the basics of reinforcement\nlearning, especially on how to derive the optimization obje ctive for the A2C method. However,\nreinforcement learning is a vast ﬁeld, and many technical de tails cannot be covered here. The in-\nterested reader can refer to reinforcement learning books f or more details [ Sutton and Barto ,2018 ;\nSzepesvári ,2010 ]. Nevertheless, we now have the necessary knowledge to furt her discuss RLHF.\nIn the subsequent subsections, we will return to the discuss ion on LLM alignment, demonstrating\nhow to use the A2C method for aligning with human preferences .\n4.3.2 Training Reward Models\nWe have shown that reward models play a very important role in the general reinforcement learn-\ning framework and form the basis for computing value functio ns. We now consider the problem\nof training these reward models.\nIn RLHF, a reward model is a neural network that maps a pair of i nput and output token\nsequences to a scalar. Given an input xand an output y, the reward can be expressed as\nr= Reward( x,y) (4.33)\nwhere Reward( ·)is the reward model. rcan be interpreted as a measure of how well the output y\naligns with the desired behavior given the input x. As discussed in the previous subsection, both x\n3The training loss for the value network (or critic network) i n A2C is generally formulated as the mean squared\nerror between the computed return rt+γV(st+1)and the predicted state value V(st). Suppose that the value network\nis parameterized by ω. The loss function is given by\nLv(ω) =1\nM∑(\nrt+γVω(st+1)−Vω(st))2(4.32)\nwhereMis the number of training samples, for example, for a sequenc e ofTtokens, we can set M=T.\n\n180 Alignment\nx0x1x2 · · ·xmy1y2 · · ·yn\n(Last Token ⟨EOS ⟩)hx0hx1hx2 · · · hxm hy1hy2 · · · hlast\nTransformer Decoder (LLM)Representation\nat Each PositionReward (Scalar)\nWr Linear Map\nFig. 4.8: Architecture of the reward model based on Transformer. The m ain component of this model is still an LLM.\nWe use the Transformer decoder as the sequence representati on model. We extract the representation of the last position\nof the decoder as the representation of the entire sequence [x,y]. We then map this representation to a scalar through a\nlinear transformation, which serves as the reward score for [x,y].\nandyare assumed to complete texts. This means that the reward mod el evaluates the relationship\nbetween inputs and outputs that provide full semantic conte nt. For example, when applying the\nreward model, it assigns a value of 0 (or another predetermin ed value) at each position tin the\noutput sequence y=y1...yn. Only at the ﬁnal position, when t=n, does the reward model\ngenerate the actual reward score. To keep the notation unclu ttered, we will use r(x,y)to denote\nthe reward model Reward( x,y)from here on.\nThere are many ways to implement the reward model. One simple approach is to build the\nreward model based on a pre-trained LLM. More speciﬁcally, w e can concatenate xandyto form\na single token sequence seqx,y= [x,y]. We run a pre-trained LLM on this sequence, as usual,\nand at each position, we obtain a representation from the top -most Transformer layer. Then, we\ntake the representation at the last position (denoted by hlast",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 151,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "abd4d8bdaeb4a561",
    "content": ". For example, when applying the\nreward model, it assigns a value of 0 (or another predetermin ed value) at each position tin the\noutput sequence y=y1...yn. Only at the ﬁnal position, when t=n, does the reward model\ngenerate the actual reward score. To keep the notation unclu ttered, we will use r(x,y)to denote\nthe reward model Reward( x,y)from here on.\nThere are many ways to implement the reward model. One simple approach is to build the\nreward model based on a pre-trained LLM. More speciﬁcally, w e can concatenate xandyto form\na single token sequence seqx,y= [x,y]. We run a pre-trained LLM on this sequence, as usual,\nand at each position, we obtain a representation from the top -most Transformer layer. Then, we\ntake the representation at the last position (denoted by hlast) and map it to a scalar via linear\ntransformation:\nr(x,y) = hlastWr (4.34)\nwhere hlastis ad-dimensional vector, and Wris ad×1linear mapping matrix. This architecture\nof the reward model is illustrated in Figure 4.8.\nTo train the reward model, the ﬁrst step is to collect human fe edback on a set of generated\noutputs. Given an input x, we use the LLM to produce multiple candidate outputs {y1,...,yN}.\nHuman feedback can be obtained in several ways:\n•Pairwise Comparison (Pairwise Ranking ). Given two different outputs, human experts\nselect which one is better.\n•Rating . Human experts provide a score or rating to each output. This score is often a\ncontinuous or discrete numerical value, such as a score on a s cale (e.g., 1-5 stars, or 1-10\npoints). In some cases, the rating might be binary, indicati ng a “yes/no” or “positive/nega-\ntive” preference.\n\n4.3 Human Preference Alignment: RLHF 181\n•Listwise Ranking . Human experts are asked to rank or order the given set of poss ible\noutputs.\nHere we consider pairwise comparison feedback as it is one of the simplest and most com-\nmon forms of human feedback used in RLHF. In this setting, eac h time, two outputs (ya,yb)are\nrandonly drawn from the candidate pool {y1,...,yN}. Human experts are then presented with\nthese pairs and asked to decide which output they prefer base d on speciﬁc criteria, such as clarity,\nrelevance, and accuracy. The human feedback can be encoded a s a binary label, ya≻ybfor a\npreference for ya, and yb≻yafor a preference for yb.\nOne simple and widely used model for describing such pairwis e comparisons is the Bradley-\nTerry model [Bradley and Terry ,1952 ]. It is a probabilistic model that estimates the probabilit y\nthat one item is preferred over another. Adapting this model to the notation used here, we can\nwrite the probability that yais preferred over ybin the form\nPr(ya≻yb|x) =er(x,ya)\ner(x,ya)+er(x,yb)\n=er(x,ya)−r(x,yb)\ner(x,ya)−r(x,yb)+ 1\n= Sigmoid( r(x,ya)−r(x,yb)) (4.35)\nWhen training the reward model, we want to maximize this pref erence probability. A loss\nfunction based on the Bradley-Terry model is given by\nLr(φ) = −E(x,ya,yb)∼D r[log Pr φ(ya≻yb|x)](4.36)\nwhere (x,ya,yb)is drawn from a human-annotated dataset Drconsisting of preference pairs of\noutputs and their corresponding inputs. φrepresents the parameters of the reward model, which\nincludes both the parameters of the Transformer decoder and the linear mapping matrix Wr. In\npractice, assuming (x,ya,yb)is uniformly sampled from Dr, we can replace the expectation with\na summation\nLr(φ) = −1\n|Dr|∑\n(x,ya,yb)∈Drlog Pr φ(ya≻yb|x) (4.37)\nThe goal of training the reward model is to ﬁnd the optimal par ameters ˆφthat minimize this\nloss function, given by\nˆφ= arg min\nφLr(φ) (4.38)\nSince the reward model itself is also an LLM, we can directly r euse the Transformer training\n",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 152,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "bb069f7383b7e03d",
    "content": "4.36)\nwhere (x,ya,yb)is drawn from a human-annotated dataset Drconsisting of preference pairs of\noutputs and their corresponding inputs. φrepresents the parameters of the reward model, which\nincludes both the parameters of the Transformer decoder and the linear mapping matrix Wr. In\npractice, assuming (x,ya,yb)is uniformly sampled from Dr, we can replace the expectation with\na summation\nLr(φ) = −1\n|Dr|∑\n(x,ya,yb)∈Drlog Pr φ(ya≻yb|x) (4.37)\nThe goal of training the reward model is to ﬁnd the optimal par ameters ˆφthat minimize this\nloss function, given by\nˆφ= arg min\nφLr(φ) (4.38)\nSince the reward model itself is also an LLM, we can directly r euse the Transformer training\nprocedure to optimize the reward model. The difference from training a standard LLM is that we\nonly need to replace the cross-entropy loss with the pairwis e comparison loss as described in Eq.\n(4.37). After the training of the reward model, we can apply the tra ined reward model rˆφ(·)to\nsupervise the target LLM for alignment.\nIt is worth noting that although we train the reward model to p erform pairwise ranking, we\napply it to score each input-output pair independently duri ng the alignment process. The pairwise\n\n182 Alignment\nranking objective ensures that the reward model is sensitiv e to subtle differences between outputs,\nbut we rely on the continuous scores produced by the reward mo del to guide the optimization of\nthe LLM. An advantage of this approach is that we can choose fr om or combine various ranking\nloss functions, and still apply the resulting reward models in the same way as we have done in this\nsubsection. This consistency ensures a uniﬁed framework fo r aligning the LLM, regardless of the\nspeciﬁc ranking loss used during reward model training.\n4.3.3 Training LLMs\nHaving obtained the reward model, we then train the policy (i .e., the LLM) via the A2C method.\nRecall from Section 4.3.1 that a state-action sequence or trajectory τcan be evaluated by the utility\nfunction\nU(τ;θ) =T∑\nt=1logπθ(at|st)A(st,at) (4.39)\nwhereA(st,at)is the advantage of taking the action atgiven the state st. An estimate of A(st,at)\nis deﬁned as the TD error rt+γV(st+1)−V(st), where the value function V(st)is trained with\nthe reward model.\nGiven this utility function, the A2C-based loss function ca n be written in the form\nL(θ) = −Eτ∼D[U(τ;θ)]\n=−Eτ∼D[T∑\nt=1logπθ(at|st)A(st,at)](4.40)\nwhere Dis a space of state-action sequences. As usual, the goal of tr aining the policy is to\nminimize this loss function\n˜θ= arg min\nθL(θ) (4.41)\nIf we map the problem back to the language modeling problem an d adopt the notation from\nLLMs, the loss function can be written as:\nL(θ) = −E(x,y)∼D[U(x,y;θ)](4.42)\nwhere\nU(x,y;θ) =T∑\nt=1logπθ(yt|x,y<t)A(x,y<t,yt) (4.43)\nHereπθ(yt|x,y<t) = Pr θ(yt|x,y<t)is the LLM parameterized by θ.\nIn general, we do not have a human annotated input-output dat asetDin RLHF, but a dataset\ncontaining inputs only. The outputs, in this case, are typic ally the predictions made by the LLM.\n\n4.3 Human Preference Alignment: RLHF 183\nThe loss function is then deﬁned as\nL(θ) = −Ex∼DEy∼πθ(·|x)[U(x,y;θ)](4.44)\nwhere Ddenotes the input-only dataset, and y∼πθ(·|x)denotes that the output yis sampled by\nthe policyπθ(·|x).\nThe above formulation provides a basic form of the A2C method for LLMs. Improved versions\nof this model are more commonly used in RLHF",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 153,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "f94670fae2b76fea",
    "content": "πθ(yt|x,y<t) = Pr θ(yt|x,y<t)is the LLM parameterized by θ.\nIn general, we do not have a human annotated input-output dat asetDin RLHF, but a dataset\ncontaining inputs only. The outputs, in this case, are typic ally the predictions made by the LLM.\n\n4.3 Human Preference Alignment: RLHF 183\nThe loss function is then deﬁned as\nL(θ) = −Ex∼DEy∼πθ(·|x)[U(x,y;θ)](4.44)\nwhere Ddenotes the input-only dataset, and y∼πθ(·|x)denotes that the output yis sampled by\nthe policyπθ(·|x).\nThe above formulation provides a basic form of the A2C method for LLMs. Improved versions\nof this model are more commonly used in RLHF. In the following discussion, we will still use\nthe reinforcement learning notation to simplify the presen tation and will get back the language\nmodeling notation later.\nOne common improvement of policy gradient methods is to use importance sampling to\nreﬁne the estimation of U(τ;θ). This can be written as\nU(τ;θ) =T∑\nt=1πθ(at|st)\nπθref(at|st)A(st,at) (4.45)\nHere we replace the log-probability logπθ(at|st)with the ratioπθ(at|st)\nπθref(at|st).θrefdenotes the pa-\nrameters of the previous policy (such as an initial model fro m which we start the training). So\nπθ(at|st)\nπθref(at|st), also called the ratio function , can be interpreted as the log-probability ratio between\nthe current policy πθand the previous policy πθref(call it the reference policy). By using the\nratio function we reweight the observed rewards based on the likelihood of the actions under the\ncurrent policy versus the reference policy. Whenπθ(at|st)\nπθref(at|st)>1, the actionatis more favored by\nthe current policy compared to the reference policy. By cont rast, whenπθ(at|st)\nπθref(at|st)<1, the action\natis less favored by the current policy4.\n4Consider a more general case where we wish to evaluate the pol icy using its expected reward (also see Eq. ( 4.18))\nJ(θ) =Eτ∼πθ[\nR(τ)]\n(4.46)\nHereτ∼πθmeans that the sequence τis generated by the policy πθ. Alternatively, we can write J(θ)in another form\nJ(θ) =Eτ∼πθref[Prθ(τ)\nPrθref(τ)R(τ)]\n(4.47)\nIt is not difﬁcult to ﬁnd that the right-hand sides of these eq uations are essentially the same since\nEτ∼πθref[\nPrθ(τ)\nPrθref(τ)R(τ)]\n=∑\nτPrθref(τ)Prθ(τ)\nPrθref(τ)R(τ) =∑\nτPrθ(τ)R(τ) =Eτ∼πθ[\nR(τ)]\nNote that this equivalence holds only when the expectation i s performed over the entire sequence space. In practice,\nhowever, we often only sample a relatively small number of se quences using a policy in policy learning. As a result,\nthe sampling method itself matters. Eq. ( 4.47) offers an interesting manner to separate the sampling and r eward\ncomputation processes: we ﬁrst use a baseline policy (with θref) to sample a number of sequences, and then use the\ntarget policy (with θ) to compute the expected reward. In this way, we separate the policy used for collecting the data,\nand the policy used for computing the gradient. This approac h avoids the need to directly sample from the policy we are\nevaluating, which can be beneﬁcial in cases where generatin g sequences from the target policy is expensive or difﬁcult.\nIn reinforcement learning, Eτ∼πθref[\nPrθ(τ)\nPrθref(τ)R(τ)]\nis often called a surrogate objective .\nEq. ( 4.47) can also be interpreted from a policy gradient perspective . ForEτ∼πθref[\nPrθ(τ)\nPrθref(τ)R(τ)]\n, the gradient at\nθ=θrefis given by\n∂\n",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 154,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "71c8a4c995d08205",
    "content": ": we ﬁrst use a baseline policy (with θref) to sample a number of sequences, and then use the\ntarget policy (with θ) to compute the expected reward. In this way, we separate the policy used for collecting the data,\nand the policy used for computing the gradient. This approac h avoids the need to directly sample from the policy we are\nevaluating, which can be beneﬁcial in cases where generatin g sequences from the target policy is expensive or difﬁcult.\nIn reinforcement learning, Eτ∼πθref[\nPrθ(τ)\nPrθref(τ)R(τ)]\nis often called a surrogate objective .\nEq. ( 4.47) can also be interpreted from a policy gradient perspective . ForEτ∼πθref[\nPrθ(τ)\nPrθref(τ)R(τ)]\n, the gradient at\nθ=θrefis given by\n∂\n∂θEτ∼πθref[Prθ(τ)\nPrθref(τ)R(τ)]⏐⏐⏐\nθ=θref=Eτ∼πθref[∂Prθ(τ)|θ=θref\n∂θR(τ)]\n(4.48)\nThe right-hand side is a standard form used in policy gradien t methods, meaning that we compute the direction of\n\n184 Alignment\nA problem with the model presented in Eq. ( 4.47) (as well as in Eq. ( 4.39)) is that the\nvariance in the gradient estimates is often high, making the learning process unstable. To mitigate\nthis issue, techniques such as clipping are often employed t o bound the importance weights and\nprevent large updates. A clipped version of the utility func tion (also called the clipped surrogate\nobjective function) is given by\nUclip(τ;θ) =T∑\nt=1Clip(πθ(at|st)\nπθref(at|st))\nA(st,at) (4.49)\nClip(πθ(at|st)\nπθref(at|st))\n= min(πθ(at|st)\nπθref(at|st),bound(πθ(at|st)\nπθref(at|st),1−ǫ,1 +ǫ))\n(4.50)\nHere the function bound(πθ(at|st)\nπθref(at|st),1−ǫ,1 +ǫ)constrains the ratio function to the range [1−\nǫ,1 +ǫ].\nA further improvement to the above model is to consider trust regions in optimization [ Schulman et al. ,\n2015 ]. In reinforcement learning, a large update to the policy ca n lead to instability, where the\nagent may start performing worse after an update. A reasonab le idea is to optimize the model in\nthe trust region, which refers to a region around the current parameter estimate where the model\nis well-behaved. One approach to incorporating trust regio ns is to impose a constraint on the size\nof the policy update, ensuring that the current policy does n ot deviate too signiﬁcantly from the\nreference policy. This can be achieved by adding a penalty ba sed on some form of divergence\nbetween the current and reference policies to the objective function. A simple form of such a\npenalty is given by the difference in the log-probability of the sequence τunder the current policy\nversus the reference policy:\nPanalty = log πθ(τ)−logπθref(τ) (4.51)\nIn practice, this penalty can be approximated by considerin g only the policy probabilities and\nignoring the dynamics. This gives\nPenalty =T∑\nt=1logπθ(at|st)−T∑\nt=1logπθref(at|st) (4.52)\nBy including this penalty in the optimization objective, we encourage the current policy to remain\nclose to the reference policy, limiting very large updates t hat could destabilize learning.\nWe can incorporate this penalty into the clipped surrogate o bjective function, and obtain\nUppo-clip(τ;θ) =Uclip(τ;θ)−βPenalty (4.53)\nwhereβis the weight of the penalty. This training method is called proximal policy optimization\n(PPO ), which is one of the most popular reinforcement learning me thods used in LLMs and many\nother ﬁelds [ Schulman et al. ,2017 ].\nNow we can write the objective of training LLMs in the form of P PO.\nthe parameter update at the point θ=θrefon the optimization surface.\n\n4.3 Human",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 155,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "951e585e6bdc5f4e",
    "content": "πθ(at|st)−T∑\nt=1logπθref(at|st) (4.52)\nBy including this penalty in the optimization objective, we encourage the current policy to remain\nclose to the reference policy, limiting very large updates t hat could destabilize learning.\nWe can incorporate this penalty into the clipped surrogate o bjective function, and obtain\nUppo-clip(τ;θ) =Uclip(τ;θ)−βPenalty (4.53)\nwhereβis the weight of the penalty. This training method is called proximal policy optimization\n(PPO ), which is one of the most popular reinforcement learning me thods used in LLMs and many\nother ﬁelds [ Schulman et al. ,2017 ].\nNow we can write the objective of training LLMs in the form of P PO.\nthe parameter update at the point θ=θrefon the optimization surface.\n\n4.3 Human Preference Alignment: RLHF 185\nU(x,y;θ) =Uppo-clip(x,y;θ)−βPenalty (4.54)\nwhere\nUppo-clip(x,y;θ) =T∑\nt=1Clip(πθ(yt|x,y<t)\nπθref(yt|x,y<t))\nA(x,y<t,yt) (4.55)\nPenalty = log Pr θ(y|x)−log Pr θref(y|x)\n=T∑\nt=1log Pr θ(yt|x,y<t)−T∑\nt=1log Pr θref(yt|x,y<t) (4.56)\nAlthough the notation here appears a bit tedious, the idea of PPO is simple: we develop an\nobjective by combining the clipped likelihood ratio of the t arget and reference policies with an\nadvantage function, and then impose a penalty that ensures p olicy updates are not too large. The\nPPO-based RLHF is illustrated in Figure 4.9.\nTo summarize, implementing RLHF requires building four mod els, all based on the Trans-\nformer decoder architecture.\n•Reward Model (rφ(·)whereφdenotes the parameters). The reward model learns from\nhuman preference data to predict the reward for each pair of i nput and output token se-\nquences. It is a Transformer decoder followed by a linear lay er that maps a sequence (the\nconcatenation of the input and output) to a real-valued rewa rd score.\n•Value Model orValue Function (Vω(·)whereωdenotes the parameters). The value func-\ntion receives reward scores from the reward model and is trai ned to predict the expected\nsum of rewards that can be obtained starting from a state. It i s generally based on the same\narchitecture as the reward model.\n•Reference Model (πθref(·) = Pr θref(·)whereθrefdenotes the parameters). The reference\nmodel is the baseline LLM that serves as a starting point for p olicy training. In RLHF, it\nrepresents the previous version of the model or a model train ed without human feedback. It\nis used to perform sampling over the space of outputs and cont ribute to the loss computation\nfor policy training.\n•Target Model orPolicy (πθ(·) = Pr θ(·)whereθdenotes the parameters). This policy\ngoverns how the LLM decides the most appropriate next token g iven its context. It is trained\nunder the supervision of both the reward model and the value m odel.\nIn practice, these models need to be trained in a certain orde r. First, we need to initialize them\nusing some other models. For example, the reward model and th e value model can be initialized\nwith a pre-trained LLM, while the reference model and the tar get model can be initialized with a\nmodel that has been instruction ﬁne-tuned. Note that, at thi s point, the reference model is ready for\nuse and will not be further updated. Second, we need to collec t human preference data and train the\nreward model on this data. Third, both the value model and the policy are trained simultaneously\nusing the reward model. At each position in an output token se quence, we update the value model\n\n186 Alignment\nReward Model\nTraining\nPolicy TrainingReward Model\nrφ(x,y)To Learn\nLLM Policy\nPrθ(yt|x,y<t)To Learn\nValue Function\nVω(x,y<t)To Learn\nRef Model\nPrθold(yt|x,y<t)FixedMinimizing the loss based on\nthe Bradley-Terry model\nmin\nφ−1\n|Dr|∑\n(x,ya,yb)",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 156,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "a969ad4932ed8e0d",
    "content": "LM, while the reference model and the tar get model can be initialized with a\nmodel that has been instruction ﬁne-tuned. Note that, at thi s point, the reference model is ready for\nuse and will not be further updated. Second, we need to collec t human preference data and train the\nreward model on this data. Third, both the value model and the policy are trained simultaneously\nusing the reward model. At each position in an output token se quence, we update the value model\n\n186 Alignment\nReward Model\nTraining\nPolicy TrainingReward Model\nrφ(x,y)To Learn\nLLM Policy\nPrθ(yt|x,y<t)To Learn\nValue Function\nVω(x,y<t)To Learn\nRef Model\nPrθold(yt|x,y<t)FixedMinimizing the loss based on\nthe Bradley-Terry model\nmin\nφ−1\n|Dr|∑\n(x,ya,yb)∈D rlogσ(rφ(x,ya)−rφ(x,yb))Human preference data Dr={(x,ya,yb)}\nInput-only data D={x}x0 x1 · · · xm y1 · · · yt−1\nState (x,y<t)x1 x2 · · · y1 y2 · · · ytAction yt\n(sampled with Prθold)Evaluate the state-action pair using the advantage\nfunction or the TD error (based on the reward\nmodel and the value function)\nLLM Policy\nLLM Policy\nMinimizing the clipped PPO loss\nwith the penalty\nmin\nθ−∑\nx∈D,y∼Prθold(·|x)∑T\nt=1[\nClip(Prθ(yt|x,y<t)\nPrθold(yt|x,y<t))\nAt−\nβ·(\nlog Pr θ(yt|x,y<t)−\nlog Pr θold(yt|x,y<t))]Value Function\nMinimizing the MSE between the\ncomputed return and the predicted\nstate value\nmin\nω1\nM∑\nx∈D∑T\nt=1\n[\nrt+γVω(x,y<t+1)−Vω(x,y<t)]2\n∗∗rt=r(x,y<t+1)denotes the reward received as step t.\n∗∗Atdenotes the advantage at step t, and can be deﬁned as rt+γVω(x,y<t+1)−Vω(x,y<t)\nFig. 4.9: Illustration of RLHF. The ﬁrst step is to collect human prefe rence data and train the reward model using this\ndata. Once the reward model is optimized, along with the refe rence model, we proceed to train both the policy and\nthe value function. At each prediction step, we compute the s um of the PPO-based loss and update the parameters of\nthe policy. This requires access to the reward model, the ref erence model, and the value function at hand. At the same\ntime, we update the parameters of the value function by minim izing the MSE loss.\nby minimizing the MSE error of value predition, and the polic y is updated by minimizing the PPO\nloss.\n\n4.4 Improved Human Preference Alignment 187\n4.4 Improved Human Preference Alignment\nIn the previous section, we reviewed the basic concepts of re inforcement learning and the general\nframework of RLHF. In this section, we will discuss some reﬁn ements of RLHF and alternative\nmethods to achieve human preference alignment.\n4.4.1 Better Reward Modeling\nIn Section 4.3.2 , we highlighted the task of learning from human preferences as well as the use\nof pairwise ranking loss for training reward models. Here we consider more methods for reward\nmodeling. Our discussion will be relatively general, and si nce the reward model is widely used in\nmany reinforcement learning problems, it will be easy for us to apply the methods discussed here\nto RLHF and related applications.\n4.4.1.1 Supervision Signals\nThe training of reward models can broadly be seen as a ranking problem, where the model learns\nto assign scores to outputs so that their order reﬂects the pr eferences indicated by humans. There\nare several methods to train a reward model from the perspect ive of ranking.\nOne approach is to extend pairwise ranking to listwise ranki ng. For each sample in a dataset,\nwe can use the LLM to generate multiple outputs, and ask human experts to order these outputs.\nFor example, given a set of four outputs {y1,y2,y3,y4}, one possible order of them can be\ny2≻y",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 157,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "416e27cf4943176d",
    "content": ". Here we consider more methods for reward\nmodeling. Our discussion will be relatively general, and si nce the reward model is widely used in\nmany reinforcement learning problems, it will be easy for us to apply the methods discussed here\nto RLHF and related applications.\n4.4.1.1 Supervision Signals\nThe training of reward models can broadly be seen as a ranking problem, where the model learns\nto assign scores to outputs so that their order reﬂects the pr eferences indicated by humans. There\nare several methods to train a reward model from the perspect ive of ranking.\nOne approach is to extend pairwise ranking to listwise ranki ng. For each sample in a dataset,\nwe can use the LLM to generate multiple outputs, and ask human experts to order these outputs.\nFor example, given a set of four outputs {y1,y2,y3,y4}, one possible order of them can be\ny2≻y3≻y1≻y4. A very simple method to model the ordering of the list is to ac cumulate the\npairwise comparison loss. For example, we can deﬁne the list wise loss by accumulating the loss\nover all pairs of outputs:\nLlist=−E(x,Y)∼D r[1\nN(N−1)∑\nya∈Y,yb∈Y\nya̸=yblog Pr( ya≻yb|x)]\n(4.57)\nwhereYis a list of outputs, and Nis the number of outputs in the list. Pr(ya≻yb|x)can be\ndeﬁned using the Bradley-Terry model, that is, Pr(ya≻yb|x) = Sigmoid( r(x,ya)−r(x,yb)).\nHere we omit the φsuperscript on the Pr(·)to keep the notation uncluttered.\nAn extension to the Bradley-Terry model for listwise rankin g could involve a ranking mecha-\nnism that takes into account the entire list of outputs rathe r than just pairwise comparisons. One\nsuch model is the Plackett-Luce model , which generalizes the Bradley-Terry model to handle\nmultiple items in a ranking [ Plackett ,1975 ]. In the Plackett-Luce model, for each item in a list,\nwe deﬁne a “worth” for this item that reﬂects its relative str ength of being chosen over other items.\nFor the reward modeling problem here, the worth of yin the listYcan be deﬁned as\nα(y) = exp( r(x,y)) (4.58)\n\n188 Alignment\nThen the probability of selecting yfromYis given by\nPr(yis selected |x,Y) =α(y)∑\ny′∈Yα(y′)\n=exp(r(x,y))∑\ny′∈Yexp(r(x,y′))(4.59)\nSuppose ˚Yis an ordered list yj1≻yj2≻ · · · ≻ yjN. The overall log-probability of this\nordered list can be deﬁned as the sum of the conditional log-p robabilities at each stage of selection,\ngiven by\nlog Pr( ˚Y|x) = log Pr( yj1≻yj2≻ · · · ≻ yjN|x)\n= log Pr( yj1|x,{yj1,yj2,...,yjN}) +\nlog Pr( yj2|x,{yj2,...,yjN}) +\n· · ·+\nlog Pr( yjN|x,{yjN})\n=N∑\nk=1log Pr( yjk|x,˚Y≥k) (4.60)\nwhere ˚Y≥krepresents the subset of the list of outputs that remain unse lected at the k-th stage, i.e.,\n˚Y≥k={yjk,...,yjN}. Given the log-probability log Pr( ˚Y|x), we can deﬁne the loss function\nbased on the Plackett-Luce model by\nLpl=−E(x,˚Y)∼D r[log Pr( ˚Y|x)](4.61)\nThere are also many other pairwise and listwise methods for m odeling rankings, such as\nRankNet [ Burges et al. ,2005 ] and ListNet [ Cao et al. ,2007 ]. All these methods can be cate-\ngorized into a large family of learning-to-rank approaches , and most of them are applicable to the\nproblem of modeling human preferences. However, discuss",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 158,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "682754828007aacd",
    "content": "4.60)\nwhere ˚Y≥krepresents the subset of the list of outputs that remain unse lected at the k-th stage, i.e.,\n˚Y≥k={yjk,...,yjN}. Given the log-probability log Pr( ˚Y|x), we can deﬁne the loss function\nbased on the Plackett-Luce model by\nLpl=−E(x,˚Y)∼D r[log Pr( ˚Y|x)](4.61)\nThere are also many other pairwise and listwise methods for m odeling rankings, such as\nRankNet [ Burges et al. ,2005 ] and ListNet [ Cao et al. ,2007 ]. All these methods can be cate-\ngorized into a large family of learning-to-rank approaches , and most of them are applicable to the\nproblem of modeling human preferences. However, discussin g these methods is beyond the scope\nof this chapter. Interested readers can refer to books on thi s topic for more details [ Liu,2009 ;Li,\n2011 ].\nIn addition to pairwise and listwise ranking, using pointwi se methods to train reward models\noffers an alternative way to capture human preferences. Unl ike methods that focus on the relative\nrankings between different outputs, pointwise methods tre at each output independently. For ex-\nample, human experts might assign a score to an individual ou tput, such as a rating on a ﬁve-point\nscale. The objective is to adjust the reward model so that its outputs align with these scores. A\nsimple way to achieve pointwise training is through regress ion techniques where the reward of\neach output is treated as a target variable. Let ϕ(x,y)be the score assigned to ygiven xby\nhumans. Pointwise reward models can be trained by minimizin g a loss function, often based on\nmean squared error or other regression losses, between the p redicted reward r(x,y)and the actual\nhuman feedback ϕ(x,y). For example, the loss function could be\nLpoint =−E[ϕ(x,y)−r(x,y)]2(4.62)\nWhile pointwise methods are conceptually simpler and can di rectly guide the reward model to\n\n4.4 Improved Human Preference Alignment 189\npredict scores, they might not always be the best choice in RL HF. A problem is that these methods\nmay struggle with high variance in human feedback, especial ly when different experts provide\ninconsistent scores for similar outputs. Because they focu s on ﬁtting to absolute scores rather than\nrelative differences, inconsistencies in scoring can lead to poor model performance. Moreover,\nﬁtting to speciﬁc scored outputs might discourage generali zation, particularly given that training\ndata is often very limited in RLHF. In contrast, methods that consider relative preferences can\npromote the learning of more generalized patterns of succes s and failure. Nevertheless, there are\nscenarios where pointwise methods might still be suitable. For example, in tasks where training\ndata is abundant and the costs of obtaining accurate, consis tent annotations are low, pointwise\nmethods can prove effective.\nIn fact, to make the supervision signal for training the rewa rd model more robust, we can also\nintroduce additional regularization terms into training. For example, if we consider the ﬁrst term\nUppo-clip(x,y;θ)in Eq. ( 4.54) as a type of generalized reward, then the second term (i.e., the\npenalty term) can be viewed as a form of regularization for th e reward model, except that here the\ngoal is to train the policy rather than the reward model. Anot her example is that Eisenstein et al.\n[2023 ] develop a regularization term based on the squared sum of re wards, and add it to the\npairwise comparison loss in RLHF:\nLreg=Lpair+ (−E(x,ya,yb)∼D r[r(x,ya) +r(x,yb)]2)\n=−E(x,ya,yb)∼D r[log Pr φ(ya≻yb|x)]\n−E(x,ya,yb)∼D r[r(x,ya) +r(x,yb)]2(4.63)\nOptimizing with this regularization term can help mitigate the underdetermination of reward mod-\nels5.\n4.4.1.2 Sparse Rewards vs. Dense Rewards\nAs discussed in Section 4.3, the rewards in RLHF are very sparse: they are observed only a t the\nend of sequences, rather than continuously throughout the g eneration process. Dealing with sparse\nrewards has long been",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 159,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "5911ca19b72e86df",
    "content": "[2023 ] develop a regularization term based on the squared sum of re wards, and add it to the\npairwise comparison loss in RLHF:\nLreg=Lpair+ (−E(x,ya,yb)∼D r[r(x,ya) +r(x,yb)]2)\n=−E(x,ya,yb)∼D r[log Pr φ(ya≻yb|x)]\n−E(x,ya,yb)∼D r[r(x,ya) +r(x,yb)]2(4.63)\nOptimizing with this regularization term can help mitigate the underdetermination of reward mod-\nels5.\n4.4.1.2 Sparse Rewards vs. Dense Rewards\nAs discussed in Section 4.3, the rewards in RLHF are very sparse: they are observed only a t the\nend of sequences, rather than continuously throughout the g eneration process. Dealing with sparse\nrewards has long been a concern in reinforcement learning, a nd has been one of the challenges in\nmany practical applications. For example, in robotics, it o ften needs to shape the reward function\nto ease optimization rather than relying solely on end-of-s equence rewards. Various methods\nhave been developed to address this issue. One common approa ch is reward shaping, where the\noriginal function is modiﬁed to include intermediate rewar ds, thereby providing more immediate\nfeedback. Also, one can adopt curriculum learning to sequen tially structure tasks in a way that the\ncomplexity gradually increases. This can help models to mas ter simpler tasks ﬁrst, which prepares\nthem for more complex challenges as their skills develop. Th ere are many such methods that can\nmitigate the impact of sparse rewards, such as Monte Carlo me thods and intrinsic motivation. Most\nof these methods are general and the discussion of them can be found in the broader literature on\nreinforcement learning, such as Sutton and Barto [2018 ]’s book.\nAlthough we do not discuss methods for mitigating sparse rew ards in detail here, an interesting\nquestion arises: why are sparse rewards so successful in RLH F? Recall from Section 4.3.1 that\nthe supervision signal received at each time step tis not the reward for the current action, but\n5A model is called underdetermined if there are multiple alte rnative sets of parameters that can achieve the same\nobjective.\n\n190 Alignment\nrather some form of the accumulated rewards from tuntil the last time step. Such supervision\nsignals are dense over the sequence, because the reward obta ined at the end of the sequence can\nbe transferred back to that time step, regardless of which ti me step it is. In other words, the sparse\nrewards are transformed into the dense supervision signals . Furthermore, from the perspective of\nreward shaping, Ng et al. [1999 ] show that the reward at tcan be deﬁned as\nr′(st,at,st+1) =r(st,at,st+1) +f(st,at,st+1) (4.64)\nwherer′(·)is the transformed reward function, r(·)is the original reward function, and f(·)is\nthe shaping reward function. To ensure the optimality of the policy under the transformed reward\nfunction, the shaping reward function can be given in the for m\nf(st,at,st+1) =γΦ(st+1)−Φ(st) (4.65)\nwhere Φ(s)is called the potential value of the state s. If we deﬁne Φ(s)as the common value\nfunction as in Eq. ( 4.15) and substitute Eq. ( 4.65) into Eq. ( 4.64), we obtain\nr′(st,at,st+1) =r(st,at,st+1) +γV(st+1)−V(st) (4.66)\nIt is interesting to see that this function is exactly the sam e as the advantage function used in PPO.\nThis relates advantage-based methods to reward shaping: th e advantage is essentially a shaped\nreward.\nOn the other hand, one of the reasons for adopting end-of-seq uence rewards lies in the nature\nof the RLHF tasks. Unlike traditional reinforcement learni ng environments where the agent in-\nteracts with a dynamic environment, RLHF tasks often involv e complex decision-making based\non linguistic or other high-level cognitive processes. The se processes do not lend themselves eas-\nily to frequent and meaningful intermediate rewards becaus e the quality and appropriateness of\nthe actions can only be fully evaluated after observing thei r impact in the larger context of the\nentire sequence or task. In this",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 160,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "0efdf3d3e68f2904",
    "content": "′(st,at,st+1) =r(st,at,st+1) +γV(st+1)−V(st) (4.66)\nIt is interesting to see that this function is exactly the sam e as the advantage function used in PPO.\nThis relates advantage-based methods to reward shaping: th e advantage is essentially a shaped\nreward.\nOn the other hand, one of the reasons for adopting end-of-seq uence rewards lies in the nature\nof the RLHF tasks. Unlike traditional reinforcement learni ng environments where the agent in-\nteracts with a dynamic environment, RLHF tasks often involv e complex decision-making based\non linguistic or other high-level cognitive processes. The se processes do not lend themselves eas-\nily to frequent and meaningful intermediate rewards becaus e the quality and appropriateness of\nthe actions can only be fully evaluated after observing thei r impact in the larger context of the\nentire sequence or task. In this case, the reward signals bas ed on human feedback, though very\nsparse, are typically very informative and accurate. Conse quently, this sparsity, together with the\nhigh informativeness and accuracy of the human feedback, ca n make the learning both robust and\nefﬁcient.\n4.4.1.3 Fine-grained Rewards\nFor many applications, our objective will be more complex th an merely evaluating an entire text.\nFor example, in sentiment analysis, we often do not just dete rmine the sentiment of a text, but need\nto analyze the sentiment in more detail by associating it wit h speciﬁc aspects of a topic discussed\nin the text. Consider the sentence \"The camera of the phone is excellent, but the battery life is\ndisappointing.\" In this example, we would need to separatel y analyze the sentiments expressed\nabout the camera and the battery. Such analysis, known as asp ect-based sentiment analysis, helps\nprovide a ﬁner-grained understanding of the customer revie w compared to general sentiment anal-\nysis.\nFor the problem of reward modeling, we often need to model dif ferent parts of a sequence as\nwell. A simple and straightforward way to do this is to divide a sequence into different segments\nand then compute the reward for each segment [ Wu et al. ,2023 ]. Suppose that an output token\n\n4.4 Improved Human Preference Alignment 191\nsequence ycan be divided into nssegments {¯y1,...,¯yns}by some criterion. We can use the\nreward model to evaluate each of these segments. By taking x,yand¯ykas input to the reward\nmodel, the reward score for the k-th segment is given by\nrk=r(x,y,¯yk) (4.67)\nThen the reward score for the entire output sequence is given by\nr(x,y) =ns∑\nk=1r(x,y,¯yk) (4.68)\nHerer(x,y)can be used to train the policy as usual.\nA problem with this model is that training reward models at th e segment level is not as straight-\nforward as learning from human preferences on entire texts, as it is difﬁcult to obtain segment-level\nhuman preference data. For rating-like problems (e.g., we r ate a segment according to its level\nof misinformation), one simple approach is to assign a ratin g score to each segment and train the\nreward model using pointwise methods. For example, we can us e a strong LLM to rate the se-\nquences ¯y1...¯yk−1and¯y1...¯yk, and obtain the scores s(¯y1...¯yk−1)ands(¯y1...¯yk). We can then\ndeﬁne the score of the segment ¯ykas the difference between s(¯y1...¯yk)ands(¯y1...¯yk−1)\ns(¯yk) =s(¯y1...¯yk)−s(¯y1...¯yk−1) (4.69)\nUsing these segment-level scores, we can train the reward mo del with a regression loss func-\ntion\nLrating =−E¯yk[s(¯yk)−r(x,y,¯yk)]2(4.70)\nSometimes, alignment can be treated as a classiﬁcation prob lem, for example, we assess\nwhether a segment has ethical issues. In this case, the segme nt can be labeled as ethical or\nunethical, either by humans or using additional classiﬁers . Given the label of the segment, we\ncan train the reward model using some classiﬁcation loss fun ction. For example, suppose that\nr(x,y,¯yk) = 1 if the segment is classi",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 161,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "20b53bff907c19b4",
    "content": "¯yk)ands(¯y1...¯yk−1)\ns(¯yk) =s(¯y1...¯yk)−s(¯y1...¯yk−1) (4.69)\nUsing these segment-level scores, we can train the reward mo del with a regression loss func-\ntion\nLrating =−E¯yk[s(¯yk)−r(x,y,¯yk)]2(4.70)\nSometimes, alignment can be treated as a classiﬁcation prob lem, for example, we assess\nwhether a segment has ethical issues. In this case, the segme nt can be labeled as ethical or\nunethical, either by humans or using additional classiﬁers . Given the label of the segment, we\ncan train the reward model using some classiﬁcation loss fun ction. For example, suppose that\nr(x,y,¯yk) = 1 if the segment is classiﬁed as unethical, and r(x,y,¯yk) =−1otherwise6. The\nhinge loss of training binary classiﬁcation models is given by\nLhinge = max(0,1−r(x,y,¯yk)·ˆr) (4.71)\nwhere ˆr∈ {1,−1}denotes the ground truth label.\nThe remaining issue here is how to split yinto segments. One approach is to deﬁne a ﬁxed-\nlength segmentation, where yis divided into equal-length chunks. However, this may not a lways\nbe ideal, as the content of the sequence may not align well wit h ﬁxed boundaries. An alternative\napproach is to segment ybased on speciﬁc linguistic or semantic cues, such as senten ce bound-\naries, topic shifts, or other meaningful structures in the t ext. Such a segmentation can be achieved\nby using linguistic segmentation systems or prompting LLMs to identify natural breaks in the se-\nquence. Another approach is to use dynamic segmentation met hods based on the complexity of\n6To allow the reward model to output categories, we can replac e the linear layer described in Section 4.3.2 with a\nSoftmax layer.\n\n192 Alignment\nthe sequence. For example, segments could be deﬁned where th ere is a signiﬁcant change in the\nreward score, which might correspond to shifts in the task be ing modeled.\n4.4.1.4 Combination of Reward Models\nA reward model can be viewed as a proxy for the environment. Si nce the true environment is often\ntoo complex or unknown, developing a perfect proxy for the en vironment is generally not possible.\nAs a result, over-aligning LLMs with this imperfect proxy mi ght lead to decreased performance,\nknown as the overoptimization problem [Stiennon et al. ,2020 ;Gao et al. ,2023a ]7. We can also\nexplain this through Goodhart’s law, which states: when a measure becomes a target, it ceases to\nbe a good measure [Goodhart ,1984 ].\nAddressing the overoptimization problem is not easy, and th ere is no mature solution yet. The\nideal approach might be to develop an oracle reward model tha t can perfectly capture the true\nobjectives of the task and prevent the agent from “tricking” . However, creating such a model is\nextremely difﬁcult due to the complexity of the real-world e nvironment, as well as the challenge\nof deﬁning all the relevant factors that contribute to the de sired outcome. Instead, a more practical\napproach is to combine multiple reward models, thereby alle viating the misalignment between\nthe training objective and the true objective that arises fr om using a single, speciﬁc reward model\n[Coste et al. ,2024 ].\nGiven a set of reward models, combining them is straightforw ard, and in some cases, we can\nsimply treat this problem as an ensemble learning problem. A simple yet common approach is to\naverage the outputs of these models to obtain a more precise r eward estimation:\nrcombine =1\nNK∑\nk=1wk·rk(x,y) (4.72)\nwhererk(·)is thek-th reward model in the ensemble, wkis the weight of rk(·), andKis the\nnumber of reward models. This combined reward can then be use d to supervise the training of\na policy. In fact, there are many ways to combine different mo dels, for example, one can make\npredictions using Bayesian model averaging or develop a fus ion network to learn to combine the\npredictions from different models. Alternatively",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 162,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "eb054f9f2e67bd21",
    "content": " using a single, speciﬁc reward model\n[Coste et al. ,2024 ].\nGiven a set of reward models, combining them is straightforw ard, and in some cases, we can\nsimply treat this problem as an ensemble learning problem. A simple yet common approach is to\naverage the outputs of these models to obtain a more precise r eward estimation:\nrcombine =1\nNK∑\nk=1wk·rk(x,y) (4.72)\nwhererk(·)is thek-th reward model in the ensemble, wkis the weight of rk(·), andKis the\nnumber of reward models. This combined reward can then be use d to supervise the training of\na policy. In fact, there are many ways to combine different mo dels, for example, one can make\npredictions using Bayesian model averaging or develop a fus ion network to learn to combine the\npredictions from different models. Alternatively, one can frame this task as a multi-objective\noptimization problem, and use multiple reward models to tra in the policy simultaneously. These\nmethods have been intensively discussed in the literature o n optimization and machine learning\n[Miettinen ,1999 ;Bishop ,2006 ].\nIn addition to model combination methods, another importan t issue is how to collect or con-\nstruct multiple different reward models. One of the simples t approaches is to employ ensemble\nlearning techniques, such as developing diverse reward mod els from different subsets of a given\ndataset or from various data sources. For RLHF, it is also pos sible to construct reward models\nbased on considerations of different aspects of alignment. For example, we can develop a reward\nmodel to evaluate the factual accuracy of the output and anot her reward model to evaluate the\n7This problem is also called reward hacking orreward gaming [Krakovna et al. ,2020 ;Skalse et al. ,2022 ;\nPan et al. ,2022 ], which refers to the phenomenon where the agent attempts to trick the reward model but fails to\nalign its actions with the true intended objectives of the ta sk. Imagine a student who is assigned homework and is re-\nwarded with points or praise for completing it. The student m ight then ﬁnd ways to ﬁnish the homework with minimal\neffort to maximize the reward, such as copying and pasting so lutions from the internet or previous assignments, rather\nthan solving the problems themselves.\n\n4.4 Improved Human Preference Alignment 193\nya≻ybPreference\nData\nReward ModelValue Function\nPolicytraining with MLETraining\nwith PPO\n(a) RLHF (PPO)\nya≻ybPreference\nData\nPolicytraining with MLE\n(b) DPO\nFig. 4.10: Standard RLHF (PPO) vs. DPO. In RLHF, the human preference da ta is used to train a reward model, which\nis then employed in training the policy as well as the value fu nction. In DPO, the use of human preference data is more\ndirect, and the policy is trained on this data without the nee d for reward model training.\ncompleteness of the output. These two models are complement ary to each other, and can be com-\nbined to improve the overall evaluation of the output. Anoth er approach is to employ different\noff-the-shelf LLMs as reward models. This approach is simpl e and practical, as there have been\na lot of well-developed LLMs and we just need to use them with n o or little modiﬁcation. An\ninteresting issue, though not closely related to the discus sion here, arises: can an LLM that aligns\nwith other LLMs outperform those LLMs? Probably not at ﬁrst g lance. In part, this is because\nthe target LLM merely imitates other LLMs based on limited su pervision and thus cannot capture\nwell the nuances of the behaviors of these supervisors. Howe ver, given the strong generalization\nability of LLMs, this approach can, in fact, be quite beneﬁci al. For example, using open-sourced\nor commercial LLMs as reward models has demonstrated strong performance in aligning LLMs,\neven achieving state-of-the-art results on several popula r tasks [ Lambert et al. ,2024 ].\n4.4.2 Direct Preference Optimization\nAlthough learning reward models is a standard step in reinfo rcement learning, it makes the entire\ntraining process much more complex than supervised trainin g. Training a reliable reward model\nis itself not an easy task, and a poorly trained reward model c an greatly affect the outcome of\npolicy learning. We now consider an alternative alignment m ",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 163,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "696524dc7d68e8e4",
    "content": " g lance. In part, this is because\nthe target LLM merely imitates other LLMs based on limited su pervision and thus cannot capture\nwell the nuances of the behaviors of these supervisors. Howe ver, given the strong generalization\nability of LLMs, this approach can, in fact, be quite beneﬁci al. For example, using open-sourced\nor commercial LLMs as reward models has demonstrated strong performance in aligning LLMs,\neven achieving state-of-the-art results on several popula r tasks [ Lambert et al. ,2024 ].\n4.4.2 Direct Preference Optimization\nAlthough learning reward models is a standard step in reinfo rcement learning, it makes the entire\ntraining process much more complex than supervised trainin g. Training a reliable reward model\nis itself not an easy task, and a poorly trained reward model c an greatly affect the outcome of\npolicy learning. We now consider an alternative alignment m ethod, called direct preference op-\ntimization (DPO ), which simpliﬁes the training framework by eliminating th e need to explicitly\nmodel rewards [ Rafailov et al. ,2024 ]. This method directly optimizes the policy based on user\npreferences, rather than developing a separate reward mode l. As a result, we can achieve human\npreference alignment in a supervised learning-like fashio n. Figure 4.10 shows a comparison of\nthe standard RLHF method and the DPO method.\nBefore deriving the DPO objective, let us ﬁrst review the obj ective of policy training used in\nRLHF. As discussed in Section 4.3.3 , the policy is typically trained by optimizing a loss functi on\n\n194 Alignment\nwith a penalty term. The DPO method assumes a simple loss func tion where the quality of the\noutput ygiven the input xis evaluated by the reward model r(x,y). The training objective is thus\ngiven by\n˜θ= arg min\nθEx∼DEy∼πθ(·|x)[−r(x,y)\nloss+β(logπθ(y|x)−logπθref(y|x))  \npenalty](4.73)\nNote that in this optimization problem, only the term πθ(y|x)depends on the target policy πθ(·).\nBoth the reward model r(x,y)and the reference model πθref(y|x)are assumed to be ﬁxed given\nxandy. This is a strong assumption compared with PPO, but as will be shown later, it simpliﬁes\nthe problem and crucial for deriving the DPO objective.\nSinceθis the variable we want to optimize, we rearrange the right-h and side of Eq. ( 4.73) to\nisolateπθ(y|x)as an independent term:\n˜θ= arg min\nθEx∼DEy∼πθ(·|x)[βlogπθ(y|x)−βlogπθref(y|x)−r(x,y)]\n= arg min\nθEx∼DEy∼πθ(·|x)[logπθ(y|x)−(logπθref(y|x) +1\nβr(x,y))]\n= arg min\nθEx∼DEy∼πθ(·|x)[logπθ(y|x)\ndependent on θ−logπθref(y|x) exp(1\nβr(x,y))\n  \nnot dependent on θ](4.74)\nThis equation deﬁnes the objective function as the differen ce between the log-probability dis-\ntribution function of yand another function of y. This form of the objective function seems not\n“ideal”, as we usually prefer to see the difference between t wo distributions, so that we can in-\nterpret this difference as some kind of divergence between t he distributions. A simple idea is\nto convert the second term (i.e., logπθref(y|x) exp(1\nβr(x,y))) into a log-probability distribution\nover the domain of y. If we treat πθref(y|x) exp(1\nβr(x,y))as an unnormalized probability of y,\nwe can convert it into a normalized probability by dividing i t by a normalization factor:\nZ(x) =∑\nyπθref(y|x) exp(1\nβr(x,y))(4.75)\nHence we can deﬁne a probability distribution by\nπ∗(y|x) =πθ",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 164,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "7a8f2aad8934bc6f",
    "content": " between the log-probability dis-\ntribution function of yand another function of y. This form of the objective function seems not\n“ideal”, as we usually prefer to see the difference between t wo distributions, so that we can in-\nterpret this difference as some kind of divergence between t he distributions. A simple idea is\nto convert the second term (i.e., logπθref(y|x) exp(1\nβr(x,y))) into a log-probability distribution\nover the domain of y. If we treat πθref(y|x) exp(1\nβr(x,y))as an unnormalized probability of y,\nwe can convert it into a normalized probability by dividing i t by a normalization factor:\nZ(x) =∑\nyπθref(y|x) exp(1\nβr(x,y))(4.75)\nHence we can deﬁne a probability distribution by\nπ∗(y|x) =πθref(y|x) exp(1\nβr(x,y))\nZ(x)(4.76)\n\n4.4 Improved Human Preference Alignment 195\nWe then rewrite Eq. ( 4.74) as\n˜θ= arg min\nθEx∼DEy∼πθ(·|x)[\nlogπθ(y|x)−logπθref(y|x) exp(1\nβr(x,y))\nZ(x)\n−logZ(x)]\n= arg min\nθEx∼DEy∼πθ(·|x)[\nlogπθ(y|x)−logπ∗(y|x)−logZ(x)]\n= arg min\nθEx∼D[\nEy∼πθ(·|x)[\nlogπθ(y|x)−logπ∗(y|x)]\n−Ey∼πθ(·|x)[logZ(x)]]\n= arg min\nθEx∼D[\nKL(πθ(·|x)||π∗(·|x))\n\nKL divergence−logZ(x)\nconstant wrt. θ]\n(4.77)\nSince logZ(x)is independent of θ, it does not affect the result of the arg minθoperation,\nand can be removed from the objective. Now we obtain a new trai ning objective which ﬁnds the\noptimal policy πθby minimizing the KL divergence between πθ(·|x)andπ∗(·|x)\n˜θ= arg min\nθEx∼D[\nKL(πθ(·|x)||π∗(·|x))]\n(4.78)\nClearly, the solution to this optimization problem is given by\nπθ(y|x) =π∗(y|x)\n=πθref(y|x) exp(1\nβr(x,y))\nZ(x) (4.79)\nGiven this equation, we can express the reward r(x,y)using the target model πθ(y|x), the\nreference model πθref(y|x), and the normalization factor Z(x):\nr(x,y) =β(\nlogπθ(y|x)\nπθref(y|x)+ logZ(x))\n(4.80)\nThis is interesting because we initially seek to learn the po licyπθ(·)using the reward model\nr(x,y), but eventually obtain a representation of the reward model based on the policy. Given the\nreward model deﬁned in Eq. ( 4.80), we can apply it to the Bradley-Terry model to calculate the\npreference probability (also see Section 4.3.2 ):\nPrθ(ya≻yb|x) = Sigmoid( r(x,ya)−r(x,yb))\n= Sigmoid(\nβ(\nlogπθ(ya|x)\nπθref(ya|x)+ logZ(x))\n−\nβ(\nlogπθ(yb|x)\nπθref(yb|x)+ logZ(x)))\n= Sigmoid(\nβlogπθ(ya|x)\nπθref(ya|x)−βlogπθ(yb|x)\nπθref(yb|x))\n(4.81)\n\n196 Alignment\nThis formula is elegant because it converts the difference i n rewards into the difference in\nratio functions, and we do not need to calculate the value of Z(x). A direct result is that we no\nlonger need a reward model, but only need the target policy an d reference model to calculate the\nprobability of preferences. Finally, we can train the targe t policy by minimizing the following\nDPO loss function\nLdpo(θ) = −E(x,ya,yb)∼D r[log Pr θ(ya≻",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 165,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "84470fce01f687e9",
    "content": "(\nβ(\nlogπθ(ya|x)\nπθref(ya|x)+ logZ(x))\n−\nβ(\nlogπθ(yb|x)\nπθref(yb|x)+ logZ(x)))\n= Sigmoid(\nβlogπθ(ya|x)\nπθref(ya|x)−βlogπθ(yb|x)\nπθref(yb|x))\n(4.81)\n\n196 Alignment\nThis formula is elegant because it converts the difference i n rewards into the difference in\nratio functions, and we do not need to calculate the value of Z(x). A direct result is that we no\nlonger need a reward model, but only need the target policy an d reference model to calculate the\nprobability of preferences. Finally, we can train the targe t policy by minimizing the following\nDPO loss function\nLdpo(θ) = −E(x,ya,yb)∼D r[log Pr θ(ya≻yb|x)](4.82)\nThe form of this loss function is very similar to that used in t raining reward models in RLHF (see\nEq. ( 4.36)). But it should be noted that the loss function here depends on the parameters of the\npolicy (i.e.,θ) rather than the parameters of the reward model (i.e., φ).\nThe main advantage of DPO lies in its simplicity and efﬁcienc y. The DPO objective is very\nstraightforward — it directly optimizes for preference-ba sed feedback, rather than relying on sep-\narately developed reward models. Moreover, DPO is generall y more sample-efﬁcient, as it learns\nfrom a ﬁxed dataset without the need for the computationally expensive sampling process used\nin PPO. This makes DPO a popular method for human preference a lignment, especially when\ndeveloping and applying reward models via reinforcement le arning is challenging.\nDPO can broadly be viewed as an ofﬂine reinforcement learning method, where the training\ndata is pre-collected and ﬁxed, and there is no exploration. In contrast, online reinforcement learn-\ning methods like PPO, which require exploring new states thr ough interaction with the environ-\nment (using the reward model as a proxy), also have their uniq ue advantages. One of the beneﬁts\nof online reinforcement learning is that it allows the agent to continuously adapt to changes in\nthe environment by learning from real-time feedback. This m eans that, unlike ofﬂine methods,\nonline methods are not constrained by the static nature of pr e-collected data and can discover\nnew problem-solving strategies. In addition, exploration can help the agent cover a wider range of\nstate-action pairs, thus improving generalization. This c ould be an important advantage for LLMs,\nas generalization is considered a critical aspect in applyi ng such large models.\n4.4.3 Automatic Preference Data Generation\nAlthough learning from human preferences is an effective an d popular method for aligning LLMs,\nannotating preference data is costly. Using human feedback does not only faces the problem of\nlimited scalability, but it may also introduce bias because human feedback is inherently subjective.\nAs a result, one can turn to AI feedback methods to address the se scalability and consistency issues\nwithout the limitations associated with human annotators.\nAs with data generation for instruction ﬁne-tuning, genera ting preference data using LLMs is\nstraightforward. Given a set of inputs, we ﬁrst use an LLM to g enerate pairs of outputs. Then, we\nprompt the LLM to label the preference between each pair of ou tputs, along with its corresponding\ninput. Below is an example of prompting the LLM to generate a p reference label for a pair of\nconsumer service responses.\n\n4.4 Improved Human Preference Alignment 197\nConsider a customer service scenario where a customer poses a request. You\nwill review two responses to this request. Please indicate w hich response is\npreferred. Note that a good response should be courteous, cl ear, and concise. It\nshould address the customer’s concern directly, provide he lpful information or a\nsolution, and maintain a positive tone.\nRequest:\nHello, I noticed that my order hasn’t arrived yet, though it w as scheduled to\narrive several days ago. Could you please update me on its sta tus? Thank you!\nResponse A:\nI’m very sorry for the delay and understand how disappointin g this can be. We’re\ndoing our best to sort this out quickly for you.\nResponse B:\nHey, stuff happens! Your package will get there when it gets t here, no need to\nstress.\nResponse A is preferred.\nOnce we collect such preference labels, we can use them, alon g with the output",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 166,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "8ad02a85d1cb50ca",
    "content": " Preference Alignment 197\nConsider a customer service scenario where a customer poses a request. You\nwill review two responses to this request. Please indicate w hich response is\npreferred. Note that a good response should be courteous, cl ear, and concise. It\nshould address the customer’s concern directly, provide he lpful information or a\nsolution, and maintain a positive tone.\nRequest:\nHello, I noticed that my order hasn’t arrived yet, though it w as scheduled to\narrive several days ago. Could you please update me on its sta tus? Thank you!\nResponse A:\nI’m very sorry for the delay and understand how disappointin g this can be. We’re\ndoing our best to sort this out quickly for you.\nResponse B:\nHey, stuff happens! Your package will get there when it gets t here, no need to\nstress.\nResponse A is preferred.\nOnce we collect such preference labels, we can use them, alon g with the output pair and input,\nto train the reward model. Of course, we can consider demonst rating a few examples or using\nadvanced prompting techniques, such as CoT, to improve labe ling performance. For example, we\ncan include in the prompt an example showing how and why one of the two responses is preferred\nbased on a CoT rationale.\nIn addition to preference labels, we can also obtain the prob ability associated with each label\n[Lee et al. ,2023 ]. A simple method is to extract the probabilities for the lab el tokens, such as “A”\nand “B”, from the probabilities output by the LLM. We can then use the Softmax function or other\nnormalization techniques to re-normalize these probabili ties into a distribution over the labels.\nThese probabilities of preferred labels can serve as pointw ise supervision signals for training the\nreward model, as discussed in Section 4.4.1 .\nFor data generation, although it is easy to scale up, it is oft en necessary to ensure the data is\naccurate and diverse. Here, the data quality and diversity i ssues involve not only the labeling of\npreferences but also the inputs and outputs of the model. The refore, we often need to use a variety\nof techniques to obtain large-scale, high-quality data. Fo r example, one can generate diverse\nmodel outputs and annotations by using different LLMs, prom pts, in-context demonstrations, and\nso on [ Cui et al. ,2024 ].Dubois et al. [2024 ] report that the variability in pairwise preference data\nis important for training LLMs from either human or AI feedba ck.\nWhile learning from AI feedback is highly scalable and gener ally objective, this method is\nmore suited to well-deﬁned tasks where objective performan ce metrics are available. By contrast,\nlearning from human feedback is more advantageous when alig ning AI systems with human val-\nues, preferences, and complex real-world tasks that requir e understanding of subtle or subjective\ncontext. These methods can be combined to train LLMs that ben eﬁt from both human insights\nand the scalability of AI feedback.\n\n198 Alignment\n4.4.4 Step-by-step Alignment\nSo far, our discussion of alignment has primarily focused on the use of reward models for evalu-\nating entire input-output sequence pairs. These methods ca n be easily adapted to scenarios where\nthe correctness of an output can be examined by checking whet her the desired result is included.\nFor example, in the task of calculating a mathematical expre ssion, a reward model can provide\npositive feedback if the answer is correct, and negative fee dback if the answer is wrong. How-\never, in many problems that require complex reasoning, simp ly examining the correctness of the\nﬁnal result is insufﬁcient for learning. Imagine a student w ho is only given the ﬁnal answer to\na challenging math problem. Knowing whether the ﬁnal answer is right or wrong does not help\nthe student ﬁgure out where they went wrong and how to calcula te the correct answer. A better\napproach would be to guide the student with a step-by-step br eakdown of the problem-solving\nprocess and encourage understanding of the underlying conc epts and logic behind these steps.\nIn Chapter 3, we studied CoT methods to prompt LLMs to explici tly write out intermediate\nsteps or the reasoning process needed to reach a conclusion o r solve a problem. We saw that\nbreaking down a problem into smaller parts could make it easi er to understand the solution path\nand increase the accuracy of the output. These methods can be naturally extended to the alignment\nof LLMs, that is, we supervise the model during the intermedi ate steps of reasoning. Consider a\nreasoning task where an LLM",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 167,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "ecfd8dc21ff1a2b4",
    "content": " the ﬁnal answer to\na challenging math problem. Knowing whether the ﬁnal answer is right or wrong does not help\nthe student ﬁgure out where they went wrong and how to calcula te the correct answer. A better\napproach would be to guide the student with a step-by-step br eakdown of the problem-solving\nprocess and encourage understanding of the underlying conc epts and logic behind these steps.\nIn Chapter 3, we studied CoT methods to prompt LLMs to explici tly write out intermediate\nsteps or the reasoning process needed to reach a conclusion o r solve a problem. We saw that\nbreaking down a problem into smaller parts could make it easi er to understand the solution path\nand increase the accuracy of the output. These methods can be naturally extended to the alignment\nof LLMs, that is, we supervise the model during the intermedi ate steps of reasoning. Consider a\nreasoning task where an LLM produces a sequence of reasoning steps y={¯y1,...,¯yns}for the\ngiven input. The result of the reasoning is assumed to be incl uded in the last step ¯yns, and can\nbe easily veriﬁed. For this reasoning problem, Uesato et al. [2022 ] categorize LLM ﬁne-tuning\napproaches into two classes:\n•Outcome-based Approaches . Supervision occurs only when the end result is veriﬁed. Thi s\nis a standard method for learning from human feedback we have discussed in this chapter.\nFor example, the LLM is optimized to maximize some form of the rewardr(x,y).\n•Process-based Approaches . Supervision is involved in all intermediate steps in addit ion to\nthe last step. To do this, we need to develop a model to give a su pervision signal at each\nstep, and develop loss functions that can make use of such sup ervision signals.\nFigure 4.11 shows two LLM outputs for an example math problem. Although t he LLM gives\nthe correct ﬁnal answer in both cases, it makes mistakes duri ng the problem-solving process in the\nsecond output. Outcome-based approaches overlook these mi stakes and give positive feedback for\nthe entire solution. By contrast, process-based approache s can take these mistakes into account\nand provide additional guidance on the detailed reasoning s teps.\nAn important issue for process-based approaches is that we n eed to get step-level feedback\nduring a (potentially) long reasoning path. We can collect o r generate reasoning paths correspond-\ning to problems from existing datasets. Human experts then a nnotate each step in these paths for\ncorrectness. These annotations can be used to directly trai n LLMS or as rewards in reward mod-\neling. However, in practice, richer annotations are often i ntroduced [ Lightman et al. ,2024 ]. In\naddition to the correct andincorrect labels, a step can also be labeled as neutral to indicate that\nwhile the step may be technically correct, it might still be p roblematic within the overall reason-\ning process. Furthermore, to improve the efﬁciency of data a nnotation, techniques such as active\nlearning can be employed. Identifying obvious errors usual ly does not signiﬁcantly contribute to\n\n4.4 Improved Human Preference Alignment 199\nQuestion (from The International Junior Math Olympiad):\nJessica has a lot of apps on her phone. The number of apps she ha s is divisible by 5. If she\ndownloaded 10 more apps, she would still have less than 50 app s. What is the maximum\noriginal number of apps Jessica could have?\nResponse 1 :\nxmust be a multiple of 5, i.e.,\nx≡0 (mod 5) .x¯y1:\nAfter downloading 10apps, the total\ntotal number of apps should still be less\nthan50:x+ 10<50. Solving this\ninequality:x<40.x¯y2:\nSincexmust be a multiple of 5and\nless than 40, the largest value of xis35.x¯y3:\nVerify the result. If x= 35 , then after\ndownloading 10 apps, Jessica would\nhave: 35 + 10 = 45 which satisfy\n45<50.x¯y4:\nThe maximum original number of\napps is 35.x¯y5:\ncorrectResponse 2 :\nThe number of apps is divisible by 5,\nsoxmust be a multiple of 5.x¯y1:\nIf she downloads 10 more apps, her\nnew total will be x+ 10 , and this must\nbe less than 50. Therefore, x+ 10<50.\nSimpl",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 168,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "76bc29985cc928f0",
    "content": "y1:\nAfter downloading 10apps, the total\ntotal number of apps should still be less\nthan50:x+ 10<50. Solving this\ninequality:x<40.x¯y2:\nSincexmust be a multiple of 5and\nless than 40, the largest value of xis35.x¯y3:\nVerify the result. If x= 35 , then after\ndownloading 10 apps, Jessica would\nhave: 35 + 10 = 45 which satisfy\n45<50.x¯y4:\nThe maximum original number of\napps is 35.x¯y5:\ncorrectResponse 2 :\nThe number of apps is divisible by 5,\nsoxmust be a multiple of 5.x¯y1:\nIf she downloads 10 more apps, her\nnew total will be x+ 10 , and this must\nbe less than 50. Therefore, x+ 10<50.\nSimplifying this gives x<40.x¯y2:\nxmust be less than or equal to\nthe largest multiple of 5, which is 40.x¯y3:\nButx+ 10 should not be more\nthan or equal to 50. So we need to\nsubtract 5from 40.x¯y4:\nTherefore, the ﬁnal result is 35. x ¯y5:\ncorrectproblematic\nproblematic\nFig. 4.11: Two LLM responses to a math problem. In response 1, both the ﬁn al result and all the reasoning steps are\ncorrect. In response 2, the ﬁnal result is correct, but there are mistakes in the reasoning process (highlighted in red).\nFor outcome-based approaches, both responses are consider ed correct. For process-based approaches, the mistakes in\nresponse 2 can be considered in reward modeling.\nlearning from reasoning mistakes. Instead, annotating ste ps that the model conﬁdently considers\ncorrect but are actually problematic is often more effectiv e.\nGiven a set of step-level annotated reasoning paths and corr esponding inputs, we can train\na reward model to provide feedback for supervising policy le arning. The reward model can be\ntreated as a classiﬁcation model, and so its architecture ca n be a Transformer decoder with a\nSoftmax layer stacked on top. At step k, the reward model takes both the problem description\n(denoted by x) and the reasoning steps generated so far (denoted by ¯y≤k) as input and outputs\na probability distribution over the label set {correct,incorrect }or{correct,incorrect,neutral }.\nThen the learned reward model is used to evaluate reasoning p aths by assessing the correctness of\neach step. A simple method to model correctness is to count th e number of steps that are classiﬁed\nascorrect , given by\nr(x,y) =ns∑\nk=1δ(correct,C(x,¯y≤k)) (4.83)\nwhereC(x,¯y≤k)denotes the label with the maximum probability. We can also u se log-probabilities\n\n200 Alignment\nof classiﬁcation to deﬁne the reward of the entire path\nr(x,y) =ns∑\nk=1log Pr( correct |x,¯y≤k) (4.84)\nwhere Pr(correct |x,¯y≤k)denotes the probability of the correct label generated by the reward\nmodel. The reward score r(x,y)can then be used to train the policy in RLHF as usual.\nWhile we restrict our discussion to math problems, the appro aches described here are general\nand can be applied to a wide variety of tasks that involve mult i-step reasoning and decision-\nmaking. Moreover, we can consider various aspects when asse ssing the quality of a step, rather\nthan just its correctness. For example, in dialogue systems , responses must not only be accurate\nbut also contextually appropriate across multiple turns of conversation. If a model provides a\ncorrect response but fails to maintain coherence in the cont ext of the ongoing dialogue, step-\nlevel feedback could help the model identify and correct suc h discrepancies. Also note that the\nprocess-based approaches are related to the ﬁne-grained re ward modeling approaches discussed\nin Section 4.4.1.3 . All these approaches essentially aim to provide more detai led supervision to\nLLMs by breaking their outputs into smaller, more manageabl e steps. However, process-based\nfeedback focuses more on evaluating the correctness of a ste p based on its preceding steps, while\nthe approaches in Section 4.4.1.3 emphasize evaluating each step independently.\nThe idea of aligning LLM",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 169,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "935987b328f05425",
    "content": " Moreover, we can consider various aspects when asse ssing the quality of a step, rather\nthan just its correctness. For example, in dialogue systems , responses must not only be accurate\nbut also contextually appropriate across multiple turns of conversation. If a model provides a\ncorrect response but fails to maintain coherence in the cont ext of the ongoing dialogue, step-\nlevel feedback could help the model identify and correct suc h discrepancies. Also note that the\nprocess-based approaches are related to the ﬁne-grained re ward modeling approaches discussed\nin Section 4.4.1.3 . All these approaches essentially aim to provide more detai led supervision to\nLLMs by breaking their outputs into smaller, more manageabl e steps. However, process-based\nfeedback focuses more on evaluating the correctness of a ste p based on its preceding steps, while\nthe approaches in Section 4.4.1.3 emphasize evaluating each step independently.\nThe idea of aligning LLMs step by step has great application p otential, especially considering\nthe recent shift towards more complex reasoning tasks in the use of LLMs. For example, both\nthe GPT-o1 and GPT-o3 models are designed with more advanced reasoning techniques (such\nas long internal CoT) to solve challenging problems like sci entiﬁc and mathematical reasoning\n[OpenAI ,2024 ]. These tasks often rely on long and complex reasoning paths , and therefore, it\nseems essential to introduce detailed supervision signals in the reasoning process. Moreover, from\na practical perspective, effective supervision on long rea soning paths not only improves reasoning\nperformance, but it also helps the model eliminate redundan t or unnecessary reasoning steps,\nthereby reducing reasoning complexity and improving efﬁci ency.\n4.4.5 Inference-time Alignment\nIn this section we explored a variety of methods to align mode ls with human preferences and an-\nnotations. However, one of the signiﬁcant limitations of ma ny such methods is that LLMs must\nbe ﬁne-tuned. For RLHF and its variants, training LLMs with r eward models can be computa-\ntionally expensive and unstable, leading to increased comp lexity and costs when applying these\napproaches. In this case, we can consider aligning models at inference time, thus avoiding the\nadditional complexity and effort involved.\nOne simple way to achieve inference-time alignment is to use the reward model to select\nthe best one from Nalternative outputs generated by the LLM, a method known as Best-of-N\nsampling (BoN sampling ). We can consider BoN sampling as a form of reranking. In fact ,\nreranking methods have been widely used in NLP tasks, such as machine translation, for a long\ntime. They are typically applied in situations where traini ng complex models is costly. In such\ncases, directly reranking the outputs allows for the incorp oration of these complex models at a\nvery low cost8.\n8Reranking methods can also help us explore what are known as m odel errors and search errors, although these\n\n4.5 Summary 201\nIn the BoN sampling process, the LLM takes the input sequence xand generates Ndifferent\noutput sequences {ˆy1,...,ˆyN}:\n{ˆy1,...,ˆyN}= argTopN\ny[Pr(y|x)] (4.85)\nwhere the argTopN operation returns the top- Noutputs that maximize the function Pr(y|x).\nThese outputs can be generated in a variety of ways, dependin g on the search algorithm used by\nthe model (e.g., sampling or beam search). Once the N-best output candidates are generated, the\nreward model is used to evaluate and select the best one:\nˆybest = max {r(x,ˆy1),...,r (x,ˆyN)} (4.86)\nIt is worth noting that the result of BoN sampling is also inﬂu enced by the diversity of the\nN-best list. This is a common issue with most reranking method s. Typically, we wish the N-best\noutput candidates to have relatively high quality but be suf ﬁciently different from each other. In\nmany text generation systems, the N-best outputs are very similar, often differing by just one\nor two words. The diversity issue is even more challenging in LLMs, as the N-best outputs\ngenerated by an LLM can be different in their wordings, yet th eir semantic meanings are often\nquite similar. In practice, one can adjust the model hyperpa rameters and/or adopt different LLMs\nto generate more diverse output candidates for reranking. N evertheless, as with many practical\nsystems, we need to",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 170,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "8549b309c4eaf272",
    "content": "�y1),...,r (x,ˆyN)} (4.86)\nIt is worth noting that the result of BoN sampling is also inﬂu enced by the diversity of the\nN-best list. This is a common issue with most reranking method s. Typically, we wish the N-best\noutput candidates to have relatively high quality but be suf ﬁciently different from each other. In\nmany text generation systems, the N-best outputs are very similar, often differing by just one\nor two words. The diversity issue is even more challenging in LLMs, as the N-best outputs\ngenerated by an LLM can be different in their wordings, yet th eir semantic meanings are often\nquite similar. In practice, one can adjust the model hyperpa rameters and/or adopt different LLMs\nto generate more diverse output candidates for reranking. N evertheless, as with many practical\nsystems, we need to make a trade-off between selecting high- quality candidates and ensuring\nsufﬁcient variation in the generated outputs.\nBoN sampling can be used for training LLMs as well. A closely r elated method is rejection\nsampling . In this method, we ﬁrst select the “best” outputs from the N-best lists via the reward\nmodel, and then take these selected outputs to ﬁne-tune the L LM. In this way, we can introduce\nhuman preferences into the training of LLMs via a much simple r approach compared to RLHF.\nMany LLMs have adopted rejection sampling for ﬁne-tuning [ Nakano et al. ,2021 ;Touvron et al. ,\n2023b ].\n4.5 Summary\nIn this chapter, we have explored a range of techniques for al igning LLMs. In particular, we\nhave discussed ﬁne-tuning methods that enable LLMs to follo w instructions and align them with\nhuman preferences. One of the beneﬁts of ﬁne-tuning LLMs is c omputation efﬁciency. Unlike\npre-training based on large-scale neural network optimiza tion, ﬁne-tuning is a post-training step\nand so is less computationally expensive. Moreover, it is be tter suited to address problems that are\nnot easily solved in pre-training, such as human value align ment. The widespread attention to the\nalignment issue has also led to a surge of research papers on t his topic, which has posed challenges\nin writing this chapter, as it is difﬁcult to cover all the lat est techniques. However, we have tried\nto provide a relatively detailed introduction to the fundam ental approaches to alignment, such as\nissues are not often discussed in the context of LLMs. For exa mple, suppose we have an old model and a new, more\npowerful model. We can use the new model to select the best out put from the N-best list of the old model as the oracle\noutput. The performance difference between the oracle outp ut and the top-1 output of the original N-best list reﬂects\nthe performance gain brought by the new model. If the perform ance gain is signiﬁcant, we can say that the old model\nhas more model errors. If the gain is small, it may indicate th at the issue lies in search errors, as the best candidates\nwere not found.\n\n202 Alignment\ninstruction ﬁne-tuning and RLHF.\nWhile we have focused on LLM alignment techniques in this cha pter, the term AI alignment\nis a wide-ranging concept. It generally refers to the proces s of ensuring that the behavior of an AI\nsystem aligns with human values, goals, and expectations. T he idea of AI alignment can be traced\nback to the early days of AI. A widely cited description of AI a lignment comes from an article by\nthe mathematician and computer scientist Norbert Wiener [ Wiener ,1960 ]. The quote is as follows\nIf we use, to achieve our purposes, a mechanical agency with w hose operation\nwe cannot efﬁciently interfere ... we had better be quite sur e that the purpose\nput into the machine is the purpose which we really desire.\nAt that time, AI alignment was a distant concern for research ers. But today, it greatly inﬂu-\nences the design of various AI systems. For example, in robot ics, alignment is critical to ensur-\ning that autonomous robots safely interact with humans and t heir environments. In autonomous\ndriving, cars must not only follow trafﬁc laws but also make c omplex, real-time decisions that\nprioritize human safety, avoid accidents, and navigate eth ical dilemmas.\nIn current AI research",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 171,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "d4a4a86de41dde28",
    "content": " days of AI. A widely cited description of AI a lignment comes from an article by\nthe mathematician and computer scientist Norbert Wiener [ Wiener ,1960 ]. The quote is as follows\nIf we use, to achieve our purposes, a mechanical agency with w hose operation\nwe cannot efﬁciently interfere ... we had better be quite sur e that the purpose\nput into the machine is the purpose which we really desire.\nAt that time, AI alignment was a distant concern for research ers. But today, it greatly inﬂu-\nences the design of various AI systems. For example, in robot ics, alignment is critical to ensur-\ning that autonomous robots safely interact with humans and t heir environments. In autonomous\ndriving, cars must not only follow trafﬁc laws but also make c omplex, real-time decisions that\nprioritize human safety, avoid accidents, and navigate eth ical dilemmas.\nIn current AI research, alignment is usually achieved by dev eloping a surrogate objective that\nis analogous to the real goal and steering the AI system towar ds this objective. However, designing\nthe objective of AI alignment is very difﬁcult. One reason is that human values are diverse and\noften context-dependent, making it difﬁcult to distill the m into a single, universally applicable\nobjective function. Also, the complexity of real-world env ironments, where values and goals often\nconﬂict or evolve over time, further complicates alignment efforts. Even if we could deﬁne an\nappropriate objective, AI systems may ﬁnd unintended ways t o achieve it, leading to “misaligned”\noutcomes that still technically satisfy the objective but i n a harmful or counterproductive way.\nThese challenges have motivated and are motivating AI resea rch towards more aligned sys-\ntems, either through developing new mechanisms for perceiv ing the world or more efﬁcient and\ngeneralizable methods to adapt these systems to given tasks . More importantly, as AI systems\nbecome more powerful and intelligent, especially given tha t recent advances in LLMs have shown\nremarkable capabilities in dealing with many challenging p roblems, the need for AI alignment\nhas become more urgent. Researchers have started to be conce rned with AI safety and warn the\ncommunity that they need to develop and release AI systems wi th great caution to prevent these\nsystems from being misaligned [ Russell ,2019 ;Bengio et al. ,2024 ].\n\nBibliography\n[Ainslie et al., 2020] Joshua Ainslie, Santiago Ontanon, Ch ris Alberti, Vaclav Cvicek, Zachary Fisher,\nPhilip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, and Li Yang. Etc: Encoding long and\nstructured inputs in transformers. In Proceedings of the 2020 Conference on Empirical Methods in\nNatural Language Processing (EMNLP) , pages 268–284, 2020.\n[Ainslie et al., 2023] Joshua Ainslie, James Lee-Thorp, Mic hiel de Jong, Yury Zemlyanskiy, Federico\nLebron, and Sumit Sanghai. Gqa: Training generalized multi -query transformer models from multi-\nhead checkpoints. In Proceedings of the 2023 Conference on Empirical Methods in N atural Language\nProcessing , pages 4895–4901, 2023.\n[Akyürek et al., 2023] Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou.\nWhat learning algorithm is in-context learning? investiga tions with linear models. In Proceedings of\nThe Eleventh International Conference on Learning Represe ntations , 2023.\n[Alabdulmohsin et al., 2022] Ibrahim M Alabdulmohsin, Behn am Neyshabur, and Xiaohua Zhai. Revisit-\ning neural scaling laws in language and vision. Advances in Neural Information Processing Systems , 35:\n22300–22312, 2022.\n[Allal et al., 2024] Loubna Ben Allal, Anton Lozhkov, and Dan iel van Strien. cosmopedia: how to create\nlarge-scale synthetic data for pre-training. https://huggingface.co/blog/cosmopedia , 2024.\n[Almazrouei et al., 2023] Ebtesam Almazrouei, Hamza Alobei dli, Abdulaziz Alshamsi, Alessandro Cap-\npelli, Ruxandra Cojocaru, Mérouane Debbah, Étienne Gofﬁne t, Daniel Hesslow",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 172,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "3fc0cb346e245567",
    "content": "ohsin et al., 2022] Ibrahim M Alabdulmohsin, Behn am Neyshabur, and Xiaohua Zhai. Revisit-\ning neural scaling laws in language and vision. Advances in Neural Information Processing Systems , 35:\n22300–22312, 2022.\n[Allal et al., 2024] Loubna Ben Allal, Anton Lozhkov, and Dan iel van Strien. cosmopedia: how to create\nlarge-scale synthetic data for pre-training. https://huggingface.co/blog/cosmopedia , 2024.\n[Almazrouei et al., 2023] Ebtesam Almazrouei, Hamza Alobei dli, Abdulaziz Alshamsi, Alessandro Cap-\npelli, Ruxandra Cojocaru, Mérouane Debbah, Étienne Gofﬁne t, Daniel Hesslow, Julien Launay, Quentin\nMalartic, Daniele Mazzotta, Badreddine Noune, Baptiste Pa nnier, and Guilherme Penedo. The falcon\nseries of open language models. arXiv preprint arXiv:2311.16867 , 2023.\n[Andreas et al., 2016] Jacob Andreas, Marcus Rohrbach, Trev or Darrell, and Dan Klein. Neural module\nnetworks. In Proceedings of the IEEE conference on computer vision and pa ttern recognition , pages\n39–48, 2016.\n[Arjovsky et al., 2016] Martin Arjovsky, Amar Shah, and Yosh ua Bengio. Unitary evolution recurrent\nneural networks. In International conference on machine learning , pages 1120–1128, 2016.\n[Aschenbrenner, 2024] Leopold Aschenbrenner. Situationa l awareness: The decade ahead, 2024. URL\nhttps://situational-awareness.ai/ .\n[Askell et al., 2021] Amanda Askell, Yuntao Bai, Anna Chen, D awn Drain, Deep Ganguli, Tom Henighan,\nAndy Jones, Nicholas Joseph, Benjamin Mann, Nova DasSarma, Nelson Elhage, Zac Hatﬁeld-Dodds,\nDanny Hernandez, Jackson Kernion, Kamal Ndousse, Catherin e Olsson, Dario Amodei, Tom B. Brown,\nJack Clark, Sam McCandlish, Chris Olah, and Jared Kaplan. A g eneral language assistant as a laboratory\nfor alignment. arXiv preprint arXiv:2112.00861 , 2021.\n[Bach et al., 2022] Stephen H. Bach, Victor Sanh, Zheng Xin Yo ng, Albert Webson, Colin Raffel, Nihal V .\nNayak, Abheesht Sharma, Taewoon Kim, M. Saiful Bari, Thibau lt Févry, Zaid Alyafeai, Manan Dey,\nAndrea Santilli, Zhiqing Sun, Srulik Ben-David, Canwen Xu, Gunjan Chhablani, Han Wang, Jason Alan\nFries, Maged Saeed AlShaibani, Shanya Sharma, Urmish Thakk er, Khalid Almubarak, Xiangru Tang,\nDragomir R. Radev, Mike Tian-Jian Jiang, and Alexander M. Ru sh. Promptsource: An integrated de-\nvelopment environment and repository for natural language prompts. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Linguistics: System Demonstrations , pages 93–104, 2022.\n[Bengio et al., 2003] Yoshua Bengio, Réjean Ducharme, Pasca l Vincent, and Christian Jauvin. A neural\nprobabilistic language model. Journal of Machine Learning Research , 3:1137–1155, 2003.\n[Bengio et al., 2006] Yoshua Bengio, Pascal Lamblin, Dan Pop ovici, and Hugo Larochelle. Greedy layer-\nwise training of deep networks. Advances in neural information processing systems , 19, 2006.\n[Bengio et al., 2024] Yoshua Bengio, Geoffrey Hinton, Andre w Yao, Dawn Song, Pieter Abbeel, Trevor\nDarrell, Yuval Noah Harari, Ya-Qin Zhang, Lan Xue, Shai Shal ev-Shwartz, Gillian K. Hadﬁeld, Jeff\n203\n\n204 Alignment\nClune, Tegan Maharaj, Frank Hutter, Atilim Gunes Baydin, Sh eila A. Mc",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 173,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "daea005fbcac734c",
    "content": " Ducharme, Pasca l Vincent, and Christian Jauvin. A neural\nprobabilistic language model. Journal of Machine Learning Research , 3:1137–1155, 2003.\n[Bengio et al., 2006] Yoshua Bengio, Pascal Lamblin, Dan Pop ovici, and Hugo Larochelle. Greedy layer-\nwise training of deep networks. Advances in neural information processing systems , 19, 2006.\n[Bengio et al., 2024] Yoshua Bengio, Geoffrey Hinton, Andre w Yao, Dawn Song, Pieter Abbeel, Trevor\nDarrell, Yuval Noah Harari, Ya-Qin Zhang, Lan Xue, Shai Shal ev-Shwartz, Gillian K. Hadﬁeld, Jeff\n203\n\n204 Alignment\nClune, Tegan Maharaj, Frank Hutter, Atilim Gunes Baydin, Sh eila A. McIlraith, Qiqi Gao, Ashwin\nAcharya, David Krueger, Anca Dragan, Philip Torr, Stuart Ru ssell, Daniel Kahneman, Jan Markus\nBrauner, and Sören Mindermann. Managing extreme ai risks am id rapid progress. Science , 384(6698):\n842–845, 2024.\n[Bentivogli and Giampiccolo, 2011] Luisa Bentivogli and Da nilo Giampiccolo. Pascal recognizing textual\nentailment challenge (rte-7) at tac 2011. https://tac.nist.gov/2011/RTE/ , 2011.\n[Besta et al., 2024] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski,\nLukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Nie wiadomski, Piotr Nyczyk, and Torsten\nHoeﬂer. Graph of thoughts: Solving elaborate problems with large language models. In Proceedings of\nthe AAAI Conference on Artiﬁcial Intelligence , volume 38, pages 17682–17690, 2024.\n[Biderman et al., 2021] Stella Biderman, Sid Black, Charles Foster, Leo Gao, Eric Hallahan, Horace He,\nBen Wang, and Phil Wang. Rotary embeddings: A relative revol ution. https://blog.eleuther.ai/\nrotary-embeddings/ , 2021.\n[Bishop, 2006] Christopher M. Bishop. Pattern Recognition and Machine Learning . Springer, 2006.\n[Blum and Mitchell, 1998] Avrim Blum and Tom Mitchell. Combi ning labeled and unlabeled data with\nco-training. In Proceedings of the eleventh annual conference on Computati onal learning theory , pages\n92–100, 1998.\n[Bradley and Terry, 1952] Ralph Allan Bradley and Milton E. T erry. Rank analysis of incomplete block\ndesigns: I. the method of paired comparisons. Biometrika , 39(3/4):324–345, 1952.\n[Brandon et al., 2024] William Brandon, Mayank Mishra, Anir uddha Nrusimha, Rameswar Panda, and\nJonathan Ragan Kelly. Reducing transformer key-value cach e size with cross-layer attention. arXiv\npreprint arXiv:2405.12981 , 2024.\n[Brill, 1992] Eric Brill. A simple rule-based part of speech tagger. In Speech and Natural Language:\nProceedings of a Workshop Held at Harriman, New York, Februa ry 23-26, 1992 , 1992.\n[Brown et al., 1993] Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer.\nThe mathematics of statistical machine translation: Param eter estimation. Computational Linguistics ,\n19(2):263–311, 1993.\n[Brown et al., 2020] Tom Brown, Benjamin Mann, Nick Ryder, Me lanie Subbiah, Jared D Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry , Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child , Aditya Ramesh, Daniel Ziegler, Jeffrey\nWu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mat eusz Litwin, Scott",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 174,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "85219d59920658d9",
    "content": "1992 , 1992.\n[Brown et al., 1993] Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer.\nThe mathematics of statistical machine translation: Param eter estimation. Computational Linguistics ,\n19(2):263–311, 1993.\n[Brown et al., 2020] Tom Brown, Benjamin Mann, Nick Ryder, Me lanie Subbiah, Jared D Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry , Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child , Aditya Ramesh, Daniel Ziegler, Jeffrey\nWu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mat eusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish, Alec Radfo rd, Ilya Sutskever, and Dario Amodei.\nLanguage models are few-shot learners. Advances in neural information processing systems , 33:1877–\n1901, 2020.\n[Bubeck et al., 2023] Sébastien Bubeck, Varun Chandrasekar an, Ronen Eldan, Johannes Gehrke, Eric\nHorvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott M. Lundberg, Harsha Nori, Hamid\nPalangi, Marco Túlio Ribeiro, and Yi Zhang. Sparks of artiﬁc ial general intelligence: Early experiments\nwith gpt-4. arXiv preprint arXiv:2303.12712 , 2023.\n[Bulatov et al., 2022] Aydar Bulatov, Yury Kuratov, and Mikh ail Burtsev. Recurrent memory transformer.\nAdvances in Neural Information Processing Systems , 35:11079–11091, 2022.\n[Burges et al., 2005] Chris Burges, Tal Shaked, Erin Renshaw , Ari Lazier, Matt Deeds, Nicole Hamilton,\nand Greg Hullender. Learning to rank using gradient descent . InProceedings of the 22nd international\nconference on Machine learning , pages 89–96, 2005.\n[Burns et al., 2023] Collin Burns, Pavel Izmailov, Jan Hendr ik Kirchner, Bowen Baker, Leo Gao, Leopold\nAschenbrenner, Yining Chen, Adrien Ecoffet, Manas Jogleka r, Jan Leike, Ilya Sutskever, and Jeff Wu.\nWeak-to-strong generalization: Eliciting strong capabil ities with weak supervision. arXiv preprint\narXiv:2312.09390 , 2023a.\n\n4.5 Summary 205\n[Burns et al., 2023] Collin Burns, Jan Leike, Leopold Aschen brenner, Jeffrey Wu, Pavel Izmailov, Leo\nGao, Bowen Baker, and Jan Hendrik Kirchner. Weak-to-strong generalization, 2023b. URL https://\nhttps://openai.com/index/weak-to-strong-generalizat ion.\n[Caballero et al., 2023] Ethan Caballero, Kshitij Gupta, Ir ina Rish, and David Krueger. Broken neural\nscaling laws. In ICLR 2023 Workshop on Mathematical and Empirical Understan ding of Foundation\nModels , 2023.\n[Cao et al., 2007] Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Ts ai, and Hang Li. Learning to rank: from\npairwise approach to listwise approach. In Proceedings of the 24th international conference on Machin e\nlearning , pages 129–136, 2007.\n[Chang et al., 2024] Kaiyan Chang, Songcheng Xu, Chenglong W ang, Yingfeng Luo, Tong Xiao, and\nJingbo Zhu. Efﬁcient prompting methods for large language m odels: A survey. arXiv preprint\narXiv:2404.01077 , 2024.\n[Charniak, 1997] Eugene Charniak. Statistical parsing wit h a context-free grammar and word statistics.\nAAAI/IAAI , 2005(598-603):18, 1997.\n[Chen et al., 2023] Banghao Chen, Zhaofeng Zhang, Nicolas",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 175,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "93da790c3542b938",
    "content": ", Tie-Yan Liu, Ming-Feng Ts ai, and Hang Li. Learning to rank: from\npairwise approach to listwise approach. In Proceedings of the 24th international conference on Machin e\nlearning , pages 129–136, 2007.\n[Chang et al., 2024] Kaiyan Chang, Songcheng Xu, Chenglong W ang, Yingfeng Luo, Tong Xiao, and\nJingbo Zhu. Efﬁcient prompting methods for large language m odels: A survey. arXiv preprint\narXiv:2404.01077 , 2024.\n[Charniak, 1997] Eugene Charniak. Statistical parsing wit h a context-free grammar and word statistics.\nAAAI/IAAI , 2005(598-603):18, 1997.\n[Chen et al., 2023] Banghao Chen, Zhaofeng Zhang, Nicolas La ngrené, and Shengxin Zhu. Unleashing\nthe potential of prompt engineering in large language model s: a comprehensive review. arXiv preprint\narXiv:2310.14735 , 2023a.\n[Chen et al., 2023] Lichang Chen, Shiyang Li, Jun Yan, Hai Wan g, Kalpa Gunaratna, Vikas Yadav, Zheng\nTang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, and Hongxi a Jin. Alpagasus: Training a better alpaca\nwith fewer data. arXiv preprint arXiv:2307.08701 , 2023b.\n[Chen et al., 2024] Lichang Chen, Shiyang Li, Jun Yan, Hai Wan g, Kalpa Gunaratna, Vikas Yadav, Zheng\nTang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, and Hongxi a Jin. Alpagasus: Training a better alpaca\nwith fewer data. In The Twelfth International Conference on Learning Represen tations , 2024a.\n[Chen et al., 2023] Shouyuan Chen, Sherman Wong, Liangjian C hen, and Yuandong Tian. Extending\ncontext window of large language models via positional inte rpolation. arXiv preprint arXiv:2306.15595 ,\n2023c.\n[Chen et al., 2020] Tianlong Chen, Jonathan Frankle, Shiyu C hang, Sijia Liu, Yang Zhang, Zhangyang\nWang, and Michael Carbin. The lottery ticket hypothesis for pre-trained bert networks. Advances in\nneural information processing systems , 33:15834–15846, 2020.\n[Chen et al., 2024] Zixiang Chen, Yihe Deng, Huizhuo Yuan, Ka ixuan Ji, and Quanquan Gu. Self-play ﬁne-\ntuning converts weak language models to strong language mod els. arXiv preprint arXiv:2401.01335 ,\n2024b.\n[Chevalier et al., 2023] Alexis Chevalier, Alexander Wetti g, Anirudh Ajith, and Danqi Chen. Adapting\nlanguage models to compress contexts. In Proceedings of the 2023 Conference on Empirical Methods\nin Natural Language Processing , pages 3829–3846, 2023.\n[Chi et al., 2022] Ta-Chung Chi, Ting-Han Fan, Peter J Ramadg e, and Alexander Rudnicky. Kerple:\nKernelized relative positional embedding for length extra polation. Advances in Neural Information Pro-\ncessing Systems , 35:8386–8399, 2022.\n[Chi et al., 2023] Ta-Chung Chi, Ting-Han Fan, Alexander Rud nicky, and Peter Ramadge. Dissecting\ntransformer length extrapolation via the lens of receptive ﬁeld analysis. In Proceedings of the 61st Annual\nMeeting of the Association for Computational Linguistics ( Volume 1: Long Papers) , pages 13522–13537,\n2023.\n[Chiang et al., 2023] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Yin g Sheng, Zhanghao Wu, Hao Zhang, Lian-\nmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzale z, Ion Stoica, and Eric P. Xing. Vicuna:\nAn open-source chatbot impressing gpt-4 with 90%* chatgpt q uality, March 2023. URL https://\nlmsys",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 176,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "e93248888cf69bcb",
    "content": ", 2022.\n[Chi et al., 2023] Ta-Chung Chi, Ting-Han Fan, Alexander Rud nicky, and Peter Ramadge. Dissecting\ntransformer length extrapolation via the lens of receptive ﬁeld analysis. In Proceedings of the 61st Annual\nMeeting of the Association for Computational Linguistics ( Volume 1: Long Papers) , pages 13522–13537,\n2023.\n[Chiang et al., 2023] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Yin g Sheng, Zhanghao Wu, Hao Zhang, Lian-\nmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzale z, Ion Stoica, and Eric P. Xing. Vicuna:\nAn open-source chatbot impressing gpt-4 with 90%* chatgpt q uality, March 2023. URL https://\nlmsys.org/blog/2023-03-30-vicuna/ .\n[Chowdhery et al., 2022] Aakanksha Chowdhery, Sharan Naran g, Jacob Devlin, Maarten Bosma, Gaurav\n\n206 Alignment\nMishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker\nSchuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abh ishek Rao, Parker Barnes, Yi Tay, Noam\nShazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hu tchinson, Reiner Pope, James Bradbury,\nJacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, To ju Duke, Anselm Levskaya, Sanjay Ghe-\nmawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Veda nt Misra, Kevin Robinson, Liam Fedus,\nDenny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Bar ret Zoph, Alexander Spiridonov, Ryan\nSepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andr ew M. Dai, Thanumalayan Sankara-\nnarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica More ira, Rewon Child, Oleksandr Polozov,\nKatherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Ma rk Diaz, Orhan Firat, Michele Catasta,\nJason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, S lav Petrov, and Noah Fiedel. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint arXiv:2204.02311 , 2022.\n[Christiano et al., 2017] Paul F Christiano, Jan Leike, Tom B rown, Miljan Martic, Shane Legg, and Dario\nAmodei. Deep reinforcement learning from human preference s.Advances in neural information pro-\ncessing systems , 30, 2017.\n[Chu et al., 2023] Zheng Chu, Jingchang Chen, Qianglong Chen , Weijiang Yu, Tao He, Haotian Wang,\nWeihua Peng, Ming Liu, Bing Qin, and Ting Liu. A survey of chai n of thought reasoning: Advances,\nfrontiers and future. arXiv preprint arXiv:2309.15402 , 2023.\n[Chung et al., 2022] Hyung Won Chung, Le Hou, S. Longpre, Barr et Zoph, Yi Tay, William Fedus,\nEric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu,\nZhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery , Dasha Valter, Sharan Narang, Gau-\nrav Mishra, Adams Wei Yu, Vincent Zhao, Yanping Huang, Andre w M. Dai, Hongkun Yu, Slav Petrov,\nEd Huai hsin Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denn y Zhou, Quoc V . Le, and Jason Wei.\nScaling instruction-ﬁnetuned language models. arXiv preprint arXiv:2210.11416 , 2022.\n[Clark et al., 2019] Kevin Clark, Minh-Thang Luong, Quoc V Le , and Christopher D Manning. Electra:\nPre-training text encoders as discriminators rather than g enerators. In Proceedings of International\nConference on Learning Representations",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 177,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "3832ee72573a5490",
    "content": ", Siddhartha Brahma, Albert Webson, Shixiang Shane Gu,\nZhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery , Dasha Valter, Sharan Narang, Gau-\nrav Mishra, Adams Wei Yu, Vincent Zhao, Yanping Huang, Andre w M. Dai, Hongkun Yu, Slav Petrov,\nEd Huai hsin Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denn y Zhou, Quoc V . Le, and Jason Wei.\nScaling instruction-ﬁnetuned language models. arXiv preprint arXiv:2210.11416 , 2022.\n[Clark et al., 2019] Kevin Clark, Minh-Thang Luong, Quoc V Le , and Christopher D Manning. Electra:\nPre-training text encoders as discriminators rather than g enerators. In Proceedings of International\nConference on Learning Representations , 2019.\n[Cobbe et al., 2021] Karl Cobbe, Vineet Kosaraju, Mohammad B avarian, Mark Chen, Heewoo Jun, Lukasz\nKaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Rei ichiro Nakano, Christopher Hesse, and John\nSchulman. Training veriﬁers to solve math word problems. arXiv preprint arXiv:2110.14168 , 2021.\n[Conneau et al., 2020] Alexis Conneau, Kartikay Khandelwal , Naman Goyal, Vishrav Chaudhary, Guil-\nlaume Wenzek, Francisco Guzmán, Édouard Grave, Myle Ott, Lu ke Zettlemoyer, and Veselin Stoyanov.\nUnsupervised cross-lingual representation learning at sc ale. In Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics , pages 8440–8451, 2020.\n[Coste et al., 2024] Thomas Coste, Usman Anwar, Robert Kirk, and David Krueger. Reward model ensem-\nbles help mitigate overoptimization. In The Twelfth International Conference on Learning Represen ta-\ntions , 2024.\n[Cui et al., 2024] Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Bingxiang He, Wei Zhu, Yuan\nNi, Guotong Xie, Ruobing Xie, Yankai Lin, Zhiyuan Liu, and Ma osong Sun. ULTRAFEEDBACK:\nBoosting language models with scaled AI feedback. In Proceedings of the 41st International Conference\non Machine Learning , volume 235, pages 9722–9744, 2024.\n[Dai et al., 2023] Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shu ming Ma, Zhifang Sui, and Furu Wei.\nWhy can gpt learn in-context? language models secretly perf orm gradient descent as meta-optimizers.\nInFindings of the Association for Computational Linguistics : ACL 2023 , pages 4005–4019, 2023.\n[Dai et al., 2019] Zihang Dai, Zhilin Yang, Yiming Yang, Jaim e G Carbonell, Quoc Le, and Ruslan\nSalakhutdinov. Transformer-xl: Attentive language model s beyond a ﬁxed-length context. In Proceed-\nings of the 57th Annual Meeting of the Association for Comput ational Linguistics , pages 2978–2988,\n2019.\n[Dao et al., 2022] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast\nand memory-efﬁcient exact attention with io-awareness. Advances in Neural Information Processing\n\n4.5 Summary 207\nSystems , 35:16344–16359, 2022.\n[Dehghani et al., 2018] Mostafa Dehghani, Stephan Gouws, Or iol Vinyals, Jakob Uszkoreit, and Łukasz\nKaiser. Universal transformers. arXiv preprint arXiv:1807.03819 , 2018.\n[Deletang et al., 2024] Gregoire Deletang, Anian Ruoss, Pau l-Ambroise Duquenne, Elliot Catt, Tim\nGenewein, Christopher Mattern, Jordi Grau-Moya, Li Kevin W enliang, Matthew Aitchison, Laurent\nOrseau, Marcus Hutter, and Joel Ven",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 178,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "55a9e7fef277f56b",
    "content": "ra, and Christopher Ré. Flashattention: Fast\nand memory-efﬁcient exact attention with io-awareness. Advances in Neural Information Processing\n\n4.5 Summary 207\nSystems , 35:16344–16359, 2022.\n[Dehghani et al., 2018] Mostafa Dehghani, Stephan Gouws, Or iol Vinyals, Jakob Uszkoreit, and Łukasz\nKaiser. Universal transformers. arXiv preprint arXiv:1807.03819 , 2018.\n[Deletang et al., 2024] Gregoire Deletang, Anian Ruoss, Pau l-Ambroise Duquenne, Elliot Catt, Tim\nGenewein, Christopher Mattern, Jordi Grau-Moya, Li Kevin W enliang, Matthew Aitchison, Laurent\nOrseau, Marcus Hutter, and Joel Veness. Language modeling i s compression. In The Twelfth Interna-\ntional Conference on Learning Representations , 2024.\n[Deng et al., 2022] Mingkai Deng, Jianyu Wang, Cheng-Ping Hs ieh, Yihan Wang, Han Guo, Tianmin Shu,\nMeng Song, Eric Xing, and Zhiting Hu. Rlprompt: Optimizing d iscrete text prompts with reinforcement\nlearning. In Proceedings of the 2022 Conference on Empirical Methods in N atural Language Processing ,\npages 3369–3391, 2022.\n[Devlin et al., 2019] Jacob Devlin, Ming-Wei Chang, Kenton L ee, and Kristina Toutanova. Bert: Pre-\ntraining of deep bidirectional transformers for language u nderstanding. In Proceedings of the 2019\nConference of the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short Papers) , pages 4171–4186, 2019.\n[Ding et al., 2024] Yiran Ding, Li Lyna Zhang, Chengruidong Z hang, Yuanyuan Xu, Ning Shang, Jiahang\nXu, Fan Yang, and Mao Yang. Longrope: Extending llm context w indow beyond 2 million tokens. arXiv\npreprint arXiv:2402.13753 , 2024.\n[Dolan and Brockett, 2005] Bill Dolan and Chris Brockett. Au tomatically constructing a corpus of senten-\ntial paraphrases. In Proceedings of Third International Workshop on Paraphrasi ng (IWP2005) , 2005.\n[Dong et al., 2019] Li Dong, Nan Yang, Wenhui Wang, Furu Wei, X iaodong Liu, Yu Wang, Jianfeng\nGao, Ming Zhou, and Hsiao-Wuen Hon. Uniﬁed language model pr e-training for natural language\nunderstanding and generation. Advances in neural information processing systems , 32, 2019.\n[Dong et al., 2022] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun,\nJingjing Xu, and Zhifang Sui. A survey on in-context learnin g.arXiv preprint arXiv:2301.00234 , 2022.\n[Dong et al., 2021] Yihe Dong, Jean-Baptiste Cordonnier, an d Andreas Loukas. Attention is not all you\nneed: Pure attention loses rank doubly exponentially with d epth. In International Conference on Machine\nLearning , pages 2793–2803. PMLR, 2021.\n[Drozdov et al., 2022] Andrew Drozdov, Nathanael Schärli, E kin Akyürek, Nathan Scales, Xinying Song,\nXinyun Chen, Olivier Bousquet, and Denny Zhou. Composition al semantic parsing with large language\nmodels. In Proceedings of The Eleventh International Conference on Le arning Representations , 2022.\n[Dua et al., 2022] Dheeru Dua, Shivanshu Gupta, Sameer Singh , and Matt Gardner. Successive prompting\nfor decomposing complex questions. In Proceedings of the 2022 Conference on Empirical Methods in\nNatural Language Processing , pages 1251–1265, 2022.\n[Dubey et al., 2024] Abhimanyu Dubey, Abhinav Jauhri, Abhin av Pandey, Abhishek Kadian, Ahmad Al-\nDahle, Aiesha Letman, Akhil Mathur,",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 179,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "88404628e9e42a7f",
    "content": " 2022] Andrew Drozdov, Nathanael Schärli, E kin Akyürek, Nathan Scales, Xinying Song,\nXinyun Chen, Olivier Bousquet, and Denny Zhou. Composition al semantic parsing with large language\nmodels. In Proceedings of The Eleventh International Conference on Le arning Representations , 2022.\n[Dua et al., 2022] Dheeru Dua, Shivanshu Gupta, Sameer Singh , and Matt Gardner. Successive prompting\nfor decomposing complex questions. In Proceedings of the 2022 Conference on Empirical Methods in\nNatural Language Processing , pages 1251–1265, 2022.\n[Dubey et al., 2024] Abhimanyu Dubey, Abhinav Jauhri, Abhin av Pandey, Abhishek Kadian, Ahmad Al-\nDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang , Angela Fan, et al. The llama 3 herd of\nmodels. arXiv preprint arXiv:2407.21783 , 2024.\n[Dubois et al., 2024] Yann Dubois, Chen Xuechen Li, Rohan Tao ri, Tianyi Zhang, Ishaan Gulrajani, Jimmy\nBa, Carlos Guestrin, Percy S Liang, and Tatsunori B Hashimot o. Alpacafarm: A simulation framework\nfor methods that learn from human feedback. Advances in Neural Information Processing Systems , 36,\n2024.\n[Eisenstein et al., 2023] Jacob Eisenstein, Chirag Nagpal, Alekh Agarwal, Ahmad Beirami, Alex D’Amour,\nDJ Dvijotham, Adam Fisch, Katherine Heller, Stephen Pfohl, Deepak Ramachandran, and Peter Shaw.\nHelping or herding? reward model ensembles mitigate but do n ot eliminate reward hacking. arXiv\npreprint arXiv:2312.09244 , 2023.\n[Elsken et al., 2019] Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search:\nA survey. Journal of Machine Learning Research , 20(55):1–21, 2019.\n\n208 Alignment\n[Erhan et al., 2010] Dumitru Erhan, Aaron Courville, Yoshua Bengio, and Pascal Vincent. Why does\nunsupervised pre-training help deep learning? In Proceedings of the thirteenth international conference\non artiﬁcial intelligence and statistics , pages 201–208, 2010.\n[Fan et al., 2019] Angela Fan, Edouard Grave, and Armand Joul in. Reducing transformer depth on demand\nwith structured dropout. In Proceedings of International Conference on Learning Repre sentations , 2019.\n[Fedus et al., 2022] William Fedus, Barret Zoph, and Noam Sha zeer. Switch transformers: Scaling to\ntrillion parameter models with simple and efﬁcient sparsit y.The Journal of Machine Learning Research ,\n23(1):5232–5270, 2022.\n[Fernandes et al., 2023] Patrick Fernandes, Aman Madaan, Em my Liu, António Farinhas, Pedro Henrique\nMartins, Amanda Bertsch, José G. C. de Souza, Shuyan Zhou, To ngshuang Wu, Graham Neubig, and\nAndré F. T. Martins. Bridging the gap: A survey on integratin g (human) feedback for natural language\ngeneration. Transactions of the Association for Computational Linguis tics, 11:1643–1668, 2023.\n[Franklin and Graesser, 1996] Stan Franklin and Art Graesse r. Is it an agent, or just a program?: A taxon-\nomy for autonomous agents. In International workshop on agent theories, architectures, and languages ,\npages 21–35. Springer, 1996.\n[Frensch and Funke, 2014] Peter A Frensch and Joachim Funke. Complex problem solving: The European\nperspective . Psychology Press, 2014.\n[Gale et al., 2019] Trevor Gale, Erich Elsen, and Sara Hooker . The state of sparsity in deep neural networks.\narXiv preprint arXiv:1902.09574 , 2019.\n[Ganguli et al., 2023] Deep Ganguli, Amanda Askell, Nichola s Schiefer, Thomas I. Liao, Kamile Luko-\nsiute",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 180,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "83bb7cde3abad304",
    "content": "3–1668, 2023.\n[Franklin and Graesser, 1996] Stan Franklin and Art Graesse r. Is it an agent, or just a program?: A taxon-\nomy for autonomous agents. In International workshop on agent theories, architectures, and languages ,\npages 21–35. Springer, 1996.\n[Frensch and Funke, 2014] Peter A Frensch and Joachim Funke. Complex problem solving: The European\nperspective . Psychology Press, 2014.\n[Gale et al., 2019] Trevor Gale, Erich Elsen, and Sara Hooker . The state of sparsity in deep neural networks.\narXiv preprint arXiv:1902.09574 , 2019.\n[Ganguli et al., 2023] Deep Ganguli, Amanda Askell, Nichola s Schiefer, Thomas I. Liao, Kamile Luko-\nsiute, Anna Chen, Anna Goldie, Azalia Mirhoseini, Catherin e Olsson, Danny Hernandez, Dawn Drain,\nDustin Li, Eli Tran-Johnson, Ethan Perez, Jackson Kernion, Jamie Kerr, Jared Mueller, Joshua Landau,\nKamal Ndousse, Karina Nguyen, Liane Lovitt, Michael Sellit to, Nelson Elhage, Noemí Mercado, Nova\nDasSarma, Oliver Rausch, Robert Lasenby, Robin Larson, Sam Ringer, Sandipan Kundu, Saurav Kada-\nvath, Scott Johnston, Shauna Kravec, Sheer El Showk, Tamera Lanham, Timothy Telleen-Lawton, Tom\nHenighan, Tristan Hume, Yuntao Bai, Zac Hatﬁeld-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph,\nSam McCandlish, Tom Brown, Christopher Olah, Jack Clark, Sa muel R. Bowman, and Jared Kaplan.\nThe capacity for moral self-correction in large language mo dels. arXiv preprint arXiv:2302.07459 , 2023.\n[Gao et al., 2023] Leo Gao, John Schulman, and Jacob Hilton. S caling laws for reward model overopti-\nmization. In International Conference on Machine Learning , pages 10835–10866. PMLR, 2023a.\n[Gao et al., 2023] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alo n, Pengfei Liu, Yiming Yang, Jamie\nCallan, and Graham Neubig. Pal: Program-aided language mod els. In International Conference on\nMachine Learning , pages 10764–10799. PMLR, 2023b.\n[Gao et al., 2023] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxian g Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei\nSun, and Haofen Wang. Retrieval-augmented generation for l arge language models: A survey. arXiv\npreprint arXiv:2312.10997 , 2023c.\n[Garg et al., 2022] Shivam Garg, Dimitris Tsipras, Percy S Li ang, and Gregory Valiant. What can trans-\nformers learn in-context? a case study of simple function cl asses. Advances in Neural Information\nProcessing Systems , 35:30583–30598, 2022.\n[Ge et al., 2024] Yuan Ge, Yilun Liu, Chi Hu, Weibin Meng, Shim in Tao, Xiaofeng Zhao, Hongxia\nMa, Li Zhang, Boxing Chen, Hao Yang, Bei Li, Tong Xiao, and Jin gbo Zhu. Clustering and rank-\ning: Diversity-preserved instruction selection through e xpert-aligned quality estimation. arXiv preprint\narXiv:2402.18191 , 2024.\n[Gemma Team, 2024] Google DeepMind Gemma Team. Gemma: Open M odels Based on Gemini Re-\nsearch and Technology, 2024.\n[Goodhart, 1984] Charles AE Goodhart. Problems of monetary management: the UK experience . Springer,\n1984.\n\n4.5 Summary 209\n[Gordon et al., 2021] Mitchell A Gordon, Kevin Duh, and Jared Kaplan. Data and parameter scaling laws\nfor neural machine translation. In Proceedings of the 2021 Conference on Empirical Methods in N atural\nLanguage Processing , pages 5915–5922, 2021.\n[Gu and Dao, 2023] Albert Gu and Tri Dao",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 181,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "59d83b7198be980e",
    "content": ", Bei Li, Tong Xiao, and Jin gbo Zhu. Clustering and rank-\ning: Diversity-preserved instruction selection through e xpert-aligned quality estimation. arXiv preprint\narXiv:2402.18191 , 2024.\n[Gemma Team, 2024] Google DeepMind Gemma Team. Gemma: Open M odels Based on Gemini Re-\nsearch and Technology, 2024.\n[Goodhart, 1984] Charles AE Goodhart. Problems of monetary management: the UK experience . Springer,\n1984.\n\n4.5 Summary 209\n[Gordon et al., 2021] Mitchell A Gordon, Kevin Duh, and Jared Kaplan. Data and parameter scaling laws\nfor neural machine translation. In Proceedings of the 2021 Conference on Empirical Methods in N atural\nLanguage Processing , pages 5915–5922, 2021.\n[Gu and Dao, 2023] Albert Gu and Tri Dao. Mamba: Linear-time s equence modeling with selective state\nspaces. arXiv preprint arXiv:2312.00752 , 2023.\n[Gunasekar et al., 2023] Suriya Gunasekar, Yi Zhang, Jyoti A neja, Caio César Teodoro Mendes, Allie Del\nGiorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann , Gustavo de Rosa, Olli Saarikivi, Adil\nSalim, Shital Shah, Harkirat Singh Behl, Xin Wang, Sébastie n Bubeck, Ronen Eldan, Adam Tauman\nKalai, Yin Tat Lee, and Yuanzhi Li. Textbooks are all you need .arXiv preprint arXiv:2306.11644 , 2023.\n[Guo et al., 2024] Qingyan Guo, Rui Wang, Junliang Guo, Bei Li , Kaitao Song, Xu Tan, Guoqing Liu, Jiang\nBian, and Yujiu Yang. Connecting large language models with evolutionary algorithms yields powerful\nprompt optimizers. In The Twelfth International Conference on Learning Represen tations , 2024.\n[Gupta and Berant, 2020] Ankit Gupta and Jonathan Berant. Gm at: Global memory augmentation for\ntransformers. arXiv preprint arXiv:2006.03274 , 2020.\n[Gupta et al., 2021] Ankit Gupta, Guy Dar, Shaya Goodman, Dav id Ciprut, and Jonathan Berant. Memory-\nefﬁcient transformers via top-k attention. In Proceedings of the Second Workshop on Simple and Efﬁcient\nNatural Language Processing , pages 39–52, 2021.\n[Han et al., 2021] Xu Han, Zhengyan Zhang, Ning Ding, Yuxian G u, Xiao Liu, Yuqi Huo, Jiezhong Qiu,\nLiang Zhang, Wentao Han, Minlie Huang, Qin Jin, Yanyan Lan, Y ang Liu, Zhiyuan Liu, Zhiwu Lu,\nXipeng Qiu, Ruihua Song, Jie Tang, Ji-Rong Wen, Jinhui Yuan, Wayne Xin Zhao, and Jun Zhu. Pre-\ntrained models: Past, present and future. AI Open , 2:225–250, 2021.\n[Han et al., 2024] Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang , and Sai Qian Zhang. Parameter-efﬁcient\nﬁne-tuning for large models: A comprehensive survey. arXiv preprint arXiv:2403.14608 , 2024.\n[Harlap et al., 2018] Aaron Harlap, Deepak Narayanan, Amar P hanishayee, Vivek Seshadri, Nikhil Deva-\nnur, Greg Ganger, and Phil Gibbons. Pipedream: Fast and efﬁc ient pipeline parallel dnn training. arXiv\npreprint arXiv:1806.03377 , 2018.\n[He et al., 2019] Kaiming He, Ross Girshick, and Piotr Dollár . Rethinking imagenet pre-training. In\nProceedings of the IEEE/CVF International Conference on Co mputer Vision , pages 4918–4927, 2019.\n[He et al., 2021] Pengcheng He, Xiaodong Liu, Jianfeng Gao, a nd Weizhu Chen. Deberta: Decoding-\nenhanced bert with disentangled attention",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 182,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "62fc80889b451a8f",
    "content": "4.\n[Harlap et al., 2018] Aaron Harlap, Deepak Narayanan, Amar P hanishayee, Vivek Seshadri, Nikhil Deva-\nnur, Greg Ganger, and Phil Gibbons. Pipedream: Fast and efﬁc ient pipeline parallel dnn training. arXiv\npreprint arXiv:1806.03377 , 2018.\n[He et al., 2019] Kaiming He, Ross Girshick, and Piotr Dollár . Rethinking imagenet pre-training. In\nProceedings of the IEEE/CVF International Conference on Co mputer Vision , pages 4918–4927, 2019.\n[He et al., 2021] Pengcheng He, Xiaodong Liu, Jianfeng Gao, a nd Weizhu Chen. Deberta: Decoding-\nenhanced bert with disentangled attention. In Proceedings of International Conference on Learning\nRepresentations , 2021.\n[Hendrycks and Gimpel, 2016] Dan Hendrycks and Kevin Gimpel . Gaussian error linear units (gelus).\narXiv preprint arXiv:1606.08415 , 2016.\n[Hendrycks et al., 2020] Dan Hendrycks, Xiaoyuan Liu, Eric W allace, Adam Dziedzic, Rishabh Krishnan,\nand Dawn Song. Pretrained transformers improve out-of-dis tribution robustness. In Proceedings of the\n58th Annual Meeting of the Association for Computational Li nguistics , pages 2744–2751, 2020.\n[Hendrycks et al., 2021] Dan Hendrycks, Collin Burns, Steve n Basart, Andy Zou, Mantas Mazeika, Dawn\nSong, and Jacob Steinhardt. Measuring massive multitask la nguage understanding. In Proceedings of\nInternational Conference on Learning Representations , 2021.\n[Hestness et al., 2017] Joel Hestness, Sharan Narang, Newsh a Ardalani, Gregory Diamos, Heewoo Jun,\nHassan Kianinejad, Md Mostofa Ali Patwary, Yang Yang, and Ya nqi Zhou. Deep learning scaling is\npredictable, empirically. arXiv preprint arXiv:1712.00409 , 2017.\n[Hewitt, 2024] John Hewitt. Instruction following without instruction tuning, 2024. URL https://nlp.\nstanford.edu/~johnhew/instruction-following.html .\n[Hewitt et al., 2024] John Hewitt, Nelson F Liu, Percy Liang, and Christopher D Manning. Instruction\nfollowing without instruction tuning. arXiv preprint arXiv:2409.14254 , 2024.\n\n210 Alignment\n[Hochreiter and Schmidhuber, 1997] Sepp Hochreiter and Jür gen Schmidhuber. Long short-term memory.\nNeural computation , 9(8):1735–1780, 1997.\n[Hoffmann et al., 2022] Jordan Hoffmann, Sebastian Borgeau d, Arthur Mensch, Elena Buchatskaya, Trevor\nCai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendric ks, Johannes Welbl, Aidan Clark, Tom\nHennigan, Eric Noland, Katie Millican, George van den Dries sche, Bogdan Damoc, Aurelia Guy, Si-\nmon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Orio l Vinyals, and Laurent Sifre. Training\ncompute-optimal large language models. arXiv preprint arXiv:2203.15556 , 2022.\n[Honovich et al., 2023] Or Honovich, Thomas Scialom, Omer Le vy, and Timo Schick. Unnatural instruc-\ntions: Tuning language models with (almost) no human labor. InProceedings of the 61st Annual Meeting\nof the Association for Computational Linguistics (Volume 1 : Long Papers) , pages 14409–14428, 2023.\n[Houlsby et al., 2019] Neil Houlsby, Andrei Giurgiu, Stanis law Jastrzebski, Bruna Morrone, Quentin\nDe Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylva in Gelly. Parameter-efﬁcient transfer\nlearning for NLP. In Proceedings of the 36th International Conference on Machin e Learning , pages\n279",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 183,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "0cad3b5766a136dd",
    "content": "iv preprint arXiv:2203.15556 , 2022.\n[Honovich et al., 2023] Or Honovich, Thomas Scialom, Omer Le vy, and Timo Schick. Unnatural instruc-\ntions: Tuning language models with (almost) no human labor. InProceedings of the 61st Annual Meeting\nof the Association for Computational Linguistics (Volume 1 : Long Papers) , pages 14409–14428, 2023.\n[Houlsby et al., 2019] Neil Houlsby, Andrei Giurgiu, Stanis law Jastrzebski, Bruna Morrone, Quentin\nDe Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylva in Gelly. Parameter-efﬁcient transfer\nlearning for NLP. In Proceedings of the 36th International Conference on Machin e Learning , pages\n2790–2799. PMLR, 2019.\n[Hu et al., 2022] Edward J Hu, yelong shen, Phillip Wallis, Ze yuan Allen-Zhu, Yuanzhi Li, Shean Wang,\nLu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International\nConference on Learning Representations , 2022.\n[Huang, 2009] Liang Huang. Dynamic programming-based sear ch algorithms in NLP. In Proceedings\nof Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the\nAssociation for Computational Linguistics, Companion Vol ume: Tutorial Abstracts , 2009.\n[Huang et al., 2019] Yanping Huang, Youlong Cheng, Ankur Bap na, Orhan Firat, Mia Xu Chen, Dehao\nChen, HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, an d Zhifeng Chen. Gpipe: Efﬁcient\ntraining of giant neural networks using pipeline paralleli sm.Advances in neural information processing\nsystems , 32, 2019.\n[Hutchins et al., 2022] DeLesley Hutchins, Imanol Schlag, Y uhuai Wu, Ethan Dyer, and Behnam\nNeyshabur. Block-recurrent transformers. Advances in neural information processing systems , 35:\n33248–33261, 2022.\n[Jelinek, 1998] Frederick Jelinek. Statistical methods for speech recognition . MIT Press, 1998.\n[Jiang et al., 2023] Albert Q Jiang, Alexandre Sablayrolles , Arthur Mensch, Chris Bamford, Deven-\ndra Singh Chaplot, Diego de las Casas, Florian Bressand, Gia nna Lengyel, Guillaume Lample, Lucile\nSaulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril,\nThomas Wang, Timothée Lacroix, and William El Sayed. Mistra l 7b. arXiv preprint arXiv:2310.06825 ,\n2023a.\n[Jiang et al., 2023] Huiqiang Jiang, Qianhui Wu, Chin-Yew Li n, Yuqing Yang, and Lili Qiu. Llmlingua:\nCompressing prompts for accelerated inference of large lan guage models. In Proceedings of the 2023\nConference on Empirical Methods in Natural Language Proces sing, pages 13358–13376, 2023b.\n[Jiang et al., 2020] Zhengbao Jiang, Frank F Xu, Jun Araki, an d Graham Neubig. How can we know what\nlanguage models know? Transactions of the Association for Computational Linguis tics, 8:423–438,\n2020.\n[Jiao et al., 2020] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xi n Jiang, Xiao Chen, Linlin Li, Fang Wang,\nand Qun Liu. Tinybert: Distilling bert for natural language understanding. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020 , pages 4163–4174, 2020.\n[Joshi et al., 2017] Mandar Joshi, Eunsol Choi, Daniel S Weld , and Luke Zettlemoyer. Triviaqa: A large\nscale distantly supervised challenge dataset for reading c omprehension. In Proceedings of the 55th\nAnnual Meeting of the Association for Computational Lingui stics (Volume 1: Long Papers) , pages 1601",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 184,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "ecb42dfdd860f44c",
    "content": "ig. How can we know what\nlanguage models know? Transactions of the Association for Computational Linguis tics, 8:423–438,\n2020.\n[Jiao et al., 2020] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xi n Jiang, Xiao Chen, Linlin Li, Fang Wang,\nand Qun Liu. Tinybert: Distilling bert for natural language understanding. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020 , pages 4163–4174, 2020.\n[Joshi et al., 2017] Mandar Joshi, Eunsol Choi, Daniel S Weld , and Luke Zettlemoyer. Triviaqa: A large\nscale distantly supervised challenge dataset for reading c omprehension. In Proceedings of the 55th\nAnnual Meeting of the Association for Computational Lingui stics (Volume 1: Long Papers) , pages 1601–\n1611, 2017.\n[Joshi et al., 2020] Mandar Joshi, Danqi Chen, Yinhan Liu, Da niel S Weld, Luke Zettlemoyer, and Omer\nLevy. Spanbert: Improving pre-training by representing an d predicting spans. Transactions of the\n\n4.5 Summary 211\nassociation for computational linguistics , 8:64–77, 2020.\n[Jurafsky and Martin, 2008] Dan Jurafsky and James H. Martin .Speech and Language Processing (2nd\ned.). Prentice Hall, 2008.\n[Kahneman, 2011] Daniel Kahneman. Thinking, fast and slow . macmillan, 2011.\n[Kaplan et al., 2020] Jared Kaplan, Sam McCandlish, Tom Heni ghan, Tom B Brown, Benjamin Chess, Re-\nwon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Am odei. Scaling laws for neural language\nmodels. arXiv preprint arXiv:2001.08361 , 2020.\n[Katharopoulos et al., 2020] Angelos Katharopoulos, Apoor v Vyas, Nikolaos Pappas, and François Fleuret.\nTransformers are rnns: Fast autoregressive transformers w ith linear attention. In International conference\non machine learning , pages 5156–5165. PMLR, 2020.\n[Khandelwal et al., 2020] Urvashi Khandelwal, Omer Levy, Da n Jurafsky, Luke Zettlemoyer, and Mike\nLewis. Generalization through memorization: Nearest neig hbor language models. In International\nConference on Learning Representations , 2020.\n[Khot et al., 2023] Tushar Khot, Harsh Trivedi, Matthew Finl ayson, Yao Fu, Kyle Richardson, Peter Clark,\nand Ashish Sabharwal. Decomposed prompting: A modular appr oach for solving complex tasks. In\nProceedings of The Eleventh International Conference on Le arning Representations , 2023.\n[Kim et al., 2023] Sehoon Kim, Coleman Hooper, Thanakul Watt anawong, Minwoo Kang, Ruohan\nYan, Hasan Genc, Grace Dinh, Qijing Huang, Kurt Keutzer, Mic hael W. Mahoney, Yakun Sophia\nShao, and Amir Gholami. Full stack optimization of transfor mer inference: a survey. arXiv preprint\narXiv:2302.14017 , 2023.\n[Kirkpatrick et al., 2017] James Kirkpatrick, Razvan Pasca nu, Neil Rabinowitz, Joel Veness, Guillaume\nDesjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago R amalho, Agnieszka Grabska-Barwinska,\nDemis Hassabis, Claudia Clopath, Dharshan Kumaran, and Rai a Hadsell. Overcoming catastrophic\nforgetting in neural networks. Proceedings of the national academy of sciences , 114(13):3521–3526,\n2017.\n[Koehn, 2010] Philipp Koehn. Statistical Machine Translation . Cambridge University Press, 2010.\n[Kojima et al., 2022] Takeshi Kojima, Shixiang Shane Gu, Mac hel Reid, Yutaka Matsuo, and Yusuke\nIwasawa. Large language models are zero-shot reasoners. Advances in neural information processing\nsystems , 35:22199–22213, 2022.\n[Korthikanti et al., 2023] Vijay Anand",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 185,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "c5383f5dd013e5ea",
    "content": "ins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago R amalho, Agnieszka Grabska-Barwinska,\nDemis Hassabis, Claudia Clopath, Dharshan Kumaran, and Rai a Hadsell. Overcoming catastrophic\nforgetting in neural networks. Proceedings of the national academy of sciences , 114(13):3521–3526,\n2017.\n[Koehn, 2010] Philipp Koehn. Statistical Machine Translation . Cambridge University Press, 2010.\n[Kojima et al., 2022] Takeshi Kojima, Shixiang Shane Gu, Mac hel Reid, Yutaka Matsuo, and Yusuke\nIwasawa. Large language models are zero-shot reasoners. Advances in neural information processing\nsystems , 35:22199–22213, 2022.\n[Korthikanti et al., 2023] Vijay Anand Korthikanti, Jared C asper, Sangkug Lym, Lawrence McAfee,\nMichael Andersch, Mohammad Shoeybi, and Bryan Catanzaro. R educing activation recomputation in\nlarge transformer models. Proceedings of Machine Learning and Systems , 5, 2023.\n[Krakovna et al., 2020] Victoria Krakovna, Jonathan Uesato , Vladimir Mikulik, Matthew\nRahtz, Tom Everitt, Ramana Kumar, Zac Kenton, Jan Leike, and Shane Legg. Speciﬁ-\ncation gaming: the ﬂip side of ai ingenuity. https://deepmind.google/discover/blog/\nspecification-gaming-the-flip-side-of-ai-ingenuity , 2020.\n[Kung and Peng, 2023] Po-Nien Kung and Nanyun Peng. Do models really learn to follow instructions?\nan empirical study of instruction tuning. arXiv preprint arXiv:2305.11383 , 2023.\n[Kwon et al., 2023] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Y ing Sheng, Lianmin Zheng, Cody Hao\nYu, Joseph E Gonzalez, Hao Zhang, and Ion Stoica. Efﬁcient me mory management for large language\nmodel serving with pagedattention. arXiv preprint arXiv:2309.06180 , 2023.\n[Lake and Baroni, 2018] Brenden Lake and Marco Baroni. Gener alization without systematicity: On\nthe compositional skills of sequence-to-sequence recurre nt networks. In International conference on\nmachine learning , pages 2873–2882. PMLR, 2018.\n[Lambert et al., 2024] Nathan Lambert, Valentina Pyatkin, J acob Morrison, LJ Miranda, Bill Yuchen\nLin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Ye jin Choi, Noah A. Smith, and Han-\nnaneh Hajishirzi. Rewardbench: Evaluating reward models f or language modeling. arXiv preprint\n\n212 Alignment\narXiv:2403.13787 , 2024.\n[Lample and Conneau, 2019] Guillaume Lample and Alexis Conn eau. Cross-lingual language model\npretraining. arXiv preprint arXiv:1901.07291 , 2019.\n[Lan et al., 2020] Zhenzhong Lan, Mingda Chen, Sebastian Goo dman, Kevin Gimpel, Piyush Sharma, and\nRadu Soricut. Albert: A lite bert for self-supervised learn ing of language representations. In Proceedings\nof International Conference on Learning Representations , 2020.\n[Lee et al., 2023] Harrison Lee, Samrat Phatale, Hassan Mans oor, Kellie Ren Lu, Thomas Mesnard, Johan\nFerret, Colton Bishop, Ethan Hall, Victor Carbune, and Abhi nav Rastogi. Rlaif: Scaling reinforcement\nlearning from human feedback with ai feedback. arXiv preprint arXiv:2309.00267 , 2023.\n[Lester et al., 2021] Brian Lester, Rami Al-Rfou, and Noah Co nstant. The power of scale for parameter-\nefﬁcient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in N atural Lan-\nguage Processing , pages 3045–3059, 2021.\n[Lewis et al., 2020] Mike Lewis, Yinhan Liu, Naman Goyal, Mar jan Ghazvininejad, Abdelrahman Mo-\nh",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 186,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "4ba98667bba991af",
    "content": "[Lee et al., 2023] Harrison Lee, Samrat Phatale, Hassan Mans oor, Kellie Ren Lu, Thomas Mesnard, Johan\nFerret, Colton Bishop, Ethan Hall, Victor Carbune, and Abhi nav Rastogi. Rlaif: Scaling reinforcement\nlearning from human feedback with ai feedback. arXiv preprint arXiv:2309.00267 , 2023.\n[Lester et al., 2021] Brian Lester, Rami Al-Rfou, and Noah Co nstant. The power of scale for parameter-\nefﬁcient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in N atural Lan-\nguage Processing , pages 3045–3059, 2021.\n[Lewis et al., 2020] Mike Lewis, Yinhan Liu, Naman Goyal, Mar jan Ghazvininejad, Abdelrahman Mo-\nhamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. B art: Denoising sequence-to-sequence\npre-training for natural language generation, translatio n, and comprehension. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational Lingui stics, pages 7871–7880, 2020.\n[Li et al., 2023] Bei Li, Rui Wang, Junliang Guo, Kaitao Song, Xu Tan, Hany Hassan, Arul Menezes, Tong\nXiao, Jiang Bian, and JingBo Zhu. Deliberate then generate: Enhanced prompting framework for text\ngeneration. arXiv preprint arXiv:2305.19835 , 2023a.\n[Li, 2011] Hang Li. Learning to Rank for Information Retrieval and Natural Lang uage Processing . Online\naccess: Morgan & Claypool Synthesis Collection Five. Morga n & Claypool Publishers, 2011. ISBN\n9781608457076.\n[Li et al., 2022] Huayang Li, Yixuan Su, Deng Cai, Yan Wang, an d Lemao Liu. A survey on retrieval-\naugmented text generation. arXiv preprint arXiv:2202.01110 , 2022.\n[Li et al., 2024] Shanda Li, Chong You, Guru Guruganesh, Josh ua Ainslie, Santiago Ontanon, Manzil\nZaheer, Sumit Sanghai, Yiming Yang, Sanjiv Kumar, and Srina dh Bhojanapalli. Functional interpolation\nfor relative positions improves long context transformers . In The Twelfth International Conference on\nLearning Representations , 2024.\n[Li et al., 2023] Shenggui Li, Fuzhao Xue, Chaitanya Baranwa l, Yongbin Li, and Yang You. Sequence\nparallelism: Long sequence training from system perspecti ve. In Proceedings of the 61st Annual Meeting\nof the Association for Computational Linguistics (Volume 1 : Long Papers) , pages 2391–2404, 2023b.\n[Li and Liang, 2021] Xiang Lisa Li and Percy Liang. Preﬁx-tun ing: Optimizing continuous prompts for\ngeneration. In Proceedings of the 59th Annual Meeting of the Association fo r Computational Linguistics\nand the 11th International Joint Conference on Natural Lang uage Processing (Volume 1: Long Papers) ,\npages 4582–4597, 2021.\n[Li, 2023] Yinheng Li. A practical survey on zero-shot promp t design for in-context learning. In Proceed-\nings of the 14th International Conference on Recent Advance s in Natural Language Processing , pages\n641–647, 2023.\n[Li et al., 2023] Yucheng Li, Bo Dong, Frank Guerin, and Cheng hua Lin. Compressing context to enhance\ninference efﬁciency of large language models. In Proceedings of the 2023 Conference on Empirical\nMethods in Natural Language Processing , pages 6342–6353, 2023c.\n[Lialin et al., 2023] Vladislav Lialin, Vijeta Deshpande, a nd Anna Rumshisky. Scaling down to scale up:\nA guide to parameter-efﬁcient ﬁne-tuning. arXiv preprint arXiv:2303.15647 , 2023.\n[Lightman et al., 2024] Hunter Lightman, Vineet Kosaraju, Y uri Burda, Harrison Edwards, Bowen Baker,\nTeddy Lee, Jan Leike, John Schul",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 187,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "457fac4b3b285326",
    "content": " Processing , pages\n641–647, 2023.\n[Li et al., 2023] Yucheng Li, Bo Dong, Frank Guerin, and Cheng hua Lin. Compressing context to enhance\ninference efﬁciency of large language models. In Proceedings of the 2023 Conference on Empirical\nMethods in Natural Language Processing , pages 6342–6353, 2023c.\n[Lialin et al., 2023] Vladislav Lialin, Vijeta Deshpande, a nd Anna Rumshisky. Scaling down to scale up:\nA guide to parameter-efﬁcient ﬁne-tuning. arXiv preprint arXiv:2303.15647 , 2023.\n[Lightman et al., 2024] Hunter Lightman, Vineet Kosaraju, Y uri Burda, Harrison Edwards, Bowen Baker,\nTeddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Kar l Cobbe. Let’s verify step by step. In The\nTwelfth International Conference on Learning Representat ions, 2024.\n[Liu et al., 2024] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wan g, Bochao Wu, Chengda Lu, Chenggang\n\n4.5 Summary 213\nZhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepsee k-v3 technical report. arXiv preprint\narXiv:2412.19437 , 2024a.\n[Liu et al., 2022] Jiachang Liu, Dinghan Shen, Yizhe Zhang, W illiam B Dolan, Lawrence Carin, and\nWeizhu Chen. What makes good in-context examples for gpt-3? InProceedings of Deep Learning Inside\nOut (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning\nArchitectures , pages 100–114, 2022.\n[Liu et al., 2023] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zheng bao Jiang, Hiroaki Hayashi, and Graham\nNeubig. Pre-train, prompt, and predict: A systematic surve y of prompting methods in natural language\nprocessing. ACM Computing Surveys , 55(9):1–35, 2023a.\n[Liu et al., 2024] Tianqi Liu, Yao Zhao, Rishabh Joshi, Misha Khalman, Mohammad Saleh, Peter J Liu, and\nJialu Liu. Statistical rejection sampling improves prefer ence optimization. In The Twelfth International\nConference on Learning Representations , 2024b.\n[Liu, 2009] Tie-Yan Liu. Learning to rank for information re trieval. Foundations and Trends® in Informa-\ntion Retrieval , 3(3):225–331, 2009.\n[Liu et al., 2023] Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming D ing, Yujie Qian, Zhilin Yang, and Jie\nTang. Gpt understands, too. AI Open , 2023b.\n[Liu et al., 2023] Xiaoxia Liu, Jingyi Wang, Jun Sun, Xiaohan Yuan, Guoliang Dong, Peng Di, Wenhai\nWang, and Dongxia Wang. Prompting frameworks for large lang uage models: A survey. arXiv preprint\narXiv:2311.12785 , 2023c.\n[Liu et al., 2024] Xinyu Liu, Runsong Zhao, Pengcheng Huang, Chunyang Xiao, Bei Li, Jingang Wang,\nTong Xiao, and Jingbo Zhu. Forgetting curve: A reliable meth od for evaluating memorization capability\nfor long-context models. In Proceedings of the 2024 Conference on Empirical Methods in N atural\nLanguage Processing , pages 4667–4682, 2024c.\n[Liu et al., 2019] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,\nMike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Robert a: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692 , 2019.\n[Longpre et al., 2023] Shayne Longpre, Le Hou, Tu Vu, Albert W ebson, Hyung Won Chung, Yi Tay,\nDenny Zhou, Quoc V . Le, Barret Zoph, Jason Wei, and Adam R",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 188,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "99e914f24079274e",
    "content": "bo Zhu. Forgetting curve: A reliable meth od for evaluating memorization capability\nfor long-context models. In Proceedings of the 2024 Conference on Empirical Methods in N atural\nLanguage Processing , pages 4667–4682, 2024c.\n[Liu et al., 2019] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,\nMike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Robert a: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692 , 2019.\n[Longpre et al., 2023] Shayne Longpre, Le Hou, Tu Vu, Albert W ebson, Hyung Won Chung, Yi Tay,\nDenny Zhou, Quoc V . Le, Barret Zoph, Jason Wei, and Adam Rober ts. The ﬂan collection: Designing\ndata and methods for effective instruction tuning. In International Conference on Machine Learning ,\npages 22631–22648. PMLR, 2023.\n[Ma et al., 2023] Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxi an He, Liangke Gui, Graham Neubig,\nJonathan May, and Luke Zettlemoyer. Mega: Moving average eq uipped gated attention. In The Eleventh\nInternational Conference on Learning Representations , 2023.\n[Ma et al., 2024] Xuezhe Ma, Xiaomeng Yang, Wenhan Xiong, Bei di Chen, Lili Yu, Hao Zhang, Jonathan\nMay, Luke Zettlemoyer, Omer Levy, and Chunting Zhou. Megalo don: Efﬁcient llm pretraining and\ninference with unlimited context length. arXiv preprint arXiv:2404.08801 , 2024.\n[Madaan et al., 2024] Aman Madaan, Niket Tandon, Prakhar Gup ta, Skyler Hallinan, Luyu Gao,\nSarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye , Yiming Yang, Shashank Gupta, Bod-\nhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck , Amir Yazdanbakhsh, and Peter Clark.\nSelf-reﬁne: Iterative reﬁnement with self-feedback. Advances in Neural Information Processing Sys-\ntems, 36, 2024.\n[Manning, 2022] Christopher D Manning. Human language unde rstanding & reasoning. Daedalus , 151\n(2):127–138, 2022.\n[Marcus, 1993] Gary F Marcus. Negative evidence in language acquisition. Cognition , 46(1):53–85, 1993.\n[Martins et al., 2022] Pedro Henrique Martins, Zita Marinho , and André FT Martins. ∞-former: Inﬁnite\nmemory transformer-former: Inﬁnite memory transformer. I nProceedings of the 60th Annual Meeting\nof the Association for Computational Linguistics (Volume 1 : Long Papers) , pages 5468–5485, 2022.\n\n214 Alignment\n[Mavi et al., 2024] Vaibhav Mavi, Anubhav Jangra, and Adam Ja towt. Multi-hop question answering.\nFoundations and Trends® in Information Retrieval , 17(5):457–586, 2024.\n[Michel et al., 2019] Paul Michel, Omer Levy, and Graham Neub ig. Are sixteen heads really better than\none? Advances in neural information processing systems , 32, 2019.\n[Micikevicius et al., 2018] Paulius Micikevicius, Sharan N arang, Jonah Alben, Gregory Diamos, Erich\nElsen, David Garcia, Boris Ginsburg, Michael Houston, Olek sii Kuchaiev, Ganesh Venkatesh, and Hao\nWu. Mixed precision training. In Proceedings of International Conference on Learning Repre sentations ,\n2018.\n[Miettinen, 1999] Kaisa Miettinen. Nonlinear multiobjective optimization , volume 12. Springer Science\n& Business Media, 1999.\n[Mikolov et al., 2013] Tomas Mikolov, Kai Chen, Greg Corrado , and Jeffrey Dean. Efﬁcient estimation\nof word representations in vector space. In Proceedings of the International Conference on Learning\nRepresentations (ICLR 2013)",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 189,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "9e38ae0461cdbd93",
    "content": " neural information processing systems , 32, 2019.\n[Micikevicius et al., 2018] Paulius Micikevicius, Sharan N arang, Jonah Alben, Gregory Diamos, Erich\nElsen, David Garcia, Boris Ginsburg, Michael Houston, Olek sii Kuchaiev, Ganesh Venkatesh, and Hao\nWu. Mixed precision training. In Proceedings of International Conference on Learning Repre sentations ,\n2018.\n[Miettinen, 1999] Kaisa Miettinen. Nonlinear multiobjective optimization , volume 12. Springer Science\n& Business Media, 1999.\n[Mikolov et al., 2013] Tomas Mikolov, Kai Chen, Greg Corrado , and Jeffrey Dean. Efﬁcient estimation\nof word representations in vector space. In Proceedings of the International Conference on Learning\nRepresentations (ICLR 2013) , 2013a.\n[Mikolov et al., 2013] Tomas Mikolov, Ilya Sutskever, Kai Ch en, Greg Corrado, and Jeffrey Dean. Dis-\ntributed representations of words and phrases and their com positionality. In Proceedings of the 26th In-\nternational Conference on Neural Information Processing S ystems - Volume 2 , pages 3111–3119, 2013b.\n[Min et al., 2019] Sewon Min, Victor Zhong, Luke Zettlemoyer , and Hannaneh Hajishirzi. Multi-hop read-\ning comprehension through question decomposition and resc oring. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguistics , pages 6097–6109, 2019.\n[Minaee et al., 2024] Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard\nSocher, Xavier Amatriain, and Jianfeng Gao. Large language models: A survey. arXiv preprint\narXiv:2402.06196 , 2024.\n[Mishra et al., 2022] Swaroop Mishra, Daniel Khashabi, Chit ta Baral, and Hannaneh Hajishirzi. Cross-\ntask generalization via natural language crowdsourcing in structions. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Linguistics ( Volume 1: Long Papers) , pages 3470–3487,\n2022.\n[Mnih et al., 2016] V olodymyr Mnih, Adrià Puigdomènech Badi a, Mehdi Mirza, Alex Graves, Tim Harley,\nTimothy P Lillicrap, David Silver, and Koray Kavukcuoglu. A synchronous methods for deep reinforce-\nment learning. In Proceedings of the 33rd International Conference on Intern ational Conference on\nMachine Learning , pages 1928–1937, 2016.\n[Mohtashami and Jaggi, 2024] Amirkeivan Mohtashami and Mar tin Jaggi. Random-access inﬁnite context\nlength for transformers. Advances in Neural Information Processing Systems , 36, 2024.\n[Mu et al., 2024] Jesse Mu, Xiang Li, and Noah Goodman. Learni ng to compress prompts with gist tokens.\nAdvances in Neural Information Processing Systems , 36, 2024.\n[Munkhdalai et al., 2024] Tsendsuren Munkhdalai, Manaal Fa ruqui, and Siddharth Gopal. Leave no context\nbehind: Efﬁcient inﬁnite context transformers with inﬁni- attention. arXiv preprint arXiv:2404.07143 ,\n2024.\n[Nakano et al., 2021] Reiichiro Nakano, Jacob Hilton, Suchi r Balaji, Jeff Wu, Long Ouyang, Christina\nKim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, Wi lliam Saunders, Xu Jiang, Karl Cobbe, Tyna\nEloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman.\nWebgpt: Browser-assisted question-answering with human f eedback. arXiv preprint arXiv:2112.09332 ,\n2021.\n[Narayanan et al., 2021] Deepak Narayanan, Mohammad Shoeyb i, Jared Casper, Patrick LeGresley,\nMostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Pre thvi Kashinkunti, Julie",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 190,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "edac88d5e0ac1f53",
    "content": ":2404.07143 ,\n2024.\n[Nakano et al., 2021] Reiichiro Nakano, Jacob Hilton, Suchi r Balaji, Jeff Wu, Long Ouyang, Christina\nKim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, Wi lliam Saunders, Xu Jiang, Karl Cobbe, Tyna\nEloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman.\nWebgpt: Browser-assisted question-answering with human f eedback. arXiv preprint arXiv:2112.09332 ,\n2021.\n[Narayanan et al., 2021] Deepak Narayanan, Mohammad Shoeyb i, Jared Casper, Patrick LeGresley,\nMostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Pre thvi Kashinkunti, Julie Bernauer, Bryan\nCatanzaro, Amar Phanishayee, and Matei Zaharia. Efﬁcient l arge-scale language model training on\ngpu clusters using megatron-lm. In Proceedings of the International Conference for High Perfo rmance\nComputing, Networking, Storage and Analysis , pages 1–15, 2021.\n\n4.5 Summary 215\n[Ng et al., 1999] Andrew Y Ng, Daishi Harada, and Stuart J Russ ell. Policy invariance under reward\ntransformations: Theory and application to reward shaping . InProceedings of the Sixteenth International\nConference on Machine Learning , pages 278–287, 1999.\n[OpenAI, 2024] OpenAI. Learning to reason with llms, Septem ber 2024. URL https://openai.com/\nindex/learning-to-reason-with-llms/ .\n[Ouyang et al., 2022] Long Ouyang, Jeffrey Wu, Xu Jiang, Diog o Almeida, Carroll L. Wainwright, Pamela\nMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Al ex Ray, John Schulman, Jacob Hilton,\nFraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, P eter Welinder, Paul F. Christiano, Jan\nLeike, and Ryan Lowe. Training language models to follow ins tructions with human feedback. Advances\nin Neural Information Processing Systems , 35:27730–27744, 2022.\n[Pal et al., 2023] Koyena Pal, Jiuding Sun, Andrew Yuan, Byro n C Wallace, and David Bau. Future lens:\nAnticipating subsequent tokens from a single hidden state. InProceedings of the 27th Conference on\nComputational Natural Language Learning (CoNLL) , pages 548–560, 2023.\n[Pan et al., 2022] Alexander Pan, Kush Bhatia, and Jacob Stei nhardt. The effects of reward misspeciﬁca-\ntion: Mapping and mitigating misaligned models. In International Conference on Learning Representa-\ntions , 2022.\n[Pan et al., 2024] Liangming Pan, Michael Saxon, Wenda Xu, De epak Nathani, Xinyi Wang, and\nWilliam Yang Wang. Automatically correcting large languag e models: Surveying the landscape of\ndiverse automated correction strategies. Transactions of the Association for Computational Linguis tics,\n12:484–506, 2024.\n[Parisi et al., 2022] Aaron Parisi, Yao Zhao, and Noah Fiedel . Talm: Tool augmented language models.\narXiv preprint arXiv:2205.12255 , 2022.\n[Parisi et al., 2019] German I Parisi, Ronald Kemker, Jose L P art, Christopher Kanan, and Stefan Wermter.\nContinual lifelong learning with neural networks: A review .Neural networks , 113:54–71, 2019.\n[Parmar et al., 2018] Niki Parmar, Ashish Vaswani, Jakob Usz koreit, Lukasz Kaiser, Noam Shazeer,\nAlexander Ku, and Dustin Tran. Image transformer. In International conference on machine learn-\ning, pages 4055–4064. PMLR, 2018.\n[Penedo et al., 2023] Guilherme Penedo, Quentin Malartic, D aniel Hesslow, Ruxandra Cojocaru, Alessan-\ndro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam A lmazrouei, and",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 191,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "d661760b81ae9886",
    "content": "[Parisi et al., 2019] German I Parisi, Ronald Kemker, Jose L P art, Christopher Kanan, and Stefan Wermter.\nContinual lifelong learning with neural networks: A review .Neural networks , 113:54–71, 2019.\n[Parmar et al., 2018] Niki Parmar, Ashish Vaswani, Jakob Usz koreit, Lukasz Kaiser, Noam Shazeer,\nAlexander Ku, and Dustin Tran. Image transformer. In International conference on machine learn-\ning, pages 4055–4064. PMLR, 2018.\n[Penedo et al., 2023] Guilherme Penedo, Quentin Malartic, D aniel Hesslow, Ruxandra Cojocaru, Alessan-\ndro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam A lmazrouei, and Julien Launay. The reﬁned-\nweb dataset for falcon llm: outperforming curated corpora w ith web data, and web data only. arXiv\npreprint arXiv:2306.01116 , 2023.\n[Peng et al., 2024] Bowen Peng, Jeffrey Quesnelle, Honglu Fa n, and Enrico Shippole. YaRN: Efﬁcient con-\ntext window extension of large language models. In The Twelfth International Conference on Learning\nRepresentations , 2024.\n[Pennington et al., 2014] Jeffrey Pennington, Richard Soch er, and Christopher D. Manning. Glove: Global\nvectors for word representation. In Proceedings of Empirical Methods in Natural Language Proce ssing\n(EMNLP) , pages 1532–1543, 2014.\n[Peters et al., 2018] Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark,\nKenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. In Proceedings of the\n2018 Conference of the North American Chapter of the Associa tion for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long Papers) , 2018.\n[Plackett, 1975] Robin L Plackett. The analysis of permutat ions. Journal of the Royal Statistical Society\nSeries C: Applied Statistics , 24(2):193–202, 1975.\n[Prasad et al., 2023] Archiki Prasad, Peter Hase, Xiang Zhou , and Mohit Bansal. Grips: Gradient-free, edit-\nbased instruction search for prompting large language mode ls. In Proceedings of the 17th Conference of\nthe European Chapter of the Association for Computational L inguistics , pages 3845–3864, 2023.\n\n216 Alignment\n[Press et al., 2022] Oﬁr Press, Noah Smith, and Mike Lewis. Tr ain short, test long: Attention with lin-\near biases enables input length extrapolation. In Proceedings of International Conference on Learning\nRepresentations , 2022.\n[Press et al., 2023] Oﬁr Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis.\nMeasuring and narrowing the compositionality gap in langua ge models. In Findings of the Association\nfor Computational Linguistics: EMNLP 2023 , pages 5687–5711, 2023.\n[Pryzant et al., 2023] Reid Pryzant, Dan Iter, Jerry Li, Yin T at Lee, Chenguang Zhu, and Michael Zeng.\nAutomatic prompt optimization with \"gradient descent\" and beam search. In The 2023 Conference on\nEmpirical Methods in Natural Language Processing , 2023.\n[Qiu et al., 2020] Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai, and Xuanjing Huang.\nPre-trained models for natural language processing: A surv ey.Science China Technological Sciences ,\n63(10):1872–1897, 2020.\n[Radford et al., 2018] Alec Radford, Karthik Narasimhan, Ti m Salimans, and Ilya Sutskever. Improving\nlanguage understanding by generative pre-training. OpenAI , 2018.\n[Radford et al., 2019] Alec Radford, Jeffrey Wu, Rewon Child , David Luan, Dario Amodei, and Ilya\nSutskever. Language models are unsupervised multitask lea rners. OpenAI blog , 1(8),",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 192,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "c585c42379567463",
    "content": "\nEmpirical Methods in Natural Language Processing , 2023.\n[Qiu et al., 2020] Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai, and Xuanjing Huang.\nPre-trained models for natural language processing: A surv ey.Science China Technological Sciences ,\n63(10):1872–1897, 2020.\n[Radford et al., 2018] Alec Radford, Karthik Narasimhan, Ti m Salimans, and Ilya Sutskever. Improving\nlanguage understanding by generative pre-training. OpenAI , 2018.\n[Radford et al., 2019] Alec Radford, Jeffrey Wu, Rewon Child , David Luan, Dario Amodei, and Ilya\nSutskever. Language models are unsupervised multitask lea rners. OpenAI blog , 1(8), 2019.\n[Radford et al., 2021] Alec Radford, Jong Wook Kim, Chris Hal lacy, Aditya Ramesh, Gabriel Goh, Sand-\nhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin , Jack Clark, Gretchen Krueger, and Ilya\nSutskever. Learning transferable visual models from natur al language supervision. In International\nconference on machine learning , pages 8748–8763. PMLR, 2021.\n[Rae et al., 2019] Jack W Rae, Anna Potapenko, Siddhant M Jaya kumar, Chloe Hillier, and Timothy P\nLillicrap. Compressive transformers for long-range seque nce modelling. In International Conference on\nLearning Representations , 2019.\n[Rafailov et al., 2024] Rafael Rafailov, Archit Sharma, Eri c Mitchell, Christopher D Manning, Stefano\nErmon, and Chelsea Finn. Direct preference optimization: Y our language model is secretly a reward\nmodel. Advances in Neural Information Processing Systems , 36, 2024.\n[Raffel et al., 2020] Colin Raffel, Noam Shazeer, Adam Rober ts, Katherine Lee, Sharan Narang, Michael\nMatena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the li mits of transfer learning with a uniﬁed\ntext-to-text transformer. Journal of Machine Learning Research , 21(140):1–67, 2020.\n[Ramachandran et al., 2017] Prajit Ramachandran, Barret Zo ph, and Quoc V Le. Searching for activation\nfunctions. arXiv preprint arXiv:1710.05941 , 2017.\n[Rolnick et al., 2019] David Rolnick, Arun Ahuja, Jonathan S chwarz, Timothy Lillicrap, and Gregory\nWayne. Experience replay for continual learning. Advances in Neural Information Processing Systems ,\n32, 2019.\n[Rosenfeld et al., 2020] Jonathan S Rosenfeld, Amir Rosenfe ld, Yonatan Belinkov, and Nir Shavit. A con-\nstructive prediction of the generalization error across sc ales. In Proceedings of International Conference\non Learning Representations , 2020.\n[Ruan et al., 2024] Junhao Ruan, Long Meng, Weiqiao Shan, Ton g Xiao, and Jingbo Zhu. A survey of llm\nsurveys. https://github.com/NiuTrans/ABigSurveyOfLLMs , 2024.\n[Rubin et al., 2022] Ohad Rubin, Jonathan Herzig, and Jonath an Berant. Learning to retrieve prompts\nfor in-context learning. In Proceedings of the 2022 Conference of the North American Cha pter of the\nAssociation for Computational Linguistics: Human Languag e Technologies , pages 2655–2671, 2022.\n[Russell, 2019] Stuart Russell. Human Compatible: Artiﬁcial Intelligence and the Problem o f Controls .\nViking, 2019.\n[Sanh et al., 2020] Victor Sanh, Thomas Wolf, and Alexander R ush. Movement pruning: Adaptive sparsity\nby ﬁne-tuning. Advances in Neural Information Processing Systems , 33:20378–20389, 2020.\n\n4.5 Summary 217\n[Sanh et al., 2022] Victor Sanh, Albert Webson, Colin Raffel , Stephen Bach, Lintang Sutawika, Zaid\nAlyafeai, Antoine Chafﬁn, Arnaud Stieg",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 193,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "dd9d7d4a598d5485",
    "content": "for in-context learning. In Proceedings of the 2022 Conference of the North American Cha pter of the\nAssociation for Computational Linguistics: Human Languag e Technologies , pages 2655–2671, 2022.\n[Russell, 2019] Stuart Russell. Human Compatible: Artiﬁcial Intelligence and the Problem o f Controls .\nViking, 2019.\n[Sanh et al., 2020] Victor Sanh, Thomas Wolf, and Alexander R ush. Movement pruning: Adaptive sparsity\nby ﬁne-tuning. Advances in Neural Information Processing Systems , 33:20378–20389, 2020.\n\n4.5 Summary 217\n[Sanh et al., 2022] Victor Sanh, Albert Webson, Colin Raffel , Stephen Bach, Lintang Sutawika, Zaid\nAlyafeai, Antoine Chafﬁn, Arnaud Stiegler, Arun Raja, Mana n Dey, M Saiful Bari, Canwen Xu, Urmish\nThakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Ki m, Gunjan Chhablani, Nihal Nayak,\nDebajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen,\nZheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht\nSharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Bider-\nman, Leo Gao, Thomas Wolf, and Alexander M Rush. Multitask pr ompted training enables zero-shot\ntask generalization. In Proceedings of International Conference on Learning Repre sentations , 2022.\n[Schick et al., 2023] Timo Schick, Jane A. Yu, Zhengbao Jiang , Fabio Petroni, Patrick Lewis, Gautier Izac-\nard, Qingfei You, Christoforos Nalmpantis, Edouard Grave, and Sebastian Riedel. PEER: A collaborative\nlanguage model. In Proceedings of The Eleventh International Conference on Le arning Representations ,\n2023.\n[Schick et al., 2024] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric\nHambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scia lom. Toolformer: Language models can\nteach themselves to use tools. Advances in Neural Information Processing Systems , 36, 2024.\n[Schmidhuber, 2015] Jürgen Schmidhuber. Deep learning in n eural networks: An overview. Neural net-\nworks , 61:85–117, 2015.\n[Schulman et al., 2015] John Schulman, Sergey Levine, Phili pp Moritz, Michael Jordan, and Pieter Abbeel.\nTrust region policy optimization. In Proceedings of the 32nd International Conference on Intern ational\nConference on Machine Learning-Volume 37 , pages 1889–1897, 2015.\n[Schulman et al., 2017] John Schulman, Filip Wolski, Praful la Dhariwal, Alec Radford, and Oleg Klimov.\nProximal policy optimization algorithms. arXiv preprint arXiv:1707.06347 , 2017.\n[Sennrich et al., 2016] Rico Sennrich, Barry Haddow, and Ale xandra Birch. Improving neural machine\ntranslation models with monolingual data. In Proceedings of the 54th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers) , pages 86–96, 2016.\n[Seo et al., 2017] Minjoon Seo, Aniruddha Kembhavi, Ali Farh adi, and Hannaneh Hajishirzi. Bidirectional\nattention ﬂow for machine comprehension. In Proceedings of International Conference on Learning\nRepresentations , 2017.\n[Shannon, 1951] Claude E Shannon. Prediction and entropy of printed english. Bell system technical\njournal , 30(1):50–64, 1951.\n[Shaw et al., 2018] Peter Shaw, Jakob Uszkoreit, and Ashish V aswani. Self-attention with relative position\nrepresentations. In Proceedings of the 2018 Conference of the North American Cha",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 194,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "c13adcffc166aec7",
    "content": "translation models with monolingual data. In Proceedings of the 54th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers) , pages 86–96, 2016.\n[Seo et al., 2017] Minjoon Seo, Aniruddha Kembhavi, Ali Farh adi, and Hannaneh Hajishirzi. Bidirectional\nattention ﬂow for machine comprehension. In Proceedings of International Conference on Learning\nRepresentations , 2017.\n[Shannon, 1951] Claude E Shannon. Prediction and entropy of printed english. Bell system technical\njournal , 30(1):50–64, 1951.\n[Shaw et al., 2018] Peter Shaw, Jakob Uszkoreit, and Ashish V aswani. Self-attention with relative position\nrepresentations. In Proceedings of the 2018 Conference of the North American Cha pter of the Associ-\nation for Computational Linguistics: Human Language Techn ologies, Volume 2 (Short Papers) , pages\n464–468, 2018.\n[Shazeer, 2019] Noam Shazeer. Fast transformer decoding: O ne write-head is all you need. arXiv preprint\narXiv:1911.02150 , 2019.\n[Shazeer, 2020] Noam Shazeer. Glu variants improve transfo rmer. arXiv preprint arXiv:2002.05202 , 2020.\n[Shen et al., 2020] Sheng Shen, Zhen Dong, Jiayu Ye, Linjian M a, Zhewei Yao, Amir Gholami, Michael W\nMahoney, and Kurt Keutzer. Q-bert: Hessian based ultra low p recision quantization of bert. In Proceed-\nings of the AAAI Conference on Artiﬁcial Intelligence , volume 34, pages 8815–8821, 2020.\n[Shoeybi et al., 2019] Mohammad Shoeybi, Mostofa Patwary, R aul Puri, Patrick LeGresley, Jared Casper,\nand Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model\nparallelism. arXiv preprint arXiv:1909.08053 , 2019.\n[Skalse et al., 2022] Joar Skalse, Nikolaus Howe, Dmitrii Kr asheninnikov, and David Krueger. Deﬁning\nand characterizing reward gaming. Advances in Neural Information Processing Systems , 35:9460–9471,\n2022.\n[Snell et al., 2022] Charlie Snell, Dan Klein, and Ruiqi Zhon g. Learning by distilling context. arXiv\n\n218 Alignment\npreprint arXiv:2209.15189 , 2022.\n[Socher et al., 2013] Richard Socher, Alex Perelygin, Jean W u, Jason Chuang, Christopher D Manning,\nAndrew Y Ng, and Christopher Potts. Recursive deep models fo r semantic compositionality over a\nsentiment treebank. In Proceedings of the 2013 conference on empirical methods in n atural language\nprocessing , pages 1631–1642, 2013.\n[Song et al., 2019] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. Mass: Masked sequence\nto sequence pre-training for language generation. In International Conference on Machine Learning ,\npages 5926–5936. PMLR, 2019.\n[Stiennon et al., 2020] Nisan Stiennon, Long Ouyang, Jeffre y Wu, Daniel Ziegler, Ryan Lowe, Chelsea\nV oss, Alec Radford, Dario Amodei, and Paul F Christiano. Lea rning to summarize with human feedback.\nAdvances in Neural Information Processing Systems , 33:3008–3021, 2020.\n[Su et al., 2024] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfen g Pan, Wen Bo, and Yunfeng Liu. Ro-\nformer: Enhanced transformer with rotary position embeddi ng.Neurocomputing , 568:127063, 2024.\n[Sun et al., 2020] Zhiqing Sun, Hongkun Yu, Xiaodan Song, Ren jie Liu, Yiming Yang, and Denny Zhou.\nMobilebert: a compact task-agnostic bert for resource-lim ited devices. In Proceedings of the 58th Annual\nMeeting",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 195,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "2895ef2d970ac37f",
    "content": " Long Ouyang, Jeffre y Wu, Daniel Ziegler, Ryan Lowe, Chelsea\nV oss, Alec Radford, Dario Amodei, and Paul F Christiano. Lea rning to summarize with human feedback.\nAdvances in Neural Information Processing Systems , 33:3008–3021, 2020.\n[Su et al., 2024] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfen g Pan, Wen Bo, and Yunfeng Liu. Ro-\nformer: Enhanced transformer with rotary position embeddi ng.Neurocomputing , 568:127063, 2024.\n[Sun et al., 2020] Zhiqing Sun, Hongkun Yu, Xiaodan Song, Ren jie Liu, Yiming Yang, and Denny Zhou.\nMobilebert: a compact task-agnostic bert for resource-lim ited devices. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics , pages 2158–2170, 2020.\n[Sutskever et al., 2014] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with\nneural networks. Advances in neural information processing systems , 27, 2014.\n[Sutton and Barto, 2018] Richard S. Sutton and Andrew G. Bart o.Reinforcement Learning: An Introduc-\ntion (2nd ed.) . The MIT Press, 2018.\n[Szepesvári, 2010] Csaba Szepesvári. Algorithms for reinf orcement learning. Synthesis Lectures on Arti-\nﬁcial Intelligence and Machine Learning , 4(1):1–103, 2010.\n[Talmor and Berant, 2018] Alon Talmor and Jonathan Berant. T he web as a knowledge-base for answering\ncomplex questions. arXiv preprint arXiv:1803.06643 , 2018.\n[Taori et al., 2023] Rohan Taori, Ishaan Gulrajani, Tianyi Z hang, Yann Dubois, Xuechen Li, Carlos\nGuestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanfor d alpaca: An instruction-following llama\nmodel. https://github.com/tatsu-lab/stanford_alpaca , 2023.\n[Tay et al., 2020] Yi Tay, Mostafa Dehghani, Dara Bahri, and D onald Metzler. Efﬁcient transformers: A\nsurvey. CoRR , abs/2009.06732, 2020.\n[Team et al., 2024] Gemma Team, Morgane Riviere, Shreya Path ak, Pier Giuseppe Sessa, Cassidy Hardin,\nSurya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Boba k Shahriari, Alexandre Ramé, et al.\nGemma 2: Improving open language models at a practical size. arXiv preprint arXiv:2408.00118 , 2024.\n[Teknium, 2023] Teknium. Openhermes 2.5: An open dataset of synthetic data for generalist llm assistants,\n2023. URL https://huggingface.co/datasets/teknium/OpenHermes- 2.5.\n[Touvron et al., 2023] Hugo Touvron, Thibaut Lavril, Gautie r Izacard, Xavier Martinet, Marie-Anne\nLachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Ro-\ndriguez, Armand Joulin, Edouard Grave, and Guillaume Lampl e. Llama: Open and efﬁcient foundation\nlanguage models. arXiv preprint arXiv:2302.13971 , 2023a.\n[Touvron et al., 2023] Hugo Touvron, Louis Martin, Kevin Sto ne, Peter Albert, Amjad Almahairi,\nYasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel,\nLukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem C ucurull, David Esiobu, Jude Fernan-\ndes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanu j Goswami, Naman Goyal, Anthony\nHart",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 196,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "1390243bf8b14bc2",
    "content": "ambro, Faisal Azhar, Aurelien Ro-\ndriguez, Armand Joulin, Edouard Grave, and Guillaume Lampl e. Llama: Open and efﬁcient foundation\nlanguage models. arXiv preprint arXiv:2302.13971 , 2023a.\n[Touvron et al., 2023] Hugo Touvron, Louis Martin, Kevin Sto ne, Peter Albert, Amjad Almahairi,\nYasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel,\nLukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem C ucurull, David Esiobu, Jude Fernan-\ndes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanu j Goswami, Naman Goyal, Anthony\nHartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kar das, Viktor Kerkez, Madian Khabsa, Is-\nabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Ann e Lachaux, Thibaut Lavril, Jenya Lee,\nDiana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, T odor Mihaylov, Pushkar Mishra, Igor\nMolybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Ra shi Rungta, Kalyan Saladi, Alan Schel-\nten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xi aoqing Ellen Tan, Binh Tang, Ross Taylor,\n\n4.5 Summary 219\nAdina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robe rt Stojnic, Sergey Edunov, and Thomas\nScialom. Llama 2: Open foundation and ﬁne-tuned chat models .arXiv preprint arXiv:2307.09288 ,\n2023b.\n[Uesato et al., 2022] Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa\nWang, Antonia Creswell, Geoffrey Irving, and Irina Higgins . Solving math word problems with process-\nand outcome-based feedback. arXiv preprint arXiv:2211.14275 , 2022.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki Pa rmar, Jakob Uszkoreit, Llion Jones, Aidan N\nGomez, Łukasz Kaiser, and Illia Polosukhin. Attention is al l you need. In Proceedings of Advances in\nNeural Information Processing Systems , volume 30, 2017.\n[V on Oswald et al., 2023] Johannes V on Oswald, Eyvind Niklas son, Ettore Randazzo, João Sacramento,\nAlexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyr ov. Transformers learn in-context by\ngradient descent. In Proceedings of International Conference on Machine Learni ng, pages 35151–\n35174. PMLR, 2023.\n[Wang et al., 2024] Chenglong Wang, Hang Zhou, Yimin Hu, Yifu Huo, Bei Li, Tongran Liu, Tong Xiao,\nand Jingbo Zhu. Esrl: Efﬁcient sampling-based reinforceme nt learning for sequence generation. In\nProceedings of the AAAI Conference on Artiﬁcial Intelligen ce, pages 19107–19115, 2024.\n[Wang et al., 2023] Liyuan Wang, Xingxing Zhang, Hang Su, and Jun Zhu. A comprehensive survey of\ncontinual learning: Theory, method and application. arXiv preprint arXiv:2302.00487 , 2023a.\n[Wang et al., 2019] Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F Wong, and\nLidia S Chao. Learning deep transformer models for machine t ranslation. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational Lingui stics, pages 1810–1822, 2019.\n[Wang et al., 2022] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and D",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 197,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "f0001f55769b6d7c",
    "content": "I Conference on Artiﬁcial Intelligen ce, pages 19107–19115, 2024.\n[Wang et al., 2023] Liyuan Wang, Xingxing Zhang, Hang Su, and Jun Zhu. A comprehensive survey of\ncontinual learning: Theory, method and application. arXiv preprint arXiv:2302.00487 , 2023a.\n[Wang et al., 2019] Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F Wong, and\nLidia S Chao. Learning deep transformer models for machine t ranslation. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational Lingui stics, pages 1810–1822, 2019.\n[Wang et al., 2022] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou.\nRationale-augmented ensembles in language models. arXiv preprint arXiv:2207.00747 , 2022a.\n[Wang et al., 2023] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang,\nAakanksha Chowdhery, and Denny Zhou. Self-consistency imp roves chain of thought reasoning in lan-\nguage models. In Proceedings of The Eleventh International Conference on Le arning Representations ,\n2023b.\n[Wang et al., 2022] Yizhong Wang, Swaroop Mishra, Pegah Alip oormolabashi, Yeganeh Kordi, Amirreza\nMirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekar an, Anjana Arunkumar, David Stap, Es-\nhaan Pathak, Giannis Karamanolakis, Haizhi Gary Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson,\nKirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel , Mehrad Moradshahi, Mihir Parmar, Mi-\nrali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit V erma, Ravsehaj Singh Puri, Rushang Karia,\nSavan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Suj an Reddy A, Sumanta Patro, Tanay Dixit,\nand Xudong Shen. Super-naturalinstructions: Generalizat ion via declarative instructions on 1600+ nlp\ntasks. In Proceedings of the 2022 Conference on Empirical Methods in N atural Language Processing ,\npages 5085–5109, 2022b.\n[Wang et al., 2023] Yizhong Wang, Hamish Ivison, Pradeep Das igi, Jack Hessel, Tushar Khot, Khy-\nathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A . Smith, Iz Beltagy, and Hannaneh\nHajishirzi. How far can camels go? exploring the state of ins truction tuning on open resources. Ad-\nvances in Neural Information Processing Systems , 36:74764–74786, 2023c.\n[Wang et al., 2023] Yizhong Wang, Yeganeh Kordi, Swaroop Mis hra, Alisa Liu, Noah A Smith, Daniel\nKhashabi, and Hannaneh Hajishirzi. Self-instruct: Aligni ng language models with self-generated in-\nstructions. In Proceedings of the 61st Annual Meeting of the Association fo r Computational Linguistics\n(Volume 1: Long Papers) , pages 13484–13508, 2023d.\n[Wang et al., 2023] Zhenyi Wang, Enneng Yang, Li Shen, and Hen g Huang. A comprehensive survey of\nforgetting in deep learning beyond continual learning. arXiv preprint arXiv:2307.09218 , 2023e.\n\n220 Alignment\n[Warstadt et al., 2019] Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network accept-\nability judgments. Transactions of the Association for Computational Linguis tics, 7:625–641, 2019.\n[Wei et al., 2022] Jason Wei, Maarten Bosma, Vincent Zhao, Ke lvin Guu, Adams Wei Yu, Brian Lester, Nan\nDu, Andrew M Dai, and Quoc",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 198,
      "total_chunks": 205
    }
  },
  {
    "chunk_id": "6e2e382d24e9cfe4",
    "content": " of the 61st Annual Meeting of the Association fo r Computational Linguistics\n(Volume 1: Long Papers) , pages 13484–13508, 2023d.\n[Wang et al., 2023] Zhenyi Wang, Enneng Yang, Li Shen, and Hen g Huang. A comprehensive survey of\nforgetting in deep learning beyond continual learning. arXiv preprint arXiv:2307.09218 , 2023e.\n\n220 Alignment\n[Warstadt et al., 2019] Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network accept-\nability judgments. Transactions of the Association for Computational Linguis tics, 7:625–641, 2019.\n[Wei et al., 2022] Jason Wei, Maarten Bosma, Vincent Zhao, Ke lvin Guu, Adams Wei Yu, Brian Lester, Nan\nDu, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In Proceedings\nof International Conference on Learning Representations , 2022a.\n[Wei et al., 2022] Jason Wei, Yi Tay, Rishi Bommasani, Colin R affel, Barret Zoph, Sebastian Borgeaud,\nDani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, E d H. Chi, Tatsunori Hashimoto, Oriol\nVinyals, Percy Liang, Jeff Dean, and William Fedus. Emergen t abilities of large language models. arXiv\npreprint arXiv:2206.07682 , 2022b.\n[Wei et al., 2022] Jason Wei, Xuezhi Wang, Dale Schuurmans, M aarten Bosma, Brian Ichter, Fei Xia,\nEd H. Chi, Quoc V . Le, and Denny Zhou. Chain-of-thought promp ting elicits reasoning in large language\nmodels. Advances in Neural Information Processing Systems , 35:24824–24837, 2022c.\n[Welleck et al., 2023] Sean Welleck, Ximing Lu, Peter West, F aeze Brahman, Tianxiao Shen, Daniel\nKhashabi, and Yejin Choi. Generating sequences by learning to self-correct. In Proceedings of The\nEleventh International Conference on Learning Representa tions , 2023.\n[Weng, 2021] Lilian Weng. How to train really large models on many gpus? lilianweng.github.io , Sep\n2021. URL https://lilianweng.github.io/posts/2021-09-25-train -large/ .\n[Wiener, 1960] Norbert Wiener. Some moral and technical con sequences of automation: As machines\nlearn they may develop unforeseen strategies at rates that b afﬂe their programmers. Science , 131(3410):\n1355–1358, 1960.\n[Williams et al., 2018] Adina Williams, Nikita Nangia, and S amuel Bowman. A broad-coverage challenge\ncorpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computational Ling uistics: Human Language Technologies,\nVolume 1 (Long Papers) , pages 1112–1122, 2018.\n[Williams, 1992] Ronald J Williams. Simple statistical gra dient-following algorithms for connectionist\nreinforcement learning. Machine learning , 8:229–256, 1992.\n[Wingate et al., 2022] David Wingate, Mohammad Shoeybi, and Taylor Sorensen. Prompt compression\nand contrastive conditioning for controllability and toxi city reduction in language models. In Findings\nof the Association for Computational Linguistics: EMNLP 20 22, pages 5621–5634, 2022.\n[Wu et al., 2024] Wilson Wu, John X Morris, and Lionel Levine. Do language models plan for future\ntokens? arXiv preprint arXiv:2404.00859 , 2024.\n[Wu et al., 2021] Yuhuai Wu, Markus Norman Rabe, DeLesley Hut chins, and Christian Szegedy. Memo-\nrizing transformers. In Proceedings of International Conference on Learning Repre sentations , 2021.\n[Wu et al., 2023] Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu,\nNoah A. Smith, Mari Ostendorf, and Hannaneh Hajishirzi.",
    "metadata": {
      "source": "C:\\Users\\sauba_xqr\\OneDrive\\Documents\\GitHub\\RAG\\data\\documents\\tmpumvxywb6.pdf",
      "filename": "tmpumvxywb6.pdf",
      "file_type": "pdf",
      "file_size": 2018896,
      "file_size_mb": 1.93,
      "file_hash": "8b8f659beda18f55ab82191bde2d0d8090ae73925b2c7ea3e8d9171857cc506a",
      "creation_date": "2025-03-15T00:49:28.708075",
      "modification_date": "2025-03-15T00:49:28.724412",
      "processing_date": "2025-03-15T00:49:52.514851",
      "Producer": "GPL Ghostscript 10.01.2",
      "CreationDate": "D:20250116201348-05'00'",
      "ModDate": "D:20250116201348-05'00'",
      "Creator": "LaTeX with hyperref",
      "Title": "",
      "Subject": "",
      "Author": "",
      "Keywords": "",
      "page_count": 231,
      "page_size_sample": [
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0",
        "595.0x842.0"
      ],
      "title": "LLM book.pdf",
      "author": "Unknown",
      "description": "",
      "original_filename": "LLM book.pdf",
      "content_type": "application/pdf",
      "batch_upload": true,
      "batch_index": 1,
      "timestamp": "2025-03-15T00:49:52.516360",
      "doc_id": "14a707ea5ba52a62",
      "chunk_index": 199,
      "total_chunks": 205
    }
  }
]